,post_id,date_created,last_activity_date,view_count,question_text,detailed_tags,user_name,user_profile_link,reputation_score
79045928,79045928,2024-10-02T08:26:43,2024-10-03 08:20:49Z,36,"I have the scenario where you have a feature, call it X, with possible values it can hold, say x1, x2, x3, x4. I want the GBM in H2O to train x1 and x3 together so that they are predicted with the same output value.


Any suggestions how to do this would be appreciated. :)","['h2o', 'gbm']",Adam,https://stackoverflow.com/users/12979649/adam,21
79033110,79033110,2024-09-27T23:03:19,2024-09-30 07:34:12Z,56,"Everytime when I run h2o on pydroid3 then it says I need Java/JRE for it to work. ERROR:


Checking whether there is an H2O instance running at http://localhost:54321..... not found.


Attempting to start a local H2O server...


Traceback (most recent call last):


File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/h2o.py"", line 270, in init


h2oconn = H2OConnection.open(url=url, ip=ip, port=port, name=name, https=https,

          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/backend/connection.py"", line 406, in open


conn._cluster = conn._test_connection(retries, messages=msgs)

                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/backend/connection.py"", line 713, in _test_connection


raise H2OConnectionError(""Could not establish link to the H2O cloud %s after %d retries\n%s""



h2o.exceptions.H2OConnectionError: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries


[22:07.97] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79554692d0>: Failed to establish a new connection: [Errno 111] Connection refused'))


[22:08.19] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x795546b1d0>: Failed to establish a new connection: [Errno 111] Connection refused'))


[22:08.41] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x795546b810>: Failed to establish a new connection: [Errno 111] Connection refused'))


[22:08.63] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7955065450>: Failed to establish a new connection: [Errno 111] Connection refused'))


[22:08.85] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7955067390>: Failed to establish a new connection: [Errno 111] Connection refused'))


During handling of the above exception, another exception occurred:


Traceback (most recent call last):


File ""/data/user/0/ru.iiec.pydroid3/files/accomp_files/iiec_run/iiec_run.py"", line 31, in 


start(fakepyfile,mainpyfile)



File ""/data/user/0/ru.iiec.pydroid3/files/accomp_files/iiec_run/iiec_run.py"", line 30, in start


exec(open(mainpyfile).read(),  __main__.__dict__)



File """", line 19, in 


File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/h2o.py"", line 287, in init


hs = H2OLocalServer.start(nthreads=nthreads, enable_assertions=enable_assertions, max_mem_size=mmax,

     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/backend/server.py"", line 139, in start


hs._launch_server(port=port, baseport=baseport, nthreads=int(nthreads), ea=enable_assertions,



File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/backend/server.py"", line 271, in _launch_server


java = self._find_java()

       ^^^^^^^^^^^^^^^^^



File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/backend/server.py"", line 453, in _find_java


raise H2OStartupError(""Cannot find Java. Please install the latest JRE from\n""



h2o.exceptions.H2OStartupError: Cannot find Java. Please install the latest JRE from


http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements


[Program finished]


I tried nothing, cant find any information to download it.","['python', 'java', 'mobile', 'h2o']",XuhanChen,https://stackoverflow.com/users/27513799/xuhanchen,1
78995266,78995266,2024-09-17T17:55:20,2024-09-18 12:11:50Z,0,"I'm attempting to implement the deeplearning function in the h2o package and obtain a persistent error despite (seemingly) following the example given in the documentation for the package:


https://www.rdocumentation.org/packages/h2o/versions/3.44.0.3/topics/h2o.deeplearning


My inputs are y, a length-n vector of 0,1 indicating binary outcomes, and an nxm matrix of integers x which are my predictor variables.


library(h2o)
h2o.init()

y = as.factor(y)
xnew = cbind(y,x)
xnew = data.frame(xnew)
#create a single data frame with response variable y and predictor variables x
x_df = as.h2o(xnew)
# formats dataframe as h2o data object
nn_model_training = h2o.deeplearning(y=1,training_frame = x_df)



which executes without issue. I now wish to use the nn_model_training to predict outcomes for a test set. To have the same column names as in the model, I take 2...m from the training_frame (i.e. exclude outcome variable y):


keep_names = names(x_df)[2:length(x_df[1,])]
x_new = Test_Mat
x_new = data.frame(x_new)
names(x_new) = keep_names
x_df_new = as.h2o(x_new)

nn_predicted = h2o.predict(nn_model_training, x_df_new)



which immediately results in the error


*Error: java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set*



despite the fact that I renamed the columns to match the names of the x-variables in the training set.


What am I doing incorrectly when implementing the h2o.predict() ?","['r', 'deep-learning', 'h2o']",Max,https://stackoverflow.com/users/5904690/max,569
78986631,78986631,2024-09-15T04:36:28,2024-09-19 21:19:57Z,0,"I would like to switch the parsinp engine to h2o and use h2o and agua packages to fit models. The following code is from the standard help site.


With 
tune::tune_grid
, I bump into the 
""Warning: All models failed ...""
, which is due to ""
Error in
 
h2o.getConnection()
: 
No active connection to an H2O cluster ...""
. Although the output of 
h2o.getConnection()
 immediately before and after 
tune_grid
 suggest everything should be OK.


library(ggplot2)
library(tidymodels)
library(agua)
#> 
#> Attaching package: 'agua'
#> The following object is masked from 'package:workflowsets':
#> 
#>     rank_results
library(h2o)
#> 
#> ----------------------------------------------------------------------
#> 
#> Your next step is to start H2O:
#>     > h2o.init()
#> 
#> For H2O package documentation, ask for help:
#>     > ??h2o
#> 
#> After starting H2O, you can use the Web UI at http://localhost:54321
#> For more information visit https://docs.h2o.ai
#> 
#> ----------------------------------------------------------------------
#> 
#> Attaching package: 'h2o'
#> The following objects are masked from 'package:stats':
#> 
#>     cor, sd, var
#> The following objects are masked from 'package:base':
#> 
#>     %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,
#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,
#>     log10, log1p, log2, round, signif, trunc

doParallel::registerDoParallel()

h2o::h2o.init()
#>  Connection successful!
#> 
#> R is connected to the H2O cluster: 
#>     H2O cluster uptime:         50 minutes 51 seconds 
#>     H2O cluster timezone:       America/New_York 
#>     H2O data parsing timezone:  UTC 
#>     H2O cluster version:        3.46.0.5 
#>     H2O cluster version age:    16 days 
#>     H2O cluster name:           H2O_started_from_R_siava_dqu656 
#>     H2O cluster total nodes:    1 
#>     H2O cluster total memory:   13.28 GB 
#>     H2O cluster total cores:    32 
#>     H2O cluster allowed cores:  32 
#>     H2O cluster healthy:        TRUE 
#>     H2O Connection ip:          localhost 
#>     H2O Connection port:        54321 
#>     H2O Connection proxy:       NA 
#>     H2O Internal Security:      FALSE 
#>     R Version:                  R version 4.4.1 (2024-06-14 ucrt)
h2o::h2o.connect()
#>  Connection successful!
#> 
#> R is connected to the H2O cluster: 
#>     H2O cluster uptime:         50 minutes 54 seconds 
#>     H2O cluster timezone:       America/New_York 
#>     H2O data parsing timezone:  UTC 
#>     H2O cluster version:        3.46.0.5 
#>     H2O cluster version age:    16 days 
#>     H2O cluster name:           H2O_started_from_R_siava_dqu656 
#>     H2O cluster total nodes:    1 
#>     H2O cluster total memory:   13.28 GB 
#>     H2O cluster total cores:    32 
#>     H2O cluster allowed cores:  32 
#>     H2O cluster healthy:        TRUE 
#>     H2O Connection ip:          localhost 
#>     H2O Connection port:        54321 
#>     H2O Connection proxy:       NA 
#>     H2O Internal Security:      FALSE 
#>     R Version:                  R version 4.4.1 (2024-06-14 ucrt)
h2o::h2o.getConnection()
#> IP Address: localhost 
#> Port      : 54321 
#> Name      : NA 
#> Session ID: _sid_b5d8 
#> Key Count : 0

data(ames)

set.seed(4595)
data_split <- ames |>
  mutate(Sale_Price = log10(Sale_Price)) |>
  initial_split(strata = Sale_Price)
ames_train <- training(data_split)
ames_test <- testing(data_split)
cv_splits <- vfold_cv(ames_train, v = 10, strata = Sale_Price)

ames_rec <-
  recipe(Sale_Price ~ Gr_Liv_Area + Longitude + Latitude, data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
  step_ns(Longitude, deg_free = tune(""long df"")) |>
  step_ns(Latitude, deg_free = tune(""lat df""))

lm_mod <- linear_reg(penalty = tune()) |>
  set_engine(""h2o"")

lm_wflow <- workflow() |>
  add_model(lm_mod) |>
  add_recipe(ames_rec)

grid <- lm_wflow |>
  extract_parameter_set_dials() |>
  grid_regular(levels = 5)

h2o.getConnection()
#> IP Address: localhost 
#> Port      : 54321 
#> Name      : NA 
#> Session ID: _sid_b5d8 
#> Key Count : 0

ames_res <- tune_grid(
  lm_wflow,
  resamples = cv_splits,
  grid = grid,
  control = control_grid(save_pred = TRUE,
                         backend_options = agua_backend_options(parallelism = 5))
)
#> Warning: All models failed. Run `show_notes(.Last.tune.result)` for more
#> information.

h2o.getConnection()
#> IP Address: localhost 
#> Port      : 54321 
#> Name      : NA 
#> Session ID: _sid_b5d8 
#> Key Count : 0

agua::h2o_running(verbose = FALSE)
#> [1] TRUE

show_notes(.Last.tune.result)
#> unique notes:
#> ────────────────────────────────────────────────────────────────────────────────
#> Error in h2o.getConnection(): No active connection to an H2O cluster. Did you run `h2o.init()` ?
Created on 2024-09-15 with reprex v2.1.1



Latest RStudio is run with both normal and administrative privileges (same issue - although we should not need admin rights) on Windows 11 Pro x64 machine. JRE (1.8.0_421) and JDK (22) are installed, and all Java paths (
JRE_HOME
, 
JDK_HOME
, 
JAVA_HOME = JRE_HOME
) are all A-OK! I went so-far as to use rJava to manually initialize a Java VM instance with no effect. The command 
demo(h2o.kmeans)
 runs without a problem and produces expected results.


The only other thing of note is that running 
agua::h2o_start()
 produces weird output that it should not be. I get ""permission denied error"" on user folder which should be accessible even without admin rights (folder-access issue is the same with h2o package) and I get ""no Java error"" that shouldn't be there.


Warning message:
JAVA not found, H2O may take minutes trying to connect. 



Could this be the root of the issue: 
""h2o cluster connection is there as far as h2o package is concerned but agua (which adds supports for tidymodels) does not recognize Java and the existing h2o connection""
?!","['r', 'connection', 'h2o', 'hyperparameters', 'tidymodels']",Unknown,,N/A
78853756,78853756,2024-08-09T16:16:57,2024-08-12 15:02:37Z,0,"I ran a GLM model using h2o in r (the model was saved as an object called mod), the dataset contains categorical and continuous predictors. I called the h2o.varimp function on mod to retrieve the variable importance for this GLM model, for categorical predictors, it seemed to calculate the relative importance for each level within that same categorical predictor. Is there a way for me to aggregate the importance across all levels so that I get a single importance metric for each unique predictor?


I first tried summing the relative importance but since they are based on standardized coefficients, I'm not sure if this is the right way.","['r', 'h2o']",ZTraveler,https://stackoverflow.com/users/22456747/ztraveler,1
78716974,78716974,2024-07-07T09:57:06,2024-07-08 13:33:34Z,0,"When I am trying to run the function 
h2o.splitFrame
 from H2O, I receive the following error:


Error en .Call(R_curl_fetch_memory, enc2utf8(url), handle, nonblocking): 
  se ha alcanzado el límite de tiempo transcurrido



I tried to check the admin task to liberate the memory from my laptop, and tried other combinations but nothing worked. Find below my complete code.


# Establecer el tiempo de espera de CURL
options(RCurlOptions = list(timeout = 600))

library(h2o)
library(parallel)
library(doParallel)

# Detectar el número de núcleos disponibles
n_cores <- parallel::detectCores()
registerDoParallel(cores=4)

# Inicializar H2O con parámetros mejorados
h2o.init(
  ip = ""localhost"",
  nthreads = max(1, n_cores - 1),
  max_mem_size = ""6g""
)

# Deshabilitamos la salida de progreso
h2o.no_progress()

data <- as.h2o(ordata.ren)

splits <- h2o.splitFrame(
  data = data, 
  ratios = c(0.7, 0.15),  # partition data into 70%, 15%, 15% chunks
  destination_frames = c(""train"", ""valid"", ""test""),  # frame ID (not required)
  seed = 1  # setting a seed will guarantee reproducibility
)

# When I run this part of the code, I receive the error message mentioned before:
# Error en .Call(R_curl_fetch_memory, enc2utf8(url), handle, nonblocking): 
#   se ha alcanzado el límite de tiempo transcurrido.

train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]

y <- ""Price""
x <- setdiff(names(data), y)
print(x)","['r', 'h2o', 'h2o.ai']",user23479948,https://stackoverflow.com/users/23479948/user23479948,1
78449790,78449790,2024-05-08T15:58:11,2024-05-08 20:57:55Z,64,"I am running H2O AutoML on the same date set with the same seed on the same laptop multiple times and getting different predictions from time to time. I wonder if it is possible to get the same numbers on every run. Here is my code:


import h2o
h2o.init()
aml = h2o.automl.H2OAutoML(max_models = 25,
 balance_classes = False, seed = 1)
aml.train(predictors, response, training_frame = h2o_df_train)
aml.leader.predict(h2o_df_test)



It looks like it is possible for 
GBM
, but I wonder if it is possible for whole AutoML run?


My appologies for cross posting, but here is a complete example:


H2O forum


Problem solved


From 
H2O docs
:


""seed: Integer. Set a seed for reproducibility. AutoML can only guarantee reproducibility under certain conditions. H2O Deep Learning models are not reproducible by default for performance reasons, 
so if the user requires reproducibility, then exclude_algos must contain ""DeepLearning"".
 In addition max_models must be used because max_runtime_secs is resource limited, meaning that if the available compute resources are not the same between runs, AutoML may be able to train more models on one run vs another. Defaults to NULL/None.""","['h2o', 'automl', 'reproducible-research']",Unknown,,N/A
78444040,78444040,2024-05-07T16:58:00,2024-05-16 20:42:45Z,0,"I have a random forest model that I'm trying to understand better.


For the sake of the example, lets say we have a grove of blueberry bushes. What we're interested in is predicting the production of rotten blueberries on a specific bush, among harvest of all blueberries of the individual bushes.


Each bush has an identifying name: 
bush_name
, such as 
'bush001'
, and we want predictions based on each individual bush. For example, I want to know if bush025 produced a rotten berry on 2/2/22.


Inputs are in a df with the following dummy structure for the sake of this example:


train_data <- data.frame(date = c(""2022-01-01"", ""2022-01-07"", ""2022-02-09"", ""2022-05-01"", ""2022-11-01"", ""2022-11-02""),
                   bush_name = c(""bush001"", ""bush001"", ""bush001"", ""bush043"", ""bush043"", ""bush043""),
                   bugs = c(2, 0, 1, 0, 3, 1),
                   has_rotten_berry = c(1, 0, 0, 1, 1, 0),
                   berry_count = c(12, 1, 7, 100, 14, 4),
                   weather = c(1, 0, 2, 0, 1, 1))



I've got a random forest model that I have set up with the following high level set up:


library(agua)
library(parsnip)
library(h2o)

h2o.init(nthreads = -1)

model_fit <- rand_forest(mtry = 10,  trees = 100) %>%
  set_engine(""h2o"") %>%
  set_mode(""classification"") %>%
  fit(has_rotten_berry ~  .,
      data = train_data) %>% 
  step_dummy(bush_name) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())



I do get this message after training:


Warning message:
In .h2o.processResponseWarnings(res) :
  Dropping bad and constant columns: [bush_name].



What I want to know is:


When I try to then predict on new data in the trained model, it seems that I am only able to input new test data with the bush_names of bushes I already trained on. 
Am I correct in assuming this model is creating bush-specific predictions? And therefore would have to input new bush information in the training in order to output a future prediction for those new bushes?


Example: I plant a new bush, bush700, and it was not present in the original training data set. If I try to predict with the new bush data without it being present in the training data, is giving me a message that there are new levels in the data. So 
I'm assuming that because it seems these predictions are bush-specific, and we can't get any new bush predictions for newly-added bushes.


Is this correct to assume?","['r', 'machine-learning', 'classification', 'h2o', 'predict']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
78250475,78250475,2024-03-31T06:26:06,2024-03-31 20:30:59Z,0,"I am trying to use 
h2o.deeplearning
 model to predict on raster data. It returns me the following error




Error: Not compatible with requested type: [type=character; target=double].




Here is a minimal, reproducible, self-contained example


library(terra)
library(h2o)
library(tidyverse)

h2o.init()

# create a RasterStack or RasterBrick with with a set of predictor layers
logo <- rast(system.file(""external/rlogo.grd"", package=""raster""))
names(logo)

# known presence and absence points
p <- matrix(c(48, 48, 48, 53, 50, 46, 54, 70, 84, 85, 74, 84, 95, 85,
              66, 42, 26, 4, 19, 17, 7, 14, 26, 29, 39, 45, 51, 56, 46, 38, 31,
              22, 34, 60, 70, 73, 63, 46, 43, 28), ncol=2)
a <- matrix(c(22, 33, 64, 85, 92, 94, 59, 27, 30, 64, 60, 33, 31, 9,
              99, 67, 15, 5, 4, 30, 8, 37, 42, 27, 19, 69, 60, 73, 3, 5, 21,
              37, 52, 70, 74, 9, 13, 4, 17, 47), ncol=2)

# extract values for points
xy <- rbind(cbind(1, p), cbind(0, a))
v <- data.frame(cbind(pa=xy[,1], terra::extract(logo, xy[,2:3]))) %>% 
  mutate(pa = as.factor(pa))

str(v) 

#### Import data to H2O cluster
df <- as.h2o(v)

#### Split data into train, validation and test dataset
splits <- h2o.splitFrame(df, c(0.70,0.15), seed=1234)
train  <- h2o.assign(splits[[1]], ""train.hex"")
valid  <- h2o.assign(splits[[2]], ""valid.hex"")
test   <- h2o.assign(splits[[3]], ""test.hex"")

#### Create response and features data sets
y <- ""pa""
x <- setdiff(names(train), y)

### Deep Learning Model
dl_model <- h2o.deeplearning(training_frame=train,                      
  validation_frame=valid,                    
  x=x,                                       
  y=y,                                      
  standardize=TRUE,                          
  seed=125)

dnn_pred <- function(model, data, ...) {
  predict(model, newdata=as.h2o(data), ...)
}

p <- predict(logo, model=dl_model, fun=dnn_pred)
plot(p)","['r', 'h2o', 'predict', 'terra']",UseR10085,https://stackoverflow.com/users/6123824/user10085,"8,062"
78245999,78245999,2024-03-29T21:21:16,2024-03-30 15:28:57Z,91,"I am getting an error for my desired dataset when trying to use an isolation forest method to detect anomalies. However I have another completely different dataset that it works fine for, what could cause this issue?


isolationforest Model Build progress: | (failed) | 0% Traceback (most recent call last): File 
""h2o_test.py"", line 149, in <module> isoforest.train(x=iso_forest.col_names[0:65], 
training_frame=iso_forest) File ""/home/ec2-user/.local/lib/python3.7/site- 
packages/h2o/estimators/estimator_base.py"", line 107, in train self._train(parms, 
verbose=verbose) File ""/home/ec2-user/.local/lib/python3.7/site- 
packages/h2o/estimators/estimator_base.py"", line 199, in _train 
job.poll(poll_updates=self._print_model_scoring_history if verbose else None) File 
""/home/ec2-user/.local/lib/python3.7/site-packages/h2o/job.py"", line 89, in poll 
""\n{}"".format(self.job_key, self.exception, self.job[""stacktrace""])) OSError: Job with key 
$03017f00000132d4ffffffff$_92ee3e892f7bc86460e80153eaec4b70 failed with an exception: 

java.lang.AssertionError stacktrace: java.lang.AssertionError at 
hex.tree.DHistogram.init(DHistogram.java:350) at 
hex.tree.DHistogram.init(DHistogram.java:343) at 
hex.tree.ScoreBuildHistogram2$ComputeHistoThread.computeChunk(ScoreBuildHistogram2.java:427) 
at hex.tree.ScoreBuildHistogram2$ComputeHistoThread.map(ScoreBuildHistogram2.java:408) at 
water.LocalMR.compute2(LocalMR.java:89) at water.LocalMR.compute2(LocalMR.java:81) at 
water.H2O$H2OCountedCompleter.compute(H2O.java:1704) at 
jsr166y.CountedCompleter.exec(CountedCompleter.java:468) at 
jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263) at 
jsr166y.ForkJoinPool$WorkQueue.popAndExecAll(ForkJoinPool.java:906) at 
jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:979) at 
jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at 
jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



with open('/home/webapp/flask-api/tmp_rows/temp_file2.csv', 'w+') as tmp_file:
        temp_name = ""/tmp_rows/temp_file2.csv""
        tmp_file.write(text_stream.getvalue())
        tmp_file.close()

h2o.init()
print(""TEMP_nAME"", temp_name)
iso_forest = h2o.import_file('/home/webapp/flask-api/{0}'.format(temp_name))
seed = 12345
ntrees = 100
isoforest = h2o.estimators.H2OIsolationForestEstimator(
ntrees=ntrees, seed=seed)
isoforest.train(x=iso_forest.col_names[0:65], training_frame=iso_forest)
predictions = isoforest.predict(iso_forest)
print(predictions)
h2o.cluster().shutdown()




The CSV is being created fine, so there doesn't seem to be an issue with that, what is causing this Java error? I even increased the size of my ec2 to have more RAM, that didn't solve it either.","['python', 'java', 'h2o']",Amon,https://stackoverflow.com/users/2621316/amon,"2,931"
78193739,78193739,2024-03-20T13:32:59,2024-03-20 13:32:59Z,34,"everyone!


I have a list of discounts, and for each discount, I iterate through them. Within each iteration, I generate a few new columns and then predict the sales amount for each product associated with that discount. I aim to compile all the results from these iterations into a single dataframe to have a comprehensive view of every discount on the list along with the predictions. This process has already been successfully implemented in native H2O using Pandas. However, I am now attempting to replicate it in Spark H2O and PySpark.


Unfortunately, when attempting to read the dataframe, I encounter the following error:


RestApiCommunicationException: H2O node http://10.159.20.11:54321 responded with org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 256.0 failed 4 times, most recent failure: Lost task 0.3 in stage 256.0 (TID 3451) (10.159.20.11 executor 0): ai.h2o.sparkling.backend.exceptions.RestApiCommunicationException: H2O node http://10.159.20.11:54321 responded with Status code: 400 : Bad Request



I am seeking a workaround or advice on whether I might be making a mistake.


Expected Behavior: I should be able to access and manipulate the dataframe without issues.


Observed Behavior: The error ""RestApiCommunicationException: H2O node 
http://10.159.20.11:54321/
 responded with"" prevents further progress.


What I am doing now is saving the predictions to a delta table and then access it, but that takes a little more time than what I expected. I know that loops are not very recommended for PySpark but I am also not seeing a better way of doing this.


Thank you so much!","['apache-spark', 'pyspark', 'databricks', 'h2o', 'sparkling-water']",Tiago,https://stackoverflow.com/users/20954952/tiago,65
78183646,78183646,2024-03-19T00:17:40,2024-03-25 17:42:30Z,58,"My code loads the h2o MOJO model to get prediction on a small dataset. However, h2o shutdown abruptly by itself. The same code is working fine on one machine with the same set of inputs but seeing abnormal h2o shutdowns on other machine.


self.test = h2o.import_file(dataset_file)
preds = imported_model.predict(self.test)



I am running this on 1TB machine with 72 cores. I can't believe this is memory issues. The most puzzling the fact is that the same code is working on other machine with the same inputs (configured differently). I don't know the full list of differences.  I was previously running with python frozen binary build and couldn't see error messages in detail. I am running python code directly and can see error messages in more detail.


 File ""h2o_model_eval.py"", line 160, in ModelEval
    preds = imported_model.predict(self.test)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.11/site-packages/h2o/model/model_base.py"", line 334, in predict
    j.poll()
  File "".venv/lib/python3.11/site-packages/h2o/job.py"", line 71, in poll
    pb.execute(self._refresh_job_status)
  File "".venv/lib/python3.11/site-packages/h2o/utils/progressbar.py"", line 187, in execute
    res = progress_fn()  # may raise StopIteration
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.11/site-packages/h2o/job.py"", line 136, in _refresh_job_status
    jobs = self._query_job_status_safe()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.11/site-packages/h2o/job.py"", line 132, in _query_job_status_safe
    raise last_err
  File "".venv/lib/python3.11/site-packages/h2o/job.py"", line 114, in _query_job_status_safe
    result = h2o.api(""GET /3/Jobs/%s"" % self.job_key)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.11/site-packages/h2o/h2o.py"", line 123, in api
    return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.11/site-packages/h2o/backend/connection.py"", line 507, in request
    raise H2OConnectionError(""Unexpected HTTP error: %s"" % e)
h2o.exceptions.H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Jobs/$03017f00000132d4ffffffff$_acab67512114e05db6ec9865ea9849d3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x2aab3f59ea90>: Failed to establish a new connection: [Errno 111] Connection refused'))
~                                                                               



How to debug this issue?","['python', 'h2o', 'h2o.ai']",Unknown,,N/A
78180809,78180809,2024-03-18T14:10:31,2024-03-18 16:55:16Z,24,"I'm running PCA in h2o (R version) and was wondering whether it's possible to specify/apply a rotation (like oblimin or promax).  I'm looking for the rotated loadings, and the reason I'm using h2o instead of other common packages for that (like ""psych"") is that my data set is huge (100000 columns) so I need to take advantage of h2o's nice parallel computing in Windows. The code I'm using currently is:


library(h2o)

h2o.init(nthreads=64)

x <- read.csv(""file_with_100000_columns.csv"")

for (i in 1:ncol(x)) {x[,i] <- as.factor(x[,i])}

x <- as.h2o(x)

mod <- h2o.prcomp(training_frame=x,k=5,use_all_factor_levels=TRUE)



Thanks!","['machine-learning', 'parallel-processing', 'pca', 'h2o', 'h2o.ai']",Tyler Moore,https://stackoverflow.com/users/10504648/tyler-moore,51
78110316,78110316,2024-03-05T20:10:23,2024-03-05 20:12:04Z,38,"I'd like to deepcopy a fitted h2o.sklearn.H2OAutoMLRegressor object. I can deepcopy prior to calling fit but not after.


Using H2O version 3.44.0.3, the following results in an error for the deepcopy of the fitted model.


import copy
import numpy as np
import h2o
from h2o.sklearn import H2OAutoMLRegressor

print(f""H2O version: {h2o.__version__}"")

X = np.random.rand(1000)
y = np.random.rand(1000)

m = H2OAutoMLRegressor(max_models=5, max_runtime_secs_per_model=30);

print(copy.deepcopy(m))

m.fit(X,y)

print(copy.deepcopy(m))



I cannot post the full trace because of StackOverflow code limits, but it looks like:


---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[7], line 17
     13 print(copy.deepcopy(m))
     15 m.fit(X,y)
---> 17 print(copy.deepcopy(m))

File ~/.pyenv/versions/3.11.7/lib/python3.11/copy.py:172, in deepcopy(x, memo, _nil)
    170                 y = x
    171             else:
--> 172                 y = _reconstruct(x, memo, *rv)
    174 # If is its own copy, don't memoize.
    175 if y is not x:

File ~/.pyenv/versions/3.11.7/lib/python3.11/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    269 if state is not None:
    270     if deep:
--> 271         state = deepcopy(state, memo)
    272     if hasattr(y, '__setstate__'):
    273         y.__setstate__(state)

...

File ~/.pyenv/versions/3.11.7/lib/python3.11/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)
    229 memo[id(x)] = y
    230 for key, value in x.items():
--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)
    232 return y

File ~/.pyenv/versions/3.11.7/lib/python3.11/copy.py:172, in deepcopy(x, memo, _nil)
    170                 y = x
    171             else:
--> 172                 y = _reconstruct(x, memo, *rv)
    174 # If is its own copy, don't memoize.
    175 if y is not x:

File ~/.pyenv/versions/3.11.7/lib/python3.11/copy.py:265, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    263 if deep and args:
    264     args = (deepcopy(arg, memo) for arg in args)
--> 265 y = func(*args)
    266 if deep:
    267     memo[id(x)] = y

File ~/.pyenv/versions/3.11.7/lib/python3.11/copyreg.py:105, in __newobj__(cls, *args)
    104 def __newobj__(cls, *args):
--> 105     return cls.__new__(cls, *args)

TypeError: H2OResponse.__new__() missing 1 required positional argument: 'keyvals'",['h2o'],Unknown,,N/A
77976819,77976819,2024-02-11T12:47:46,2024-02-14 12:56:50Z,0,"I am trying to train two GBM models, the first one takes the frequency as a response variable and the second takes number of claims as a response and exposure as on offset column, however, I did not see any difference between the two best models when I make hyperparameters tuning.
I get the same RMSE.



DF=data[-extreme_ind, ] 
DF[,c(4:60)]<- lapply(DF[,c(4:60)], factor)


df=as.h2o(DF)
splits <- h2o.splitFrame(df, 0.8, seed=1234)  
train <- h2o.assign(splits[[1]], ""train.hex"")  
valid <- h2o.assign(splits[[2]], ""valid.hex"") 

MOD_1_v2 <- h2o.gbm(x=c(4:56, 58:60),y = 61, training_frame = train, validation_frame =valid, ntrees=200) #100
summary(MOD_1_v2)

plot(MOD_1_v2,timestep=""number_of_trees"",metric=""RMSE"") 





gbm1_parameters <- list(learn_rate = c(0.01,0.05, 0.1),
                        max_depth = c(3, 5, 6),
                        sample_rate = c(0.7, 0.75, 0.8),  
                        col_sample_rate = c(0.2, 0.5, 1.0))



gbm1_grid <- h2o.grid(""gbm"", x = c(4:56, 58:60), y = 61,
                      grid_id = ""gbm_grid"",
                      training_frame = train,
                      validation_frame = valid,  
                      ntrees=20, #30
                      seed = 1,
                      hyper_params = gbm1_parameters)



gbm1_gridp<- h2o.getGrid(grid_id = ""gbm_grid"",
                         sort_by = ""rmse"",
                         decreasing  = FALSE)
print(gbm1_gridp)


best_MOD_1=h2o.getModel(gbm1_gridp@model_ids[[1]])

summary(best_MOD_1)




best_gbm_perf1 <- h2o.performance(model = best_MOD_1,newdata = valid)
best_gbm_perf1



plot(best_MOD_1,timestep=""number_of_trees"",metric=""rmse"")
h2o.varimp_plot(best_MOD_1)



MOD_2_v2 <- h2o.gbm(x=c(4:56, 58:60),y = 2,offset_column=""APVI"", training_frame = train, validation_frame = valid,ntrees=55) 

summary(MOD_2_v2) #apres supp outliers 

plot(MOD_2_v2,timestep=""number_of_trees"",metric=""RMSE"")


gbm2_parameters <- list(learn_rate = c(0.01,0.05, 0.1),
                        max_depth = c(3, 5),
                        sample_rate = c(0.7, 0.75, 0.8),  
                        col_sample_rate = c(0.2, 0.5, 1.0))




gbm2_grid <- h2o.grid(""gbm"", x = c(4:56, 58:60), y = 2,
                      grid_id = ""gbm_grid"",
                      training_frame = train,
                      validation_frame = valid, 
                      ntrees=55, #10
                      seed = 123,
                      hyper_params = gbm2_parameters)


gbm2_gridp<- h2o.getGrid(grid_id = ""gbm_grid"",
                         sort_by = ""rmse"",
                         decreasing  = FALSE)
print(gbm2_gridp)



best_MOD_2=h2o.getModel(gbm2_gridp@model_ids[[1]])
summary(best_MOD_2)


best_gbm_perf2 <- h2o.performance(model = best_MOD_2,newdata = valid)
best_gbm_perf2



How Can I fix this problem ?","['r', 'offset', 'h2o', 'hyperparameters', 'gbm']",wej,https://stackoverflow.com/users/14061266/wej,21
77973356,77973356,2024-02-10T14:04:40,2024-02-13 09:03:02Z,0,"I'm using 
h2o
 in R and RStudio, and 
h2o
 is working fine. However, when I try to use the 
automl()
 function, the process starts, RStudio shows progress bars, but after the progress bar reaches 100% no results are returned. The R session and the process just continues running and the R session remains busy. Using the exact same code with 
randomForest()
 works fine.


Training frame:


h2o.init()
peng <- as.h2o(penguins)



This doesn't worK:


aml <- h2o.automl(y = c(""body_mass_g""), training_frame = peng, max_runtime_secs = 30)



however, this does:


rf <- h2o.randomForest(y = c(""body_mass_g""), training_frame = peng, max_runtime_secs = 30)","['r', 'h2o', 'automl']",L Tyrone,https://stackoverflow.com/users/2530121/l-tyrone,"6,579"
77908088,77908088,2024-01-30T16:54:19,2024-01-31 08:44:31Z,0,"Im trying to upload a csv file to a 
shiny
 app as a 
h2o
 object based on this 
process
 but I get 
Warning: Error in .key.validate: 
key
 must match the regular expression '^[a-zA-Z_][a-zA-Z0-9_.]*$': 0_sid_b018_4
. Im not sure if there is a problem with path reading or the table


#install h2o first
if (""package:h2o"" %in% search()) { detach(""package:h2o"", unload=TRUE) }
if (""h2o"" %in% rownames(installed.packages())) { remove.packages(""h2o"") }


if (! (""methods"" %in% rownames(installed.packages()))) { install.packages(""methods"") }
if (! (""statmod"" %in% rownames(installed.packages()))) { install.packages(""statmod"") }
if (! (""stats"" %in% rownames(installed.packages()))) { install.packages(""stats"") }
if (! (""graphics"" %in% rownames(installed.packages()))) { install.packages(""graphics"") }
if (! (""RCurl"" %in% rownames(installed.packages()))) { install.packages(""RCurl"") }
if (! (""jsonlite"" %in% rownames(installed.packages()))) { install.packages(""jsonlite"") }
if (! (""tools"" %in% rownames(installed.packages()))) { install.packages(""tools"") }
if (! (""utils"" %in% rownames(installed.packages()))) { install.packages(""utils"") }


install.packages(""h2o"", type = ""source"", repos = (c(""http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R"")))
library(h2o)
h2o.init()

#app
library(shiny)
library(h2o)
h2o.init()

# Define UI for data upload app ----
ui <- fluidPage(
  
  # App title ----
  titlePanel(""Uploading Files""),
  
  # Sidebar layout with input and output definitions ----
  sidebarLayout(
    
    # Sidebar panel for inputs ----
    sidebarPanel(
      # Input: Select a file ----
      fileInput(""file1"", ""Choose CSV File"",
                multiple = FALSE,
                accept = c(""text/csv"",
                           ""text/comma-separated-values,text/plain"","".xlsx"",
                           "".csv"")),
      
      # Horizontal line ----
      tags$hr(),
      
      # Input: Checkbox if file has header ----
      checkboxInput(""header"", ""Header"", TRUE),
      
      # Input: Select separator ----
      radioButtons(""sep"", ""Separator"",
                   choices = c(Comma = "","",
                               Semicolon = "";"",
                               Tab = ""\t""),
                   selected = "",""),
      
      # Input: Select quotes ----
      radioButtons(""quote"", ""Quote"",
                   choices = c(None = """",
                               ""Double Quote"" = '""',
                               ""Single Quote"" = ""'""),
                   selected = '""'),
      
      # Horizontal line ----
      tags$hr(),
      
      # Input: Select number of rows to display ----
      radioButtons(""disp"", ""Display"",
                   choices = c(Head = ""head"",
                               All = ""all""),
                   selected = ""head"")
      
    ),
    
    # Main panel for displaying outputs ----
    mainPanel(
      
      # Output: Data file ----
      tableOutput(""contents"")
      
    )
    
  )
)

# Define server logic to read selected file ----
server <- function(input, output) {
  file_info <- reactive({
    # input$file1 will be NULL initially. After the user selects
    # and uploads a file, head of that data file by default,
    # or all rows if selected, will be shown.
    
    req(input$file1)
   
      # when reading semicolon separated files,
      # having a comma separator causes `read.csv` to error
      tryCatch(
        {
          path <- input$file1$datapath
          data <- h2o.uploadFile(path = path)
          df <- read.csv(data,
                         header = input$header,
                         sep = input$sep,
                         quote = input$quote)
        },
        error = function(e) {
          # return a safeError if a parsing error occurs
          stop(safeError(e))
        }
      )
      
      if(input$disp == ""head"") {
        return(head(df))
      }
      else {
        return(df)
      }
    
    
  })
  output$contents <- renderTable({
    
    file_info()
  })
  
}

# Create Shiny app ----
shinyApp(ui, server)","['r', 'shiny', 'h2o']",firmo23,https://stackoverflow.com/users/9198260/firmo23,"8,384"
77874825,77874825,2024-01-24T16:50:31,2024-01-24 18:02:09Z,91,"I am working on a hybrid model that uses CNN (time series input) and H2oRandom forest (tabular data input) combined at the fully connected layer to solve regression problems. I would like to optimize the hyperparameters of CNN and RF. The only way I figured out is to optimize two models separately and combine them at the FC layer to obtain the output. I use RandomSeach Keras Tuner for CNN and Grid RandomSearch for H2O Random Forest hyperparameters.


I am uncertain if optimizing the models separately is the most effective way to enhance the model's performance. Is there a different approach to optimizing hybrid models? Please let me know if there is a better approach.","['optimization', 'deep-learning', 'conv-neural-network', 'random-forest', 'h2o']",Geerthy Thambiraj,https://stackoverflow.com/users/11698173/geerthy-thambiraj,21
77849328,77849328,2024-01-19T23:37:26,2024-01-22 23:55:53Z,13,"I'm interested in using h2o's suite of algorithms to perform some analysis on data, specifically with ordinal logistic regression. I was looking through the available GLMBooklet.pdf and on page 21 of the version available it references a loss function that actually doesn't appear; does anyone know where I can get the details of the loss function, or an updated version of GLMBooklet.pdf with the loss function present? Thanks!


I haven't been able to find the loss function anywhere, but maybe I'm not looking in the right places?",['h2o'],Rich,https://stackoverflow.com/users/23271364/rich,1
77838561,77838561,2024-01-18T10:15:28,2024-01-18 18:16:22Z,86,"I have been getting error in pyspark while running h2o model prediction.


file ""/usr/spark/python/pyspark/cloudpickle.py"", line 562, in subimport
ModuleNotFoundError; No Modele named h2o


i created pandas udf


`def predict_h2o_model(*cols)
    x=pd.concat(cols,axis=1)
    h2odataframe=h2o.H2OFrame(x)
    scores=model.predict(h2odataframe)
    return pd.series(scores)`



I' scoring using pyspark dataframe


`df_scores=sparf_df.select(F.col(""cust_id""),predict_h2o_model(*cols).alias('model_score'))`



I was expecting h2o model scores in spark_df dataframe","['pyspark', 'h2o']",Nithin Reddy,https://stackoverflow.com/users/22314796/nithin-reddy,1
77774644,77774644,2024-01-07T20:01:26,2024-01-07 20:01:26Z,174,"When I run h2o AutoML in python on my MacOS with Apple M1 chip, I get a message ""
XGBoost is not available; skipping it.
"". I found a 
similar issue
 from more than a year ago. So I am wondering if there has been any progress (and therefore my issue has different causes), or XGBoost has still not been compiled for Apple M1.


Python: 3.9.2

h2o: 3.44.0.2

processor: Apple M1","['xgboost', 'apple-m1', 'h2o']",sofi,https://stackoverflow.com/users/23209807/sofi,11
77679172,77679172,2023-12-18T12:44:49,2023-12-18 16:03:49Z,103,"I am trying to upgrade a binary (DRF) H2O model to a higher version (from v3.28.1.2 to v3.42.0.3). Due to tecnical restrictions I cannot use the MOJO format for deployment. Is it possible to do the following instead?




load the binary model in the older version


export the model as MOJO format


load the model in MOJO format in the newer version


save the model in Binary in the newer version




I am able to save the model in the newer version. However, when I try to load the model object (using h2o.load_model()), I get the following error:



H2OServerError: HTTP 500 Server Error:
Server error java.lang.NullPointerException:
  Error: Caught exception: java.lang.NullPointerException
  Request: None
  Stacktrace: java.lang.NullPointerException
      hex.generic.GenericModel$GenModelSource.backingByteVec(GenericModel.java:391)
      hex.generic.GenericModel$GenModelSource.get(GenericModel.java:373)
      hex.generic.GenericModel.genModel(GenericModel.java:325)
      hex.generic.GenericModel.havePojo(GenericModel.java:546)
      water.api.schemas3.ModelSchemaV3.fillFromImpl(ModelSchemaV3.java:80)
      water.api.schemas3.ModelSchemaV3.fillFromImpl(ModelSchemaV3.java:22)
      water.api.ModelsHandler.importModel(ModelsHandler.java:263)
      sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
      sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","['binary', 'version', 'h2o', 'mojo']",Unknown,,N/A
77549868,77549868,2023-11-25T23:03:26,2023-11-27 21:01:59Z,94,"I am using h2o package in python to build a fairly complex model.
It has around 1500 features, but I know that most of them are not really important and I would like to extract the subset of a given size (let's say 100) that maximizes the R squared of my model.

Is there some method that is already implementing this for h2o in python?


Otherwise I would need to code it myself, but that also implies to run the model multiple times and I am not sure i would code it in the correct way.


One possible way to code it is this one:




Save the R2 for the model, then remove the k less important features


Create a second model without the removed features


Calculate the R2 for the new model and compare to the previous R2. Use a metric to decide whether to keep the new model or stick with the old.


Iterate these steps until the previous step chooses the old model as the best one
I am pretty sure this will not give me the 'best subset' of feature but I really hope it would be sufficient.




The second method I thought of is the following:




set the number of feature 
N
 you want in the new model and the number of iterations 
K


Save the original model R2 as reference


Extract 
N
 features at random from the original model, using their relative importance as probability of being extracted (more important features more likely extracted)


For each model save the list of features and the new R2


After iterating 
K
 times stop the algorithm and compare the R2


Choose the set of features with the closest R2 to the original one","['python', 'machine-learning', 'random-forest', 'h2o', 'feature-selection']",Mirko,https://stackoverflow.com/users/7214340/mirko,233
77498397,77498397,2023-11-16T22:38:15,2023-11-17 14:07:48Z,78,"I am working in Databricks with Sparkling Water 3.40.0.4; I have a total driver memory of 512 GB and six workers with 64 GB each. When I call


hc = H2OContext.getOrCreate()



the internal H2O cluster is created across six workers, but the total cluster memory size is roughly 60 GB. I can create a normal, non-sparkling water H2O cluster and pass the max_mem_size and min_mem_size arguments to the init() method, which will return a much larger sized cluster, but I can't seem to find how to do that on Databricks.


h2o.init(max_mem_size=""200g"")



That returns roughly 200 GB of memory for the cluster.


I created a local Spark installation and changed the spark.driver.memory property, and changing this resulted in a larger sparkling water cluster size, but explicitly setting that property in Data Bricks has no change on the sparkling water cluster there.


Is there a configuration I can pass to Spark or an internal H2O cluster to set a larger memory size on Databricks?","['python', 'apache-spark', 'databricks', 'h2o', 'sparkling-water']",Alex Ott,https://stackoverflow.com/users/18627/alex-ott,86.7k
77476421,77476421,2023-11-13T19:36:59,2023-11-13 19:36:59Z,0,"Trying to use DALEX on my data. Getting following error in line 
pb_h2o_automl <- predict_parts(explainer_h2o_automl,new_observation = new_date_birth,type=""break_down"")


Error


Error in contribution[nrow(contribution), ] <- cumulative[nrow(contribution),  : 
  incorrect number of subscripts on matrix



Code


rm( list = ls() )

library(DALEX) ; library(h2o) ; library(DALEXtra) ; library(readxl) ; library(dplyr)
set.seed(17)

setwd( 'E:\\projects\\political_analysis' )

df0 = read_excel('training.xlsx')

df0$age = as.numeric( df0$age)

df1 <- df0[c(""area"", ""district"", ""assembly_constituency"", ""gender"", ""age"", ""party_assembly_election_2018"",
             ""party_current_year_election"", ""chief_minister"", ""leader_vote_for_mla"", ""benefit_govt_scheme"",
             ""benefit_current_budget_scheme"", ""occupation"", ""education"", ""social_category"", ""caste"", ""caste_other"",'party_upcoming_election')]

df1 <- df1 %>% mutate_all(~ifelse(is.na(.), as.character(names(which.max(table(na.omit(.))))), as.character(.))) %>% mutate_at(vars(-age), as.factor)

h2o.init()

target <- ""party_upcoming_election""
df <- as.h2o(df1)

model_h2o_automl <- h2o.automl(y = target, training_frame = df, max_models = 5, max_runtime_secs = 600  )

leader_board <- h2o.get_leaderboard(model_h2o_automl)
head(leader_board)

test_df_0 = df1[1,]

explainer_h2o_automl <- DALEXtra::explain_h2o(model = model_h2o_automl, 
                                              data = test_df_0,
                                              y = test_df_0$party_upcoming_election,
                                              label = ""h2o automl"",
                                              colorize = T)

new_date_birth <- test_df_0 %>% select( - c('party_upcoming_election'))
pb_h2o_automl <- predict_parts(explainer_h2o_automl,new_observation = new_date_birth,type=""break_down"")



Have pasted first 50 rows of data here :


https://pastebin.com/C6ETyJbp","['r', 'h2o', 'dalex']",Soumya Boral,https://stackoverflow.com/users/8315634/soumya-boral,"1,329"
77475890,77475890,2023-11-13T17:57:13,2023-11-15 13:39:15Z,43,"I built an 
H2ORandomForestEstimator
 model using 
train()
 method on my spark dataframe with the target column containing values 0 or 1. I downloaded and printed its mojo files using 
model.download_mojo(MOJO_ZIP_PATH)
 and 
h2o.print_mojo(MOJO_ZIP_PATH, tree_index=tree_ind)
 functions respectively. A partial such output tree is shown below.


As can be seen, leaf nodes have a field named 
predValue
 containing a value between 0 and 1. What is the meaning of this 
predValue
 field? Does it mean that the target variable is likely to contain the value contained in 
predValue
 field if the input variables happen to meet this root to leaf path when 
predict()
 is called on them?


Moreover, I want to preprocess the output model of 
H2ORandomForestEstimator
 and filter only those rules (root to leaf paths) for which my model will predict 1. Is there a way to filter such rules by parsing the mojo files without actually running the 
predict()
 function on input variables? 
predValue
 field in the output mojo files looked promising to solve this problem but I could not figure out its co-relation with the output variable. Can it be used to figure out the top-N rules?


'trees': [{
    'root': {
        'nodeNumber': 0,
        'weight': 18319.0,
        'colId': 169,
        'colName': 'pkg_items_gl_product_group_desc_1.gl_electronics',
        'leftward': True,
        'isCategorical': False,
        'inclusiveNa': True,
        'splitValue': 0.5,
        'rightChild': {
            'nodeNumber': 25,
            'weight': 462.0,
            'predValue': 0.9935065
        },
        'leftChild': {
            'nodeNumber': 1,
            'weight': 17857.0,
            'colId': 0,
            'colName': 'pkg_attr_total_pkg_price',
            'leftward': True,
            'isCategorical': False,
            'inclusiveNa': True,
            'splitValue': 186.52805,
            'rightChild': {
                'nodeNumber': 26,
                'weight': 201.0,
                'predValue': 0.9900498
            },
            'leftChild': {
                'nodeNumber': 3,
                'weight': 13184.0,
                'colId': 149,
                'colName': 'pkg_items_gl_product_group_desc_1.gl_automotive',
                'leftward': True,
                'isCategorical': False,
                'inclusiveNa': True,
                'splitValue': 0.5,
                'rightChild': {
                    'nodeNumber': 27,
                    'weight': 312.0,
                    'predValue': 0.99038464
                },","['machine-learning', 'pyspark', 'random-forest', 'decision-tree', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
77208586,77208586,2023-09-30T21:59:06,2023-10-01 11:53:51Z,0,"I am getting an error. It occurs when I use the deep learning function in h2o in R. My response is a categorical variable which takes on 3 values so I can't change it to binary labels.




Error: java.lang.IllegalArgumentException: Actual column must contain binary class labels, but found cardinality 3!




This is my input:


h2o.init()
dat_h20 = data.frame(Event=as.factor(space_data$Event), TrajA= space_data$TrajA, AcousticA = space_data$AcousticA, HullScan= as.factor(space_data$HullScan), MCStatus = as.factor(space_data$MCStatus))
set.seed(2023)

set = sample(1:150, 150 , replace = FALSE)
data_train = as.h2o(dat_h20[set,])
head(data_train)
data_val = as.h2o(dat_h20[-set,])

value = exp(seq(-10,-3, length = 20))
value
validation_errors = numeric(20) # validation error for each regularisation parameter
?h2o.deeplearning
dat_h20[1]
for (i in 1:length(value))
{
model = h2o.deeplearning(x = 2:5, y = 1 ,
                         training_frame = data_train, 
                         validation_frame = data_val,
                         standardize = TRUE, 
                         hidden = c(5,5), 
                         activation = 'Rectifier', 
                         distribution = 'multinomial',
                         loss = 'CrossEntropy',
                         l2 = value[i],
                         rate = 0.01,
                         adaptive_rate = FALSE,
                         epochs = 1000,
                         reproducible = TRUE,
                         seed = 2,
                         )
validation_errors[i]= h2o.logloss(model, train = TRUE, valid = TRUE)
}

plot(value, validation_errors)","['r', 'deep-learning', 'h2o']",jonrsharpe,https://stackoverflow.com/users/3001761/jonrsharpe,121k
77186876,77186876,2023-09-27T11:01:32,2023-09-27 16:49:27Z,0,"I can't seem to get h2o running in R.

I can easily import h2o using 
library(h2o)
 which displays


----------------------------------------------------------------------

Your next step is to start H2O:
    > h2o.init()

For H2O package documentation, ask for help:
    > ??h2o

After starting H2O, you can use the Web UI at http://localhost:54321
For more information visit https://docs.h2o.ai

----------------------------------------------------------------------


Attaching package: ‘h2o’

The following objects are masked from ‘package:stats’:

    cor, sd, var

The following objects are masked from ‘package:base’:

    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames, colnames<-,
    ifelse, is.character, is.factor, is.numeric, log, log10, log1p, log2,
    round, signif, trunc

Warning message:
package ‘h2o’ was built under R version 4.1.3 



But the issue is whenever I run 
h2o.init()
 to start the h2o cluster, I get the following error message


H2O is not running yet, starting it now...
<simpleError in system2(command, ""-version"", stdout = TRUE, stderr = TRUE): '""C:\Program Files\Java\jdk-20\bin\bin\java.exe""' not found>
Error in value[[3L]](cond) : 
  You have a 32-bit version of Java. H2O works best with 64-bit Java.
Please download the latest Java SE JDK from the following URL:
https://www.oracle.com/technetwork/java/javase/downloads/index.html
In addition: Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :
  path[1]=""C:\Program Files\Java\jdk-20\bin/bin/java.exe"": The system cannot find the path specified



My laptop is HP 64-bit and the Java JDK I downloaded and installed is jdk-20_windows-x64_bin (which is 64-bit).

I have also set the required Java Path in Environment Variables.


Below is the Java version installed


Microsoft Windows [Version 10.0.19044.1889]
(c) Microsoft Corporation. All rights reserved.

C:\Users\user>java -version
java version ""20.0.2"" 2023-07-18
Java(TM) SE Runtime Environment (build 20.0.2+9-78)
Java HotSpot(TM) 64-Bit Server VM (build 20.0.2+9-78, mixed mode, sharing)



I don't understand why h2o cannot detect my 64-bit Java already installed and work with it.

Did I download/install the wrong Java JDK file?


My R version is 4.1.0


Below are images of my JAVA_HOME variable and Path variable","['java', 'r', 'h2o', 'h2o.ai']",Unknown,,N/A
77184526,77184526,2023-09-27T03:26:48,2023-10-20 20:03:43Z,54,"Having had AutoML() recommend a model after I experimented with a single column of data and needed to identify a target as a step-1 of the single data to train the successful model I now want to use the model on test (live) data. The model however wants a 2 column input and when I use a padded second column ""target"" = [0] then the model returns the same predictive output and I am guessing it is because of the second null zero entry column. Is there a way around this issue please ?


I have tried using the model with just one column and it errored and I have added the second padding column all zeros and while the code worked the prediction is always the same with differing inputs",['h2o'],marc_s,https://stackoverflow.com/users/13302/marc-s,752k
77064065,77064065,2023-09-08T04:01:47,2024-03-23 05:09:27Z,0,"I'm very new to generative AI. I have 64gb RAM and 20GB GPU. I used some opensource model from Huggingface and used Python to simply prompt it with out of box model and displaying the result. I downloaded the model to local using 
save_pretrained
 and trying to load the model from local there after. It works. But everytime I run the python file it takes more than 10 mins to display the results.


There is a step 
Loading checkpoint shards
 that takes 6-7 mins everytime. Am I doing anything wrong? why it has to load something everytime even though the model is refered from local.


I tried using 
local_files_only=True, cache_dir=cache_dir, low_cpu_mem_usage=True, max_shard_size=""200MB""
 , none solved the time issue .


How to prompt the saved model directly without so much delay as user usable. Any help would be highly appreciated","['huggingface-transformers', 'h2o', 'huggingface', 'huggingface-tokenizers', 'llama']",Khaleel,https://stackoverflow.com/users/3867290/khaleel,"1,372"
77016904,77016904,2023-08-31T14:43:01,2023-09-06 14:39:48Z,173,"The 
documentation 
 for distributed random forest in h2o states that, for multiclass problems, ""a tree is used to estimate the probability of each class separately"". I can see this in visualising the trees that each class indeed seems to have a completely independent ""one-vs-rest"" tree.


I am wondering how the scores from these trees are combined into the final score vector - are they just normalized to sum to one?


I would also like to understand why this approach was chosen and how it compares to the usual approach of handling multiple classes within a single tree. For individual classes we see that the performance of the multiclass classifier is typically worse than a dedicated one-vs-rest classifier with the same hyperparameters, even though under the hood the multiclass classifier should be very similar.","['random-forest', 'h2o', 'multiclass-classification']",nickc,https://stackoverflow.com/users/22442035/nickc,11
76950243,76950243,2023-08-22T05:09:39,2023-08-23 03:33:35Z,47,"I tried to build H2O open source, nomatter stable version or clone version, build process via 
./gradlew build
 are failed due to ""error: cannot find symbol"", the detailed error info as below:


Updated build log:


here is the full log from output for the latest rel-3.42.0 branch:

(python) wayahead@ubox:~/workspace/h2o/github$ git clone --branch rel-3.42.0 https://github.com/h2oai/h2o-3.git
Cloning into 'h2o-3'...
remote: Enumerating objects: 428884, done.
remote: Counting objects: 100% (6618/6618), done.
remote: Compressing objects: 100% (3895/3895), done.
remote: Total 428884 (delta 3654), reused 5264 (delta 2604), pack-reused 422266
Receiving objects: 100% (428884/428884), 604.35 MiB | 9.36 MiB/s, done.
Resolving deltas: 100% (263828/263828), done.
(python) wayahead@ubox:~/workspace/h2o/github$ javac -version
javac 1.8.0_382
(python) wayahead@ubox:~/workspace/h2o/github$ cd h2o-3/
(python) wayahead@ubox:~/workspace/h2o/github/h2o-3$ ./gradlew build -x test
Starting a Gradle Daemon (subsequent builds will be faster)

> Configure project :
The project project ':h2o-persist-s3' needs CI for running tests! You can pass `-PdoCI=true` to force CI behaviour.

> Configure project :h2o-assemblies:main
== :h2o-assemblies:main: Using optional component: xgboost, version 3.42.0
== :h2o-assemblies:main: Using optional component: jython-cfunc, version 3.42.0
== :h2o-assemblies:main: Using optional component: jgrapht, version 3.42.0
== :h2o-assemblies:main: Using optional component: mojo-pipeline, version 3.42.0

> Configure project :h2o-r


> Configure project :h2o-assemblies:genmodel
== :h2o-assemblies:genmodel: Using optional genmodel component: xgboost, version 3.42.0
== :h2o-assemblies:genmodel: Using optional genmodel component: jgrapht, version 3.42.0
== :h2o-assemblies:genmodel: Using optional genmodel component: mojo-pipeline, version 3.42.0

> Configure project :h2o-assemblies:minimal
== :h2o-assemblies:minimal: Using optional component: xgboost, version 3.42.0
== :h2o-assemblies:minimal: Using optional component: jython-cfunc, version 3.42.0
== :h2o-assemblies:minimal: Using optional component: jgrapht, version 3.42.0
== :h2o-assemblies:minimal: Using optional component: mojo-pipeline, version 3.42.0

> Configure project :h2o-assemblies:steam
== :h2o-assemblies:steam: Using optional component: xgboost, version 3.42.0
== :h2o-assemblies:steam: Using optional component: jython-cfunc, version 3.42.0
== :h2o-assemblies:steam: Using optional component: jgrapht, version 3.42.0
== :h2o-assemblies:steam: Using optional component: mojo-pipeline, version 3.42.0

> Task :h2o-core:generateBuildVersionJava
NOTE: emitBuildVersionJava found no file, emitting new file
> Task :h2o-genmodel:generateBuildVersionJava
NOTE: emitBuildVersionJava found no file, emitting new file
> Task :h2o-genmodel:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:h2o-genmodel:compileJava took 1.141 secs

> Task :h2o-core:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:h2o-core:compileJava took 7.099 secs

> Task :h2o-algos:compileJava
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:24: error: cannot find symbol
  public static final class GLMModelOutputV3 extends ModelOutputSchemaV3<GLMOutput, GLMModelOutputV3> {
                                                     ^
  symbol:   class ModelOutputSchemaV3
  location: class hex.schemas.GLMModelV3
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:24: error: cannot find symbol
  public static final class GLMModelOutputV3 extends ModelOutputSchemaV3<GLMOutput, GLMModelOutputV3> {
                                                                         ^
  symbol:   class GLMOutput
  location: class hex.schemas.GLMModelV3
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:22: error: type argument hex.schemas.GLMModelV3.GLMModelOutputV3 is not within bounds of type-variable OS
public class GLMModelV3 extends ModelSchemaV3<GLMModel, GLMModelV3, GLMModel.GLMParameters, GLMV3.GLMParametersV3, GLMOutput, GLMModelV3.GLMModelOutputV3> {
                                                                                                                                        ^
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:196: error: method does not override or implement a method from a supertype
    @Override
    ^
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:198: error: non-static variable super cannot be referenced from a static context
      super.fillFromImpl(impl);
      ^
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:198: error: no suitable method found for fillFromImpl(hex.glm.GLMModel.GLMOutput)
      super.fillFromImpl(impl);
           ^
    method water.api.Schema.fillFromImpl(hex.glm.GLMModel) is not applicable
      (argument mismatch; hex.glm.GLMModel.GLMOutput cannot be converted to hex.glm.GLMModel)
    method water.api.schemas3.ModelSchemaV3.fillFromImpl(hex.glm.GLMModel) is not applicable
      (argument mismatch; hex.glm.GLMModel.GLMOutput cannot be converted to hex.glm.GLMModel)
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
Note: Some messages have been simplified; recompile with -Xdiags:verbose to get full output
6 errors
:h2o-algos:compileJava took 2.255 secs

> Task :h2o-algos:compileJava FAILED

Task timings:
   7.099 secs  :h2o-core:compileJava
   2.255 secs  :h2o-algos:compileJava
   1.141 secs  :h2o-genmodel:compileJava
   0.366 secs  :h2o-core:generateBuildVersionJava
   0.290 secs  All others

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':h2o-algos:compileJava'.
> Compilation failed; see the compiler error output for details.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0.

You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.

See https://docs.gradle.org/7.2/userguide/command_line_interface.html#sec:command_line_warnings

BUILD FAILED in 29s
14 actionable tasks: 13 executed, 1 up-to-date



==============================================
The environment: Ubuntu 22.04, openjdk version ""1.8.0_382""


I did look into the FQA and README of h2o-3 at github, and tried many times 
./gladew clean
. Accorindg to the above error, seems machine learning algo 
GLMModelV3
 is mising in the code base, but there is no info about how to fetch those code/jar. So I hope the expert can explain more details about how to prepare the build environment.",['h2o'],Unknown,,N/A
76931856,76931856,2023-08-18T18:40:12,2023-08-22 22:25:05Z,47,"I am trying to migrate a model from Hadoop to GCP. Model MOJO will not be retrained. I am running the model in Dataproc using Airflow spark submit.
Source data format matches with Hadoop source and everything. While running the model, I am getting this error:


Caused by: hex.genmodel.easy.exception.PredictUnknownCategoricalLevelException: Unknown categorical level (my_column,Y)



This column has the same values as we have on Hadoop, and in there everything works fine.
Model was created on H20 version 
3.30.0.4
 and MOJO version is 
1.4
.


While running the dataproc cluster I am using 
""PIP_PACKAGES"": ""h2o_pysparkling_3.1""


Not sure what the issue is? Please help.","['h2o', 'sparkling-water', 'h2o.ai']",Unknown,,N/A
76850663,76850663,2023-08-07T10:06:32,2023-08-10 09:23:45Z,0,"I am trying to build a model of Cox Proportional Hazard with h2o. In fact a I have succesfully built a model using deeplearning about survival in gastric cancer dataset. When trying to do it with h2o.coxph, I always get ""ERRR on field: _train: Training data must have at least 2 features (incl. response)"". I have checked that my data train frame has the column of ""event/response"" (Have tried with factor/numeric/integer with same results). Also I have tried with an invented  10 observations dataset which include ""start column (all zeros), stop colum (random>0), event column (random (0,1)) and a predictor (eg:age (random>20), with the same results.


But when I import the data (csv file) that under the ""help section"" is suggested, and check the datatype of the start,stop and event column, are similar to those I have, but in this case I can build the model. I don't know which the problem is.


Any suggestions??


This is my code:


h2o.init()

#Load data into the cluster
datos_h2o<-as.h2o(prueba_2)

#Build the model
modelo_coxph <- h2o.coxph(x = ""edad"", training_frame = datos_h2o,
                         event_column = ""event"",
                         start_column= ""inicio"",
                         stop_column = ""final""
                        )




Thanks in advance","['r', 'h2o']",ilpadrino,https://stackoverflow.com/users/1974542/ilpadrino,13
76844317,76844317,2023-08-06T03:30:58,2023-08-09 20:31:49Z,346,"I'm stuck with H2oGPT, I can't let it run.


I am running on a Windows PC with




11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz   2.30 GHz


32GB memory 64bit


GPU Nvidia GeForce RTX 3050




I followed all the procedure described in the 
H2oGPT Windows Installation Page
.


This is the command I launch.


$ python generate.py --base_model='llama' --prompt_type=llama2 --score_model=None --langchain_mode='UserData' --user_path=./my_documents



This is the error stack trace:


Traceback (most recent call last):
  File ""C:\Users\dulcin\Documents\Java\UltraGPT\h2ogpt\generate.py"", line 7, in <module>
    from src.gen import main
  File ""C:\Users\dulcin\Documents\Java\UltraGPT\h2ogpt\src\gen.py"", line 32, in <module>
    from utils import set_seed, clear_torch_cache, NullContext, wrapped_partial, EThread, get_githash, \
ImportError: cannot import name 'set_seed' from 'utils' (C:\Users\dulcin\AppData\Local\Programs\Python\Python310\lib\site-packages\utils\__init__.py)



My questions are:




Should I install ""utils"" seaprately?


Is my laynch command ok? ./my_documents right now only contains one text document.","['h2o', 'openai-api']",donnadulcinea,https://stackoverflow.com/users/3200736/donnadulcinea,"1,864"
