,post_id,date_created,last_activity_date,view_count,question_text,detailed_tags,user_name,user_profile_link,reputation_score
79194524,79194524,2024-11-16T04:29:33,2024-11-16 04:29:33Z,9,"I'm working with h2o in RStudio, specifically the h2o.glm() model, and I'm trying to manually calculate the std_error of my coefficients. Does anyone know what formula is used?


I tried using the formula SE(Beta Hat) = sqrt(MSE/sum((Xi-Xbar)^2)) with the code below. I don't expect the answer to be the exact SE since it's an estimation, but the answer I got wasn't close.


teststack <- model_stack$variable
denom <- sum((teststack$variable-mean(teststack$variable))^2)
se <- sqrt(h2o.mse(model_fit_uc)/denom)
se",['h2o'],Camino Stephanie,https://stackoverflow.com/users/28324064/camino-stephanie,11
79045928,79045928,2024-10-02T08:26:43,2024-10-03 08:20:49Z,36,"I have the scenario where you have a feature, call it X, with possible values it can hold, say x1, x2, x3, x4. I want the GBM in H2O to train x1 and x3 together so that they are predicted with the same output value.


Any suggestions how to do this would be appreciated. :)","['h2o', 'gbm']",Adam,https://stackoverflow.com/users/12979649/adam,21
79033110,79033110,2024-09-27T23:03:19,2024-09-30 07:34:12Z,56,"Everytime when I run h2o on pydroid3 then it says I need Java/JRE for it to work. ERROR:


Checking whether there is an H2O instance running at http://localhost:54321..... not found.


Attempting to start a local H2O server...


Traceback (most recent call last):


File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/h2o.py"", line 270, in init


h2oconn = H2OConnection.open(url=url, ip=ip, port=port, name=name, https=https,

          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/backend/connection.py"", line 406, in open


conn._cluster = conn._test_connection(retries, messages=msgs)

                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/backend/connection.py"", line 713, in _test_connection


raise H2OConnectionError(""Could not establish link to the H2O cloud %s after %d retries\n%s""



h2o.exceptions.H2OConnectionError: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries


[22:07.97] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79554692d0>: Failed to establish a new connection: [Errno 111] Connection refused'))


[22:08.19] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x795546b1d0>: Failed to establish a new connection: [Errno 111] Connection refused'))


[22:08.41] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x795546b810>: Failed to establish a new connection: [Errno 111] Connection refused'))


[22:08.63] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7955065450>: Failed to establish a new connection: [Errno 111] Connection refused'))


[22:08.85] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7955067390>: Failed to establish a new connection: [Errno 111] Connection refused'))


During handling of the above exception, another exception occurred:


Traceback (most recent call last):


File ""/data/user/0/ru.iiec.pydroid3/files/accomp_files/iiec_run/iiec_run.py"", line 31, in 


start(fakepyfile,mainpyfile)



File ""/data/user/0/ru.iiec.pydroid3/files/accomp_files/iiec_run/iiec_run.py"", line 30, in start


exec(open(mainpyfile).read(),  __main__.__dict__)



File """", line 19, in 


File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/h2o.py"", line 287, in init


hs = H2OLocalServer.start(nthreads=nthreads, enable_assertions=enable_assertions, max_mem_size=mmax,

     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/backend/server.py"", line 139, in start


hs._launch_server(port=port, baseport=baseport, nthreads=int(nthreads), ea=enable_assertions,



File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/backend/server.py"", line 271, in _launch_server


java = self._find_java()

       ^^^^^^^^^^^^^^^^^



File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/h2o/backend/server.py"", line 453, in _find_java


raise H2OStartupError(""Cannot find Java. Please install the latest JRE from\n""



h2o.exceptions.H2OStartupError: Cannot find Java. Please install the latest JRE from


http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements


[Program finished]


I tried nothing, cant find any information to download it.","['python', 'java', 'mobile', 'h2o']",XuhanChen,https://stackoverflow.com/users/27513799/xuhanchen,1
78995266,78995266,2024-09-17T17:55:20,2024-09-18 12:11:50Z,0,"I'm attempting to implement the deeplearning function in the h2o package and obtain a persistent error despite (seemingly) following the example given in the documentation for the package:


https://www.rdocumentation.org/packages/h2o/versions/3.44.0.3/topics/h2o.deeplearning


My inputs are y, a length-n vector of 0,1 indicating binary outcomes, and an nxm matrix of integers x which are my predictor variables.


library(h2o)
h2o.init()

y = as.factor(y)
xnew = cbind(y,x)
xnew = data.frame(xnew)
#create a single data frame with response variable y and predictor variables x
x_df = as.h2o(xnew)
# formats dataframe as h2o data object
nn_model_training = h2o.deeplearning(y=1,training_frame = x_df)



which executes without issue. I now wish to use the nn_model_training to predict outcomes for a test set. To have the same column names as in the model, I take 2...m from the training_frame (i.e. exclude outcome variable y):


keep_names = names(x_df)[2:length(x_df[1,])]
x_new = Test_Mat
x_new = data.frame(x_new)
names(x_new) = keep_names
x_df_new = as.h2o(x_new)

nn_predicted = h2o.predict(nn_model_training, x_df_new)



which immediately results in the error


*Error: java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set*



despite the fact that I renamed the columns to match the names of the x-variables in the training set.


What am I doing incorrectly when implementing the h2o.predict() ?","['r', 'deep-learning', 'h2o']",Max,https://stackoverflow.com/users/5904690/max,569
78986631,78986631,2024-09-15T04:36:28,2024-09-19 21:19:57Z,0,"I would like to switch the parsinp engine to h2o and use h2o and agua packages to fit models. The following code is from the standard help site.


With 
tune::tune_grid
, I bump into the 
""Warning: All models failed ...""
, which is due to ""
Error in
 
h2o.getConnection()
: 
No active connection to an H2O cluster ...""
. Although the output of 
h2o.getConnection()
 immediately before and after 
tune_grid
 suggest everything should be OK.


library(ggplot2)
library(tidymodels)
library(agua)
#> 
#> Attaching package: 'agua'
#> The following object is masked from 'package:workflowsets':
#> 
#>     rank_results
library(h2o)
#> 
#> ----------------------------------------------------------------------
#> 
#> Your next step is to start H2O:
#>     > h2o.init()
#> 
#> For H2O package documentation, ask for help:
#>     > ??h2o
#> 
#> After starting H2O, you can use the Web UI at http://localhost:54321
#> For more information visit https://docs.h2o.ai
#> 
#> ----------------------------------------------------------------------
#> 
#> Attaching package: 'h2o'
#> The following objects are masked from 'package:stats':
#> 
#>     cor, sd, var
#> The following objects are masked from 'package:base':
#> 
#>     %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,
#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,
#>     log10, log1p, log2, round, signif, trunc

doParallel::registerDoParallel()

h2o::h2o.init()
#>  Connection successful!
#> 
#> R is connected to the H2O cluster: 
#>     H2O cluster uptime:         50 minutes 51 seconds 
#>     H2O cluster timezone:       America/New_York 
#>     H2O data parsing timezone:  UTC 
#>     H2O cluster version:        3.46.0.5 
#>     H2O cluster version age:    16 days 
#>     H2O cluster name:           H2O_started_from_R_siava_dqu656 
#>     H2O cluster total nodes:    1 
#>     H2O cluster total memory:   13.28 GB 
#>     H2O cluster total cores:    32 
#>     H2O cluster allowed cores:  32 
#>     H2O cluster healthy:        TRUE 
#>     H2O Connection ip:          localhost 
#>     H2O Connection port:        54321 
#>     H2O Connection proxy:       NA 
#>     H2O Internal Security:      FALSE 
#>     R Version:                  R version 4.4.1 (2024-06-14 ucrt)
h2o::h2o.connect()
#>  Connection successful!
#> 
#> R is connected to the H2O cluster: 
#>     H2O cluster uptime:         50 minutes 54 seconds 
#>     H2O cluster timezone:       America/New_York 
#>     H2O data parsing timezone:  UTC 
#>     H2O cluster version:        3.46.0.5 
#>     H2O cluster version age:    16 days 
#>     H2O cluster name:           H2O_started_from_R_siava_dqu656 
#>     H2O cluster total nodes:    1 
#>     H2O cluster total memory:   13.28 GB 
#>     H2O cluster total cores:    32 
#>     H2O cluster allowed cores:  32 
#>     H2O cluster healthy:        TRUE 
#>     H2O Connection ip:          localhost 
#>     H2O Connection port:        54321 
#>     H2O Connection proxy:       NA 
#>     H2O Internal Security:      FALSE 
#>     R Version:                  R version 4.4.1 (2024-06-14 ucrt)
h2o::h2o.getConnection()
#> IP Address: localhost 
#> Port      : 54321 
#> Name      : NA 
#> Session ID: _sid_b5d8 
#> Key Count : 0

data(ames)

set.seed(4595)
data_split <- ames |>
  mutate(Sale_Price = log10(Sale_Price)) |>
  initial_split(strata = Sale_Price)
ames_train <- training(data_split)
ames_test <- testing(data_split)
cv_splits <- vfold_cv(ames_train, v = 10, strata = Sale_Price)

ames_rec <-
  recipe(Sale_Price ~ Gr_Liv_Area + Longitude + Latitude, data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
  step_ns(Longitude, deg_free = tune(""long df"")) |>
  step_ns(Latitude, deg_free = tune(""lat df""))

lm_mod <- linear_reg(penalty = tune()) |>
  set_engine(""h2o"")

lm_wflow <- workflow() |>
  add_model(lm_mod) |>
  add_recipe(ames_rec)

grid <- lm_wflow |>
  extract_parameter_set_dials() |>
  grid_regular(levels = 5)

h2o.getConnection()
#> IP Address: localhost 
#> Port      : 54321 
#> Name      : NA 
#> Session ID: _sid_b5d8 
#> Key Count : 0

ames_res <- tune_grid(
  lm_wflow,
  resamples = cv_splits,
  grid = grid,
  control = control_grid(save_pred = TRUE,
                         backend_options = agua_backend_options(parallelism = 5))
)
#> Warning: All models failed. Run `show_notes(.Last.tune.result)` for more
#> information.

h2o.getConnection()
#> IP Address: localhost 
#> Port      : 54321 
#> Name      : NA 
#> Session ID: _sid_b5d8 
#> Key Count : 0

agua::h2o_running(verbose = FALSE)
#> [1] TRUE

show_notes(.Last.tune.result)
#> unique notes:
#> ────────────────────────────────────────────────────────────────────────────────
#> Error in h2o.getConnection(): No active connection to an H2O cluster. Did you run `h2o.init()` ?
Created on 2024-09-15 with reprex v2.1.1



Latest RStudio is run with both normal and administrative privileges (same issue - although we should not need admin rights) on Windows 11 Pro x64 machine. JRE (1.8.0_421) and JDK (22) are installed, and all Java paths (
JRE_HOME
, 
JDK_HOME
, 
JAVA_HOME = JRE_HOME
) are all A-OK! I went so-far as to use rJava to manually initialize a Java VM instance with no effect. The command 
demo(h2o.kmeans)
 runs without a problem and produces expected results.


The only other thing of note is that running 
agua::h2o_start()
 produces weird output that it should not be. I get ""permission denied error"" on user folder which should be accessible even without admin rights (folder-access issue is the same with h2o package) and I get ""no Java error"" that shouldn't be there.


Warning message:
JAVA not found, H2O may take minutes trying to connect. 



Could this be the root of the issue: 
""h2o cluster connection is there as far as h2o package is concerned but agua (which adds supports for tidymodels) does not recognize Java and the existing h2o connection""
?!","['r', 'connection', 'h2o', 'hyperparameters', 'tidymodels']",Unknown,,N/A
78853756,78853756,2024-08-09T16:16:57,2024-08-12 15:02:37Z,0,"I ran a GLM model using h2o in r (the model was saved as an object called mod), the dataset contains categorical and continuous predictors. I called the h2o.varimp function on mod to retrieve the variable importance for this GLM model, for categorical predictors, it seemed to calculate the relative importance for each level within that same categorical predictor. Is there a way for me to aggregate the importance across all levels so that I get a single importance metric for each unique predictor?


I first tried summing the relative importance but since they are based on standardized coefficients, I'm not sure if this is the right way.","['r', 'h2o']",ZTraveler,https://stackoverflow.com/users/22456747/ztraveler,1
78716974,78716974,2024-07-07T09:57:06,2024-07-08 13:33:34Z,0,"When I am trying to run the function 
h2o.splitFrame
 from H2O, I receive the following error:


Error en .Call(R_curl_fetch_memory, enc2utf8(url), handle, nonblocking): 
  se ha alcanzado el límite de tiempo transcurrido



I tried to check the admin task to liberate the memory from my laptop, and tried other combinations but nothing worked. Find below my complete code.


# Establecer el tiempo de espera de CURL
options(RCurlOptions = list(timeout = 600))

library(h2o)
library(parallel)
library(doParallel)

# Detectar el número de núcleos disponibles
n_cores <- parallel::detectCores()
registerDoParallel(cores=4)

# Inicializar H2O con parámetros mejorados
h2o.init(
  ip = ""localhost"",
  nthreads = max(1, n_cores - 1),
  max_mem_size = ""6g""
)

# Deshabilitamos la salida de progreso
h2o.no_progress()

data <- as.h2o(ordata.ren)

splits <- h2o.splitFrame(
  data = data, 
  ratios = c(0.7, 0.15),  # partition data into 70%, 15%, 15% chunks
  destination_frames = c(""train"", ""valid"", ""test""),  # frame ID (not required)
  seed = 1  # setting a seed will guarantee reproducibility
)

# When I run this part of the code, I receive the error message mentioned before:
# Error en .Call(R_curl_fetch_memory, enc2utf8(url), handle, nonblocking): 
#   se ha alcanzado el límite de tiempo transcurrido.

train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]

y <- ""Price""
x <- setdiff(names(data), y)
print(x)","['r', 'h2o', 'h2o.ai']",user23479948,https://stackoverflow.com/users/23479948/user23479948,1
78449790,78449790,2024-05-08T15:58:11,2024-05-08 20:57:55Z,64,"I am running H2O AutoML on the same date set with the same seed on the same laptop multiple times and getting different predictions from time to time. I wonder if it is possible to get the same numbers on every run. Here is my code:


import h2o
h2o.init()
aml = h2o.automl.H2OAutoML(max_models = 25,
 balance_classes = False, seed = 1)
aml.train(predictors, response, training_frame = h2o_df_train)
aml.leader.predict(h2o_df_test)



It looks like it is possible for 
GBM
, but I wonder if it is possible for whole AutoML run?


My appologies for cross posting, but here is a complete example:


H2O forum


Problem solved


From 
H2O docs
:


""seed: Integer. Set a seed for reproducibility. AutoML can only guarantee reproducibility under certain conditions. H2O Deep Learning models are not reproducible by default for performance reasons, 
so if the user requires reproducibility, then exclude_algos must contain ""DeepLearning"".
 In addition max_models must be used because max_runtime_secs is resource limited, meaning that if the available compute resources are not the same between runs, AutoML may be able to train more models on one run vs another. Defaults to NULL/None.""","['h2o', 'automl', 'reproducible-research']",Unknown,,N/A
78444040,78444040,2024-05-07T16:58:00,2024-05-16 20:42:45Z,0,"I have a random forest model that I'm trying to understand better.


For the sake of the example, lets say we have a grove of blueberry bushes. What we're interested in is predicting the production of rotten blueberries on a specific bush, among harvest of all blueberries of the individual bushes.


Each bush has an identifying name: 
bush_name
, such as 
'bush001'
, and we want predictions based on each individual bush. For example, I want to know if bush025 produced a rotten berry on 2/2/22.


Inputs are in a df with the following dummy structure for the sake of this example:


train_data <- data.frame(date = c(""2022-01-01"", ""2022-01-07"", ""2022-02-09"", ""2022-05-01"", ""2022-11-01"", ""2022-11-02""),
                   bush_name = c(""bush001"", ""bush001"", ""bush001"", ""bush043"", ""bush043"", ""bush043""),
                   bugs = c(2, 0, 1, 0, 3, 1),
                   has_rotten_berry = c(1, 0, 0, 1, 1, 0),
                   berry_count = c(12, 1, 7, 100, 14, 4),
                   weather = c(1, 0, 2, 0, 1, 1))



I've got a random forest model that I have set up with the following high level set up:


library(agua)
library(parsnip)
library(h2o)

h2o.init(nthreads = -1)

model_fit <- rand_forest(mtry = 10,  trees = 100) %>%
  set_engine(""h2o"") %>%
  set_mode(""classification"") %>%
  fit(has_rotten_berry ~  .,
      data = train_data) %>% 
  step_dummy(bush_name) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())



I do get this message after training:


Warning message:
In .h2o.processResponseWarnings(res) :
  Dropping bad and constant columns: [bush_name].



What I want to know is:


When I try to then predict on new data in the trained model, it seems that I am only able to input new test data with the bush_names of bushes I already trained on. 
Am I correct in assuming this model is creating bush-specific predictions? And therefore would have to input new bush information in the training in order to output a future prediction for those new bushes?


Example: I plant a new bush, bush700, and it was not present in the original training data set. If I try to predict with the new bush data without it being present in the training data, is giving me a message that there are new levels in the data. So 
I'm assuming that because it seems these predictions are bush-specific, and we can't get any new bush predictions for newly-added bushes.


Is this correct to assume?","['r', 'machine-learning', 'classification', 'h2o', 'predict']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
78250475,78250475,2024-03-31T06:26:06,2024-03-31 20:30:59Z,0,"I am trying to use 
h2o.deeplearning
 model to predict on raster data. It returns me the following error




Error: Not compatible with requested type: [type=character; target=double].




Here is a minimal, reproducible, self-contained example


library(terra)
library(h2o)
library(tidyverse)

h2o.init()

# create a RasterStack or RasterBrick with with a set of predictor layers
logo <- rast(system.file(""external/rlogo.grd"", package=""raster""))
names(logo)

# known presence and absence points
p <- matrix(c(48, 48, 48, 53, 50, 46, 54, 70, 84, 85, 74, 84, 95, 85,
              66, 42, 26, 4, 19, 17, 7, 14, 26, 29, 39, 45, 51, 56, 46, 38, 31,
              22, 34, 60, 70, 73, 63, 46, 43, 28), ncol=2)
a <- matrix(c(22, 33, 64, 85, 92, 94, 59, 27, 30, 64, 60, 33, 31, 9,
              99, 67, 15, 5, 4, 30, 8, 37, 42, 27, 19, 69, 60, 73, 3, 5, 21,
              37, 52, 70, 74, 9, 13, 4, 17, 47), ncol=2)

# extract values for points
xy <- rbind(cbind(1, p), cbind(0, a))
v <- data.frame(cbind(pa=xy[,1], terra::extract(logo, xy[,2:3]))) %>% 
  mutate(pa = as.factor(pa))

str(v) 

#### Import data to H2O cluster
df <- as.h2o(v)

#### Split data into train, validation and test dataset
splits <- h2o.splitFrame(df, c(0.70,0.15), seed=1234)
train  <- h2o.assign(splits[[1]], ""train.hex"")
valid  <- h2o.assign(splits[[2]], ""valid.hex"")
test   <- h2o.assign(splits[[3]], ""test.hex"")

#### Create response and features data sets
y <- ""pa""
x <- setdiff(names(train), y)

### Deep Learning Model
dl_model <- h2o.deeplearning(training_frame=train,                      
  validation_frame=valid,                    
  x=x,                                       
  y=y,                                      
  standardize=TRUE,                          
  seed=125)

dnn_pred <- function(model, data, ...) {
  predict(model, newdata=as.h2o(data), ...)
}

p <- predict(logo, model=dl_model, fun=dnn_pred)
plot(p)","['r', 'h2o', 'predict', 'terra']",UseR10085,https://stackoverflow.com/users/6123824/user10085,"8,062"
78245999,78245999,2024-03-29T21:21:16,2024-03-30 15:28:57Z,91,"I am getting an error for my desired dataset when trying to use an isolation forest method to detect anomalies. However I have another completely different dataset that it works fine for, what could cause this issue?


isolationforest Model Build progress: | (failed) | 0% Traceback (most recent call last): File 
""h2o_test.py"", line 149, in <module> isoforest.train(x=iso_forest.col_names[0:65], 
training_frame=iso_forest) File ""/home/ec2-user/.local/lib/python3.7/site- 
packages/h2o/estimators/estimator_base.py"", line 107, in train self._train(parms, 
verbose=verbose) File ""/home/ec2-user/.local/lib/python3.7/site- 
packages/h2o/estimators/estimator_base.py"", line 199, in _train 
job.poll(poll_updates=self._print_model_scoring_history if verbose else None) File 
""/home/ec2-user/.local/lib/python3.7/site-packages/h2o/job.py"", line 89, in poll 
""\n{}"".format(self.job_key, self.exception, self.job[""stacktrace""])) OSError: Job with key 
$03017f00000132d4ffffffff$_92ee3e892f7bc86460e80153eaec4b70 failed with an exception: 

java.lang.AssertionError stacktrace: java.lang.AssertionError at 
hex.tree.DHistogram.init(DHistogram.java:350) at 
hex.tree.DHistogram.init(DHistogram.java:343) at 
hex.tree.ScoreBuildHistogram2$ComputeHistoThread.computeChunk(ScoreBuildHistogram2.java:427) 
at hex.tree.ScoreBuildHistogram2$ComputeHistoThread.map(ScoreBuildHistogram2.java:408) at 
water.LocalMR.compute2(LocalMR.java:89) at water.LocalMR.compute2(LocalMR.java:81) at 
water.H2O$H2OCountedCompleter.compute(H2O.java:1704) at 
jsr166y.CountedCompleter.exec(CountedCompleter.java:468) at 
jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263) at 
jsr166y.ForkJoinPool$WorkQueue.popAndExecAll(ForkJoinPool.java:906) at 
jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:979) at 
jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at 
jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



with open('/home/webapp/flask-api/tmp_rows/temp_file2.csv', 'w+') as tmp_file:
        temp_name = ""/tmp_rows/temp_file2.csv""
        tmp_file.write(text_stream.getvalue())
        tmp_file.close()

h2o.init()
print(""TEMP_nAME"", temp_name)
iso_forest = h2o.import_file('/home/webapp/flask-api/{0}'.format(temp_name))
seed = 12345
ntrees = 100
isoforest = h2o.estimators.H2OIsolationForestEstimator(
ntrees=ntrees, seed=seed)
isoforest.train(x=iso_forest.col_names[0:65], training_frame=iso_forest)
predictions = isoforest.predict(iso_forest)
print(predictions)
h2o.cluster().shutdown()




The CSV is being created fine, so there doesn't seem to be an issue with that, what is causing this Java error? I even increased the size of my ec2 to have more RAM, that didn't solve it either.","['python', 'java', 'h2o']",Amon,https://stackoverflow.com/users/2621316/amon,"2,931"
78193739,78193739,2024-03-20T13:32:59,2024-03-20 13:32:59Z,34,"everyone!


I have a list of discounts, and for each discount, I iterate through them. Within each iteration, I generate a few new columns and then predict the sales amount for each product associated with that discount. I aim to compile all the results from these iterations into a single dataframe to have a comprehensive view of every discount on the list along with the predictions. This process has already been successfully implemented in native H2O using Pandas. However, I am now attempting to replicate it in Spark H2O and PySpark.


Unfortunately, when attempting to read the dataframe, I encounter the following error:


RestApiCommunicationException: H2O node http://10.159.20.11:54321 responded with org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 256.0 failed 4 times, most recent failure: Lost task 0.3 in stage 256.0 (TID 3451) (10.159.20.11 executor 0): ai.h2o.sparkling.backend.exceptions.RestApiCommunicationException: H2O node http://10.159.20.11:54321 responded with Status code: 400 : Bad Request



I am seeking a workaround or advice on whether I might be making a mistake.


Expected Behavior: I should be able to access and manipulate the dataframe without issues.


Observed Behavior: The error ""RestApiCommunicationException: H2O node 
http://10.159.20.11:54321/
 responded with"" prevents further progress.


What I am doing now is saving the predictions to a delta table and then access it, but that takes a little more time than what I expected. I know that loops are not very recommended for PySpark but I am also not seeing a better way of doing this.


Thank you so much!","['apache-spark', 'pyspark', 'databricks', 'h2o', 'sparkling-water']",Tiago,https://stackoverflow.com/users/20954952/tiago,65
78183646,78183646,2024-03-19T00:17:40,2024-03-25 17:42:30Z,58,"My code loads the h2o MOJO model to get prediction on a small dataset. However, h2o shutdown abruptly by itself. The same code is working fine on one machine with the same set of inputs but seeing abnormal h2o shutdowns on other machine.


self.test = h2o.import_file(dataset_file)
preds = imported_model.predict(self.test)



I am running this on 1TB machine with 72 cores. I can't believe this is memory issues. The most puzzling the fact is that the same code is working on other machine with the same inputs (configured differently). I don't know the full list of differences.  I was previously running with python frozen binary build and couldn't see error messages in detail. I am running python code directly and can see error messages in more detail.


 File ""h2o_model_eval.py"", line 160, in ModelEval
    preds = imported_model.predict(self.test)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.11/site-packages/h2o/model/model_base.py"", line 334, in predict
    j.poll()
  File "".venv/lib/python3.11/site-packages/h2o/job.py"", line 71, in poll
    pb.execute(self._refresh_job_status)
  File "".venv/lib/python3.11/site-packages/h2o/utils/progressbar.py"", line 187, in execute
    res = progress_fn()  # may raise StopIteration
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.11/site-packages/h2o/job.py"", line 136, in _refresh_job_status
    jobs = self._query_job_status_safe()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.11/site-packages/h2o/job.py"", line 132, in _query_job_status_safe
    raise last_err
  File "".venv/lib/python3.11/site-packages/h2o/job.py"", line 114, in _query_job_status_safe
    result = h2o.api(""GET /3/Jobs/%s"" % self.job_key)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.11/site-packages/h2o/h2o.py"", line 123, in api
    return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.11/site-packages/h2o/backend/connection.py"", line 507, in request
    raise H2OConnectionError(""Unexpected HTTP error: %s"" % e)
h2o.exceptions.H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Jobs/$03017f00000132d4ffffffff$_acab67512114e05db6ec9865ea9849d3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x2aab3f59ea90>: Failed to establish a new connection: [Errno 111] Connection refused'))
~                                                                               



How to debug this issue?","['python', 'h2o', 'h2o.ai']",Unknown,,N/A
78180809,78180809,2024-03-18T14:10:31,2024-03-18 16:55:16Z,24,"I'm running PCA in h2o (R version) and was wondering whether it's possible to specify/apply a rotation (like oblimin or promax).  I'm looking for the rotated loadings, and the reason I'm using h2o instead of other common packages for that (like ""psych"") is that my data set is huge (100000 columns) so I need to take advantage of h2o's nice parallel computing in Windows. The code I'm using currently is:


library(h2o)

h2o.init(nthreads=64)

x <- read.csv(""file_with_100000_columns.csv"")

for (i in 1:ncol(x)) {x[,i] <- as.factor(x[,i])}

x <- as.h2o(x)

mod <- h2o.prcomp(training_frame=x,k=5,use_all_factor_levels=TRUE)



Thanks!","['machine-learning', 'parallel-processing', 'pca', 'h2o', 'h2o.ai']",Tyler Moore,https://stackoverflow.com/users/10504648/tyler-moore,51
78110316,78110316,2024-03-05T20:10:23,2024-03-05 20:12:04Z,38,"I'd like to deepcopy a fitted h2o.sklearn.H2OAutoMLRegressor object. I can deepcopy prior to calling fit but not after.


Using H2O version 3.44.0.3, the following results in an error for the deepcopy of the fitted model.


import copy
import numpy as np
import h2o
from h2o.sklearn import H2OAutoMLRegressor

print(f""H2O version: {h2o.__version__}"")

X = np.random.rand(1000)
y = np.random.rand(1000)

m = H2OAutoMLRegressor(max_models=5, max_runtime_secs_per_model=30);

print(copy.deepcopy(m))

m.fit(X,y)

print(copy.deepcopy(m))



I cannot post the full trace because of StackOverflow code limits, but it looks like:


---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[7], line 17
     13 print(copy.deepcopy(m))
     15 m.fit(X,y)
---> 17 print(copy.deepcopy(m))

File ~/.pyenv/versions/3.11.7/lib/python3.11/copy.py:172, in deepcopy(x, memo, _nil)
    170                 y = x
    171             else:
--> 172                 y = _reconstruct(x, memo, *rv)
    174 # If is its own copy, don't memoize.
    175 if y is not x:

File ~/.pyenv/versions/3.11.7/lib/python3.11/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    269 if state is not None:
    270     if deep:
--> 271         state = deepcopy(state, memo)
    272     if hasattr(y, '__setstate__'):
    273         y.__setstate__(state)

...

File ~/.pyenv/versions/3.11.7/lib/python3.11/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)
    229 memo[id(x)] = y
    230 for key, value in x.items():
--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)
    232 return y

File ~/.pyenv/versions/3.11.7/lib/python3.11/copy.py:172, in deepcopy(x, memo, _nil)
    170                 y = x
    171             else:
--> 172                 y = _reconstruct(x, memo, *rv)
    174 # If is its own copy, don't memoize.
    175 if y is not x:

File ~/.pyenv/versions/3.11.7/lib/python3.11/copy.py:265, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    263 if deep and args:
    264     args = (deepcopy(arg, memo) for arg in args)
--> 265 y = func(*args)
    266 if deep:
    267     memo[id(x)] = y

File ~/.pyenv/versions/3.11.7/lib/python3.11/copyreg.py:105, in __newobj__(cls, *args)
    104 def __newobj__(cls, *args):
--> 105     return cls.__new__(cls, *args)

TypeError: H2OResponse.__new__() missing 1 required positional argument: 'keyvals'",['h2o'],Unknown,,N/A
77976819,77976819,2024-02-11T12:47:46,2024-02-14 12:56:50Z,0,"I am trying to train two GBM models, the first one takes the frequency as a response variable and the second takes number of claims as a response and exposure as on offset column, however, I did not see any difference between the two best models when I make hyperparameters tuning.
I get the same RMSE.



DF=data[-extreme_ind, ] 
DF[,c(4:60)]<- lapply(DF[,c(4:60)], factor)


df=as.h2o(DF)
splits <- h2o.splitFrame(df, 0.8, seed=1234)  
train <- h2o.assign(splits[[1]], ""train.hex"")  
valid <- h2o.assign(splits[[2]], ""valid.hex"") 

MOD_1_v2 <- h2o.gbm(x=c(4:56, 58:60),y = 61, training_frame = train, validation_frame =valid, ntrees=200) #100
summary(MOD_1_v2)

plot(MOD_1_v2,timestep=""number_of_trees"",metric=""RMSE"") 





gbm1_parameters <- list(learn_rate = c(0.01,0.05, 0.1),
                        max_depth = c(3, 5, 6),
                        sample_rate = c(0.7, 0.75, 0.8),  
                        col_sample_rate = c(0.2, 0.5, 1.0))



gbm1_grid <- h2o.grid(""gbm"", x = c(4:56, 58:60), y = 61,
                      grid_id = ""gbm_grid"",
                      training_frame = train,
                      validation_frame = valid,  
                      ntrees=20, #30
                      seed = 1,
                      hyper_params = gbm1_parameters)



gbm1_gridp<- h2o.getGrid(grid_id = ""gbm_grid"",
                         sort_by = ""rmse"",
                         decreasing  = FALSE)
print(gbm1_gridp)


best_MOD_1=h2o.getModel(gbm1_gridp@model_ids[[1]])

summary(best_MOD_1)




best_gbm_perf1 <- h2o.performance(model = best_MOD_1,newdata = valid)
best_gbm_perf1



plot(best_MOD_1,timestep=""number_of_trees"",metric=""rmse"")
h2o.varimp_plot(best_MOD_1)



MOD_2_v2 <- h2o.gbm(x=c(4:56, 58:60),y = 2,offset_column=""APVI"", training_frame = train, validation_frame = valid,ntrees=55) 

summary(MOD_2_v2) #apres supp outliers 

plot(MOD_2_v2,timestep=""number_of_trees"",metric=""RMSE"")


gbm2_parameters <- list(learn_rate = c(0.01,0.05, 0.1),
                        max_depth = c(3, 5),
                        sample_rate = c(0.7, 0.75, 0.8),  
                        col_sample_rate = c(0.2, 0.5, 1.0))




gbm2_grid <- h2o.grid(""gbm"", x = c(4:56, 58:60), y = 2,
                      grid_id = ""gbm_grid"",
                      training_frame = train,
                      validation_frame = valid, 
                      ntrees=55, #10
                      seed = 123,
                      hyper_params = gbm2_parameters)


gbm2_gridp<- h2o.getGrid(grid_id = ""gbm_grid"",
                         sort_by = ""rmse"",
                         decreasing  = FALSE)
print(gbm2_gridp)



best_MOD_2=h2o.getModel(gbm2_gridp@model_ids[[1]])
summary(best_MOD_2)


best_gbm_perf2 <- h2o.performance(model = best_MOD_2,newdata = valid)
best_gbm_perf2



How Can I fix this problem ?","['r', 'offset', 'h2o', 'hyperparameters', 'gbm']",wej,https://stackoverflow.com/users/14061266/wej,21
77973356,77973356,2024-02-10T14:04:40,2024-02-13 09:03:02Z,0,"I'm using 
h2o
 in R and RStudio, and 
h2o
 is working fine. However, when I try to use the 
automl()
 function, the process starts, RStudio shows progress bars, but after the progress bar reaches 100% no results are returned. The R session and the process just continues running and the R session remains busy. Using the exact same code with 
randomForest()
 works fine.


Training frame:


h2o.init()
peng <- as.h2o(penguins)



This doesn't worK:


aml <- h2o.automl(y = c(""body_mass_g""), training_frame = peng, max_runtime_secs = 30)



however, this does:


rf <- h2o.randomForest(y = c(""body_mass_g""), training_frame = peng, max_runtime_secs = 30)","['r', 'h2o', 'automl']",L Tyrone,https://stackoverflow.com/users/2530121/l-tyrone,"6,599"
77908088,77908088,2024-01-30T16:54:19,2024-01-31 08:44:31Z,0,"Im trying to upload a csv file to a 
shiny
 app as a 
h2o
 object based on this 
process
 but I get 
Warning: Error in .key.validate: 
key
 must match the regular expression '^[a-zA-Z_][a-zA-Z0-9_.]*$': 0_sid_b018_4
. Im not sure if there is a problem with path reading or the table


#install h2o first
if (""package:h2o"" %in% search()) { detach(""package:h2o"", unload=TRUE) }
if (""h2o"" %in% rownames(installed.packages())) { remove.packages(""h2o"") }


if (! (""methods"" %in% rownames(installed.packages()))) { install.packages(""methods"") }
if (! (""statmod"" %in% rownames(installed.packages()))) { install.packages(""statmod"") }
if (! (""stats"" %in% rownames(installed.packages()))) { install.packages(""stats"") }
if (! (""graphics"" %in% rownames(installed.packages()))) { install.packages(""graphics"") }
if (! (""RCurl"" %in% rownames(installed.packages()))) { install.packages(""RCurl"") }
if (! (""jsonlite"" %in% rownames(installed.packages()))) { install.packages(""jsonlite"") }
if (! (""tools"" %in% rownames(installed.packages()))) { install.packages(""tools"") }
if (! (""utils"" %in% rownames(installed.packages()))) { install.packages(""utils"") }


install.packages(""h2o"", type = ""source"", repos = (c(""http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R"")))
library(h2o)
h2o.init()

#app
library(shiny)
library(h2o)
h2o.init()

# Define UI for data upload app ----
ui <- fluidPage(
  
  # App title ----
  titlePanel(""Uploading Files""),
  
  # Sidebar layout with input and output definitions ----
  sidebarLayout(
    
    # Sidebar panel for inputs ----
    sidebarPanel(
      # Input: Select a file ----
      fileInput(""file1"", ""Choose CSV File"",
                multiple = FALSE,
                accept = c(""text/csv"",
                           ""text/comma-separated-values,text/plain"","".xlsx"",
                           "".csv"")),
      
      # Horizontal line ----
      tags$hr(),
      
      # Input: Checkbox if file has header ----
      checkboxInput(""header"", ""Header"", TRUE),
      
      # Input: Select separator ----
      radioButtons(""sep"", ""Separator"",
                   choices = c(Comma = "","",
                               Semicolon = "";"",
                               Tab = ""\t""),
                   selected = "",""),
      
      # Input: Select quotes ----
      radioButtons(""quote"", ""Quote"",
                   choices = c(None = """",
                               ""Double Quote"" = '""',
                               ""Single Quote"" = ""'""),
                   selected = '""'),
      
      # Horizontal line ----
      tags$hr(),
      
      # Input: Select number of rows to display ----
      radioButtons(""disp"", ""Display"",
                   choices = c(Head = ""head"",
                               All = ""all""),
                   selected = ""head"")
      
    ),
    
    # Main panel for displaying outputs ----
    mainPanel(
      
      # Output: Data file ----
      tableOutput(""contents"")
      
    )
    
  )
)

# Define server logic to read selected file ----
server <- function(input, output) {
  file_info <- reactive({
    # input$file1 will be NULL initially. After the user selects
    # and uploads a file, head of that data file by default,
    # or all rows if selected, will be shown.
    
    req(input$file1)
   
      # when reading semicolon separated files,
      # having a comma separator causes `read.csv` to error
      tryCatch(
        {
          path <- input$file1$datapath
          data <- h2o.uploadFile(path = path)
          df <- read.csv(data,
                         header = input$header,
                         sep = input$sep,
                         quote = input$quote)
        },
        error = function(e) {
          # return a safeError if a parsing error occurs
          stop(safeError(e))
        }
      )
      
      if(input$disp == ""head"") {
        return(head(df))
      }
      else {
        return(df)
      }
    
    
  })
  output$contents <- renderTable({
    
    file_info()
  })
  
}

# Create Shiny app ----
shinyApp(ui, server)","['r', 'shiny', 'h2o']",firmo23,https://stackoverflow.com/users/9198260/firmo23,"8,384"
77874825,77874825,2024-01-24T16:50:31,2024-01-24 18:02:09Z,91,"I am working on a hybrid model that uses CNN (time series input) and H2oRandom forest (tabular data input) combined at the fully connected layer to solve regression problems. I would like to optimize the hyperparameters of CNN and RF. The only way I figured out is to optimize two models separately and combine them at the FC layer to obtain the output. I use RandomSeach Keras Tuner for CNN and Grid RandomSearch for H2O Random Forest hyperparameters.


I am uncertain if optimizing the models separately is the most effective way to enhance the model's performance. Is there a different approach to optimizing hybrid models? Please let me know if there is a better approach.","['optimization', 'deep-learning', 'conv-neural-network', 'random-forest', 'h2o']",Geerthy Thambiraj,https://stackoverflow.com/users/11698173/geerthy-thambiraj,21
77849328,77849328,2024-01-19T23:37:26,2024-01-22 23:55:53Z,13,"I'm interested in using h2o's suite of algorithms to perform some analysis on data, specifically with ordinal logistic regression. I was looking through the available GLMBooklet.pdf and on page 21 of the version available it references a loss function that actually doesn't appear; does anyone know where I can get the details of the loss function, or an updated version of GLMBooklet.pdf with the loss function present? Thanks!


I haven't been able to find the loss function anywhere, but maybe I'm not looking in the right places?",['h2o'],Rich,https://stackoverflow.com/users/23271364/rich,1
77838561,77838561,2024-01-18T10:15:28,2024-01-18 18:16:22Z,86,"I have been getting error in pyspark while running h2o model prediction.


file ""/usr/spark/python/pyspark/cloudpickle.py"", line 562, in subimport
ModuleNotFoundError; No Modele named h2o


i created pandas udf


`def predict_h2o_model(*cols)
    x=pd.concat(cols,axis=1)
    h2odataframe=h2o.H2OFrame(x)
    scores=model.predict(h2odataframe)
    return pd.series(scores)`



I' scoring using pyspark dataframe


`df_scores=sparf_df.select(F.col(""cust_id""),predict_h2o_model(*cols).alias('model_score'))`



I was expecting h2o model scores in spark_df dataframe","['pyspark', 'h2o']",Nithin Reddy,https://stackoverflow.com/users/22314796/nithin-reddy,1
77774644,77774644,2024-01-07T20:01:26,2024-01-07 20:01:26Z,174,"When I run h2o AutoML in python on my MacOS with Apple M1 chip, I get a message ""
XGBoost is not available; skipping it.
"". I found a 
similar issue
 from more than a year ago. So I am wondering if there has been any progress (and therefore my issue has different causes), or XGBoost has still not been compiled for Apple M1.


Python: 3.9.2

h2o: 3.44.0.2

processor: Apple M1","['xgboost', 'apple-m1', 'h2o']",sofi,https://stackoverflow.com/users/23209807/sofi,11
77679172,77679172,2023-12-18T12:44:49,2023-12-18 16:03:49Z,104,"I am trying to upgrade a binary (DRF) H2O model to a higher version (from v3.28.1.2 to v3.42.0.3). Due to tecnical restrictions I cannot use the MOJO format for deployment. Is it possible to do the following instead?




load the binary model in the older version


export the model as MOJO format


load the model in MOJO format in the newer version


save the model in Binary in the newer version




I am able to save the model in the newer version. However, when I try to load the model object (using h2o.load_model()), I get the following error:



H2OServerError: HTTP 500 Server Error:
Server error java.lang.NullPointerException:
  Error: Caught exception: java.lang.NullPointerException
  Request: None
  Stacktrace: java.lang.NullPointerException
      hex.generic.GenericModel$GenModelSource.backingByteVec(GenericModel.java:391)
      hex.generic.GenericModel$GenModelSource.get(GenericModel.java:373)
      hex.generic.GenericModel.genModel(GenericModel.java:325)
      hex.generic.GenericModel.havePojo(GenericModel.java:546)
      water.api.schemas3.ModelSchemaV3.fillFromImpl(ModelSchemaV3.java:80)
      water.api.schemas3.ModelSchemaV3.fillFromImpl(ModelSchemaV3.java:22)
      water.api.ModelsHandler.importModel(ModelsHandler.java:263)
      sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
      sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)","['binary', 'version', 'h2o', 'mojo']",Unknown,,N/A
77549868,77549868,2023-11-25T23:03:26,2023-11-27 21:01:59Z,96,"I am using h2o package in python to build a fairly complex model.
It has around 1500 features, but I know that most of them are not really important and I would like to extract the subset of a given size (let's say 100) that maximizes the R squared of my model.

Is there some method that is already implementing this for h2o in python?


Otherwise I would need to code it myself, but that also implies to run the model multiple times and I am not sure i would code it in the correct way.


One possible way to code it is this one:




Save the R2 for the model, then remove the k less important features


Create a second model without the removed features


Calculate the R2 for the new model and compare to the previous R2. Use a metric to decide whether to keep the new model or stick with the old.


Iterate these steps until the previous step chooses the old model as the best one
I am pretty sure this will not give me the 'best subset' of feature but I really hope it would be sufficient.




The second method I thought of is the following:




set the number of feature 
N
 you want in the new model and the number of iterations 
K


Save the original model R2 as reference


Extract 
N
 features at random from the original model, using their relative importance as probability of being extracted (more important features more likely extracted)


For each model save the list of features and the new R2


After iterating 
K
 times stop the algorithm and compare the R2


Choose the set of features with the closest R2 to the original one","['python', 'machine-learning', 'random-forest', 'h2o', 'feature-selection']",Mirko,https://stackoverflow.com/users/7214340/mirko,233
77498397,77498397,2023-11-16T22:38:15,2023-11-17 14:07:48Z,78,"I am working in Databricks with Sparkling Water 3.40.0.4; I have a total driver memory of 512 GB and six workers with 64 GB each. When I call


hc = H2OContext.getOrCreate()



the internal H2O cluster is created across six workers, but the total cluster memory size is roughly 60 GB. I can create a normal, non-sparkling water H2O cluster and pass the max_mem_size and min_mem_size arguments to the init() method, which will return a much larger sized cluster, but I can't seem to find how to do that on Databricks.


h2o.init(max_mem_size=""200g"")



That returns roughly 200 GB of memory for the cluster.


I created a local Spark installation and changed the spark.driver.memory property, and changing this resulted in a larger sparkling water cluster size, but explicitly setting that property in Data Bricks has no change on the sparkling water cluster there.


Is there a configuration I can pass to Spark or an internal H2O cluster to set a larger memory size on Databricks?","['python', 'apache-spark', 'databricks', 'h2o', 'sparkling-water']",Alex Ott,https://stackoverflow.com/users/18627/alex-ott,86.8k
77475890,77475890,2023-11-13T17:57:13,2023-11-15 13:39:15Z,43,"I built an 
H2ORandomForestEstimator
 model using 
train()
 method on my spark dataframe with the target column containing values 0 or 1. I downloaded and printed its mojo files using 
model.download_mojo(MOJO_ZIP_PATH)
 and 
h2o.print_mojo(MOJO_ZIP_PATH, tree_index=tree_ind)
 functions respectively. A partial such output tree is shown below.


As can be seen, leaf nodes have a field named 
predValue
 containing a value between 0 and 1. What is the meaning of this 
predValue
 field? Does it mean that the target variable is likely to contain the value contained in 
predValue
 field if the input variables happen to meet this root to leaf path when 
predict()
 is called on them?


Moreover, I want to preprocess the output model of 
H2ORandomForestEstimator
 and filter only those rules (root to leaf paths) for which my model will predict 1. Is there a way to filter such rules by parsing the mojo files without actually running the 
predict()
 function on input variables? 
predValue
 field in the output mojo files looked promising to solve this problem but I could not figure out its co-relation with the output variable. Can it be used to figure out the top-N rules?


'trees': [{
    'root': {
        'nodeNumber': 0,
        'weight': 18319.0,
        'colId': 169,
        'colName': 'pkg_items_gl_product_group_desc_1.gl_electronics',
        'leftward': True,
        'isCategorical': False,
        'inclusiveNa': True,
        'splitValue': 0.5,
        'rightChild': {
            'nodeNumber': 25,
            'weight': 462.0,
            'predValue': 0.9935065
        },
        'leftChild': {
            'nodeNumber': 1,
            'weight': 17857.0,
            'colId': 0,
            'colName': 'pkg_attr_total_pkg_price',
            'leftward': True,
            'isCategorical': False,
            'inclusiveNa': True,
            'splitValue': 186.52805,
            'rightChild': {
                'nodeNumber': 26,
                'weight': 201.0,
                'predValue': 0.9900498
            },
            'leftChild': {
                'nodeNumber': 3,
                'weight': 13184.0,
                'colId': 149,
                'colName': 'pkg_items_gl_product_group_desc_1.gl_automotive',
                'leftward': True,
                'isCategorical': False,
                'inclusiveNa': True,
                'splitValue': 0.5,
                'rightChild': {
                    'nodeNumber': 27,
                    'weight': 312.0,
                    'predValue': 0.99038464
                },","['machine-learning', 'pyspark', 'random-forest', 'decision-tree', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
77208586,77208586,2023-09-30T21:59:06,2023-10-01 11:53:51Z,0,"I am getting an error. It occurs when I use the deep learning function in h2o in R. My response is a categorical variable which takes on 3 values so I can't change it to binary labels.




Error: java.lang.IllegalArgumentException: Actual column must contain binary class labels, but found cardinality 3!




This is my input:


h2o.init()
dat_h20 = data.frame(Event=as.factor(space_data$Event), TrajA= space_data$TrajA, AcousticA = space_data$AcousticA, HullScan= as.factor(space_data$HullScan), MCStatus = as.factor(space_data$MCStatus))
set.seed(2023)

set = sample(1:150, 150 , replace = FALSE)
data_train = as.h2o(dat_h20[set,])
head(data_train)
data_val = as.h2o(dat_h20[-set,])

value = exp(seq(-10,-3, length = 20))
value
validation_errors = numeric(20) # validation error for each regularisation parameter
?h2o.deeplearning
dat_h20[1]
for (i in 1:length(value))
{
model = h2o.deeplearning(x = 2:5, y = 1 ,
                         training_frame = data_train, 
                         validation_frame = data_val,
                         standardize = TRUE, 
                         hidden = c(5,5), 
                         activation = 'Rectifier', 
                         distribution = 'multinomial',
                         loss = 'CrossEntropy',
                         l2 = value[i],
                         rate = 0.01,
                         adaptive_rate = FALSE,
                         epochs = 1000,
                         reproducible = TRUE,
                         seed = 2,
                         )
validation_errors[i]= h2o.logloss(model, train = TRUE, valid = TRUE)
}

plot(value, validation_errors)","['r', 'deep-learning', 'h2o']",jonrsharpe,https://stackoverflow.com/users/3001761/jonrsharpe,121k
77186876,77186876,2023-09-27T11:01:32,2023-09-27 16:49:27Z,0,"I can't seem to get h2o running in R.

I can easily import h2o using 
library(h2o)
 which displays


----------------------------------------------------------------------

Your next step is to start H2O:
    > h2o.init()

For H2O package documentation, ask for help:
    > ??h2o

After starting H2O, you can use the Web UI at http://localhost:54321
For more information visit https://docs.h2o.ai

----------------------------------------------------------------------


Attaching package: ‘h2o’

The following objects are masked from ‘package:stats’:

    cor, sd, var

The following objects are masked from ‘package:base’:

    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames, colnames<-,
    ifelse, is.character, is.factor, is.numeric, log, log10, log1p, log2,
    round, signif, trunc

Warning message:
package ‘h2o’ was built under R version 4.1.3 



But the issue is whenever I run 
h2o.init()
 to start the h2o cluster, I get the following error message


H2O is not running yet, starting it now...
<simpleError in system2(command, ""-version"", stdout = TRUE, stderr = TRUE): '""C:\Program Files\Java\jdk-20\bin\bin\java.exe""' not found>
Error in value[[3L]](cond) : 
  You have a 32-bit version of Java. H2O works best with 64-bit Java.
Please download the latest Java SE JDK from the following URL:
https://www.oracle.com/technetwork/java/javase/downloads/index.html
In addition: Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :
  path[1]=""C:\Program Files\Java\jdk-20\bin/bin/java.exe"": The system cannot find the path specified



My laptop is HP 64-bit and the Java JDK I downloaded and installed is jdk-20_windows-x64_bin (which is 64-bit).

I have also set the required Java Path in Environment Variables.


Below is the Java version installed


Microsoft Windows [Version 10.0.19044.1889]
(c) Microsoft Corporation. All rights reserved.

C:\Users\user>java -version
java version ""20.0.2"" 2023-07-18
Java(TM) SE Runtime Environment (build 20.0.2+9-78)
Java HotSpot(TM) 64-Bit Server VM (build 20.0.2+9-78, mixed mode, sharing)



I don't understand why h2o cannot detect my 64-bit Java already installed and work with it.

Did I download/install the wrong Java JDK file?


My R version is 4.1.0


Below are images of my JAVA_HOME variable and Path variable","['java', 'r', 'h2o', 'h2o.ai']",Unknown,,N/A
77184526,77184526,2023-09-27T03:26:48,2023-10-20 20:03:43Z,54,"Having had AutoML() recommend a model after I experimented with a single column of data and needed to identify a target as a step-1 of the single data to train the successful model I now want to use the model on test (live) data. The model however wants a 2 column input and when I use a padded second column ""target"" = [0] then the model returns the same predictive output and I am guessing it is because of the second null zero entry column. Is there a way around this issue please ?


I have tried using the model with just one column and it errored and I have added the second padding column all zeros and while the code worked the prediction is always the same with differing inputs",['h2o'],marc_s,https://stackoverflow.com/users/13302/marc-s,752k
77064065,77064065,2023-09-08T04:01:47,2024-03-23 05:09:27Z,0,"I'm very new to generative AI. I have 64gb RAM and 20GB GPU. I used some opensource model from Huggingface and used Python to simply prompt it with out of box model and displaying the result. I downloaded the model to local using 
save_pretrained
 and trying to load the model from local there after. It works. But everytime I run the python file it takes more than 10 mins to display the results.


There is a step 
Loading checkpoint shards
 that takes 6-7 mins everytime. Am I doing anything wrong? why it has to load something everytime even though the model is refered from local.


I tried using 
local_files_only=True, cache_dir=cache_dir, low_cpu_mem_usage=True, max_shard_size=""200MB""
 , none solved the time issue .


How to prompt the saved model directly without so much delay as user usable. Any help would be highly appreciated","['huggingface-transformers', 'h2o', 'huggingface', 'huggingface-tokenizers', 'llama']",Khaleel,https://stackoverflow.com/users/3867290/khaleel,"1,372"
77016904,77016904,2023-08-31T14:43:01,2023-09-06 14:39:48Z,173,"The 
documentation 
 for distributed random forest in h2o states that, for multiclass problems, ""a tree is used to estimate the probability of each class separately"". I can see this in visualising the trees that each class indeed seems to have a completely independent ""one-vs-rest"" tree.


I am wondering how the scores from these trees are combined into the final score vector - are they just normalized to sum to one?


I would also like to understand why this approach was chosen and how it compares to the usual approach of handling multiple classes within a single tree. For individual classes we see that the performance of the multiclass classifier is typically worse than a dedicated one-vs-rest classifier with the same hyperparameters, even though under the hood the multiclass classifier should be very similar.","['random-forest', 'h2o', 'multiclass-classification']",nickc,https://stackoverflow.com/users/22442035/nickc,11
76950243,76950243,2023-08-22T05:09:39,2023-08-23 03:33:35Z,47,"I tried to build H2O open source, nomatter stable version or clone version, build process via 
./gradlew build
 are failed due to ""error: cannot find symbol"", the detailed error info as below:


Updated build log:


here is the full log from output for the latest rel-3.42.0 branch:

(python) wayahead@ubox:~/workspace/h2o/github$ git clone --branch rel-3.42.0 https://github.com/h2oai/h2o-3.git
Cloning into 'h2o-3'...
remote: Enumerating objects: 428884, done.
remote: Counting objects: 100% (6618/6618), done.
remote: Compressing objects: 100% (3895/3895), done.
remote: Total 428884 (delta 3654), reused 5264 (delta 2604), pack-reused 422266
Receiving objects: 100% (428884/428884), 604.35 MiB | 9.36 MiB/s, done.
Resolving deltas: 100% (263828/263828), done.
(python) wayahead@ubox:~/workspace/h2o/github$ javac -version
javac 1.8.0_382
(python) wayahead@ubox:~/workspace/h2o/github$ cd h2o-3/
(python) wayahead@ubox:~/workspace/h2o/github/h2o-3$ ./gradlew build -x test
Starting a Gradle Daemon (subsequent builds will be faster)

> Configure project :
The project project ':h2o-persist-s3' needs CI for running tests! You can pass `-PdoCI=true` to force CI behaviour.

> Configure project :h2o-assemblies:main
== :h2o-assemblies:main: Using optional component: xgboost, version 3.42.0
== :h2o-assemblies:main: Using optional component: jython-cfunc, version 3.42.0
== :h2o-assemblies:main: Using optional component: jgrapht, version 3.42.0
== :h2o-assemblies:main: Using optional component: mojo-pipeline, version 3.42.0

> Configure project :h2o-r


> Configure project :h2o-assemblies:genmodel
== :h2o-assemblies:genmodel: Using optional genmodel component: xgboost, version 3.42.0
== :h2o-assemblies:genmodel: Using optional genmodel component: jgrapht, version 3.42.0
== :h2o-assemblies:genmodel: Using optional genmodel component: mojo-pipeline, version 3.42.0

> Configure project :h2o-assemblies:minimal
== :h2o-assemblies:minimal: Using optional component: xgboost, version 3.42.0
== :h2o-assemblies:minimal: Using optional component: jython-cfunc, version 3.42.0
== :h2o-assemblies:minimal: Using optional component: jgrapht, version 3.42.0
== :h2o-assemblies:minimal: Using optional component: mojo-pipeline, version 3.42.0

> Configure project :h2o-assemblies:steam
== :h2o-assemblies:steam: Using optional component: xgboost, version 3.42.0
== :h2o-assemblies:steam: Using optional component: jython-cfunc, version 3.42.0
== :h2o-assemblies:steam: Using optional component: jgrapht, version 3.42.0
== :h2o-assemblies:steam: Using optional component: mojo-pipeline, version 3.42.0

> Task :h2o-core:generateBuildVersionJava
NOTE: emitBuildVersionJava found no file, emitting new file
> Task :h2o-genmodel:generateBuildVersionJava
NOTE: emitBuildVersionJava found no file, emitting new file
> Task :h2o-genmodel:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:h2o-genmodel:compileJava took 1.141 secs

> Task :h2o-core:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:h2o-core:compileJava took 7.099 secs

> Task :h2o-algos:compileJava
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:24: error: cannot find symbol
  public static final class GLMModelOutputV3 extends ModelOutputSchemaV3<GLMOutput, GLMModelOutputV3> {
                                                     ^
  symbol:   class ModelOutputSchemaV3
  location: class hex.schemas.GLMModelV3
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:24: error: cannot find symbol
  public static final class GLMModelOutputV3 extends ModelOutputSchemaV3<GLMOutput, GLMModelOutputV3> {
                                                                         ^
  symbol:   class GLMOutput
  location: class hex.schemas.GLMModelV3
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:22: error: type argument hex.schemas.GLMModelV3.GLMModelOutputV3 is not within bounds of type-variable OS
public class GLMModelV3 extends ModelSchemaV3<GLMModel, GLMModelV3, GLMModel.GLMParameters, GLMV3.GLMParametersV3, GLMOutput, GLMModelV3.GLMModelOutputV3> {
                                                                                                                                        ^
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:196: error: method does not override or implement a method from a supertype
    @Override
    ^
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:198: error: non-static variable super cannot be referenced from a static context
      super.fillFromImpl(impl);
      ^
/home/wayahead/workspace/h2o/github/h2o-3/h2o-algos/src/main/java/hex/schemas/GLMModelV3.java:198: error: no suitable method found for fillFromImpl(hex.glm.GLMModel.GLMOutput)
      super.fillFromImpl(impl);
           ^
    method water.api.Schema.fillFromImpl(hex.glm.GLMModel) is not applicable
      (argument mismatch; hex.glm.GLMModel.GLMOutput cannot be converted to hex.glm.GLMModel)
    method water.api.schemas3.ModelSchemaV3.fillFromImpl(hex.glm.GLMModel) is not applicable
      (argument mismatch; hex.glm.GLMModel.GLMOutput cannot be converted to hex.glm.GLMModel)
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
Note: Some messages have been simplified; recompile with -Xdiags:verbose to get full output
6 errors
:h2o-algos:compileJava took 2.255 secs

> Task :h2o-algos:compileJava FAILED

Task timings:
   7.099 secs  :h2o-core:compileJava
   2.255 secs  :h2o-algos:compileJava
   1.141 secs  :h2o-genmodel:compileJava
   0.366 secs  :h2o-core:generateBuildVersionJava
   0.290 secs  All others

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':h2o-algos:compileJava'.
> Compilation failed; see the compiler error output for details.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0.

You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.

See https://docs.gradle.org/7.2/userguide/command_line_interface.html#sec:command_line_warnings

BUILD FAILED in 29s
14 actionable tasks: 13 executed, 1 up-to-date



==============================================
The environment: Ubuntu 22.04, openjdk version ""1.8.0_382""


I did look into the FQA and README of h2o-3 at github, and tried many times 
./gladew clean
. Accorindg to the above error, seems machine learning algo 
GLMModelV3
 is mising in the code base, but there is no info about how to fetch those code/jar. So I hope the expert can explain more details about how to prepare the build environment.",['h2o'],Unknown,,N/A
76931856,76931856,2023-08-18T18:40:12,2023-08-22 22:25:05Z,47,"I am trying to migrate a model from Hadoop to GCP. Model MOJO will not be retrained. I am running the model in Dataproc using Airflow spark submit.
Source data format matches with Hadoop source and everything. While running the model, I am getting this error:


Caused by: hex.genmodel.easy.exception.PredictUnknownCategoricalLevelException: Unknown categorical level (my_column,Y)



This column has the same values as we have on Hadoop, and in there everything works fine.
Model was created on H20 version 
3.30.0.4
 and MOJO version is 
1.4
.


While running the dataproc cluster I am using 
""PIP_PACKAGES"": ""h2o_pysparkling_3.1""


Not sure what the issue is? Please help.","['h2o', 'sparkling-water', 'h2o.ai']",Unknown,,N/A
76850663,76850663,2023-08-07T10:06:32,2023-08-10 09:23:45Z,0,"I am trying to build a model of Cox Proportional Hazard with h2o. In fact a I have succesfully built a model using deeplearning about survival in gastric cancer dataset. When trying to do it with h2o.coxph, I always get ""ERRR on field: _train: Training data must have at least 2 features (incl. response)"". I have checked that my data train frame has the column of ""event/response"" (Have tried with factor/numeric/integer with same results). Also I have tried with an invented  10 observations dataset which include ""start column (all zeros), stop colum (random>0), event column (random (0,1)) and a predictor (eg:age (random>20), with the same results.


But when I import the data (csv file) that under the ""help section"" is suggested, and check the datatype of the start,stop and event column, are similar to those I have, but in this case I can build the model. I don't know which the problem is.


Any suggestions??


This is my code:


h2o.init()

#Load data into the cluster
datos_h2o<-as.h2o(prueba_2)

#Build the model
modelo_coxph <- h2o.coxph(x = ""edad"", training_frame = datos_h2o,
                         event_column = ""event"",
                         start_column= ""inicio"",
                         stop_column = ""final""
                        )




Thanks in advance","['r', 'h2o']",ilpadrino,https://stackoverflow.com/users/1974542/ilpadrino,13
76844317,76844317,2023-08-06T03:30:58,2023-08-09 20:31:49Z,346,"I'm stuck with H2oGPT, I can't let it run.


I am running on a Windows PC with




11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz   2.30 GHz


32GB memory 64bit


GPU Nvidia GeForce RTX 3050




I followed all the procedure described in the 
H2oGPT Windows Installation Page
.


This is the command I launch.


$ python generate.py --base_model='llama' --prompt_type=llama2 --score_model=None --langchain_mode='UserData' --user_path=./my_documents



This is the error stack trace:


Traceback (most recent call last):
  File ""C:\Users\dulcin\Documents\Java\UltraGPT\h2ogpt\generate.py"", line 7, in <module>
    from src.gen import main
  File ""C:\Users\dulcin\Documents\Java\UltraGPT\h2ogpt\src\gen.py"", line 32, in <module>
    from utils import set_seed, clear_torch_cache, NullContext, wrapped_partial, EThread, get_githash, \
ImportError: cannot import name 'set_seed' from 'utils' (C:\Users\dulcin\AppData\Local\Programs\Python\Python310\lib\site-packages\utils\__init__.py)



My questions are:




Should I install ""utils"" seaprately?


Is my laynch command ok? ./my_documents right now only contains one text document.","['h2o', 'openai-api']",donnadulcinea,https://stackoverflow.com/users/3200736/donnadulcinea,"1,864"
76790068,76790068,2023-07-28T17:55:50,2024-11-15 21:26:11Z,65,"I want to retrain my h2o model on a new set of observations using checkpoint but facing errors. My code is failing on the train step when using checkpoint. My original model is created using h2o automl and I verified aml.leader is the GBM model.


The error is related to max_depth field can't be modified. However, I am not modifying max_depth paramter in the gbm_continued definiton.


#ds_file is my local dataset with 4k rows
ds= h2o.import_file(ds_file)
splits = ds.split_frame(ratios= [0.8], seed=1)
train = splits[0]
test = splits[1]
aml = H2OAutoML(max_runtime_secs = 60, seed = 1 , project_name = 'test')
aml.train(y=y, training_frame = train, leaderboard_frame = test)
#verify that aml.leader is the GBM model
print(aml.leader)
#H2OGradientBoostingEstimator : Gradient Boosting Machine
#Model Key: GBM_1_AutoML_1_20230727_145804
#ds2_file is my local dataset with 30k rows
ds2 = h2o.import_file(ds2_file)
Splits2 = ds2.split_frame(ratios= [0.8], seed=1)
train2 = splits2[0]
test2 = splits2[1]
gbm_continued = H2OGradientBoostingEstimator(model_id = 'gbm_continued', checkpoint = aml.leader)
gbm_continued.train(x=predictors, y = y, training_frame = train2)




Here is the error message:


>>> gbm_continued.train(x=predictors, y = y, training_frame = train2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""h2o-dev/lib/lib/python3.8/site-packages/h2o/estimators/estimator_base.py"", line 108, in train
    self._train(parms, verbose=verbose)
  File ""dev_items/h2o-dev/lib/lib/python3.8/site-packages/h2o/estimators/estimator_base.py"", line 187, in _train
    model_builder_json = h2o.api(""POST /%d/ModelBuilders/%s"" % (rest_ver, self.algo), data=parms)
  File ""h2o-dev/lib/lib/python3.8/site-packages/h2o/h2o.py"", line 124, in api
    return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
  File ""h2o-dev/lib/lib/python3.8/site-packages/h2o/backend/connection.py"", line 498, in request
    return self._process_response(resp, save_to)
  File ""h2o-dev/lib/lib/python3.8/site-packages/h2o/backend/connection.py"", line 852, in _process_response
    raise H2OResponseError(data)
h2o.exceptions.H2OResponseError: ModelBuilderErrorV3  (water.exceptions.H2OModelBuilderIllegalArgumentException):
    timestamp = 1690566243266
    error_url = '/3/ModelBuilders/gbm'
    msg = 'Illegal argument(s) for GBM model: gbm_continued.  Details: ERRR on field: _max_depth: Field _max_depth cannot be modified if checkpoint is specified!\nERRR on field: _ntrees: If checkpoint is specified then requested ntrees must be higher than 409'
    dev_msg = 'Illegal argument(s) for GBM model: gbm_continued.  Details: ERRR on field: _max_depth: Field _max_depth cannot be modified if checkpoint is specified!\nERRR on field: _ntrees: If checkpoint is specified then requested ntrees must be higher than 409'
    http_status = 412




I found 
one related question
 on this topic but that do not address this question.","['python', 'h2o']",mindstorm84,https://stackoverflow.com/users/9228207/mindstorm84,102
76774699,76774699,2023-07-26T20:06:37,2023-07-26 23:53:24Z,61,"I want to enable incremental training using H2O models. I save the trained model using MOJO format for existing dataset/observations. Upon receiving new observations, I would like to load the MOJO-based model and retrain the existing model on new observations. However, this is not working.


Alternatively, I can train the model using specific model classes e.g. H2OGradientBoostingEstimator on the combined dataset but that will require me to keep track of all previous observations and cause higher disk usage.


H2O documentation
 for H2OGenericEstimator shows support for training function. However, based on experiments, the train function doesn't really make any difference.


from h2o.estimators import H2OGenericEstimator, H2OXGBoostEstimator
import tempfile
airlines= h2o.import_file(""https://s3.amazonaws.com/h2o-public-test-data/smalldata/testng/airlines_train.csv"")
y = ""IsDepDelayed""
x = [""fYear"",""fMonth"",""Origin"",""Dest"",""Distance""]
xgb = H2OXGBoostEstimator(ntrees=1, nfolds=3)
xgb.train(x=x, y=y, training_frame=airlines)
original_model_filename = tempfile.mkdtemp()
original_model_filename = xgb.download_mojo(original_model_filename)
key = h2o.lazy_import(original_model_filename)
fr = h2o.get_frame(key[0])
model = H2OGenericEstimator(model_key=fr)
model.train()
model.auc()



Is there any way to train the model loaded using MOJO file?","['python', 'h2o']",mindstorm84,https://stackoverflow.com/users/9228207/mindstorm84,102
76722818,76722818,2023-07-19T15:17:22,2023-07-19 16:20:37Z,25,"I'm watching an H2O demo, but one of the code lines are cut off, and I have no clue what is supposed to be in the rest.




the gv_call assignment is cut off. Does anyone know what is supposed to be in the rest. I'm not familiar with the packages used.","['python', 'operating-system', 'subprocess', 'decision-tree', 'h2o']",Hai Vu,https://stackoverflow.com/users/459745/hai-vu,40.5k
76659512,76659512,2023-07-11T07:18:50,2023-07-11 16:25:08Z,34,"I am using the h20 cluster to train the model using tuned random forest and plot pdp plots using the below code


tx = tuned_rf.explain(test_data, top_n_features=5, include_explanations='pdp')



This is returning pdp plots in graphical format the graph is 
features vs mean response
  but I want to  get the pdp plots in tabular form(below shows the expected output)


Feature          Mean response 
Sub-feature-1      0.18
Sub-feature-2      0.15



but right now i am getting the same in graphical format I want to get the output in the tabular format how can I  do this using h20 cluster","['python', 'h2o', 'h2o.ai', 'h2o4gpu', 'h2o-wave']",rqqa,https://stackoverflow.com/users/18110623/rqqa,25
76659362,76659362,2023-07-11T06:54:35,2023-07-11 16:01:23Z,0,"I want to do deeplearning using h2o with custom initial weights, if I do not add the initial_weights parameter the code runs fine, but if I add it, the error is keep showing.


wt1 = as.h2o(weight1)
   wt2 = as.h2o(weight2)
   wt3 = as.h2o(weight3)
   wt4 = as.h2o(weight4)

   #building the model
   ann_h2o_model <- h2o.deeplearning(x = features,
                                  y = target,
                                  training_frame = Train_set_b,
                                  standardize = T,
                                  activation = ""Tanh"",
                                  hidden = c(6,6,6),
                                  epochs = 2500,
                                  adaptive_rate = TRUE,
                                  initial_weights = c(wt1,wt2,wt3,wt4))
java.lang.NullPointerException error","['r', 'h2o']",Baglan Baglan,https://stackoverflow.com/users/22207713/baglan-baglan,1
76523487,76523487,2023-06-21T12:49:48,2023-06-21 15:08:57Z,0,"I'm using the H2O package for machine learning. The goal is to use my dataset to estimate new predicted ages for each participant in my dataset. My setup for the code was to get rid of all unnecessary columns in my dataset like sex, race, other demographics, etc and just leave age and some biomarkers I'd like to compare it with.


My first attempt went well because I made my dataset cross-sectional (limiting us to one entry per participant) before getting rid of columns, so I don't need to worry about grouping. However, I want to also test it out longitudinally too. In my original dataset (before making it cross-sectional), I have a column called ""ID"", and sometimes rows have the same ID if a participant in our study visited multiple times. I'd like to factor this in to machine learning but I'm wondering how to do so.


Below is my code for the cross-sectional analysis. I'm not sure if I need to adjust anything here or earlier when I'm changing to factors or using as.h2o


aml <- h2o.automl(y = ""age"", 
               training_frame = train_long2, 
               max_models = 300, 
               stopping_metric = ""MSE"", 
               nfolds = 5, 
               seed = 1, 
               include_algos = ""GBM"", 
               sort_metric = ""MSE"", 
               keep_cross_validation_predictions = TRUE)","['r', 'machine-learning', 'h2o', 'automl', 'longitudinal']",Unknown,,N/A
76503651,76503651,2023-06-19T04:42:06,2023-06-19 14:09:47Z,0,"Good day! I had initially built a deep learning estimator on the h2o library in R, and I was thinking of using the h2o4gpu library instead to build it in order to expedite the process. Does anyone know if there is an existing function in h2o4gpu that is equivalent to the h2o.DeepLearningEstimator function in h2o? If not, is there a way for my h2o function to incorporate utilizing the GPU? Thank you!


I used the following function in R:


model_one = h2o.deeplearning(x = independent_variables,
                             training_frame = results_h2o,
                             autoencoder = TRUE,
                             #reproducible = TRUE,
                             seed = -1,
                             hidden = c(2000,1000,500,250,125,50),
                             epochs = 30,
                             activation = ""Tanh""#
)","['r', 'deep-learning', 'h2o', 'autoencoder', 'h2o4gpu']",Aengus,https://stackoverflow.com/users/22039632/aengus,13
76488419,76488419,2023-06-16T08:40:25,2023-06-17 20:14:38Z,0,"Good day! I was training a h2o autoencoder and everytime I did so, the anomaly results (reconstruction score) of the variables I received would always be different, although the data I am using was more or less always the same. I was just wondering if this was supposed to be the case? Thank you!


I implemented the h2o.DeepLearningEstimator in R","['r', 'h2o', 'autoencoder']",Aengus,https://stackoverflow.com/users/22039632/aengus,13
76428833,76428833,2023-06-08T05:24:36,2023-06-08 19:40:35Z,109,"I was wondering if anyone knows if there are any glaring problems with the way my H2o autoencoder is being trained that could cause it to take so long? Or if anyone knows of any way I can reduce the time it takes to train this model, both with the dataset and the model construction. Any help would be greatly appreciated! Thank you very much!


I have been training a H2o autoencoder on a dataset consisting of just one-hot-encoded categorical columns. The dataset is of shape (7762,2232) and the model took about 5 hours to train. The code for building the model is as follows:


model = H2ODeepLearningEstimator(
    autoencoder = True,
    seed = -1
    hidden = [2000,1000,500,250,125,50],
    epochs = 30,
    activation = ""Tanh""
)","['pandas', 'deep-learning', 'h2o', 'autoencoder', 'one-hot-encoding']",Unknown,,N/A
76412118,76412118,2023-06-06T06:55:36,2023-06-07 05:39:43Z,44,"Following the proposed tree interpreter approach (
http://blog.datadive.net/interpreting-random-forests/
) one can explain a tree-based model prediction using info from the decision path.


I've built tree models with H2o and exported them as PMML to do so.
However, only the terminal nodes contain the probability scores, but not the branching nodes which are needed for the tree interpreter approach.


I've tested with packages from R (rpart, randomForest) and python (sklearn) but it seems they tend not to store the split info in the resulting model.
So far only BigML seems to produce the needed PMML structure.


Do you know which other libraries I can try?
What is the workaround strategy to compute sample split values and then generate a correesponding PMML file?


Thanks
K","['scikit-learn', 'decision-tree', 'h2o', 'rpart', 'pmml']",kevin5jan,https://stackoverflow.com/users/16538388/kevin5jan,39
76391929,76391929,2023-06-02T16:24:53,2023-06-03 06:30:18Z,38,"My h2o code used to return useful information like printing table and metrics but for some reason now always returns memory address.


Example:


import pandas as pd
import h2o
from h2o.estimators import H2OGeneralizedLinearEstimator
h2o.init()

df = pd.read_csv('winequality-white.csv', sep=';')
df.loc[df.quality <= 6, 'quality'] = 0
df.loc[df.quality > 6, 'quality'] = 1
hf = h2o.H2OFrame(df)
print(hf.head())



output:


Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%
<h2o.frame.H2OFrame object at 0x0000025D22121EB0>



wine_split = hf.split_frame(ratios = [0.8], seed = 1234)
wine_train = wine_split[0] 
wine_test = wine_split[1]  
gbm_default = H2OGeneralizedLinearEstimator(seed = 1234, family='binomial', alpha=1)
gbm_default.train(x = features, y = 'quality', training_frame = wine_train)
print(gbm_default)



output:


██████████████████████████████████████████████████████| (done) 100%
<h2o.estimators.glm.H2OGeneralizedLinearEstimator object at 0x0000025D74E225E0>



I have tried shutting down everything and restarting everything but nothing changes.","['python', 'h2o']",Unknown,,N/A
76316226,76316226,2023-05-23T15:09:51,2023-05-23 17:33:55Z,0,"I am trying to install h2o (R version) on Databricks.


I started with the following:


%sh
apt-get install libcurl4-openssl-dev -y
apt-get update -y
apt-get install -y r-cran-jsonlite



It all ran fine. Then I tried:


install.packages(""h2o"", type=""source"", repos=(c(""http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R"")))



but got back the following error:


Warning message:
This project is configured to use R version '4.0.4', but '4.2.2' is currently being used. 
ERROR: dependency ‘jsonlite’ is not available for package ‘h2o’
* removing ‘/local_disk0/.ephemeral_nfs/envs/rEnv-2a5edebe-8ede-4459-8a8b-72c0047b8a01/h2o’
Installing package into ‘/local_disk0/.ephemeral_nfs/envs/rEnv-2a5edebe-8ede-4459-8a8b-72c0047b8a01’
(as ‘lib’ is unspecified)
trying URL 'http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R/src/contrib/h2o_3.40.0.4.tar.gz'
Content type 'application/x-tar' length 177590345 bytes (169.4 MB)
==================================================
downloaded 169.4 MB

Warning in utils::install.packages(pkgs, ...) :
  installation of package ‘h2o’ had non-zero exit status

The downloaded source packages are in
    ‘/tmp/RtmpXqq9V0/downloaded_packages’



What is the best way to troubleshoot?","['r', 'databricks', 'h2o', 'jsonlite']",user1700890,https://stackoverflow.com/users/1700890/user1700890,"7,672"
76310451,76310451,2023-05-22T23:28:38,2023-05-23 14:43:40Z,0,"I just installed h2o (3.40.0.4) in R. I ran the following:


library(h2o)
h2o.init()



and got back the following:


H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    C:\Users\some_user~1\AppData\Local\Temp\RtmpQLhwRy\file313c1b97627f/h2o_some_userdov_started_from_r.out
    C:\Users\some_user~1\AppData\Local\Temp\RtmpQLhwRy\file313c4c7f3347/h2o_some_userdov_started_from_r.err

Picked up JAVA_TOOL_OPTIONS: -Djava.vendor=""Sun Microsystems Inc.""
java version ""11.0.6"" 2020-01-14 LTS
Java(TM) SE Runtime Environment 18.9 (build 11.0.6+8-LTS)
Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.6+8-LTS, mixed mode)

Starting H2O JVM and connecting: ............................................................Diagnostic HTTP Request:
   HTTP Status Code: -1
HTTP Error Message: Failed to connect to localhost port 54321: Connection refused
18:12:11.226 [main] WARN  hex.tree.xgboost.XGBoostExtension - Cannot initialize XGBoost backend! Xgboost (enabled GPUs) needs: 
  - CUDA 8.0
XGboost (minimal version) needs: 
  - GCC 4.7+
For more details, run in debug mode: `java -Dlog4j.configuration=file:///tmp/log4j.properties -jar h2o.jar`

05-22 18:12:14.024 127.0.0.1:54321       10320        main  INFO water.default: ----- H2O started  -----
05-22 18:12:14.026 127.0.0.1:54321       10320        main  INFO water.default: Build git branch: rel-zz_kurka
05-22 18:12:14.029 127.0.0.1:54321       10320        main  INFO water.default: Build git hash: 5ff8870f912c6110d7b6988f577c020de10496ec
05-22 18:12:14.031 127.0.0.1:54321       10320        main  INFO water.default: Build git describe: jenkins-3.40.0.3-122-g5ff8870
05-22 18:12:14.032 127.0.0.1:54321       10320        main  INFO water.default: Build project version: 3.40.0.4
05-22 18:12:14.033 127.0.0.1:54321       10320        main  INFO water.default: Build age: 24 days
05-22 18:12:14.034 127.0.0.1:54321       10320        main  INFO water.default: Built by: 'jenkins'
05-22 18:12:14.035 127.0.0.1:54321       10320        main  INFO water.default: Built on: '2023-04-28 12:08:23'
05-22 18:12:14.036 127.0.0.1:54321       10320        main  INFO water.default: Found H2O Core extensions: [KrbStandalone, Infogram]
05-22 18:12:14.037 127.0.0.1:54321       10320        main  INFO water.default: Processed H2O arguments: [-name, H2O_started_from_R_some_userdov_swz423, -ip, localhost, -web_ip, localhost, -port, 54321, -ice_root, C:/Users/some_user~1/AppData/Local/Temp/RtmpQLhwRy, -nthreads, 6, -allow_unsupported_java]
05-22 18:12:14.038 127.0.0.1:54321       10320        main  INFO water.default: Java availableProcessors: 8
05-22 18:12:14.039 127.0.0.1:54321       10320        main  INFO water.default: Java heap totalMemory: 510.0 MB
05-22 18:12:14.040 127.0.0.1:54321       10320        main  INFO water.default: Java heap maxMemory: 7.96 GB
05-22 18:12:14.041 127.0.0.1:54321       10320        main  INFO water.default: Java version: Java 11.0.6 (from Sun Microsystems Inc.)
05-22 18:12:14.042 127.0.0.1:54321       10320        main  INFO water.default: JVM launch parameters: [-Djava.vendor=Sun Microsystems Inc., -ea, -agentpath:C:\Program Files\Palo Alto Networks\Traps\cyjagent.dll]
05-22 18:12:14.043 127.0.0.1:54321       10320        main  INFO water.default: JVM process id: 10320@5CG9061HY2
05-22 18:12:14.044 127.0.0.1:54321       10320        main  INFO water.default: OS version: Windows 10 10.0 (amd64)
05-22 18:12:14.045 127.0.0.1:54321       10320        main  INFO water.default: Machine physical memory: 31.85 GB
05-22 18:12:14.046 127.0.0.1:54321       10320        main  INFO water.default: Machine locale: en_US
05-22 18:12:14.047 127.0.0.1:54321       10320        main  INFO water.default: X-h2o-cluster-id: 1684797130431
05-22 18:12:14.047 127.0.0.1:54321       10320        main  INFO water.default: User name: 'some_userdov'
05-22 18:12:14.048 127.0.0.1:54321       10320        main  INFO water.default: IPv6 stack selected: false
05-22 18:12:14.049 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: lo (Software Loopback Interface 1), 127.0.0.1
05-22 18:12:14.050 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: lo (Software Loopback Interface 1), 0:0:0:0:0:0:0:1
05-22 18:12:14.051 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:net0 (Microsoft 6to4 Adapter)
05-22 18:12:14.052 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:ppp0 (WAN Miniport (PPPOE))
05-22 18:12:14.052 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth0 (WAN Miniport (IPv6))
05-22 18:12:14.053 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:ppp1 (RAS Async Adapter)
05-22 18:12:14.055 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan0 (Microsoft Wi-Fi Direct Virtual Adapter)
05-22 18:12:14.057 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:net1 (Microsoft IP-HTTPS Platform Adapter)
05-22 18:12:14.058 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth1 (VirtualBox Host-Only Ethernet Adapter)
05-22 18:12:14.059 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth2 (Microsoft Kernel Debug Network Adapter)
05-22 18:12:14.060 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan1 (Intel(R) Dual Band Wireless-AC 8265-WFP Native MAC Layer LightWeight Filter-0000)
05-22 18:12:14.061 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth3 (Bluetooth Device (Personal Area Network))
05-22 18:12:14.061 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: eth4 (Intel(R) Ethernet Connection (4) I219-LM), 192.168.0.116
05-22 18:12:14.062 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: eth4 (Intel(R) Ethernet Connection (4) I219-LM), 2601:483:900:21f8:be28:2df4:49bb:2c7a
05-22 18:12:14.063 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: eth4 (Intel(R) Ethernet Connection (4) I219-LM), 2601:483:900:21f8:7cbe:3d33:6313:e3a8
05-22 18:12:14.064 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: eth4 (Intel(R) Ethernet Connection (4) I219-LM), fe80:0:0:0:9c65:405f:3791:3f1b%eth4
05-22 18:12:14.065 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:net2 (WAN Miniport (L2TP))
05-22 18:12:14.066 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth5 (Remote NDIS based Internet Sharing Device)
05-22 18:12:14.066 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth6 (WAN Miniport (IP))
05-22 18:12:14.067 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth7 (Realtek USB GbE Family Controller)
05-22 18:12:14.068 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth8 (PANGP Virtual Ethernet Adapter)
05-22 18:12:14.069 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:net3 (WAN Miniport (SSTP))
05-22 18:12:14.070 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:net4 (Microsoft Teredo Tunneling Adapter)
05-22 18:12:14.071 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: eth9 (PANGP Virtual Ethernet Adapter #3), 10.92.145.169
05-22 18:12:14.072 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: eth9 (PANGP Virtual Ethernet Adapter #3), fe80:0:0:0:c86a:aa5:322a:855d%eth9
05-22 18:12:14.073 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan2 (Intel(R) Dual Band Wireless-AC 8265)
05-22 18:12:14.074 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth10 (PANGP Virtual Ethernet Adapter #2)
05-22 18:12:14.075 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth11 (WAN Miniport (IP)-WFP Native MAC Layer LightWeight Filter-0000)
05-22 18:12:14.075 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: eth12 (VirtualBox Host-Only Ethernet Adapter), 192.168.56.1
05-22 18:12:14.076 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: eth12 (VirtualBox Host-Only Ethernet Adapter), fe80:0:0:0:1cd4:83df:3d19:8a23%eth12
05-22 18:12:14.077 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:net5 (WAN Miniport (IKEv2))
05-22 18:12:14.078 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth13 (Cisco AnyConnect Secure Mobility Client Virtual Miniport Adapter for Windows x64)
05-22 18:12:14.079 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:net6 (WAN Miniport (PPTP))
05-22 18:12:14.080 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth14 (VirtualBox Host-Only Ethernet Adapter)
05-22 18:12:14.081 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan3 (Microsoft Wi-Fi Direct Virtual Adapter #2)
05-22 18:12:14.082 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth15 (WAN Miniport (Network Monitor))
05-22 18:12:14.083 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth16 (VirtualBox Host-Only Ethernet Adapter-WFP Native MAC Layer LightWeight Filter-0000)
05-22 18:12:14.084 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth17 (VirtualBox Host-Only Ethernet Adapter-QoS Packet Scheduler-0000)
05-22 18:12:14.084 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth18 (VirtualBox Host-Only Ethernet Adapter-WFP 802.3 MAC Layer LightWeight Filter-0000)
05-22 18:12:14.085 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan4 (Intel(R) Dual Band Wireless-AC 8265-Virtual WiFi Filter Driver-0000)
05-22 18:12:14.086 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth19 (WAN Miniport (IP)-QoS Packet Scheduler-0000)
05-22 18:12:14.087 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth20 (WAN Miniport (IPv6)-WFP Native MAC Layer LightWeight Filter-0000)
05-22 18:12:14.115 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth21 (WAN Miniport (IPv6)-QoS Packet Scheduler-0000)
05-22 18:12:14.116 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth22 (WAN Miniport (Network Monitor)-WFP Native MAC Layer LightWeight Filter-0000)
05-22 18:12:14.117 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth23 (Intel(R) Ethernet Connection (4) I219-LM-WFP Native MAC Layer LightWeight Filter-0000)
05-22 18:12:14.118 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth24 (Intel(R) Ethernet Connection (4) I219-LM-VirtualBox NDIS Light-Weight Filter-0000)
05-22 18:12:14.119 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth25 (Intel(R) Ethernet Connection (4) I219-LM-QoS Packet Scheduler-0000)
05-22 18:12:14.120 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth26 (Intel(R) Ethernet Connection (4) I219-LM-WFP 802.3 MAC Layer LightWeight Filter-0000)
05-22 18:12:14.121 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan5 (Intel(R) Dual Band Wireless-AC 8265-Native WiFi Filter Driver-0000)
05-22 18:12:14.122 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan6 (Intel(R) Dual Band Wireless-AC 8265-VirtualBox NDIS Light-Weight Filter-0000)
05-22 18:12:14.122 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan7 (Intel(R) Dual Band Wireless-AC 8265-QoS Packet Scheduler-0000)
05-22 18:12:14.123 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan8 (Intel(R) Dual Band Wireless-AC 8265-WFP 802.3 MAC Layer LightWeight Filter-0000)
05-22 18:12:14.124 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth27 (WAN Miniport (Network Monitor)-QoS Packet Scheduler-0000)
05-22 18:12:14.125 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan9 (Microsoft Wi-Fi Direct Virtual Adapter-WFP Native MAC Layer LightWeight Filter-0000)
05-22 18:12:14.125 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan10 (Microsoft Wi-Fi Direct Virtual Adapter-Native WiFi Filter Driver-0000)
05-22 18:12:14.126 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan11 (Microsoft Wi-Fi Direct Virtual Adapter-VirtualBox NDIS Light-Weight Filter-0000)
05-22 18:12:14.127 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan12 (Microsoft Wi-Fi Direct Virtual Adapter-QoS Packet Scheduler-0000)
05-22 18:12:14.127 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan13 (Microsoft Wi-Fi Direct Virtual Adapter-WFP 802.3 MAC Layer LightWeight Filter-0000)
05-22 18:12:14.128 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth28 (Hyper-V Virtual Switch Extension Adapter)
05-22 18:12:14.129 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth29 (Hyper-V Virtual Switch Extension Adapter-Hyper-V Virtual Switch Extension Filter-0000)
05-22 18:12:14.130 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth30 (Hyper-V Virtual Ethernet Adapter-WFP Native MAC Layer LightWeight Filter-0000)
05-22 18:12:14.130 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth31 (Hyper-V Virtual Ethernet Adapter-VirtualBox NDIS Light-Weight Filter-0000)
05-22 18:12:14.131 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth32 (Hyper-V Virtual Ethernet Adapter-QoS Packet Scheduler-0000)
05-22 18:12:14.132 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth33 (Hyper-V Virtual Ethernet Adapter-WFP 802.3 MAC Layer LightWeight Filter-0000)
05-22 18:12:14.133 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: eth34 (Hyper-V Virtual Ethernet Adapter), 172.24.80.1
05-22 18:12:14.134 127.0.0.1:54321       10320        main  INFO water.default: Possible IP Address: eth34 (Hyper-V Virtual Ethernet Adapter), fe80:0:0:0:203e:c59a:d223:79c8%eth34
05-22 18:12:14.135 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth35 (PANGP Virtual Ethernet Adapter #3-WFP Native MAC Layer LightWeight Filter-0000)
05-22 18:12:14.136 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth36 (PANGP Virtual Ethernet Adapter #3-VirtualBox NDIS Light-Weight Filter-0000)
05-22 18:12:14.137 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth37 (PANGP Virtual Ethernet Adapter #3-QoS Packet Scheduler-0000)
05-22 18:12:14.138 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:eth38 (PANGP Virtual Ethernet Adapter #3-WFP 802.3 MAC Layer LightWeight Filter-0000)
05-22 18:12:14.139 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan14 (Microsoft Wi-Fi Direct Virtual Adapter #2-WFP Native MAC Layer LightWeight Filter-0000)
05-22 18:12:14.140 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan15 (Microsoft Wi-Fi Direct Virtual Adapter #2-Native WiFi Filter Driver-0000)
05-22 18:12:14.140 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan16 (Microsoft Wi-Fi Direct Virtual Adapter #2-VirtualBox NDIS Light-Weight Filter-0000)
05-22 18:12:14.141 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan17 (Microsoft Wi-Fi Direct Virtual Adapter #2-QoS Packet Scheduler-0000)
05-22 18:12:14.142 127.0.0.1:54321       10320        main  INFO water.default: Network interface is down: name:wlan18 (Microsoft Wi-Fi Direct Virtual Adapter #2-WFP 802.3 MAC Layer LightWeight Filter-0000)
05-22 18:12:14.143 127.0.0.1:54321       10320        main FATAL water.default: On localhost/127.0.0.1 some of the required ports 54321, 54322 are not available, change -port PORT and try again. 
Error Output:
   Picked up JAVA_TOOL_OPTIONS: -Djava.vendor=""Sun Microsystems Inc.""



I apologize for this extra long error message, but how can I troubleshoot?","['r', 'h2o']",user1700890,https://stackoverflow.com/users/1700890/user1700890,"7,672"
76274774,76274774,2023-05-17T17:34:43,2023-05-17 21:34:48Z,0,"I am trying to train a neural network with massive amounts of data. I have around 2million rows and 200k columns of data. My data is in a sparse matrix, which I cannot feed into a neural network. I am required to convert it to a dataframe, but the size is too large for me to do that.


Hence, I am trying to convert it to smaller H2O dataframes, and then merge them combine all the small H2O dataframes into one. However, It does not seem to be working. I assumed that H2O could deal with large amounts of data, and so this is something that can be done. But it doesn't seem to be the case.
Can someone tell me what the purpose of H2O is? Is it only about utilizing fast machine learning models that it offers, along with some sort of parallization or is there more to it? Also, is there a way for me to do what I am looking forward to do. Also, when I use H2O with R, am I using R to run Java code in the background
FYI - I am doing all of this in R.","['r', 'h2o']",Vortenzie,https://stackoverflow.com/users/13806364/vortenzie,85
76201233,76201233,2023-05-08T13:47:47,2023-05-08 20:11:25Z,337,"I am quite new to docker. I am trying to deliver some scores via my h2o models through docker in real time (have not tried this yet: 
link
). Here is my Dockerfile and docker-compose/yml.


FROM python:3.9

WORKDIR /app

RUN apt-get update && apt-get install -y default-jre

RUN pip install h2o==3.38.0.3
RUN pip install pandas==1.3.5

COPY . /app

#EXPOSE 8000

CMD [""python"", ""my_models.py""]



docker-compose.yml:


version: '3'
services:
  click_app:
    image: h2oai/h2o-open-source-k8s
    container_name: click_models
    restart: unless-stopped
    build: .
    ports:
      - ""8001:8001""



and my code:


h2o.init(port=23023, nthreads=10)

def main():
    df_h2o = h2o.import_file('input.csv')
    df_h2o[""PhoneNumber""] = df_h2o[""PhoneNumber""].ascharacter()
    
    dfScore = df_h2o[df_h2o['NotDateMonth']==5]
    
    ms1gbm, ms1rf, ms1glm, ms2gbm, ms2rf, ms2glm = CallModels()
    
    predictions = Score(ms1gbm, ms1rf, ms1glm, ms2gbm, ms2rf, ms2glm, dfScore)
    
    print(""NOTIFY..."", predictions)
    
    return 0

if __name__ == '__main__':
    
    print(sys.argv)
    
    main()



So as long as I have 
restart: unless-stopped
 in my yml file, h2o server starts and my code runs doing the scoring and the process keeps repeating infinitely (reinitiating the same h2o over and over again). If I remove it of course it only runs once. What I want is to initiate h2o once keep the container up and whenever a new dataset shows up then do the scoring. What would be the ways to achieve this?","['docker', 'real-time', 'h2o', 'scoring']",mlee_jordan,https://stackoverflow.com/users/3198674/mlee-jordan,842
76200605,76200605,2023-05-08T12:28:26,2023-05-08 13:19:06Z,0,"I would like reduce the dimensionality of a mixed data set with the help of the 
h2o.glrm()
 function from the R package 
h2o
. My data set includes binary variables (nominal variables with two possible levels), nominal variables (with three or more possible levels), and ordinal variables (with three or more possible levels). I'm using logistic loss for binary variables and ordinal loss and categorical loss for ordinal variables and nominal variables, respectively.


Here is a minimal, reproducible example of my problem.


# Load packages
library(tibble)
library(h2o)

# Example data for MRE
my_data <- tibble::tibble(
  var.1 = as.factor(rep(1, 10)),
  var.2 = as.factor(c(NA, 1, 1, -1, -1, -1, 1, 1, 1, 1)),
  var.3 = as.factor(rep(-1, 10)),
  var.4 = as.factor(c(-1, 1, 1, 1, 1, 1, -1, 1, 1, 1)),
  var.5 = as.factor(rep(-1, 10)),
  var.6 = as.factor(c(1, 2, 3, 1, 2, 2, 2, 2, 2, 3)),
  var.7 = as.factor(c(NA, 2, 3, 2, 2, 2, 2, 3, 1, 2)),
  var.8 = as.factor(c(2, 3, 2, 2, 2, 2, 3, 2, 2, 2)),
  var.9 = as.factor(c(1, 2, 3, 4, 1, 2, 3, 4, 1, 3)),
  var.10 = as.factor(c(1, 1, 1, 1, NA, 1, 1, -1, -1, 1))
)

my_data_types <- tibble::tibble(
  var_name = paste(""var"", 1:10, sep = "".""),
  var_type = c(rep(""binary"", 5),
               rep(""ordinal"", 3),
               ""nominal"", ""binary"")
)

# Initialize h2o cluster
h2o::h2o.init()
h2o::h2o.no_progress()

# Convert data to h2o object
my_data_h2o <- h2o::as.h2o(my_data)

# Define loss function for ordinal and nominal variables
losses <- tibble::tibble(
  index = which(my_data_types$var_type %in% c(""ordinal"", ""nominal"")) - 1,
  loss = NA_character_
)

for (i in seq_along(losses$index)) {
  losses$loss[i] <-
    ifelse(my_data_types$var_type[losses$index[i] + 1] == ""ordinal"", ""Ordinal"",
           ifelse(my_data_types$var_type[losses$index[i] + 1] == ""nominal"", ""Categorical"", NA))
}

# Run GLRM
my_glrm <- h2o::h2o.glrm(
  training_frame = my_data_h2o,
  k = 2,
  loss = ""Logistic"",
  loss_by_col_idx = losses$index,
  loss_by_col = losses$loss,
  regularization_x = ""None"",
  regularization_y = ""None"",
  transform = ""NONE"",
  max_iterations = 2000,
  seed = 12345
)



When I run the above model, I receive the following error message:


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  
ERROR MESSAGE:

Illegal argument(s) for GLRM model: GLRM_model_R_1683532209346_20.  Details: ERRR on field: _loss: Logistic is not a numeric loss function



Although I don't think that this is what the error message tells me, I also ran the model on an alternative version of the data set in which binary variables are not defined as factors.


# Alternative example data for MRE
my_data_2 <- tibble::tibble(
  var.1 = rep(1, 10),
  var.2 = c(NA, 1, 1, -1, -1, -1, 1, 1, 1, 1),
  var.3 = rep(-1, 10),
  var.4 = c(-1, 1, 1, 1, 1, 1, -1, 1, 1, 1),
  var.5 = rep(-1, 10),
  var.6 = as.factor(c(1, 2, 3, 1, 2, 2, 2, 2, 2, 3)),
  var.7 = as.factor(c(NA, 2, 3, 2, 2, 2, 2, 3, 1, 2)),
  var.8 = as.factor(c(2, 3, 2, 2, 2, 2, 3, 2, 2, 2)),
  var.9 = as.factor(c(1, 2, 3, 4, 1, 2, 3, 4, 1, 3)),
  var.10 = c(1, 1, 1, 1, NA, 1, 1, -1, -1, 1)
)

# Convert data to h2o object
my_data_2_h2o <- h2o::as.h2o(my_data_2)

# Run GLRM
my_glrm_2 <- h2o::h2o.glrm(
  training_frame = my_data_2_h2o,
  k = 2,
  loss = ""Logistic"",
  loss_by_col_idx = losses$index,
  loss_by_col = losses$loss,
  regularization_x = ""None"",
  regularization_y = ""None"",
  transform = ""NONE"",
  max_iterations = 2000,
  seed = 12345
)



When I run the model on the alternative version of the data set, I receive the following error:


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  
ERROR MESSAGE:

Illegal argument(s) for GLRM model: GLRM_model_R_1683532209346_21.  Details: ERRR on field: _loss: Logistic is not a numeric loss function
ERRR on field: _loss_by_col: Loss function Logistic cannot be applied to numeric column 0
ERRR on field: _loss_by_col: Loss function Logistic cannot be applied to numeric column 1
ERRR on field: _loss_by_col: Loss function Logistic cannot be applied to numeric column 6



I would be very grateful if anyone could tell me what I'm doing wrong here.","['r', 'h2o', 'dimensionality-reduction']",Dan,https://stackoverflow.com/users/5120828/dan,57
76199437,76199437,2023-05-08T09:54:46,2023-05-11 15:07:11Z,64,"I have a model developed with h2o 3.38. I calibrated it with a separate data. Now when I score a new data set, I see the calibrated probabilities. If  I save the model as:


h2o.save_model(gbm_model, path=model_path', force=True)
 and load it with 
h2o.load_model(model_path)

and score it I can get calibrated probabilities.


However, if I use mojo as: 
gbm.save_mojo(model_path)
 and load it as 
h2o.import_mojo(model_path)

and score the data I don't get calibrated probabilities.


Is it not possible to get calibrated probabilities with mojo?","['h2o', 'calibration', 'mojo']",mlee_jordan,https://stackoverflow.com/users/3198674/mlee-jordan,842
76173043,76173043,2023-05-04T12:11:39,2023-05-04 16:15:08Z,177,"I am currently using h2o autoML to train a model on a binary classification problem. I have a train (70% ~200k rows), valid (10% ~30k rows), test (10% ~30k rows) and blend (10% ~30k rows) datasets all coming from the time sensitive splitting of the original dataset (~300k rows).


When checking the training confusion matrix I only see 10k total cases instead of ~200k.


I create the model like this :


#  Create model
aml = H2OAutoML(
    max_runtime_secs=max_runtime_secs,
    stopping_metric=stopping_metric, # ""AUCPR""
    sort_metric=sort_metric, # ""AUCPR""
    nfolds=nfolds, # set to 0
    distribution=distribution, # ""bernoulli""
    verbosity=verbosity,
    balance_classes=balance_classes, # False
    seed=seed,
)

aml.train(
    y=outcome_column,
    training_frame=train,
    validation_frame=valid,
    leaderboard_frame=test,
    blending_frame=blend,
)

# Get the best model
best_model = aml.get_best_model()

# get the performance on test
performance = best_model.model_performance(test)

# define the threshold based on the desired metric
best_threshold = best_model.find_threshold_by_max_metric(
        metric=metric_to_use, valid=True)

# inspect confusion matrix on training set using that threshold
train_confusion = best_model.confusion_matrix(
        thresholds=best_threshold, train=True)

# inspect confusion matrix on test using that threshold
test_confusion = performance.confusion_matrix(thresholds=best_threshold)

# confusion matrix validation using that threshold
valid_confusion = best_model.confusion_matrix(
    thresholds=best_threshold, valid=True)
)



These are the resulting confusion matrix:


confusion matrix train: Confusion Matrix (Act/Pred) @ threshold = 0.35701837501784456
       False    True    Error    Rate
-----  -------  ------  -------  --------------
False  8589     190     0.0216   (190.0/8779.0)
True   272      904     0.2313   (272.0/1176.0)
Total  8861     1094    0.0464   (462.0/9955.0) 

confusion matrix valid: Confusion Matrix (Act/Pred) @ threshold = 0.3555305434918455
       False    True    Error    Rate
-----  -------  ------  -------  ----------------
False  23367    802     0.0332   (802.0/24169.0)
True   1486     1580    0.4847   (1486.0/3066.0)
Total  24853    2382    0.084    (2288.0/27235.0) 

confusion matrix test: Confusion Matrix (Act/Pred) @ threshold = 0.3546996890950105
       False    True    Error    Rate
-----  -------  ------  -------  ----------------
False  23399    769     0.0318   (769.0/24168.0)
True   1537     1529    0.5013   (1537.0/3066.0)
Total  24936    2298    0.0847   (2306.0/27234.0) 



We can see that on valid and test confusion matrix I have my ~30k totals cases but I only have ~10k total cases on train confusion matrix instead of the initial ~200k rows. Why ?


EDIT 1:
Here is the leaderboard of the models :


LEADERBOARD:
model_id                                                    aucpr       auc    logloss    mean_per_class_error      rmse        mse    training_time_ms    predict_time_per_row_ms  algo
StackedEnsemble_BestOfFamily_1_AutoML_1_20230504_164001  0.632635  0.876718   0.226965                0.260764  0.253097  0.0640579                4397                   0.041607  StackedEnsemble
GBM_1_AutoML_1_20230504_164001                           0.632514  0.876067   0.237024                0.262188  0.254668  0.0648556               24116                   0.038866  GBM
StackedEnsemble_BestOfFamily_2_AutoML_1_20230504_164001  0.631139  0.87681    0.22705                 0.262454  0.253345  0.0641838                1421                   0.049998  StackedEnsemble
GBM_4_AutoML_1_20230504_164001                           0.491181  0.810158   0.338646                0.308554  0.311599  0.0970939                 638                   0.001294  GBM
GBM_3_AutoML_1_20230504_164001                           0.374094  0.762457   0.342545                0.353879  0.312943  0.0979334                 642                   0.001166  GBM
DRF_1_AutoML_1_20230504_164001                           0.373471  0.755862   2.01974                 0.290401  0.311465  0.0970105                1735                   0.001758  DRF
GBM_2_AutoML_1_20230504_164001                           0.355587  0.758635   0.343282                0.371797  0.3132    0.0980943                 960                   0.001194  GBM
GLM_1_AutoML_1_20230504_164001                           0.330708  0.727998   0.312086                0.362253  0.298647  0.08919                 23648                   0.001599  GLM
[8 rows x 10 columns]","['python', 'machine-learning', 'h2o', 'automl', 'h2o.ai']",Unknown,,N/A
76131318,76131318,2023-04-28T15:58:41,2023-04-28 20:11:42Z,119,"The documentation in standardize section 
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/standardize.html
 only includes these algorithms: Deep Learning, GLM, GAM, K-Means.


I have two questions:




Does it mean that other algorithms such as Random Forest, Gradient Boosting, etc, are not standardizing (at least automatically in AutoML)?




Does 
standardize = TRUE
 in Deep Learning, GLM, ..., standardize the target variable altogether, or only features?






A related question is 
Feature Standardize in AutoML H2O
.","['machine-learning', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
76130631,76130631,2023-04-28T14:32:16,2023-04-28 18:15:38Z,862,"I recently started exploring the field of Data Engineering and came across some difficulties. I have a bucket in GCS with millions of parquet files and I want to create an Anomaly Detection model with them. I was trying to ingest that data into Databricks and store it all in a Delta Table, using Autoloader, and then using H2O Sparkling Water for the Anomaly Detection model.


I have run into some memory issues when I read the Delta Table into a Pyspark Dataframe and try to pass it to an H2O Frame.


I'm using a cluster with 30 GB of memory and 4 cores and the Dataframe has shape (30448674, 213). The error I'm getting is the following:




The spark driver has stopped unexpectedly and is restarting. Your notebook will be reattached automatically.




Through an analysis in Ganglia UI my driver graph is always in red when executing the asH2OFrame() function.


This is the code I am using:


# Imports

import h2o

from pysparkling import *

from pyspark.sql import SparkSession

# Creates an H2OContext Object.

hc = H2OContext.getOrCreate(spark)

# Read the Data from Delta Table to PySpark Dataframe

df = spark.read.format(""delta"").load(""path/to/delta_table"")

# Converting the PySpark Dataframe to an H2OFrame

h2o_df = hc.asH2OFrame(df)

# H2O Model for Anomaly Detection

isolation_model = H2OIsolationForestEstimator(model_id,
                                              ntrees,
                                              seed)

isolation_model.train(training_frame = h2o_df)

# Making predictions

predictions = isolation_model.predict(h2o_df)","['pyspark', 'databricks', 'parquet', 'h2o', 'delta-live-tables']",Unknown,,N/A
76123086,76123086,2023-04-27T17:16:01,2023-04-29 00:12:47Z,66,"I am looking to create a confidence interval for one of my model's outputs and I need to get the model outputs before the link function is applied. From what I've read, it seems like I am interested in getting the stage results of the model.


So far I have created a model with the proper parameter, fit it, verified the parameter value, and obtained predictions, but I don't see the information in the detailed predictions column regardless.


estimator = H2OGLMClassifier(family='binomial', featureCols=feature_columns, labelCol=response, withStageResults=True)
model = estimator.fit(training_data)

predictions = model.transform(training_data)



the predictions will then have the 
detailed_predictions
 column, but it will only contain JSON with the prediction probabilities, the same as if the stage results weren't listed, e.g.


{
  ""label"": ""1"",
  ""probabilities"": {""0"": "".814"", ""1"": "".176""}
}



Is there something else that needs to be done to obtain the stage results? Are the stage results not the correct way to get what I am looking for?


Thanks","['pyspark', 'logistic-regression', 'h2o', 'sparkling-water', 'h2o.ai']",samkart,https://stackoverflow.com/users/8279585/samkart,"6,604"
76032896,76032896,2023-04-17T08:01:55,2023-04-17 09:16:55Z,0,"I am using h2o.automl function in R and here you can find the function below;


h2o.automl(
  x = x_name,
  y = y_name,
  training_frame = as.h2o(train),
  leaderboard_frame = as.h2o(test),
  max_runtime_secs = 20*60,
  exclude_algos = c(""XGBoost"")
)



So, I'm confused about the last final fit on the entire dataset after getting the leader model from this function. In this case, cross-validation will be applied to the training data to find the best models and leaderboard_frame is only used for scoring. So the test subset is not used in any training process? After finding the best model for training with cross-validation folds, does h2o.automl fit a model on the entire dataset?


Because I would like to use this model operationally and use the entire dataset as well since I do not want to lose any information/data on the operational model. What about if I don't give any leaderboard_frame? I know that the performance on the cross-validation folds will be shown in this case, but will h2o.automl model fit a final model to the entire dataset after finding the best hyperparameters and models by using cross-validation folds?


In other words, in a Kaggle competition, how can I use the h2o.automl to make sure to use the entire dataset to predict unseen data? By the way, it is a time-series forecasting competition and the time of the year has also a very crucial effect on the model. They've given a 10-year-long hourly time-series data and June is the month that the competition hosts would like you to predict. I would like my model to perform better in June by using h2o.automl, what do you suggest in this case?


One last question; for having a July-specific model, would you train the model by filtering out the July months from the training dataset and finding the best hyperparameters that perform well in July months? Or would you include the July months in the data? In this case, what would be your train/test/validation and cross-validation subsets? Since I would like to use h2o.automl function, can you please apply your answer to the h2o.automl?","['r', 'validation', 'h2o', 'train-test-split', 'automl']",Cem,https://stackoverflow.com/users/8751136/cem,138
75982995,75982995,2023-04-11T06:31:56,2023-04-17 07:15:05Z,54,"I have used XGBoost for my model.  i have noticed that h2o cluster not share memory while on this model process. master A server RAM utilization is very high and master B RAM utilization  is very low.  i checked h2o logs on both servers and noticed master A log file continuously updating while on model processing but master B log file not updating. it shows cluster created logs only


Some times while on model processing master A h2o jar down due to high memory usage.


I'm using h2o-3.36.1.1 version and created two node cluster.  Cluster successfully created and logged cluster details on log file.


i have check master A & B connectivity and did curl with both side. all work fine and cluster is work well.




H2O_cluster_uptime:         15 mins 14 secs


H2O_cluster_timezone:       Asia/Colombo


H2O_data_parsing_timezone:  UTC


H2O_cluster_version:           3.36.1.1


H2O_cluster_version_age:    11 months and 28 days !!!


H2O_cluster_name:           XXXXXX


H2O_cluster_total_nodes:    2


H2O_cluster_free_memory:    43.36 Gb


H2O_cluster_total_cores:    30


H2O_cluster_allowed_cores:  30


H2O_cluster_status:         locked,


healthy H2O_connection_url:         http://localhost:54321


H2O_connection_proxy:       {""http"": null, ""https"": null}


H2O_internal_security:      False


Python_version:             3.7.11 final




Could anyone please help me to troubleshoot these issues.


Why both servers not share server resources while on model processing ?


Why master B h2o log not update ?


Why master A h2o jar down on high memory usage ?


Master A log


            main  INFO water.default: Open H2O Flow in your web browser: http://xxx.xxx.xxx.xx:54321
        main  INFO water.default: 
   FJ-126-15  INFO water.default: Cloud of size 2 formed [master01.user.com/xxx.xxx.xxx.xx:54321, master02.user.com/xxx.xxx.xxx.xx:54321]
  058452-166  INFO water.default: GET /3/Metadata/schemas/CloudV3, parms: {}
  058452-166  INFO water.default: Locking cloud to new members, because water.api.schemas3.MetadataV3
  4058452-14  INFO water.default: GET /3/Metadata/schemas/H2OErrorV3, parms: {}
  4058452-15  INFO water.default: GET /3/Metadata/schemas/H2OModelBuilderErrorV3, parms: {}
  4058452-18  INFO water.default: POST /4/sessions, parms: {}
  4058452-16  INFO water.default: POST /99/Rapids, parms: {ast=(setTimeZone ""UTC""), session_id=_sid_a391}
  4058452-13  INFO water.default: DELETE /3/DKV, parms: {}
  4058452-13  INFO water.default: Removing all objects
  4058452-13  INFO water.default: Finished removing objects
  4058452-12  INFO water.default: DELETE /3/DKV, parms: {}
  4058452-12  INFO water.default: Removing all objects
  4058452-12  INFO water.default: Finished removing objects
  058452-170  INFO water.default: DELETE /3/DKV, parms: {}
  058452-170  INFO water.default: Removing all objects
  058452-170  INFO water.default: Finished removing objects
  4058452-14  INFO water.default: GET /3/Metadata/schemas/CloudV3, parms: {}
  058452-169  INFO water.default: GET /3/Metadata/schemas/H2OErrorV3, parms: {}
  058452-166  INFO water.default: GET /3/Metadata/schemas/H2OModelBuilderErrorV3, parms: {}
  4058452-19  INFO water.default: POST /4/sessions, parms: {}
  4058452-18  INFO water.default: POST /99/Rapids, parms: {ast=(setTimeZone ""UTC""), session_id=_sid_bfac}
  058452-170  INFO water.default: Reading byte InputStream into Frame:
  058452-170  INFO water.default:     frameKey:    upload_bbcd4f6aeb3c1095e63f66a89cdd4756
  058452-170  INFO water.default:     totalChunks: 2
  058452-170  INFO water.default:     totalBytes:  4404663
  058452-170  INFO water.default:     Success.
  058452-167  INFO water.default: POST /3/ParseSetup, parms: {single_quotes=False, source_frames=[""upload_bbcd4f6aeb3c1095e63f66a89cdd4756""], check_header=0}
  058452-169  INFO water.default: Total file size: 4.2 MB
  058452-169  INFO water.default: Parse chunk size 4194304
     FJ-1-15  INFO water.default: Parse result for Key_Frame__upload_bbcd4f6aeb3c1095e63f66a89cdd4756.hex (2023 rows, 436 columns):
     FJ-1-15  INFO water.default:                               ColV2    type          min          max         mean        sigma         NAs constant cardinality
     FJ-1-15  INFO water.default:                                COL1:  factor    011022232    YA9854024                                                  1334
     FJ-1-15  INFO water.default:                      COL2: numeric      2019.00      2020.00      2019.70     0.457960                            
     FJ-1-15  INFO water.default:                     COL3: numeric      1.00000      12.0000      6.07860      2.82287                            
     FJ-1-15  INFO water.default:                         COL4:  factor |00011000813 |09988000074                                                  1334
     FJ-1-15  INFO water.default:                       COL5:  factor    CUST NAME     CUSTOMER                                                     2
     FJ-1-15  INFO water.default:                         COL6: numeric  1.14005e+08  4.10024e+08  2.96146e+08  4.57328e+07                            
     FJ-1-15  INFO water.default:                    COL7: numeric      10000.0      30000.0      28294.6      5573.93           3                
     FJ-1-15  INFO water.default:                     COL8:  factor                       USD                                                     4
     FJ-1-15  INFO water.default:                              COL9:  factor          927         RM17                                                    20
     FJ-1-15  INFO water.default:               COL10:  factor           NO          YES                                                     2
     FJ-1-15  INFO water.default: Additional column information only sent to log file...
     FJ-1-15  INFO water.default:                COL11: numeric     -1.00000      175.250      1.07602      5.07740                            
     FJ-1-15  INFO water.default:                COL12: numeric     -1.00000      97.2262     0.447662      3.19167                            
     FJ-1-15  INFO water.default:                COL13: numeric     -1.00000      124.206      1.03933      3.94221                            
     FJ-1-15  INFO water.default:                      response_class:  factor           1A to_be_filled                                                     5
     FJ-1-15  INFO water.default:                    response_class_5:  factor           1B          1B1                                                     2
     FJ-1-15  INFO water.default:                    response_class_4:  factor           1A NON_PERFORME                                                     4
     FJ-1-15  INFO water.default:                    response_class_3:  factor           1A NON_PERFORME                                                     4
     FJ-1-15  INFO water.default:                    response_class_2:  factor           1A NON_PERFORME                                                     4
     FJ-1-15  INFO water.default:                    response_class_1:  factor           1A NON_PERFORME                                                     4
     FJ-1-15  INFO water.default:                              subset:  factor         test        train                                                     2
     FJ-1-15  INFO water.default: Chunk compression summary:
     FJ-1-15  INFO water.default:   Chunk Type                 Chunk Name       Count  Count Percentage        Size  Size Percentage
     FJ-1-15  INFO water.default:          C0L              Constant long          74           8.486 %      5.8 KB          0.207 %
     FJ-1-15  INFO water.default:          CBS                     Binary          19           2.179 %      4.4 KB          0.159 %
     FJ-1-15  INFO water.default:          CXI            Sparse Integers          80           9.174 %     25.0 KB          0.897 %
     FJ-1-15  INFO water.default:          CXF               Sparse Reals          50           5.734 %     48.9 KB          1.753 %
     FJ-1-15  INFO water.default:           C1            1-Byte Integers           7           0.803 %     11.8 KB          0.423 %
     FJ-1-15  INFO water.default:          C1N  1-Byte Integers (w/o NAs)          92          10.550 %    104.0 KB          3.731 %
     FJ-1-15  INFO water.default:          C1S           1-Byte Fractions         142          16.284 %    118.4 KB          4.245 %
     FJ-1-15  INFO water.default:           C2            2-Byte Integers          72           8.257 %    231.7 KB          8.309 %
     FJ-1-15  INFO water.default:          C2S           2-Byte Fractions          18           2.064 %     22.9 KB          0.822 %
     FJ-1-15  INFO water.default:           C4            4-Byte Integers          50           5.734 %    109.1 KB          3.913 %
     FJ-1-15  INFO water.default:          C4S           4-Byte Fractions         127          14.564 %    360.5 KB         12.925 %
     FJ-1-15  INFO water.default:           C8            8-byte Integers           1           0.115 %     15.0 KB          0.539 %
     FJ-1-15  INFO water.default:          CUD               Unique Reals           5           0.573 %     13.2 KB          0.472 %
     FJ-1-15  INFO water.default:          C8D               64-bit Reals         135          15.482 %      1.7 MB         61.606 %
     FJ-1-15  INFO water.default: Frame distribution summary:
     FJ-1-15  INFO water.default:                             Size  Number of Rows  Number of Chunks per Column  Number of Chunks



Master B


    main  INFO water.default: H2O started in 4906ms
     main  INFO water.default: 
     main  INFO water.default: Open H2O Flow in your web browser: http://xxx.xxx.xxx.xx:54321
     main  INFO water.default: 
FJ-126-15  INFO water.default: Cloud of size 2 formed [master01.user.com/xxx.xxx.xxx.xx:54321, master02.user.com/xxx.xxx.xxx.xx:54321]
FJ-123-15  INFO water.default: Locking cloud to new members, because Class Id=56
  FJ-2-15  INFO water.default: Key upload_bbcd4f6aeb3c1095e63f66a89cdd4756 will be parsed using method DistributedParse.
  FJ-2-21  INFO water.default: Key upload_902bcdd31a4aea9f65690f1bc6074886 will be parsed using method DistributedParse.","['linux', 'data-science', 'xgboost', 'h2o', 'h2o.ai']",Unknown,,N/A
75873650,75873650,2023-03-29T06:03:57,2023-04-11 12:38:38Z,38,"I've started H2O 3 nodes instance on Hadoop (yarn), at night something went wrong at cluster and I see job container attempt 1 was killed and yarn started new job on different node attempt 2. 3 mapper job is running. How can find H2O UI now ? http://different_node:54321/ is not working, mapper nodes also is not responding at port 54321. Looks like H2O unable to restore UI application after failure.",['h2o'],Triffids,https://stackoverflow.com/users/3274529/triffids,155
75816459,75816459,2023-03-22T19:19:22,2023-03-27 02:06:01Z,395,I'm checking how we can use H2O MOJO models to run scoring models at prod using java api. I've received MOJO model in zip file from data scientists teams and managed to produce some predictions. Now I have a question will it be possible for data scientists to apply transformations on features (data set) which was used by model. Assume they will be not able to use data set as is and simple transformations will be required.,['h2o'],Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
75749800,75749800,2023-03-15T20:32:28,2023-03-16 10:13:25Z,36,"I have a monthly timeseries dataframe grouped by date and team name. I want to convert it to a h2o frame to run the automl models but the index gets dropped in hf_train. How do I group this data in h2o so that the model runs per team?


data=[
['01/01/18','A','Team A',1.5,8.9,'Pop'],
['02/01/18','A','Team AA',1.5,8.9,'Pop'],
['03/01/18','A','Team AA',3.1,11,'Div'],
['04/01/18','A','Team AC',1.5,6,'Div'],
['01/01/18','A','Team A',1.5,8.9,'Pop'],
['02/01/18','A','Team BA',1.5,8.9,'Pop'],
['03/01/18','A','Team BA',3.1,11,'Div'],
['04/01/18','A','Team BC',1.5,6,'Div'],
['01/01/18','C','Team C',1.5,8.9,'Pop'],
['02/01/18','C','Team CA',1.5,8.9,'Pop'],
['03/01/18','C','Team CC',3.1,11,'Div'],
['04/01/18','C','Team CA',1.5,6,'Div']
]

import pandas as pd
df = pd.DataFrame(l, columns=['Dt','Co','Team','Val1','Val2','Type'])
df =df.set_index(['Dt','Co'])


import h2o
h2o.init()

hf_train = h2o.H2OFrame(df)
hf_train



H2o Automl


aml = H2OAutoML(max_runtime_secs = 600, seed = 42)

aml.train(x = x, 
          y = y,
          training_frame = hf_train,
          leaderboard_frame = hf_test)","['h2o', 'automl', 'h2o.ai']",apprunner2186,https://stackoverflow.com/users/19163185/apprunner2186,279
75722820,75722820,2023-03-13T13:53:13,2023-03-13 14:30:29Z,0,"I would like to increase point in h2o SHAP summary plot. I tried to modify object that is returned by h2o.shap_summary_plot:


plt=h2o.shap_summary_plot(model, test)
plt=plt+geom_point(size = 4)



Unfortunately old points remain in the background, and position of the new/large points is changed - these are plotted on straight lines:","['r', 'ggplot2', 'h2o', 'shap']",MLearner,https://stackoverflow.com/users/11067633/mlearner,63
75701517,75701517,2023-03-10T22:26:54,2023-04-15 21:22:31Z,0,"I am experiencing what appears to be strange behavior while trying to set base/reference levels for a categorical variable in a simple glm model using H2O. To illustrate, I have added a few additional lines to the documentation example for the h2o.relevel function in R.


 ## Not run: 
 library(h2o)
 h2o.init()

 # Convert iris dataset to an H2OFrame
 iris_hf <- as.h2o(iris)
 # Look at current ordering of the Species column levels
 h2o.levels(iris_hf[""Species""])
 # ""setosa""     ""versicolor"" ""virginica"" 

 # fit glm
 h2o.glm(""Species"", ""Sepal.Length"", iris_hf)




You can already see a problem here, because 'setosa' is supposed to be the reference level, but the glm is using 'versicolor' instead.


In base R, I would use 'relevel' to change the base level, and this works as expected with base R's glm function. In H2O, there is an equivalent h2o.relevel function. But as stated, this does not seem to influence the glm output in any way.


 # Change the reference level to ""virginica""
 iris_hf[""Species""] <- h2o.relevel(x = iris_hf[""Species""], y = 
 ""virginica"")
 # Observe new ordering
 h2o.levels(iris_hf[""Species""])
 # ""virginica""  ""setosa""     ""versicolor""

 h2o.glm(""Species"", ""Sepal.Length"", iris_hf)





As can be seen, the ordering of the variable names changes in the output table, but there is no change in the actual parameter estimates.


The documentation for H2O that I have read implies that h2o.relevel should do what I am expecting, and that the h2o.glm function should, by default, use the first level of a factor as the reference level when estimating coefficients. This seems to not be the case though.","['r', 'h2o', 'h2o.ai']",darkness,https://stackoverflow.com/users/3380142/darkness,85
75700445,75700445,2023-03-10T19:49:38,2023-03-13 14:29:00Z,96,"I'm pretty sure that 
h2o.varimp
 is not based on data other than train or validation since the test data is never put into the model.


I was reading the h2o documents about 
Variable Importance
 but couldn't find what it is based on. Is it based on training or a validation set? Is there a way to check the importance on the test data?",['h2o'],Matthew Son,https://stackoverflow.com/users/10484383/matthew-son,"1,385"
75689169,75689169,2023-03-09T19:23:37,2023-03-10 22:19:41Z,0,"I'm using RStudio server and 
h2o.splitFrame
 is causing some error, and I'm not sure where to look at.



h2o.init(max_mem_size = '950G', nthreads = 127) 

...

parts = h2o.splitFrame(data_h2o, ratios = c(0.7,0.15), seed =1) # train-valid-test split
Error in .Call(R_curl_fetch_memory, enc2utf8(url), handle, nonblocking) : 
  reached elapsed time limit



The behavior is quite strange in that sometimes 
parts
 is generated fine regardless of the error. However, occasionally it fails and prints errors like:


> parts
[[1]]
# train split was OK    
5 -1.110098e-16   -0.30177617
6  9.999955e-01   12.29344288

[487719970 rows x 16 columns] 

[[2]]

ERROR: Unexpected HTTP Status code: 400 Bad Request (url = http://localhost:54321/99/Rapids)

java.lang.IllegalArgumentException
 [1] ""java.lang.IllegalArgumentException: Name lookup of 'RTMP_sid_bbef_2' failed""                                 
 [2] ""    water.rapids.Env.lookup(Env.java:458)""                                                                   
 [3] ""    water.rapids.ast.params.AstId.exec(AstId.java:33)""                                                       
 [4] ""    water.rapids.ast.prims.mungers.AstRowSlice.apply(AstRowSlice.java:39)""                                   
 [5] ""    water.rapids.ast.prims.mungers.AstRowSlice.apply(AstRowSlice.java:20)""                                   
 [6] ""    water.rapids.ast.AstExec.exec(AstExec.java:63)""                                                          
 [7] ""    water.rapids.ast.prims.assign.AstTmpAssign.apply(AstTmpAssign.java:48)""                                  
 [8] ""    water.rapids.ast.prims.assign.AstTmpAssign.apply(AstTmpAssign.java:17)""                                  
 [9] ""    water.rapids.ast.AstExec.exec(AstExec.java:63)""                                                          
[10] ""    water.rapids.Session.exec(Session.java:99)""                                                              
[11] ""    water.rapids.Rapids.exec(Rapids.java:94)""                                                                
[12] ""    water.api.RapidsHandler.exec(RapidsHandler.java:34)""                                                     
[13] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                          
[14] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""        
[15] ""    java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""
[16] ""    java.base/java.lang.reflect.Method.invoke(Method.java:566)""                                              
[17] ""    water.api.Handler.handle(Handler.java:60)""                                                               
[18] ""    water.api.RequestServer.serve(RequestServer.java:472)""                                                   
[19] ""    water.api.RequestServer.doGeneric(RequestServer.java:303)""                                               
[20] ""    water.api.RequestServer.doPost(RequestServer.java:227)""                                                  
[21] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:707)""                                            
[22] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:790)""                                            
[23] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)""                                  
[24] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)""                              
[25] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)""                       
[26] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)""                      
[27] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)""                        
[28] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)""                               
[29] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)""                        
[30] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)""                       
[31] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)""                           
[32] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                   
[33] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                         
[34] ""    water.webserver.jetty9.Jetty9ServerAdapter$LoginHandler.handle(Jetty9ServerAdapter.java:130)""            
[35] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                   
[36] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                         
[37] ""    org.eclipse.jetty.server.Server.handle(Server.java:531)""                                                 
[38] ""    org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)""                                       
[39] ""    org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)""                             
[40] ""    org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)""             
[41] ""    org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)""                                       
[42] ""    org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)""                                    
[43] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)""                  
[44] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)""                
[45] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)""               
[46] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)""                      
[47] ""    org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)""
[48] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)""                        
[49] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)""                         
[50] ""    java.base/java.lang.Thread.run(Thread.java:829)""                                                         

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  

ERROR MESSAGE:

Name lookup of 'RTMP_sid_bbef_2' failed



sessionInfo()
R version 4.2.2 Patched (2022-11-10 r83330)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 22.04.2 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8     LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                  LC_ADDRESS=C               LC_TELEPHONE=C             LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] hrbrthemes_0.8.0  kableExtra_1.3.4  fixest_0.11.1     RPostgres_1.4.5   dbplyr_2.3.0      tictoc_1.1        ggthemr_1.1.0     hms_1.1.2         arrow_10.0.1     
[10] data.table_1.14.6 forcats_1.0.0     stringr_1.5.0     dplyr_1.1.0       purrr_1.0.1       readr_2.1.3       tidyr_1.3.0       tibble_3.1.8      ggplot2_3.4.0    
[19] tidyverse_1.3.2   h2o_3.40.0.1","['r', 'h2o']",Matthew Son,https://stackoverflow.com/users/10484383/matthew-son,"1,385"
75653463,75653463,2023-03-06T16:37:33,2023-03-07 08:06:03Z,85,"I'm investigating the possibility of storing MOJOs in cloud storage blobs and/or a database.  I have proof-of-concept code working that saves the MOJO to a file then loads the file and stores to the target (and vice-versa for loading), but I'd like to know if there's any way to skip the file step?  I've looked into python's BytesIO, but since the h2o mojo APIs all require a file-path I don't think I can use it.","['python', 'machine-learning', 'h2o']",8forty,https://stackoverflow.com/users/1854159/8forty,565
75546765,75546765,2023-02-23T15:16:37,2023-02-23 17:20:27Z,100,"I have the following code:


import h2o
from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o.grid.grid_search import H2OGridSearch

h2o.init()
data=h2o.import_file('dataset.csv')
train,test= train.split_frame(ratios=[0.8])

n_trees = [50, 100, 200, 300]
max_depth = [5, 6, 7]
learn_rate = [0.01, 0.05, 0.1]
min_rows = [10,15,20]
min_split_improvement = [0.00001, 0.0001]
hyper_parameters = {""ntrees"":n_trees, 
                   ""max_depth"":max_depth,
                   ""learn_rate"":learn_rate,
                   ""min_rows"":min_rows}

gs=H2OGridSearch(model=H2OGradientBoostingEstimator, hyper_params=hyper_parameters)
gs.train(x=train.columns, y=target_column, training_frame=train, validation_frame=test, distribution='bernoulli')

grid_perf=gs.get_grid(sort_by='auc',decreasing=True)



This produces a grid search of GBMs on the dataset.
I want to be able to save the result of the grid search, grid_perf, as a csv.


Something along the lines of:

h2o.export_file(grid_perf,'grid_search_results.csv')


Note: the code above works, so no debugging necessary, thanks.


Tried using the above line, but it gives me a 
Argument python_obj should be a None | list | tuple | dict | numpy.ndarray | pandas.DataFrame | scipy.sparse.issparse, got H2OGridSearch
 error.","['json', 'h2o', 'grid-search', 'h2o.ai']",Adam,https://stackoverflow.com/users/12979649/adam,21
75511989,75511989,2023-02-20T16:41:56,2023-02-21 17:29:35Z,0,"I installed H2o (3.4), Java SE SDK (17), and use R (4.1.3). I am working on deep autoencoder. I was able to split my data using SplitFrame and generated autoencoder model and everything was working well. A week later, when I was working with the same data to regenerate my model because I didn't save my workspace, I had to split again, and then the following error came up. I have been battling with it for 3 days now. See the error below:


ERROR: Unexpected HTTP Status code: 404 Not Found (url = http://localhost:54321/3/Frames/RTMP_sid_afb5_1?row_count=10)


water.exceptions.H2OKeyNotFoundArgumentException
 [1] ""water.exceptions.H2OKeyNotFoundArgumentException: Object 'RTMP_sid_afb5_1' not found for argument: key""      
 [2] ""    water.api.FramesHandler.getFromDKV(FramesHandler.java:136)""                                              
 [3] ""    water.api.FramesHandler.doFetch(FramesHandler.java:226)""                                                 
 [4] ""    water.api.FramesHandler.doFetch(FramesHandler.java:221)""                                                 
 [5] ""    water.api.FramesHandler.fetch(FramesHandler.java:200)""                                                   
 [6] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                          
 [7] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)""        
 [8] ""    java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""
 [9] ""    java.base/java.lang.reflect.Method.invoke(Method.java:568)""                                              
[10] ""    water.api.Handler.handle(Handler.java:60)""                                                               
[11] ""    water.api.RequestServer.serve(RequestServer.java:472)""                                                   
[12] ""    water.api.RequestServer.doGeneric(RequestServer.java:303)""                                               
[13] ""    water.api.RequestServer.doGet(RequestServer.java:225)""                                                   
[14] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:687)""                                            
[15] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:790)""                                            
[16] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)""                                  
[17] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)""                              
[18] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)""                       
[19] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)""                      
[20] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)""                        
[21] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)""                               
[22] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)""                        
[23] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)""                       
[24] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)""                           
[25] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                   
[26] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                         
[27] ""    water.webserver.jetty9.Jetty9ServerAdapter$LoginHandler.handle(Jetty9ServerAdapter.java:130)""            
[28] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                   
[29] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                         
[30] ""    org.eclipse.jetty.server.Server.handle(Server.java:531)""                                                 
[31] ""    org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)""                                       
[32] ""    org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)""                             
[33] ""    org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)""             
[34] ""    org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)""                                       
[35] ""    org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)""                                    
[36] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)""                  
[37] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)""                
[38] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)""               
[39] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)""                      
[40] ""    org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)""
[41] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)""                        
[42] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)""                         
[43] ""    java.base/java.lang.Thread.run(Thread.java:833)""                                                         

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  

ERROR MESSAGE:

Object 'RTMP_sid_afb5_1' not found for argument: key```","['java', 'r', 'windows', 'eclipse', 'h2o']",Hakeem Ojulari,https://stackoverflow.com/users/7119300/hakeem-ojulari,51
75483947,75483947,2023-02-17T11:51:58,2023-02-18 22:20:34Z,99,"I am trying to proccess the dataset 
kr-vs-kp
 using AutoML H2O. The dataset has two possible target values ""nowin"" and ""win"", so I suppose it should be binary classification. But after the model is found it turns out that H2O regarded it as a multiclass classification problem (accuracy score is absent, and confusion matrix is present).
Why does that happend, and what I have to fix so that it will be binary classification problem?


The code to run AutoML is as follow:


info = h2o.import_file(""kr-vs-kp.csv"")
train,test = info.split_frame(ratios=[.75])

x = train.columns
y = x.pop()

train[y] = train[y].asfactor() #doesn't change anything
test[y] = test[y].asfactor()   #doesn't change anything

automl = h2o.automl.H2OAutoML(max_runtime_secs=900)

automl.train(x=x, y=y, training_frame=train)

perf = automl.leader.model_performance(test)

print(""perf type:"", type(perf))

print(""Algorithm"", automl.leader.show())

print(""Confusion Matrix"", perf.confusion_matrix())
print(""Accuracy score"", perf.accuracy())



The output is as follow:


perf type: <class 'h2o.model.metrics.multinomial.H2OMultinomialModelMetrics'>

Algorithm GBM_1_AutoML_1_20230217_142818

Confusion Matrix Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
class   nowin   won Error   Rate
-------  -------  -----  ---------  --------
0       0       0   nan     0 / 0
0       341     14  0.0394366  14 / 355
0       16      402 0.0382775  16 / 418
0       357     416 0.0388098  30 / 773

AttributeError: type object 'MetricsBase' has no attribute 'accuracy'



Upd. OK, It seems that I have found the source of the problem. For some strange reason the first line in the file is regarded not as columns' names, but as data, so target has three values: win, nowin, class. But why? All other files that I tried so far were processed normally with first line being columns' names.


First line in file, with columns' names:


bkblk,bknwy,bkon8,bkona,bkspr,bkxbq,bkxcr,bkxwp,blxwp,bxqsq,cntxt,dsopp,dwipd,hdchk,katri,mulch,qxmsq,r2ar8,reskd,reskr,rimmx,rkxwp,rxmsq,simpl,skach,skewr,skrxp,spcop,stlmt,thrsk,wkcti,wkna8,wknck,wkovl,wkpos,wtoeg,class","['python', 'h2o', 'automl', 'multinomial']",Unknown,,N/A
75473825,75473825,2023-02-16T14:39:06,2023-02-17 14:38:43Z,47,"The parameter called 
ignored_columns
 (see 
link
) helps user to keep a feature that you want to be ignored when building a model.


When I build a simple ML model and analyze the feature importance, I can see that 
h2o
 ignores the column that I speficied during the training process, which can be observed from the feature importance. As shown below, column 
c
 is not used during training.


import pandas as pd 
import h2o
from h2o.estimators import H2ODeepLearningEstimator
from h2o.grid.grid_search import H2OGridSearch
from h2o.estimators.random_forest import H2ORandomForestEstimator

h2o.init()

x = pd.DataFrame([[0, 1, 4], [5, 1, 6], [15, 2, 0], [25, 5 , 32], 
                  [35, 11 ,89], [45, 15, 1], [55, 34,3], [60, 35,4]], columns = ['a','b','c'])
y = pd.DataFrame([4, 5, 20, 14, 32, 22, 38, 43], columns = ['label'])
hf = h2o.H2OFrame( pd.concat([x,y], axis=""columns""))

X = hf.col_names[:-1]   
y = hf.col_names[-1] 

model=  H2ORandomForestEstimator(ignored_columns = ['c'])

model.train(y = y, training_frame=hf)
model.varimp(use_pandas=True)

variable    relative_importance scaled_importance   percentage
0   b   33876.328125    1.000000    0.540893
1   a   28753.998047    0.848793    0.459107



However, when I turn on the grid search for the hyper parameter tunning, it does not seem like working.


params = {'max_depth': list(range(7, 16)),   'sample_rate': [0.8], }
criteria = {'strategy': 'RandomDiscrete', 'max_models': 4}
grid = H2OGridSearch(model= H2ORandomForestEstimator(ignored_columns = ['c']),
                              search_criteria=criteria,
                              hyper_params=params )

grid.train( y = y, training_frame=hf)
best_model =  grid.get_grid(sort_by='rmse', decreasing=False)[0]
best_model.varimp(use_pandas=True)
    variable    relative_importance scaled_importance   percentage
0   a   33525.109375    1.000000    0.516545
1   b   23314.916016    0.695446    0.359230
2   c   8062.515137 0.240492    0.124225","['python', 'debugging', 'h2o', 'hyperparameters']",Unknown,,N/A
75346452,75346452,2023-02-04T16:14:41,2023-02-21 17:27:31Z,119,"I'm trying to specify the folds for cross-validation in H20. I want to select the subset from the data to be a specific fold. For example, fold number 1 corresponds to my subject 1 data. The fold number 2 to my subject 2 and so on. Is there a way to do it?


I checked the docs for H20 but I didn't find a way for specifying the fold based on my data.","['machine-learning', 'deep-learning', 'h2o', 'automl', 'h2o.ai']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
75308849,75308849,2023-02-01T10:31:33,2023-02-02 13:44:29Z,0,"I like to to run the lilikoi example code with the mock data provided by the lilikoi R package, however, I am stuck at the lilikoi.machine_learning() command due to a connection error with H2O.


I downloaded the H2O file and unzipped it in the R Terminal, but now I cannot connect to http://localhost:54321 as indicated by your website (
https://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/4/index.html
).


Below is the error message I get, copied from my R console:


H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    C:\Users\LUISEB~1\AppData\Local\Temp\Rtmp6HnISl\file2abc38ab39e6/h2o_Luise_Bellach_started_from_r.out
    C:\Users\LUISEB~1\AppData\Local\Temp\Rtmp6HnISl\file2abc5e38704f/h2o_Luise_Bellach_started_from_r.err

java version ""1.8.0_361""
Java(TM) SE Runtime Environment (build 1.8.0_361-b09)
Java HotSpot(TM) 64-Bit Server VM (build 25.361-b09, mixed mode)

Starting H2O JVM and connecting: . Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         1 seconds 582 milliseconds 
    H2O cluster timezone:       Europe/Berlin 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.38.0.1 
    H2O cluster version age:    4 months and 11 days !!! 
    H2O cluster name:           H2O_started_from_R_Luise_Bellach_gwz511 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   3.51 GB 
    H2O cluster total cores:    16 
    H2O cluster allowed cores:  16 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    R Version:                  R version 4.2.2 (2022-10-31 ucrt) 
Warnung: 
Your H2O cluster version is too old (4 months and 11 days)!
Please download and install the latest version from http://h2o.ai/download/
  |=========================================================================================================================| 100%
  |                                                                                                                         |   0%Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  : 
  Unexpected CURL error: Operation was aborted by an application callback
[1] ""Job request failed Unexpected CURL error: Operation was aborted by an application callback, will retry after 3s.""



This goes on multiple times and the % bar doesn't move. Can anyone help me out with this issue?


Best


Dr. Luise Bellach


I downloaded the H2O file and unzipped it in the R Terminal, but now I cannot connect to http://localhost:54321 as indicated by your website (
https://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/4/index.html
).","['r', 'h2o']",Marco,https://stackoverflow.com/users/8833311/marco,"2,797"
75188789,75188789,2023-01-20T19:52:48,2023-01-30 19:59:58Z,508,"When I have categorical features in my dataset, 
h20
 implies one-hot encoding and start the training process. When I call 
summary
 method to see the feature importance tho, it treats each encoded categorical feature as a feature. My question is that how can I get the feature importance information for the original features?


#import libraries
import pandas as pd
import h2o
import random
from h2o.estimators.glm import H2OGeneralizedLinearEstimator

#initiate h20
h2o.init(ip ='localhost')   
h2o.remove_all()  

#load a fake data
training_data = h2o.import_file(""http://h2o-public-test-data.s3.amazonaws.com/smalldata/glm_test/gamma_dispersion_factor_9_10kRows.csv"")


#Spesifcy the predictors (x) and the response (y). I add a dummy categorical column named ""0""
myPredictors = [""abs.C1."", ""abs.C2."", ""abs.C3."", ""abs.C4."", ""abs.C5."", '0']
myResponse = ""resp""

#add a dummy column consisting of random string values
train = h2o.as_list(training_data)
train = pd.concat([train, pd.DataFrame(random.choices(['ya','ne','agh','c','de'],  k=len(training_data)))], axis=1)
train = h2o.H2OFrame(train)


#define linear regression method
def linearRegression(df, predictors, response):
    
    model = H2OGeneralizedLinearEstimator(family=""gaussian"",  lambda_ = 0, standardize = True)
    model.train(x=predictors, y=response, training_frame=df)
    print(model.summary)

linearRegression(train, myPredictors, myResponse)   



Once I run the model, here's the summary of feature importance reported by 
h20
.


Variable Importances: 
variable    relative_importance scaled_importance   percentage
0   abs.C5. 1.508031    1.000000    0.257004
1   abs.C4. 1.364653    0.904924    0.232569
2   abs.C3. 1.158184    0.768011    0.197382
3   abs.C2. 0.766653    0.508380    0.130656
4   abs.C1. 0.471997    0.312989    0.080440
5   0.de    0.275667    0.182799    0.046980
6   0.ne    0.210085    0.139311    0.035803
7   0.ya    0.078100    0.051789    0.013310
8   0.c 0.034353    0.022780    0.005855



Is there a method that I'd get the feature importance for column 
0
. Note that in real, I have way more categorical feature, this is just a MWE.","['python', 'regression', 'h2o', 'feature-selection']",sergey_208,https://stackoverflow.com/users/7447976/sergey-208,649
75169165,75169165,2023-01-19T07:56:28,2023-04-27 17:16:44Z,125,"I have a PySpark code to train an H2o DRF model. I need to save this model to disk and then load it.


from pysparkling.ml import H2ODRF
drf = H2ODRF(featuresCols = predictors,
                labelCol = response,
                columnsToCategorical = [response])



I can not find any document on this so I am asking this question here.","['pyspark', 'h2o', 'sparkling-water']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
75130927,75130927,2023-01-16T06:40:32,2023-01-27 10:46:38Z,99,"I have hot encoded data separately (there are multiple categories under a single main variable and 30 variables). I want to know if this will effect GB, GL, DRF  in H2O . the documentation says for XGBOOST it internally encodes to one-hot For deep learning models i can may be use All factor parameter but I cannot find how to stop implicit hot encoding or let it be as the results will be same?


A little detail on why i needed to preprocess and encode. The original data i have consist of 30 columns and each row is answer from participant and each column row data has multiple categories as string separated by new line . The logical solution was to use hot encode suing dummy coding to split each cell and encode to get columns. The columns are not 150 and rows are 250. I want to find out if the hot encoded data is handled automatically in H2O ?


I have read documentation and tutorial published by amazonaws, may be I am missing something.","['machine-learning', 'h2o', 'one-hot-encoding']",Unknown,,N/A
74815510,74815510,2022-12-15T17:38:05,2023-01-28 00:51:33Z,71,"When running 
xGboost
 Package in 
H2o
 throws Java heap space error. But when the memory is cleared manually it works fine.


I often use
del df
del something
import gc
gc.collect()


in order to clear memory. Any ideas are appreciated.


import h2o
from h2o.tree import H2OTree
from h2o.estimators import H2OIsolationForestEstimator, H2OXGBoostEstimator,

encoding = ""one_hot_explicit""

baseModel = H2OXGBoostEstimator(model_id = modelId, ntrees = 100, 
                                    max_depth = 3,seed = 0xDECAF,
                                    sample_rate = 1,
                                    categorical_encoding = encoding,
                                    keep_cross_validation_predictions=True, 
                                    nfolds = 10
                                    )
    ## TRAIN DATA
    baseModel.train(x = predictor_columns, y = ""label"", training_frame = train.rbind(valid))



Error Trace:


Traceback (most recent call last):
      File ""/docs/code/000_pyGraph/dec_rf_gb_xgb.py"", line 151, in <module>
        decxgb.xgb_cvs(df=df, year=year, model_path=model_path, 
      File ""/docs/code/000_pyGraph/dec_xgb.py"", line 90, in xgb_cvs
        baseModel.train(x = predictor_columns, y = ""label"", training_frame = train.rbind(valid))
      File ""/home/miniconda3/envs/tf-gpu-mem-day/lib/python3.10/site-packages/h2o/estimators/estimator_base.py"", line 123, in train
        self._train(parms, verbose=verbose)
      File ""/home/miniconda3/envs/tf-gpu-mem-day/lib/python3.10/site-packages/h2o/estimators/estimator_base.py"", line 215, in _train
        job.poll(poll_updates=self._print_model_scoring_history if verbose else None)
      File ""/home/miniconda3/envs/tf-gpu-mem-day/lib/python3.10/site-packages/h2o/job.py"", line 90, in poll
        raise EnvironmentError(""Job with key {} failed with an exception: {}\nstacktrace: ""
    OSError: Job with key $03017f00000132d4ffffffff$_8508e7043b6647f7868aa83a3f6842d4 failed with an exception: DistributedException from /127.0.0.1:54321: 'Java heap space', caused by java.lang.OutOfMemoryError: Java heap space
    stacktrace: 
    DistributedException from /127.0.0.1:54321: 'Java heap space', caused by java.lang.OutOfMemoryError: Java heap space
        at water.MRTask.getResult(MRTask.java:660)
        at water.MRTask.getResult(MRTask.java:670)
        at water.MRTask.doAll(MRTask.java:530)
        at water.MRTask.doAll(MRTask.java:412)
        at water.MRTask.doAll(MRTask.java:397)
        at water.fvec.Vec.doCopy(Vec.java:514)
        at water.fvec.Vec.makeCopy(Vec.java:500)
        at water.fvec.Vec.makeCopy(Vec.java:493)
        at water.fvec.Vec.makeCopy(Vec.java:487)
        at water.util.FrameUtils$CategoricalOneHotEncoder$CategoricalOneHotEncoderDriver.compute2(FrameUtils.java:768)
        at water.H2O$H2OCountedCompleter.compute(H2O.java:1677)
        at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
        at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
        at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:976)
        at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
        at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
    Caused by: java.lang.OutOfMemoryError: Java heap space","['python', 'java', 'xgboost', 'h2o']",lpt,https://stackoverflow.com/users/5120817/lpt,975
74795331,74795331,2022-12-14T08:20:57,2022-12-20 04:29:32Z,0,"I have the following 
script using tidymodels' agua
 package:


library(tidymodels)
library(agua)
library(ggplot2)
theme_set(theme_bw())
h2o_start()

data(concrete)
set.seed(4595)
concrete_split <- initial_split(concrete, strata = compressive_strength)
concrete_train <- training(concrete_split)
concrete_test <- testing(concrete_split)

# run for a maximum of 120 seconds
auto_spec <-
  auto_ml() %>%
  set_engine(""h2o"", max_runtime_secs = 120, seed = 1) %>%
  set_mode(""regression"")

normalized_rec <-
  recipe(compressive_strength ~ ., data = concrete_train) %>%
  step_normalize(all_predictors())

auto_wflow <-
  workflow() %>%
  add_model(auto_spec) %>%
  add_recipe(normalized_rec)

auto_fit <- fit(auto_wflow, data = concrete_train)
saveRDS(auto_fit, file = ""test.h2o.auto_fit.rds"") #save the object
h2o_end()



There I tried to save the 
auto_fit
 object into a file.
But when I tried to retrieve it and use it to predict test data:


h2o_start()
auto_fit <- readRDS(""test.h2o.auto_fit.rds"")
predict(auto_fit, new_data = concrete_test)



I got an error:


Error in `h2o_get_model()`:
! Model id does not exist on the h2o server.



What's the way to go about it?


The expected result is:


predict(auto_fit, new_data = concrete_test)
#> # A tibble: 260 × 1
#>    .pred
#>    <dbl>
#>  1  40.0
#>  2  43.0
#>  3  38.2
#>  4  55.7
#>  5  41.4
#>  6  28.1
#>  7  53.2
#>  8  34.5
#>  9  51.1
#> 10  37.9
#> # … with 250 more rows





Update


After following Simon Couch advice


auto_fit <- fit(auto_wflow, data = concrete_train)
auto_fit_bundle <- bundle(auto_fit)
saveRDS(auto_fit_bundle, file = ""test.h2o.auto_fit.rds"") #save the object
h2o_end()

# to reload
h2o_start()
auto_fit_bundle <- readRDS(""test.h2o.auto_fit.rds"")
auto_fit <- unbundle(auto_fit_bundle)
predict(auto_fit, new_data = concrete_test)

rank_results(auto_fit)



I got this error message:


Error in UseMethod(""rank_results"") : 
  no applicable method for 'rank_results' applied to an object of class ""c('H2ORegressionModel', 'H2OModel', 'Keyed')""","['r', 'h2o', 'tidymodels', 'parsnip']",Unknown,,N/A
74559844,74559844,2022-11-24T11:13:19,2022-11-24 12:10:46Z,0,"I am getting the following error, after trying to get most important variables with H2o Package in a classification binary problem with Rstudio.




Error in h(simpleError(msg, call)) :
error in evaluating the argument 'object' in selecting a method for function 'h2o.varimp': no slot of name ""leader"" for this
object of class ""H2OBinomialModel""




Previous Error comes after applying the following steps.


# Lookup best Algorithm for this classification challenge (binary).
rautoml<- h2o.automl(y = target,x = independientes,
      training_frame = train_h2o,
      validation_frame = test_h2o,  # Podría probar hacer el test contra el futuro.
      nfolds = 3,
      max_runtime_secs = 300,
      sort_metric = 'AUC'
    )
#Get the best model from previous step
    rautoml_winner <- rautoml@leader
###### Winner model is a StackedEnsemble_AllModels####

#Get the most important variables.
    h2o.varimp(rautoml_winner@leader)



And then last code produces this error.




Error in h(simpleError(msg, call)) :
error in evaluating the argument 'object' in selecting a method for function 'h2o.varimp': no slot of name ""leader"" for this
object of class ""H2OBinomialModel""","['r', 'h2o']",Konrad Rudolph,https://stackoverflow.com/users/1968/konrad-rudolph,544k
74486548,74486548,2022-11-18T07:59:58,2022-11-18 09:27:45Z,430,"I'm using H2O AutoML to do binary classification, and the classes are imbalanced.


I've set 
balance_classes = TRUE
 and 
max_after_balance_size = 100
 in 
h2o.automl()
 function to oversample the minority class. However, the metric ""area under the Precision-Recall curve (AUCPR)"" of the leader model is not very good, ~ 0.10.


May I ask, are there any tips (e.g., preprocessing steps, parameter setting in 
h2o.automl()
) to handle the class imbalance problem with H2O AutoML?


Your kind guidance is much appreciated!","['classification', 'h2o', 'automl']",Xiaochi,https://stackoverflow.com/users/9929576/xiaochi,65
74368804,74368804,2022-11-09T00:55:23,2023-08-28 18:21:43Z,0,"I'm using h2o.r2(), but it gives me a very different value from what I'm computing manually... It doesn't seem to ALWAYS have this behavior... e.g. for simple linear models it seems to work.


Anyways I'm not sure if I am using it wrong somehow or if this is a bug?


library(tidyverse)
# fit AML, get leader...
fit_aml_H2O = function(X_df, Y_vec) {
  library(h2o); h2o.init()
  
  df = cbind(X_df, Y_vec)
  colnames(df)[[ncol(df)]]='Y'
  df = as.h2o(df)
  aml <- h2o.automl(x=colnames(X_df), y='Y', training_frame=df, nfolds=0,
                    max_models = 15, max_runtime_secs=90)
  leader = h2o.get_best_model(aml)
  cat('R^2: ', h2o.r2(leader, train=T))
  return(leader)
}

# manually compute R^2 (verified to work)
R2 = function(Y_pred, Y_true) {
  MSE = mean((as.numeric(Y_true)-as.numeric(Y_pred))**2)
  R2 = 1-MSE/var(Y_true)
  return(R2)
}

data(iris)

X_df = iris %>% select(-Petal.Length)
Y = iris %>% select(Petal.Length)
model = fit_aml_H2O(X_df, Y)

X_df = as.h2o(X_df)
Y_pred = h2o.predict(model, newdata=X_df)$predict
(R2_train = R2(Y_pred[,1], Y[,1]))
cat('R^2 (manually computed): ', R2_train, '\n')
cat('R^2 (reported by H2O): ', h2o.r2(model,train=T), '\n')
cat('difference between manual R^2 & H2O reported R^2: ',
    abs(R2_train-h2o.r2(model,train=T)), '\n')
stopifnot(all.equal(h2o.r2(model,train=T), R2_train))","['r', 'h2o']",Unknown,,N/A
74294256,74294256,2022-11-02T18:45:02,2022-12-09 19:27:08Z,132,"Based on H2O's 
documentation
 it would seem as though 
relevel('most_frequency_category')
 and 
relevel_by_frequency()
 should accomplish the same thing. However the coefficient estimates are different depending on which method is used to set the reference level for a factor column.


Using an open source dataset from sklearn demonstrates how the GLM coefficients are misaligned when the base level is set using the two releveling methods. Why do the coefficient estimates vary when the base level is the same between the two models?


import pandas as pd
from sklearn.datasets import fetch_openml

import h2o
from h2o.estimators.glm import H2OGeneralizedLinearEstimator

h2o.init(max_mem_size=8)


def load_mtpl2(n_samples=100000):
    """"""
    Fetch the French Motor Third-Party Liability Claims dataset.
    https://scikit-learn.org/stable/auto_examples/linear_model/plot_tweedie_regression_insurance_claims.html
    
    Parameters
    ----------
    n_samples: int, default=100000
      number of samples to select (for faster run time). Full dataset has
      678013 samples.
    """"""
    # freMTPL2freq dataset from https://www.openml.org/d/41214
    df_freq = fetch_openml(data_id=41214, as_frame=True)[""data""]
    df_freq[""IDpol""] = df_freq[""IDpol""].astype(int)
    df_freq.set_index(""IDpol"", inplace=True)

    # freMTPL2sev dataset from https://www.openml.org/d/41215
    df_sev = fetch_openml(data_id=41215, as_frame=True)[""data""]

    # sum ClaimAmount over identical IDs
    df_sev = df_sev.groupby(""IDpol"").sum()

    df = df_freq.join(df_sev, how=""left"")
    df[""ClaimAmount""].fillna(0, inplace=True)

    # unquote string fields
    for column_name in df.columns[df.dtypes.values == object]:
        df[column_name] = df[column_name].str.strip(""'"")
    return df.iloc[:n_samples]


df = load_mtpl2()
df.loc[(df[""ClaimAmount""] == 0) & (df[""ClaimNb""] >= 1), ""ClaimNb""] = 0
df[""Exposure""] = df[""Exposure""].clip(upper=1)
df[""ClaimAmount""] = df[""ClaimAmount""].clip(upper=100000)
df[""PurePremium""] = df[""ClaimAmount""] / df[""Exposure""]

X_freq = h2o.H2OFrame(df)
X_freq[""VehBrand""] = X_freq[""VehBrand""].asfactor()
X_freq[""VehBrand""] = X_freq[""VehBrand""].relevel_by_frequency()

X_relevel = h2o.H2OFrame(df)
X_relevel[""VehBrand""] = X_relevel[""VehBrand""].asfactor()
X_relevel[""VehBrand""] = X_relevel[""VehBrand""].relevel(""B1"") # most frequent category

response_col = ""PurePremium""
weight_col = ""Exposure""
predictors = ""VehBrand""

glm_freq = H2OGeneralizedLinearEstimator(family=""tweedie"",
                                      solver='IRLSM',
                                      tweedie_variance_power=1.5,
                                      tweedie_link_power=0,
                                      lambda_=0,
                                      compute_p_values=True,
                                      remove_collinear_columns=True,
                                      seed=1)

glm_relevel = H2OGeneralizedLinearEstimator(family=""tweedie"",
                                      solver='IRLSM',
                                      tweedie_variance_power=1.5,
                                      tweedie_link_power=0,
                                      lambda_=0,
                                      compute_p_values=True,
                                      remove_collinear_columns=True,
                                      seed=1)

glm_freq.train(x=predictors, y=response_col, training_frame=X_freq, weights_column=weight_col)
glm_relevel.train(x=predictors, y=response_col, training_frame=X_relevel, weights_column=weight_col)

print('GLM with the reference level set using relevel_by_frequency()')
print(glm_freq._model_json['output']['coefficients_table'])
print('\n')
print('GLM with the reference level manually set using relevel()')
print(glm_relevel._model_json['output']['coefficients_table'])



Output


GLM with the reference level set using relevel_by_frequency()
Coefficients: glm coefficients
names         coefficients    std_error    z_value     p_value      standardized_coefficients
------------  --------------  -----------  ----------  -----------  ---------------------------
Intercept     5.40413         1.24082      4.35531     1.33012e-05  5.40413
VehBrand.B2   -0.398721       1.2599       -0.316472   0.751645     -0.398721
VehBrand.B12  -0.061573       1.46541      -0.0420176  0.966485     -0.061573
VehBrand.B3   -0.393908       1.30712      -0.301356   0.763144     -0.393908
VehBrand.B5   -0.282484       1.31929      -0.214118   0.830455     -0.282484
VehBrand.B6   -0.387747       1.25943      -0.307876   0.758177     -0.387747
VehBrand.B4   0.391771        1.45615      0.269047    0.787894     0.391771
VehBrand.B10  -0.0542706      1.35049      -0.040186   0.967945     -0.0542706
VehBrand.B13  -0.306381       1.4628       -0.209449   0.834098     -0.306381
VehBrand.B11  -0.435297       1.29155      -0.337035   0.736091     -0.435297
VehBrand.B14  -0.304243       1.34781      -0.225732   0.821411     -0.304243


GLM with the reference level manually set using relevel()
Coefficients: glm coefficients
names         coefficients    std_error    z_value     p_value     standardized_coefficients
------------  --------------  -----------  ----------  ----------  ---------------------------
Intercept     5.01639         0.215713     23.2549     2.635e-119  5.01639
VehBrand.B10  0.081366        0.804165     0.101181    0.919407    0.081366
VehBrand.B11  0.779518        0.792003     0.984237    0.325001    0.779518
VehBrand.B12  -0.0475497      0.41834      -0.113663   0.909505    -0.0475497
VehBrand.B13  0.326174        0.80891      0.403227    0.686782    0.326174
VehBrand.B14  0.387747        1.25943      0.307876    0.758177    0.387747
VehBrand.B2   -0.010974       0.306996     -0.0357465  0.971485    -0.010974
VehBrand.B3   -0.00616108     0.464188     -0.0132728  0.98941     -0.00616108
VehBrand.B4   0.333477        0.575082     0.579877    0.561999    0.333477
VehBrand.B5   0.105263        0.497431     0.211613    0.832409    0.105263
VehBrand.B6   0.0835042       0.568769     0.146816    0.883278    0.0835042","['python', 'glm', 'h2o', 'categorical-data']",Unknown,,N/A
74240019,74240019,2022-10-28T19:22:56,2024-09-03 17:30:05Z,472,"I am trying to run h2o in following the installation guide from 
here
. I expect a successful initialization with my code but instead get an error.


I am using 
Python 3.10.8
 on Mac M1 macOS Monterey. Is it possible that there is no M1 support yet?


Code


import h2o
h2o.init()
h2o.demo(""glm"")



Error


CalledProcessError: Command '['/usr/bin/java', '-version']' returned non-zero exit status 1.","['python', 'h2o']",yemy,https://stackoverflow.com/users/14606987/yemy,838
74097863,74097863,2022-10-17T13:10:45,2022-10-17 13:39:11Z,557,"I working with the Titanic dataset and made some basic preprocessing (such as normalization, ohe, etc.).


Then, I tried to use H2O algorithm and got following error:


from h2o.estimators.gbm import H2OGradientBoostingEstimator
classifier  =  H2OGradientBoostingEstimator(nfolds =    5,
                                            ntrees =   15,
                                            seed   =    42,
                                            max_depth = 4)

classifier.train(predictors, target, training_frame = train_data)





H2OTypeError: Argument 
x
 should be a None | integer | string |
ModelBase | list(string | integer) | set(integer | string), got
H2OFrame




My target is 
train_data[""Survived""].asfactor()


I tried to read to dataframe from file, instead of coverting the preprocessed df into H2OFrame but to no vail.


Any ideas would be appreciated.","['python', 'data-science', 'h2o']",Stanislav Jirák,https://stackoverflow.com/users/10978122/stanislav-jir%c3%a1k,485
73995371,73995371,2022-10-08T08:03:02,2022-10-08 08:03:02Z,114,"I am trying to learn how to use PySpark and H20 and most of the concepts underneath are very new to me. I want to implement a random forest algorithm using H2O but I have a Spark dataframe to work with. I am using Google Colab Notebook. Below is how my spark dataframe and environment were set up:


# Install java
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# get spark
VERSION='3.2.2'
!wget https://dlcdn.apache.org/spark/spark-$VERSION/spark-$VERSION-bin-hadoop3.2.tgz

# decompress spark
!tar xf spark-$VERSION-bin-hadoop3.2.tgz

# install python package to help with system paths
!pip install -q findspark'''
# Let Colab know where the java and spark folders are

import os
os.environ[""JAVA_HOME""] = ""/usr/lib/jvm/java-8-openjdk-amd64""
os.environ[""SPARK_HOME""] = f""/content/spark-{VERSION}-bin-hadoop3.2""

#add pyspark to sys.path using findspark
import findspark
findspark.init()

# get a spark session
from pyspark.sql import SparkSession
spark = SparkSession.builder.master(""local[*]"").getOrCreate()
sc = spark.sparkContext



Obtaining the data from Google cloud Storage buckets


! wget https://storage.googleapis.com/bdt-spark-store/internal_data.csv -O gcs_internal_data.csv

! wget https://storage.googleapis.com/bdt-spark-store/external_sources.csv -O gcs_external_sources.csv


#Reading into spark

df1 = spark.read.csv('/content/gcs_internal_data.csv', sep =',', inferSchema=True, 
header = True)


df2 = spark.read.csv('/content/gcs_external_sources.csv', sep =',', inferSchema=True, 
header = True)

joinedDF = df1.join(df2, df1.SK_ID_CURR == df2.SK_ID_CURR)



filtering the data


df_spk = joinedDF.select('EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',
                          'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE',
                           'DAYS_ID_PUBLISH', 'CODE_GENDER', 'AMT_ANNUITY',
                           'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',
                           'ORGANIZATION_TYPE', 'DAYS_LAST_PHONE_CHANGE',
                            'NAME_INCOME_TYPE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE', 
                            'TARGET')



Convert spark dataframe to H2O frame


! apt-get install default-jre
! java -version 
! pip install h2o_pysparkling_3.2


import pysparkling
 
from pysparkling import *
hc= pysparkling.context.H2OContext(sc)



this is the error I get


TypeError                                 Traceback (most recent call last)
<ipython-input-51-507df42899af> in <module>
----> 1 hc= pysparkling.context.H2OContext(sc)

TypeError: __init__() takes 1 positional argument but 2 were given



and if I run:


h2oFrame = hc.asH2OFrame(df_spk)



I get the error:


> AttributeError                            Traceback (most recent call last)
><ipython-input-49-d563908a6983> in <module>
>----> 1 h2oFrame = hc.asH2OFrame(df_spk)

>/usr/local/lib/python3.7/dist-packages/ai/h2o/sparkling/H2OContext.py in 
asH2OFrame(self, >sparkFrame, h2oFrameName, fullCols)
>    199         assert_is_type(sparkFrame, DataFrame, RDD)
>    200 
>--> 201         df = H2OContext.__prepareSparkDataForConversion(self._jvm, sparkFrame)
>    202         if h2oFrameName is None:
>    203             key = self._jhc.asH2OFrame(df._jdf).frameId()
>
>AttributeError: 'H2OContext' object has no attribute '_jvm","['pyspark', 'h2o']",Katlego_mich,https://stackoverflow.com/users/16274568/katlego-mich,31
73903859,73903859,2022-09-30T03:55:51,2022-09-30 08:15:29Z,457,"I am using 
h2o
 automl library from python with scikit-learn wrapper to create a pipeline for training my model. I follow 
this example
, recommended by the official documentation:


from sklearn import datasets
from sklearn.feature_selection import f_classif, SelectKBest
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures

from h2o.sklearn import H2OAutoMLClassifier


X_classes_train, X_classes_test, y_classes_train, y_classes_test = train_test_split(X_classes, y_classes, test_size=0.33)

pipeline = Pipeline([
    ('polyfeat', PolynomialFeatures(degree=2)),
    ('featselect', SelectKBest(f_classif, k=5)),
    ('classifier', H2OAutoMLClassifier(max_models=10, seed=2022, sort_metric='logloss'))
])

pipeline.fit(X_classes_train, y_classes_train)
preds = pipeline.predict(X_classes_test)



So, I've trained my pipeline/model, now I want to get an 
H2OAutoML
 object out of 
H2OAutoMLClassifier
 wrapper to invoke 
.explain()
 method on it and get some insight about the features and models.


How do I do that?","['python', 'scikit-learn', 'h2o', 'automl']",Boris Burkov,https://stackoverflow.com/users/1360544/boris-burkov,14.3k
73831597,73831597,2022-09-23T18:21:48,2022-09-26 13:39:12Z,0,"I'm wondering why 
h2o.performance
 report is different from the standard definition of 
rmse
 on the test data. 
h2o
's performance report seems to overstate.


Below is a reprex.



iris_h2o = as.h2o(iris)
parts = h2o.splitFrame(iris_h2o, ratios = c(0.5,0.25), seed = 1)
train = parts[[1]]
valid = parts[[2]]
test = parts[[3]]

x = c('Sepal.Width','Petal.Length','Petal.Width')
y = 'Sepal.Length'
auto_gbm = h2o.automl(x= x,
                      y= y,
                      training_frame = train,
                      validation_frame = valid,
                      nfolds = 0,
                      include_algos = c('GBM'),
                      max_models = 5,
                      seed = 1
                      )
best_gbm = h2o.get_best_model(auto_gbm)
 
h2o.performance(best_gbm, test)



The above performance result is


H2ORegressionMetrics: gbm

MSE:  0.1152907
RMSE:  0.3395449
MAE:  0.2675279
RMSLE:  0.04744378
Mean Residual Deviance :  0.1152907




However, when I generate a prediction on the test dataset and calculate 
RMSE
 manually, the value diverges a lot.


rmse = function(y, y_predict){
  N = length(y)
  RMSE = sqrt(sum((y-y_predict)^2,na.rm=T)/N)
  return(RMSE)
}

test['predicted'] = h2o.predict(best_gbm, test)

rmse(test['Sepal.Length'], test['predicted'])

[1] 1.890506




H2O's performance report on RMSE : 0.33


Manual calculation on RMSE : 1.89


which is more than 5 times bigger. Why am I seeing this inconsistency?


H2O cluster version:        3.36.1.4","['r', 'h2o']",Unknown,,N/A
73804485,73804485,2022-09-21T17:00:15,2022-10-19 06:03:15Z,0,"I'm wondering how to standardize features when using 
h2o
's 
AutoML
 with deep learning and GLM algorithms.


Seems it is supported to deep learning and GLM models (
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/standardize.html
), but in 
h2o.automl
 it does not accept the 
standardize = TRUE
 argument.


My questions are:




Does autoML automatically scales (i.e. standardizes) the features when deeplearning or GLM algorithm is used?




If true, does it automatically standardize also when I predict on new test data?




If 1) is not true, is there a built-in 
h2o
 function that achieves this so that I can do it manually? What's the recommended workflow for this with 
AutoML
?","['r', 'h2o', 'h2o.ai']",Matthew Son,https://stackoverflow.com/users/10484383/matthew-son,"1,385"
73548237,73548237,2022-08-30T20:41:24,2022-08-31 16:03:28Z,296,"Here is my code:


import pandas as pd
import h2o

h2o.init(nthreads = 5)
my_df = pd.DataFrame({'col1':[12,3,4,5,45,6,45,7,4,2],
                      'col2': [12,3,1,23,43,1,3,12,32,1],
                      'y':[0,1,1,0,1,0,1,1,0,0]})
my_df_h2o = h2o.H2OFrame(my_df)
aml = h2o.automl.H2OAutoML(max_runtime_secs=30, seed=123)
aml.train(x=['col1', 'col2'], y='y', training_frame=my_df_h2o)



it returns:


Traceback (most recent call last):
  File ""D:\user_data\Program_Files\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 3441, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-9-031544adf3aa>"", line 1, in <module>
    aml.train(x=['col1', 'col2'], y='y', training_frame=my_df_h2o)
  File ""D:\user_data\Program_Files\Anaconda3\envs\tensorflow\lib\site-packages\h2o\automl\autoh2o.py"", line 557, in train
    resp = self._build_resp = h2o.api('POST /99/AutoMLBuilder', json=automl_build_params)
  File ""D:\user_data\Program_Files\Anaconda3\envs\tensorflow\lib\site-packages\h2o\h2o.py"", line 113, in api
    return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
  File ""D:\user_data\Program_Files\Anaconda3\envs\tensorflow\lib\site-packages\h2o\backend\connection.py"", line 481, in request
    return self._process_response(resp, save_to)
  File ""D:\user_data\Program_Files\Anaconda3\envs\tensorflow\lib\site-packages\h2o\backend\connection.py"", line 823, in _process_response
    raise H2OServerError(""HTTP %d %s:\n%s"" % (status_code, response.reason, data))
h2o.exceptions.H2OServerError: HTTP 500 Server Error:
Server error java.lang.NullPointerException:
  Error: Caught exception: java.lang.NullPointerException
  Request: None



The h2o server seems to be running just fine. I am not sure how to troubleshoot.


UPDATE


As per suggestion below here are the log (available from web gui). This only partial log file as full log did not fit:


08-31 10:50:23.742 127.0.0.1:54321       20836        main  INFO water.default: H2O started in 5837ms
08-31 10:50:23.742 127.0.0.1:54321       20836        main  INFO water.default: 
08-31 10:50:23.743 127.0.0.1:54321       20836        main  INFO water.default: Open H2O Flow in your web browser: http://127.0.0.1:54321
08-31 10:50:23.743 127.0.0.1:54321       20836        main  INFO water.default: 
08-31 10:50:23.820 127.0.0.1:54321       20836  0228826-17  INFO water.default: GET /3/Metadata/schemas/CloudV3, parms: {}
08-31 10:50:23.847 127.0.0.1:54321       20836  0228826-17  INFO water.default: Locking cloud to new members, because water.api.schemas3.MetadataV3
08-31 10:50:23.984 127.0.0.1:54321       20836  0228826-12  INFO water.default: GET /3/Metadata/schemas/H2OErrorV3, parms: {}
08-31 10:50:23.995 127.0.0.1:54321       20836  0228826-13  INFO water.default: GET /3/Metadata/schemas/H2OModelBuilderErrorV3, parms: {}
08-31 10:50:24.042 127.0.0.1:54321       20836  0228826-11  INFO water.default: POST /4/sessions, parms: {}
08-31 10:50:24.055 127.0.0.1:54321       20836  0228826-17  INFO water.default: POST /99/Rapids, parms: {ast=(setTimeZone ""UTC""), session_id=_sid_9a7a}
08-31 10:50:24.468 127.0.0.1:54321       20836  0228826-15  INFO water.default: GET /3/Capabilities/API, parms: {}
08-31 10:50:37.122 127.0.0.1:54321       20836  ice Thread  WARN water.default: Unblock allocations; cache below desired, but also OOM: GC CALLBACK, (K/V:Zero   + POJO:198.8 MB + FREE:48.7 MB == MEM_MAX:247.5 MB), desiredKV=30.9 MB OOM!
08-31 10:56:12.695 127.0.0.1:54321       20836  0228826-10  INFO water.default: GET /3/Metadata/schemas/CloudV3, parms: {}
08-31 10:56:14.752 127.0.0.1:54321       20836  0228826-12  INFO water.default: GET /3/Metadata/schemas/H2OErrorV3, parms: {}
08-31 10:56:16.800 127.0.0.1:54321       20836  0228826-17  INFO water.default: GET /3/Metadata/schemas/H2OModelBuilderErrorV3, parms: {}
08-31 10:56:20.892 127.0.0.1:54321       20836  0228826-11  INFO water.default: POST /4/sessions, parms: {}
08-31 10:56:21.171 127.0.0.1:54321       20836  0228826-12  INFO water.default: GET /flow/index.html, parms: {}
08-31 10:56:21.730 127.0.0.1:54321       20836  0228826-25  INFO water.default: GET /flow/fonts/Lato-Regular.woff2, parms: {}
08-31 10:56:21.735 127.0.0.1:54321       20836  0228826-12  INFO water.default: GET /flow/fonts/fa-solid-900.woff2, parms: {}
08-31 10:56:21.736 127.0.0.1:54321       20836  0228826-15  INFO water.default: GET /flow/fonts/SourceCodePro-Regular.ttf.woff2, parms: {}
08-31 10:56:22.428 127.0.0.1:54321       20836  0228826-25  INFO water.default: GET /3/Metadata/endpoints, parms: {}
08-31 10:56:22.761 127.0.0.1:54321       20836  0228826-12  INFO water.default: GET /3/NodePersistentStorage/notebook, parms: {}
08-31 10:56:22.783 127.0.0.1:54321       20836  0228826-17  INFO water.default: GET /3/NodePersistentStorage/categories/environment/names/clips/exists, parms: {}
08-31 10:56:22.787 127.0.0.1:54321       20836  0228826-25  INFO water.default: GET /flow/help/catalog.json, parms: {}
08-31 10:56:22.798 127.0.0.1:54321       20836  0228826-17  INFO water.default: GET /3/About, parms: {}
08-31 10:56:22.804 127.0.0.1:54321       20836  0228826-12  INFO water.default: GET /3/ModelBuilders, parms: {}
08-31 10:56:22.938 127.0.0.1:54321       20836  0228826-11  INFO water.default: POST /99/Rapids, parms: {ast=(setTimeZone ""UTC""), session_id=_sid_9257}
08-31 10:56:23.041 127.0.0.1:54321       20836  0228826-15  INFO water.default: GET /flow/fonts/fa-regular-400.woff2, parms: {}
08-31 10:56:23.059 127.0.0.1:54321       20836  0228826-12  INFO water.default: GET /flow/fonts/Lato-Italic.woff2, parms: {}
08-31 10:56:27.015 127.0.0.1:54321       20836  0228826-17  INFO water.default: GET /3/Capabilities/API, parms: {}
08-31 10:57:06.289 127.0.0.1:54321       20836  0228826-15  INFO water.default: Reading byte InputStream into Frame:
08-31 10:57:06.290 127.0.0.1:54321       20836  0228826-15  INFO water.default:     frameKey:    upload_891ff2a7def056d5c12dba0cc8e04532
08-31 10:57:06.308 127.0.0.1:54321       20836  0228826-15  INFO water.default:     totalChunks: 1
08-31 10:57:06.308 127.0.0.1:54321       20836  0228826-15  INFO water.default:     totalBytes:  108
08-31 10:57:06.316 127.0.0.1:54321       20836  0228826-15  INFO water.default:     Success.
08-31 10:57:08.370 127.0.0.1:54321       20836  0228826-11  INFO water.default: POST /3/ParseSetup, parms: {single_quotes=False, source_frames=[""upload_891ff2a7def056d5c12dba0cc8e04532""], check_header=1, separator=44}
08-31 10:57:10.584 127.0.0.1:54321       20836  0228826-17  INFO water.default: POST /3/Parse, parms: {number_columns=3, source_frames=[""upload_891ff2a7def056d5c12dba0cc8e04532""], column_types=[""Numeric"",""Numeric"",""Numeric""], single_quotes=False, parse_type=CSV, destination_frame=Key_Frame__upload_891ff2a7def056d5c12dba0cc8e04532.hex, column_names=[""col1"",""col2"",""y""], delete_on_done=True, check_header=1, separator=44, blocking=False, escapechar=0, chunk_size=4194304}
08-31 10:57:10.586 127.0.0.1:54321       20836  0228826-17  INFO water.default: Total file size:  108  B
08-31 10:57:10.587 127.0.0.1:54321       20836  0228826-17  INFO water.default: Parse chunk size 4194304
08-31 10:57:10.610 127.0.0.1:54321       20836     FJ-2-15  INFO water.default: Key upload_891ff2a7def056d5c12dba0cc8e04532 will be parsed using method DistributedParse.
08-31 10:57:10.640 127.0.0.1:54321       20836     FJ-1-15  INFO water.default: Parse result for Key_Frame__upload_891ff2a7def056d5c12dba0cc8e04532.hex (10 rows, 3 columns):
08-31 10:57:10.648 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:  ColV2    type          min          max         mean        sigma         NAs constant cardinality
08-31 10:57:10.649 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:  col1: numeric      2.00000      45.0000      13.3000      16.9316                            
08-31 10:57:10.649 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:  col2: numeric      1.00000      43.0000      13.1000      14.8881                            
08-31 10:57:10.649 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:     y: numeric      0.00000      1.00000     0.500000     0.527046                            
08-31 10:57:10.658 127.0.0.1:54321       20836     FJ-1-15  INFO water.default: Chunk compression summary:
08-31 10:57:10.659 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:   Chunk Type                 Chunk Name       Count  Count Percentage        Size  Size Percentage
08-31 10:57:10.659 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:          CBS                     Binary           1          33.333 %       72  B         31.579 %
08-31 10:57:10.659 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:          C1N  1-Byte Integers (w/o NAs)           2          66.667 %      156  B         68.421 %
08-31 10:57:10.659 127.0.0.1:54321       20836     FJ-1-15  INFO water.default: Frame distribution summary:
08-31 10:57:10.659 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:                        Size  Number of Rows  Number of Chunks per Column  Number of Chunks
08-31 10:57:10.659 127.0.0.1:54321       20836     FJ-1-15  INFO water.default: 127.0.0.1:54321      228  B              10                            1                 3
08-31 10:57:10.659 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:            mean      228  B       10.000000                     1.000000          3.000000
08-31 10:57:10.659 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:             min      228  B       10.000000                     1.000000          3.000000
08-31 10:57:10.660 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:             max      228  B       10.000000                     1.000000          3.000000
08-31 10:57:10.660 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:          stddev        0  B        0.000000                     0.000000          0.000000
08-31 10:57:10.660 127.0.0.1:54321       20836     FJ-1-15  INFO water.default:           total      228  B              10                            1                 3
08-31 10:57:14.722 127.0.0.1:54321       20836  0228826-15  INFO water.default: GET /3/Frames/Key_Frame__upload_891ff2a7def056d5c12dba0cc8e04532.hex, parms: {column_offset=0, full_column_count=-1, row_count=10, row_offset=0, column_count=-1}
08-31 10:57:19.923 127.0.0.1:54321       20836  0228826-11  INFO water.default: GET /3/Metadata/schemas/AutoMLV99, parms: {}
08-31 10:57:26.195 127.0.0.1:54321       20836  0228826-17  INFO water.default: POST /99/AutoMLBuilder, parms: {}
08-31 10:57:26.221 127.0.0.1:54321       20836  0228826-17  INFO water.default: Workflow: Project: AutoML_1_20220831_105726
08-31 10:57:26.226 127.0.0.1:54321       20836  0228826-17  INFO water.default: Validation: Setting stopping tolerance adaptively based on the training frame: 0.05
08-31 10:57:26.226 127.0.0.1:54321       20836  0228826-17  INFO water.default: Validation: Build control seed: 123
08-31 10:57:26.226 127.0.0.1:54321       20836  0228826-17  INFO water.default: DataImport: training frame: Frame key: AutoML_1_20220831_105726_training_Key_Frame__upload_891ff2a7def056d5c12dba0cc8e04532.hex    cols: 3    rows: 10  chunks: 1    size: 228  checksum: -2705803362319216029
08-31 10:57:26.226 127.0.0.1:54321       20836  0228826-17  INFO water.default: DataImport: validation frame: NULL
08-31 10:57:26.227 127.0.0.1:54321       20836  0228826-17  INFO water.default: DataImport: leaderboard frame: NULL
08-31 10:57:26.227 127.0.0.1:54321       20836  0228826-17  INFO water.default: DataImport: blending frame: NULL
08-31 10:57:26.227 127.0.0.1:54321       20836  0228826-17  INFO water.default: DataImport: response column: y
08-31 10:57:26.227 127.0.0.1:54321       20836  0228826-17  INFO water.default: DataImport: fold column: null
08-31 10:57:26.227 127.0.0.1:54321       20836  0228826-17  INFO water.default: DataImport: weights column: null
08-31 10:57:26.234 127.0.0.1:54321       20836  0228826-17  WARN water.default: Workflow: AutoML: XGBoost is not available; skipping it.
08-31 10:57:26.234 127.0.0.1:54321       20836  0228826-17  INFO water.default: Workflow: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (6g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (6g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (10g, 60w)]}, {StackedEnsemble : [best_of_family_1 (1g, 5w), best_of_family_2 (2g, 5w), best_of_family_3 (3g, 5w), best_of_family_4 (4g, 5w), best_of_family_5 (5g, 5w), all_2 (2g, 10w), all_3 (3g, 10w), all_4 (4g, 10w), all_5 (5g, 10w), monotonic (6g, 10w), best_of_family_xgboost (6g, 10w), best_of_family_gbm (6g, 10w), all_xgboost (7g, 10w), all_gbm (7g, 10w), best_of_family_xglm (8g, 10w), all_xglm (8g, 10w), best_of_family (10g, 10w), best_N (10g, 10w)]}]
08-31 10:57:26.235 127.0.0.1:54321       20836  0228826-17 ERROR water.default: 
java.lang.NullPointerException: null
    at water.nbhm.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:369) ~[h2o.jar:?]
    at water.nbhm.NonBlockingHashMap.put(NonBlockingHashMap.java:320) ~[h2o.jar:?]
    at ai.h2o.automl.AutoMLSession.getModelingSteps(AutoMLSession.java:76) ~[h2o.jar:?]
    at ai.h2o.automl.ModelingStepsRegistry.getOrderedSteps(ModelingStepsRegistry.java:54) ~[h2o.jar:?]
    at ai.h2o.automl.AutoML.getExecutionPlan(AutoML.java:330) ~[h2o.jar:?]
    at ai.h2o.automl.AutoML.planWork(AutoML.java:359) ~[h2o.jar:?]
    at ai.h2o.automl.AutoML.submit(AutoML.java:395) ~[h2o.jar:?]
    at ai.h2o.automl.AutoML.startAutoML(AutoML.java:80) ~[h2o.jar:?]
    at water.automl.api.AutoMLBuilderHandler.build(AutoMLBuilderHandler.java:15) ~[h2o.jar:?]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_341]
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_341]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_341]
    at java.lang.reflect.Method.invoke(Unknown Source) ~[?:1.8.0_341]
    at water.api.Handler.handle(Handler.java:60) ~[h2o.jar:?]
    at water.api.RequestServer.serve(RequestServer.java:470) [h2o.jar:?]
    at water.api.RequestServer.doGeneric(RequestServer.java:301) [h2o.jar:?]
    at water.api.RequestServer.doPost(RequestServer.java:227) [h2o.jar:?]
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:707) [h2o.jar:?]
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [h2o.jar:?]
    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865) [h2o.jar:?]
    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535) [h2o.jar:?]
    at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255) [h2o.jar:?]
    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317) [h2o.jar:?]
    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203) [h2o.jar:?]
    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473) [h2o.jar:?]
    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201) [h2o.jar:?]
    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219) [h2o.jar:?]
    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144) [h2o.jar:?]
    at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126) [h2o.jar:?]
    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132) [h2o.jar:?]
    at water.webserver.jetty9.Jetty9ServerAdapter$LoginHandler.handle(Jetty9ServerAdapter.java:130) [h2o.jar:?]
    at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126) [h2o.jar:?]
    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132) [h2o.jar:?]
    at org.eclipse.jetty.server.Server.handle(Server.java:531) [h2o.jar:?]
    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352) [h2o.jar:?]
    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260) [h2o.jar:?]
    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281) [h2o.jar:?]
    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102) [h2o.jar:?]
    at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118) [h2o.jar:?]
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333) [h2o.jar:?]
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310) [h2o.jar:?]
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168) [h2o.jar:?]
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126) [h2o.jar:?]
    at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366) [h2o.jar:?]
    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762) [h2o.jar:?]
    at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680) [h2o.jar:?]
    at java.lang.Thread.run(Unknown Source) [?:1.8.0_341]
08-31 10:57:26.242 127.0.0.1:54321       20836  0228826-17 ERROR water.default: Caught exception: 
08-31 10:57:26.242 127.0.0.1:54321       20836  0228826-17 ERROR water.default: 
08-31 10:57:26.242 127.0.0.1:54321       20836  0228826-17 ERROR water.default: ERROR MESSAGE:
08-31 10:57:26.242 127.0.0.1:54321       20836  0228826-17 ERROR water.default: 
08-31 10:57:26.243 127.0.0.1:54321       20836  0228826-17 ERROR water.default: Caught exception: java.lang.NullPointerException from: water.nbhm.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:369)
08-31 10:57:26.243 127.0.0.1:54321       20836  0228826-17 ERROR water.default: 
08-31 10:57:26.243 127.0.0.1:54321       20836  0228826-17 ERROR water.default: ; Stacktrace: [java.lang.NullPointerException,     water.nbhm.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:369),     water.nbhm.NonBlockingHashMap.put(NonBlockingHashMap.java:320),     ai.h2o.automl.AutoMLSession.getModelingSteps(AutoMLSession.java:76),     ai.h2o.automl.ModelingStepsRegistry.getOrderedSteps(ModelingStepsRegistry.java:54),     ai.h2o.automl.AutoML.getExecutionPlan(AutoML.java:330),     ai.h2o.automl.AutoML.planWork(AutoML.java:359),     ai.h2o.automl.AutoML.submit(AutoML.java:395),     ai.h2o.automl.AutoML.startAutoML(AutoML.java:80),     water.automl.api.AutoMLBuilderHandler.build(AutoMLBuilderHandler.java:15),     sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method),     sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source),     sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source),     java.lang.reflect.Method.invoke(Unknown Source),     water.api.Handler.handle(Handler.java:60),     water.api.RequestServer.serve(RequestServer.java:470),     water.api.RequestServer.doGeneric(RequestServer.java:301),     water.api.RequestServer.doPost(RequestServer.java:227),     javax.servlet.http.HttpServlet.service(HttpServlet.java:707),     javax.servlet.http.HttpServlet.service(HttpServlet.java:790),     org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865),     org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535),     org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255),     org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317),     org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203),     org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473),     org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201),     org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219),     org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144),     org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126),     org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132),     water.webserver.jetty9.Jetty9ServerAdapter$LoginHandler.handle(Jetty9ServerAdapter.java:130),     org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126),     org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132),     org.eclipse.jetty.server.Server.handle(Server.java:531),     org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352),     org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260),     org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281),     org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102),     org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118),     org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333),     org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310),     org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168),     org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126),     org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366),     org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762),     org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680),     java.lang.Thread.run(Unknown Source)];parms={}
08-31 10:59:11.676 127.0.0.1:54321       20836  0228826-10  INFO water.default: GET /3/Logs/nodes/self/files/info, parms: {}
08-31 10:59:11.701 127.0.0.1:54321       20836  0228826-13  INFO water.default: GET /3/Logs/nodes/127.0.0.1:54321/files/info, parms: {}","['python', 'nullpointerexception', 'h2o', 'automl']",Unknown,,N/A
73532614,73532614,2022-08-29T17:36:32,2022-08-30 18:14:50Z,0,"H2O document doesn't detail on these two hyper parameters. It only says these are L1 and L2 regularization parameters, with default values as 0 and 1. I can't find more info googling it either. Can someone provide any insight?  TIA!",['h2o'],SSSGGG,https://stackoverflow.com/users/19786641/sssggg,11
73419125,73419125,2022-08-19T15:34:56,2022-08-30 03:48:13Z,499,"I have a dataset of around 1M rows with a high imbalance (743 / 1072780). I am training xgboost model in h2o with the following parameters and it looks like it is overfitting


H2OXGBoostEstimator(max_depth=10,
                                   subsample=0.7,
                                   ntrees=200,
                                   learn_rate=0.5,
                                   min_rows=3,
                                   col_sample_rate_per_tree = .75,
                                   reg_lambda=2.0,
                                   reg_alpha=2.0,
                                   sample_rate = .5,
                                   booster='gbtree',
                                   nfolds=10,
                                   keep_cross_validation_predictions = True,
                                   stopping_metric = 'AUCPR',
                                   min_split_improvement= 1e-5,
                                   categorical_encoding  = 'OneHotExplicit',
                                    weights_column = ""Products""
                                  )



The output is:


Training data AUCPR: 0.6878932664592388       Validation data AUCPR: 0.04033158660014747
Training data AUC: 0.9992170372214433           Validation data AUC: 0.7000804189162043
Training data MSE: 0.0005722912424124134           Validation data MSE: 0.0010002949568585474
Training data RMSE: 0.023922609439866994         Validation data RMSE: 0.03162743993526108
Training data Gini: 0.9984340744428866         Validation data Gini: 0.40016083783240863
Confusion Matrix for Training Data:
 
Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.15900755567210062: 
       0       1    Error    Rate
-----  ------  ---  -------  ----------------
0      709201  337  0.0005   (337.0/709538.0)
1      189     516  0.2681   (189.0/705.0)
Total  709390  853  0.0007   (526.0/710243.0)

Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.047459165255228676: 
       0       1    Error    Rate
-----  ------  ---  -------  ----------------
0      202084  365  0.0018   (365.0/202449.0)
1      140     52   0.7292   (140.0/192.0)
Total  202224  417  0.0025   (505.0/202641.0)
{'train': , 'valid': }



I am using h2o 3.32.0.1 version (since it's a requirement), xgboost h2o doesnt support balance_classes or scale_pos_weight hyperparameters.


What can cause this to have such performance? Also, What can be improved here for such an imbalanced dataset that might improve the performance?","['machine-learning', 'xgboost', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
73404520,73404520,2022-08-18T14:08:49,2022-08-23 08:59:05Z,107,"I am having problems in using future package in h2o:


 base) usr@usr-X705UDR:~$ pip install future
        Requirement already satisfied: future in /usr/lib/python3/dist-packages (0.18.2)
        (base) usr@usr-X705UDR:~$ python
        Python 3.8.12 (default, Oct 12 2021, 13:49:34) 
        [GCC 7.5.0] :: Anaconda, Inc. on linux
        Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
        >>> import future
        Traceback (most recent call last):
          File ""<stdin>"", line 1, in <module>
        ModuleNotFoundError: No module named 'future'
        >>> 



How to rectify this?","['python', 'python-3.x', 'future', 'h2o']",Mad Physicist,https://stackoverflow.com/users/2988730/mad-physicist,113k
73355181,73355181,2022-08-14T21:17:49,2022-08-15 12:53:18Z,533,"Hey I am using anaconda environment, and have successfully installed h20-py library and all. It's just that when I try to run h2o.init() it gives me the following error




H2OConnectionError                        Traceback (most recent call last)
~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/h2o/h2o.py in init(url, ip, port, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, **kwargs)
    251                                      _msgs=(""Checking whether there is an H2O instance running at {url}"",
--> 252                                             ""connected."", ""not found.""))
    253     except H2OConnectionError:

~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/h2o/backend/connection.py in open(server, url, ip, port, https, auth, verify_ssl_certificates, proxy, cookies, verbose, _msgs)
    317             conn._timeout = 3.0
--> 318             conn._cluster = conn._test_connection(retries, messages=_msgs)
    319             # If a server is unable to respond within 1s, it should be considered a bug. However we disable this

~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/h2o/backend/connection.py in _test_connection(self, max_retries, messages)
    587             raise H2OConnectionError(""Could not establish link to the H2O cloud %s after %d retries\n%s""
--> 588                                      % (self._base_url, max_retries, ""\n"".join(errors)))
    589 

H2OConnectionError: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries
[06:14.50] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd0f913b390>: Failed to establish a new connection: [Errno 61] Connection refused',))
[06:14.71] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd0f913ba58>: Failed to establish a new connection: [Errno 61] Connection refused',))
[06:14.92] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd1186fd160>: Failed to establish a new connection: [Errno 61] Connection refused',))
[06:15.14] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd1186fd828>: Failed to establish a new connection: [Errno 61] Connection refused',))
[06:15.35] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd1186fdef0>: Failed to establish a new connection: [Errno 61] Connection refused',))

During handling of the above exception, another exception occurred:

H2OStartupError                           Traceback (most recent call last)
<ipython-input-3-95453bf1556d> in <module>
----> 1 h2o.init()

~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/h2o/h2o.py in init(url, ip, port, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, **kwargs)
    259             raise H2OConnectionError('Can only start H2O launcher if IP address is localhost.')
    260         hs = H2OLocalServer.start(nthreads=nthreads, enable_assertions=enable_assertions, max_mem_size=mmax,
--> 261                                   min_mem_size=mmin, ice_root=ice_root, port=port, extra_classpath=extra_classpath)
    262         h2oconn = H2OConnection.open(server=hs, https=https, verify_ssl_certificates=not insecure,
    263                                      auth=auth, proxy=proxy,cookies=cookies, verbose=True)

~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/h2o/backend/server.py in start(jar_path, nthreads, enable_assertions, max_mem_size, min_mem_size, ice_root, port, extra_classpath, verbose)
    110         hs = H2OLocalServer()
    111         hs._verbose = bool(verbose)
--> 112         hs._jar_path = hs._find_jar(jar_path)
    113         hs._extra_classpath = extra_classpath
    114         hs._ice_root = ice_root

~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/h2o/backend/server.py in _find_jar(self, path0)
    204                 return jp
    205         raise H2OStartupError(""Cannot start local server: h2o.jar not found. Paths searched:\n"" +
--> 206                               """".join(""    %s\n"" % s for s in searched_paths))
    207 
    208     @staticmethod

H2OStartupError: Cannot start local server: h2o.jar not found. Paths searched:
    /Users/sharozearcher/opt/anaconda3/envs/py36/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar
    /Users/sharozearcher/opt/anaconda3/envs/py36/h2o_jar/h2o.jar
    /usr/local/h2o_jar/h2o.jar
    /Users/sharozearcher/opt/anaconda3/envs/py36/local/h2o_jar/h2o.jar
    /Users/sharozearcher/.local/h2o_jar/h2o.jar
    /Users/sharozearcher/opt/anaconda3/envs/py36/h2o_jar/h2o.jar



Any help would be appreciated.


P.S. I'm using Jupyter Notebook","['python', 'jupyter-notebook', 'anaconda', 'h2o']",Sharoze Archer,https://stackoverflow.com/users/19764102/sharoze-archer,1
73316915,73316915,2022-08-11T07:35:13,2022-08-11 13:26:34Z,225,"i am using spark standalone cluster and running h2o pysparkling in it.
I am unable to find the function for getting the leader feature importances. please help


Code:


import pandas as pd
from pyspark.sql import SparkSession
from pysparkling import *
import h2o
from pyspark import SparkFiles
from pysparkling.ml import H2OAutoML
spark = SparkSession.builder.appName('SparkApplication').getOrCreate()

conf = H2OConf()
hc = H2OContext.getOrCreate(conf)

def xgb_automl_features_importance(data, target_metric):
    # Converting DataFrame in H2OFrame
    hf = h2o.H2OFrame(data)
    sparkDF = hc.asSparkFrame(hf)
    # Identify predictors and response
    y = target_metric
    aml = H2OAutoML(labelCol=y)
    aml.setIncludeAlgos([""XGBoost""])
    aml.setMaxModels(1)
    aml.fit(sparkDF)
    print('-----------****************')
    print(aml.getLeaderboard().show(truncate=False))","['h2o', 'sparkling-water']",Priyadutt Bhatt,https://stackoverflow.com/users/19646755/priyadutt-bhatt,13
73294368,73294368,2022-08-09T15:28:21,2022-08-09 16:14:53Z,81,"I'm not able to understand the models generated by the 
H20 automl
!


The output is like this, for example:

StackedEnsemble_AllModels_1_AutoML_1_20220809_134944


How can I know which base models used by stacked?","['python', 'h2o', 'automl']",Jamiu S.,https://stackoverflow.com/users/19290081/jamiu-s,"5,693"
73278630,73278630,2022-08-08T13:26:44,2022-08-08 13:26:44Z,111,"I am using H2O Auto ML and while comparing the time for XGBoost and D-RandomForest, DRF is taking a lot more time than XGBoost!


Code for DRF:


x = hf.columns
y = target_metric
x.remove(y)
aml = H2OAutoML(max_models=1, seed=1, include_algos=[""DRF""])
aml.train(x=x, y=y, training_frame=hf)
m = aml.leader
varimp = m.varimp(use_pandas=True)



Code for XGboost:


x = hf.columns
y = target_metric
x.remove(y)
aml = H2OAutoML(max_models=1, seed=1, include_algos=[""XGBoost""])
aml.train(x=x, y=y, training_frame=hf)
m = aml.leader
varimp = m.varimp(use_pandas=True)



H2O-AUTOML","['xgboost', 'h2o', 'automl', 'h2o.ai']",Priyadutt Bhatt,https://stackoverflow.com/users/19646755/priyadutt-bhatt,13
73276305,73276305,2022-08-08T10:17:00,2022-08-08 10:57:51Z,133,"Code:


import pandas as pd
from pyspark.sql import SparkSession
from pysparkling import *
import h2o
from pysparkling.ml import H2OAutoML
spark = SparkSession.builder.appName('SparkApplication').getOrCreate()
hc = H2OContext.getOrCreate()



Spark-submit Command:




spark-submit --master spark://local:7077 --py-files
sparkling-water-3.36.1.3-1-3.2/py/h2o_pysparkling_3.2-3.36.1.3-1-3.2.zip
--conf ""spark.ext.h2o.backend.cluster.mode=external"" --conf spark.ext.h2o.external.start.mode=""auto"" --conf
spark.ext.h2o.external.h2o.driver=""/home/whiz/spark/h2odriver-3.36.1.3.jar""
--conf spark.ext.h2o.external.cluster.size=2 spark_h20/h2o_script.py




Error Logs:
py4j.protocol.Py4JJavaError: An error occurred while calling o58.getOrCreate.
: java.io.IOException: Cannot run program ""hadoop"": error=2, No such file or directory**","['apache-spark', 'h2o', 'sparkling-water']",Priyadutt Bhatt,https://stackoverflow.com/users/19646755/priyadutt-bhatt,13
73267375,73267375,2022-08-07T12:22:09,2022-08-07 12:45:25Z,0,"I am running a H2o deep learning model in R (binary classification) using the following grid search code


hyper_params <- list(
    activation = c(""Rectifier"", ""Maxout"", ""Tanh"", ""RectifierWithDropout"", ""MaxoutWithDropout"", ""TanhWithDropout""),
    hidden = list(c(5, 5, 5, 5, 5), c(10, 10, 10, 10), c(50, 50, 50), c(100, 100, 100)),
    epochs = c(50, 100, 200),
    l1 = c(0, 0.00001, 0.0001),
    l2 = c(0, 0.00001, 0.0001),
    rate = c(0, 01, 0.005, 0.001),
    rate_annealing = c(1e-8, 1e-7, 1e-6),
    rho = c(0.9, 0.95, 0.99, 0.999),
    epsilon = c(1e-10, 1e-8, 1e-6, 1e-4),
    momentum_start = c(0, 0.5),
    momentum_stable = c(0.99, 0.5, 0),
    input_dropout_ratio = c(0, 0.1, 0.2),
    max_w2 = c(10, 100, 1000, 3.4028235e+38)
)
search_criteria <- list(strategy = ""RandomDiscrete"",
                                                max_models = 100,
                                                max_runtime_secs = 12000,
                                                stopping_tolerance = 0.001,
                                                stopping_rounds = 15,
                                                seed = 42)
dl_grid <- h2o.grid(algorithm = ""deeplearning"",
                                        x = x,
                                        y = y,
                                        grid_id = ""dl_grid"",
                                        training_frame = df,
                                        validation_frame = tst,
                                        nfolds = 25,                          
                                        fold_assignment = ""Stratified"",
                                        hyper_params = hyper_params,
                                        search_criteria = search_criteria,
                                        seed = 42
)



However I am getting the error




""na .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix, : Unexpected CURL error: Timeout was reached: [localhost:54321] Resolving timed out after 10180 milliseconds [1] ""Job request failed Unexpected CURL error: Timeout was reached: [localhost:54321] Resolving timed out after 10180 milliseconds, will retry after 3s.""




My R version details are


platform       x86_64-w64-mingw32

arch           x86_64

os             mingw32

crt            ucrt

system         x86_64, mingw32

status

major          4

minor          2.0

year           2022

month          04

day            22

svn rev        82229

language       R

version.string R version 4.2.0 (2022-04-22 ucrt)
nickname       Vigorous Calisthenics****","['r', 'deep-learning', 'h2o', 'rcurl', 'h2o.ai']",Donald Seinen,https://stackoverflow.com/users/14853907/donald-seinen,"4,419"
73255104,73255104,2022-08-05T20:45:13,2022-08-05 20:45:13Z,107,"I followed the directions to create an H2o cluster in K8s 
here
.  The headless service description that I deployed is:


apiVersion: v1
kind: Service
metadata:
  name: h2o-service
  namespace: dasi-ml-elr-dev
spec:
  type: ClusterIP
  clusterIP: None
  selector:
    app: h2o-k8s
  ports:
  - protocol: TCP
    port: 54321



and the StatefulSet description that I deployed is:


apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: h2o-stateful-set
  namespace: dasi-ml-elr-dev 
spec:
  serviceName: h2o-service
  podManagementPolicy: ""Parallel""
  replicas: 3
  selector:
    matchLabels:
      app: h2o-k8s
  template:
    metadata:
      labels:
        app: h2o-k8s
    spec:
      containers:
      - name: h2o-k8s
        image: h2oai/h2o-open-source-k8s:latest
        resources:
          limits:
            cpu: ""4000m""
            memory: ""8Gi""
          requests:
            cpu: ""4000m""
            memory: ""8Gi"" 
        ports:
          - containerPort: 54321
            protocol: TCP
        readinessProbe:
          httpGet:
            path: /kubernetes/isLeaderNode
            port: 8081
          initialDelaySeconds: 5
          periodSeconds: 5
          failureThreshold: 1
        env:
        - name: H2O_KUBERNETES_SERVICE_DNS
          value: h2o-service.dasi-ml-elr-dev.svc.cluster.local 
        - name: H2O_NODE_LOOKUP_TIMEOUT
          value: '180'
        - name: H2O_NODE_EXPECTED_COUNT
          value: '3'
        - name: H2O_KUBERNETES_API_PORT
          value: '8081'



I can see that the pods in the cluster are running:


$ kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
h2o-stateful-set-0          1/1     Running   0          5h27m
h2o-stateful-set-1          1/1     Running   0          5h27m
h2o-stateful-set-2          1/1     Running   0          5h27m



Although the logs from each pod include 
""Using the following pods to form H2O cluster: [192.168.38.24,192.168.32.78,192.168.16.159]""
 they also all include 
""Created cluster of size 1, leader node IP is 'h2o-stateful-set-1.h2o-service.dasi-ml-elr-dev.svc.cluster.local/192.168.32.78'""
 or 
""Created cluster of size 1, leader node IP is 'h2o-stateful-set-2.h2o-service.dasi-ml-elr-dev.svc.cluster.local/192.168.38.24'""
 or 
""Created cluster of size 1, leader node IP is '/192.168.16.159'""
 depending on which pod's log I'm looking at.  Each pod appears to think that it's the leader.  Does anyone know why and how to fix this?  Here is a full log example from one of the pods.


$ kubectl logs h2o-stateful-set-1
15:00:35.214 [main] WARN  hex.tree.xgboost.util.NativeLibrary - Cannot load library from path lib/linux_64/libxgboost4j_gpu.so
15:00:35.217 [main] WARN  hex.tree.xgboost.util.NativeLibrary - Failed to load library from both native path and jar!
15:00:35.218 [main] INFO  hex.tree.xgboost.util.NativeLibraryLoaderChain - Cannot load library: xgboost4j_gpu (lib/linux_64/libxgboost4j_gpu.so)
15:00:35.242 [main] INFO  hex.tree.xgboost.util.NativeLibrary - Loaded library from lib/linux_64/libxgboost4j_minimal.so (/tmp/libxgboost4j_minimal11832558774653271011.so)
15:00:35.256 [main] INFO  water.k8s.H2OCluster - Starting Kubernetes-related REST API services
15:00:35.273 [main] INFO  water.k8s.H2OCluster - Kubernetes REST API services successfully started.
15:00:35.273 [main] INFO  water.k8s.H2OCluster - Initializing H2O Kubernetes cluster
15:00:35.274 [main] INFO  water.k8s.H2OCluster - Timeout contraint: 180 seconds.
15:00:35.274 [main] INFO  water.k8s.H2OCluster - Cluster size constraint: 3 nodes.
15:00:35.289 [main] INFO  water.k8s.lookup.KubernetesDnsLookup - Timeout for node discovery is set to 180 seconds.
15:00:35.289 [main] INFO  water.k8s.lookup.KubernetesDnsLookup - Desired cluster size is set to 3 nodes.
15:00:35.346 [main] INFO  water.k8s.lookup.KubernetesDnsLookup - New H2O pod with DNS record 'h2o-stateful-set-0.h2o-service.dasi-ml-elr-dev.svc.cluster.local./192.168.16.159' discovered.
15:00:40.448 [main] INFO  water.k8s.lookup.KubernetesDnsLookup - New H2O pod with DNS record 'h2o-stateful-set-1.h2o-service.dasi-ml-elr-dev.svc.cluster.local./192.168.32.78' discovered.
15:02:30.513 [main] INFO  water.k8s.lookup.KubernetesDnsLookup - New H2O pod with DNS record 'h2o-stateful-set-2.h2o-service.dasi-ml-elr-dev.svc.cluster.local./192.168.38.24' discovered.
15:02:31.515 [main] INFO  water.k8s.H2OCluster - **Using the following pods to form H2O cluster: [192.168.38.24,192.168.32.78,192.168.16.159]**
2022-08-05 15:02:31.587:INFO::main: Logging initialized @117689ms to org.eclipse.jetty.util.log.StdErrLog
08-05 15:02:31.778 192.168.32.78:54321   1            main  INFO water.default: Dynamically loaded 'water.k8s.KubernetesEmbeddedConfigProvider' as AbstractEmbeddedH2OConfigProvider.
08-05 15:02:31.779 192.168.32.78:54321   1            main  INFO water.default: ----- H2O started  -----
08-05 15:02:31.780 192.168.32.78:54321   1            main  INFO water.default: Build git branch: rel-zumbo
08-05 15:02:31.780 192.168.32.78:54321   1            main  INFO water.default: Build git hash: 0c87fbeb537d68e7d140dd25fdedc4bb5f02f402
08-05 15:02:31.780 192.168.32.78:54321   1            main  INFO water.default: Build git describe: jenkins-3.36.1.3-23-g0c87fbe
08-05 15:02:31.781 192.168.32.78:54321   1            main  INFO water.default: Build project version: 3.36.1.4
08-05 15:02:31.781 192.168.32.78:54321   1            main  INFO water.default: Build age: 1 day
08-05 15:02:31.781 192.168.32.78:54321   1            main  INFO water.default: Built by: 'jenkins'
08-05 15:02:31.781 192.168.32.78:54321   1            main  INFO water.default: Built on: '2022-08-03 17:21:00'
08-05 15:02:31.782 192.168.32.78:54321   1            main  INFO water.default: Found H2O Core extensions: [XGBoost, KrbStandalone, Infogram]
08-05 15:02:31.782 192.168.32.78:54321   1            main  INFO water.default: Processed H2O arguments: []
08-05 15:02:31.782 192.168.32.78:54321   1            main  INFO water.default: Java availableProcessors: 4
08-05 15:02:31.782 192.168.32.78:54321   1            main  INFO water.default: Java heap totalMemory: 130.0 MB
08-05 15:02:31.783 192.168.32.78:54321   1            main  INFO water.default: Java heap maxMemory: 4.00 GB
08-05 15:02:31.783 192.168.32.78:54321   1            main  INFO water.default: Java version: Java 11.0.16 (from Red Hat, Inc.)
08-05 15:02:31.783 192.168.32.78:54321   1            main  INFO water.default: JVM launch parameters: [-XX:+UseContainerSupport, -XX:MaxRAMPercentage=50]
08-05 15:02:31.783 192.168.32.78:54321   1            main  INFO water.default: JVM process id: 1@h2o-stateful-set-1
08-05 15:02:31.784 192.168.32.78:54321   1            main  INFO water.default: OS version: Linux 4.18.0-240.22.1.el8_3.x86_64 (amd64)
08-05 15:02:31.784 192.168.32.78:54321   1            main  INFO water.default: Machine physical memory: 8.00 GB
08-05 15:02:31.784 192.168.32.78:54321   1            main  INFO water.default: Machine locale: en_US
08-05 15:02:31.784 192.168.32.78:54321   1            main  INFO water.default: X-h2o-cluster-id: 1659711634049
08-05 15:02:31.785 192.168.32.78:54321   1            main  INFO water.default: User name: 'root'
08-05 15:02:31.785 192.168.32.78:54321   1            main  INFO water.default: IPv6 stack selected: false
08-05 15:02:31.785 192.168.32.78:54321   1            main  INFO water.default: Possible IP Address: eth0 (eth0), 192.168.32.78
08-05 15:02:31.785 192.168.32.78:54321   1            main  INFO water.default: Possible IP Address: lo (lo), 127.0.0.1
08-05 15:02:31.786 192.168.32.78:54321   1            main  INFO water.default: H2O node running in unencrypted mode.
08-05 15:02:31.787 192.168.32.78:54321   1            main  INFO water.default: Internal communication uses port: 54322
08-05 15:02:31.787 192.168.32.78:54321   1            main  INFO water.default: Listening for HTTP and REST traffic on http://192.168.32.78:54321/
08-05 15:02:31.788 192.168.32.78:54321   1            main  INFO water.default: H2O cloud name: 'root' on /192.168.32.78:54321, static configuration based on -flatfile null
08-05 15:02:31.788 192.168.32.78:54321   1            main  INFO water.default: If you have trouble connecting, try SSH tunneling from your local machine (e.g., via port 55555):
08-05 15:02:31.789 192.168.32.78:54321   1            main  INFO water.default:   1. Open a terminal and run 'ssh -L 55555:localhost:54321 
[email protected]
'
08-05 15:02:31.789 192.168.32.78:54321   1            main  INFO water.default:   2. Point your browser to http://localhost:55555
08-05 15:02:32.406 192.168.32.78:54321   1            main  INFO water.default: Kerberos not configured
08-05 15:02:32.408 192.168.32.78:54321   1            main  INFO water.default: Log dir: '/tmp/h2o-root/h2ologs'
08-05 15:02:32.408 192.168.32.78:54321   1            main  INFO water.default: Cur dir: '/'
08-05 15:02:32.419 192.168.32.78:54321   1            main  INFO water.default: Subsystem for distributed import from HTTP/HTTPS successfully initialized
08-05 15:02:32.420 192.168.32.78:54321   1            main  INFO water.default: HDFS subsystem successfully initialized
08-05 15:02:32.423 192.168.32.78:54321   1            main  INFO water.default: S3 subsystem successfully initialized
08-05 15:02:32.433 192.168.32.78:54321   1            main  INFO water.default: GCS subsystem successfully initialized
08-05 15:02:32.434 192.168.32.78:54321   1            main  INFO water.default: Flow dir: '/root/h2oflows'
08-05 15:02:32.443 192.168.32.78:54321   1            main  INFO water.default: Cloud of size 1 formed [/192.168.32.78:54321]
08-05 15:02:32.443 192.168.32.78:54321   1            main  INFO water.default: Created cluster of size 1, leader node IP is 'h2o-stateful-set-1.h2o-service.dasi-ml-elr-dev.svc.cluster.local/192.168.32.78'
08-05 15:02:32.451 192.168.32.78:54321   1            main  INFO water.default: Registered parsers: [GUESS, ARFF, XLS, SVMLight, AVRO, PARQUET, CSV]
08-05 15:02:32.468 192.168.32.78:54321   1            main  INFO water.default: XGBoost extension initialized
08-05 15:02:32.469 192.168.32.78:54321   1            main  INFO water.default: KrbStandalone extension initialized
08-05 15:02:32.469 192.168.32.78:54321   1            main  INFO water.default: Infogram extension initialized
08-05 15:02:32.469 192.168.32.78:54321   1            main  INFO water.default: Registered 3 core extensions in: 1143ms
08-05 15:02:32.470 192.168.32.78:54321   1            main  INFO water.default: Registered H2O core extensions: [XGBoost, KrbStandalone, Infogram]
08-05 15:02:32.470 192.168.32.78:54321   1            main  INFO water.default: Registered: 1 auth extensions in: 116272ms
08-05 15:02:32.470 192.168.32.78:54321   1            main  INFO water.default: Registered Auth extensions: [LeaderNodeRequestFilter]
08-05 15:02:32.476 192.168.32.78:54321   1            main  INFO hex.tree.xgboost.XGBoostExtension: Found XGBoost backend with library: xgboost4j_minimal
08-05 15:02:32.476 192.168.32.78:54321   1            main  WARN hex.tree.xgboost.XGBoostExtension: Your system supports only minimal version of XGBoost (no GPUs, no multithreading)!
08-05 15:02:32.726 192.168.32.78:54321   1            main  INFO water.default: Registered: 270 REST APIs in: 256ms
08-05 15:02:32.727 192.168.32.78:54321   1            main  INFO water.default: Registered REST API extensions: [XGBoost, Amazon S3, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4]
08-05 15:02:32.826 192.168.32.78:54321   1            main  INFO water.default: Registered: 318 schemas in 98ms
08-05 15:02:32.827 192.168.32.78:54321   1            main  INFO water.default: H2O started in 118774ms
08-05 15:02:32.827 192.168.32.78:54321   1            main  INFO water.default: 
08-05 15:02:32.828 192.168.32.78:54321   1            main  INFO water.default: Open H2O Flow in your web browser: http://192.168.32.78:54321
08-05 15:02:32.828 192.168.32.78:54321   1            main  INFO water.default:","['kubernetes', 'h2o']",Matthew,https://stackoverflow.com/users/1280065/matthew,21
73215715,73215715,2022-08-03T02:34:07,2022-08-23 15:37:57Z,248,"I've been looking at the deviance calculation for negative binomial model in H2O  (
code
 line 580/959) and I'm struggling to reason why it is 0 when yr or ym is/are 0.


(yr==0||ym==0)?0:2*((_invTheta+yr)*Math.log((1+_theta*ym)/(1+_theta*yr))+yr*Math.log(yr/ym))




The formula for the deviance calculation is as below (from 
H2O Documentation
):




Going with maths, I don't see the deviance is 0 unless both yr and ym are 0.


Does anyone happen to know if there is a special case where deviance for negative binomial needs to be set to 0 when either of the yr and ym is/are 0?


Thanks!","['glm', 'h2o']",Rabbid76,https://stackoverflow.com/users/5577765/rabbid76,210k
73164701,73164701,2022-07-29T09:43:54,2022-07-29 13:13:25Z,376,"Code:


from pyspark.sql import SparkSession
from pysparkling import *

hc = H2OContext.getOrCreate()



I am using spark standalone cluster 3.2.1 and try to initiate H2OContext in python file. while trying to run the script using spark-submit, i am getting following error:




hc = H2OContext.getOrCreate() NameError: name 'H2OContext' is not defined





Spark-submit command:




spark-submit --master spark://local:7077 --packages
ai.h2o:sparkling-water-package_2.12:3.36.1.3-1-3.2 spark_h20/h2o.py","['python', 'apache-spark', 'h2o', 'sparkling-water', 'h2o.ai']",Priyadutt Bhatt,https://stackoverflow.com/users/19646755/priyadutt-bhatt,13
73138864,73138864,2022-07-27T13:30:00,2022-07-27 13:30:00Z,0,"Recently I started working with the h2o package in R-Studio. But now I have a different problem: everytime I try to 
load_all()
 my own project package, which has absolutely nothing to do with h2o, it initiates h2o without me saying. This causes different problems with functions that are masked by h2o.


> devtools::load_all()

ℹ Loading my_own_package
 Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:        blabla 
    H2O cluster timezone:       blabla 
    H2O data parsing timezone:  blabla
    H2O cluster version:        blabla
    H2O cluster version age:    blabla 
    H2O cluster name:           blabla
    H2O cluster total nodes:    blabla
    H2O cluster total memory:   blabla
    H2O cluster total cores:    blabla 
    H2O cluster allowed cores:  blabla
    H2O cluster healthy:        blabla
    H2O Connection ip:          blabla
    H2O Connection port:        blabla
    H2O Connection proxy:       blabla
    H2O Internal Security:      blabla
    R Version:                  blabla 

Error in UseMethod(""mutate"") : 
  no applicable method for 'mutate' applied to an object of class ""function""
In addition: There were 50 or more warnings (use warnings() to see the first 50)



Is there a way such that I can 
devtools::load_all()
 my own package but not load h2o?","['r', 'h2o', 'devtools']",MJimitater,https://stackoverflow.com/users/12097191/mjimitater,939
73137419,73137419,2022-07-27T11:51:28,2022-07-27 13:17:59Z,0,"im getting different output for feature importance, when I run the automl in azure, google and h2o. even though the data is same and all the features are also same. what would be the reason for it.
is there any other method to compare the models","['azure', 'h2o', 'automl', 'google-cloud-vertex-ai']",molbdnilo,https://stackoverflow.com/users/404970/molbdnilo,66.3k
72960854,72960854,2022-07-13T04:11:13,2022-07-14 02:20:06Z,80,"I'm using the 
h2o
 package in Python.


When 
binomial_double_trees == True
, I want to know information about internal all trees for a class.


This is my parameter:


h2o_rfe = H2ORandomForestEstimator(ntrees=3, max_depth=12, min_rows=10, binomial_double_trees=True)



If I use:


h2o_rfe.predict_leaf_node_assignment()



I can see that each tree is classified according to a separate tree and is also in 
h2o
 flow web page.


However, if I use:


list_of_trees = [H2OTree(model=h2o_rfe, tree_number=t, tree_class=None) for t in range(h2o_rfe.params['ntrees']['actual'])]'



I can't know the information of each internal tree according to the class.


Also I tried to set the 
tree_class
 (domain is 0,1):


list_of_trees = [H2OTree(model=h2o_rfe, tree_number=t, tree_class=""0"") for t in range(h2o_rfe.params['ntrees']['actual'])]'



When 
tree_class
 is 
'0'
, it is okay:


list_of_trees = [H2OTree(model=h2o_rfe, tree_number=t, tree_class=""1"") for t in range(h2o_rfe.params['ntrees']['actual'])]'



But when 
tree_class
 is 
'1'
, this error occurs:




Error: For binomial, only one tree class has been built per each iteration: 0




How to check the internal trees?","['python-3.x', 'random-forest', 'decision-tree', 'h2o', 'h2o.ai']",Daniel Walker,https://stackoverflow.com/users/8075540/daniel-walker,"6,648"
72900276,72900276,2022-07-07T15:10:38,2022-07-07 20:19:43Z,0,"I am trying to understand how the confusion matrix in h2o.explain is generated.
If I use the following code:
h2o.explain(model@leader, test_set, include_explanations=""confusion_matrix""), is the generated confusion matrix evaluating the model accuracy on the test set?
How would this be different from using h2o.predict on the test set (e.g. h2o.predict(model@leader, test_set)?","['r', 'h2o', 'confusion-matrix', 'automl']",M--,https://stackoverflow.com/users/6461462/m,28.4k
72888069,72888069,2022-07-06T18:02:36,2022-07-13 06:02:57Z,0,"I am trying to create an Azure python function which uses H20 module. When I tried to test it locally I am getting module not available error even though I have specified it in requirements.txt and it seem to be installed in the virtual env and I am able to run using virtual environment manually.


Minimal Python code:


import datetime
import logging

import h2o
import azure.functions as func

def main(mytimer: func.TimerRequest) -> None:
    utc_timestamp = datetime.datetime.utcnow().replace(
        tzinfo=datetime.timezone.utc).isoformat()
    if mytimer.past_due:
        logging.info('The timer is past due!')
    logging.info('Python timer trigger function ran at %s', utc_timestamp)



requirement.txt


h2o==3.32.0.2



Error.txt:




Python version 3.9m Windows 10 OS.","['python-3.x', 'azure-functions', 'h2o']",The6thSense,https://stackoverflow.com/users/4251775/the6thsense,"8,325"
72665283,72665283,2022-06-17T22:16:33,2022-06-17 22:37:50Z,156,"I just installed the most recent version of h2o for Python.


And it generates the following error:


import h2o
h2o.init()
h2o_df = h2o.H2OFrame(some_df)



the error:


Traceback (most recent call last):
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_console_utils.py"", line 417, in execTableCommand
    success, res = exec_table_command(command, command_type,
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydevd_bundle\pydevd_tables.py"", line 43, in exec_table_command
    res += repr(tmp_var.head().to_html(notebook=True,
AttributeError: 'H2OFrame' object has no attribute 'to_html'
Traceback (most recent call last):
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_console_utils.py"", line 417, in execTableCommand
    success, res = exec_table_command(command, command_type,
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydevd_bundle\pydevd_tables.py"", line 43, in exec_table_command
    res += repr(tmp_var.head().to_html(notebook=True,
AttributeError: 'H2OFrame' object has no attribute 'to_html'
Traceback (most recent call last):
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_console_utils.py"", line 417, in execTableCommand
    success, res = exec_table_command(command, command_type,
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydevd_bundle\pydevd_tables.py"", line 43, in exec_table_command
    res += repr(tmp_var.head().to_html(notebook=True,
AttributeError: 'H2OFrame' object has no attribute 'to_html'
Traceback (most recent call last):
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_console_utils.py"", line 417, in execTableCommand
    success, res = exec_table_command(command, command_type,
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydevd_bundle\pydevd_tables.py"", line 43, in exec_table_command
    res += repr(tmp_var.head().to_html(notebook=True,
AttributeError: 'H2OFrame' object has no attribute 'to_html'
Traceback (most recent call last):
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_console_utils.py"", line 417, in execTableCommand
    success, res = exec_table_command(command, command_type,
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydevd_bundle\pydevd_tables.py"", line 43, in exec_table_command
    res += repr(tmp_var.head().to_html(notebook=True,
AttributeError: 'H2OFrame' object has no attribute 'to_html'
Traceback (most recent call last):
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_console_utils.py"", line 417, in execTableCommand
    success, res = exec_table_command(command, command_type,
  File ""C:\Users\some_user\AppData\Local\JetBrains\DataSpell 2022.1\plugins\python-ce\helpers\pydev\_pydevd_bundle\pydevd_tables.py"", line 43, in exec_table_command
    res += repr(tmp_var.head().to_html(notebook=True,
AttributeError: 'H2OFrame' object has no attribute 'to_html'



It also dumps all previous calls and output of h2o. What is wrong here?


UPDATE


I guess I have to add that I am running it in DataSpell. Everything seems to be fine in Jupyter notebook.","['python', 'h2o', 'dataspell']",Unknown,,N/A
72562340,72562340,2022-06-09T14:44:20,2022-07-07 11:55:28Z,784,"I am trying to save the output images (graphs) I get when I use 
explain()
 in H2O models. Currently I am just saving the SHAP output using the 
model.shap_summary_plot(test, save_plot_path=`shap_summary.png`)
. There is no 
save_plot_path
 for explain.






import h2o
from h2o.automl import H2OAutoML

h2o.init()

df = h2o.import_file(""https://h2o-public-test-data.s3.amazonaws.com/smalldata/wine/winequality-redwhite-no-BOM.csv"")

response = ""quality""

predictors = [
  ""fixed acidity"", ""volatile acidity"", ""citric acid"", ""residual sugar"", ""chlorides"", ""free sulfur dioxide"",
  ""total sulfur dioxide"", ""density"", ""pH"", ""sulphates"", ""alcohol"",  ""type""
]


train, test = df.split_frame(seed=1)

aml = H2OAutoML(max_runtime_secs=120, seed=1)
aml.train(x=predictors, y=response, training_frame=train)

leader_model = aml.leader 

leader_model.explain(test) # save this output








However I want to save all the graphs generated via 
explain()
 instead of creating them individually. Also I want it to run as a script and not as a jupyter notebook.


Here is sample code,(edited 
Explain-wine-example
)


H2O explain docs","['python', 'h2o', 'h2o.ai']",Andreas Rossberg,https://stackoverflow.com/users/1097780/andreas-rossberg,36k
72560442,72560442,2022-06-09T12:40:15,2022-06-09 17:41:56Z,150,"It is a strange error when I use the 
col_names=
 argument in 
h2o.import_file
. However, setting the column names by a separate line works fine.


import os
import h2o

h2o.init() # It shows H2O_cluster_version 3.36.1.2 and Python version 3.9.7 final

os.system(""wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/ijcnn1.tr.bz2"")
os.system(""bzip2 -d ijcnn1.tr.bz2"")

# These lines work
col_names = ['class'] + ['F' + str(i) for i in range(22)]
df1 = h2o.import_file(path=""ijcnn1.tr"")
df1.columns = col_names

# But this line does not work
df2 = h2o.import_file(path=""ijcnn1.tr"", col_names=col_names)

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_20191/3817572867.py in <module>
----> 1 df2 = h2o.import_file(path=""ijcnn1.tr"", col_names=col_names)

~/anaconda3/lib/python3.9/site-packages/h2o/h2o.py in import_file(path, destination_frame, parse, header, sep, col_names, col_types, na_strings, pattern, skipped_columns, custom_non_data_line_markers, partition_by, quotechar, escapechar)
    498         return lazy_import(path, pattern)
    499     else:
--> 500         return H2OFrame()._import_parse(path, pattern, destination_frame, header, sep, col_names, col_types, na_strings,
    501                                         skipped_columns, custom_non_data_line_markers, partition_by, quotechar, escapechar)
    502 

~/anaconda3/lib/python3.9/site-packages/h2o/frame.py in _import_parse(self, path, pattern, destination_frame, header, separator, column_names, column_types, na_strings, skipped_columns, custom_non_data_line_markers, partition_by, quotechar, escapechar)
    459             path = os.path.abspath(path)
    460         rawkey = h2o.lazy_import(path, pattern)
--> 461         self._parse(rawkey, destination_frame, header, separator, column_names, column_types, na_strings,
    462                     skipped_columns, custom_non_data_line_markers, partition_by, quotechar, escapechar)
    463         return self

~/anaconda3/lib/python3.9/site-packages/h2o/frame.py in _parse(self, rawkey, destination_frame, header, separator, column_names, column_types, na_strings, skipped_columns, custom_non_data_line_markers, partition_by, quotechar, escapechar)
    476                na_strings=None, skipped_columns=None, custom_non_data_line_markers=None, partition_by=None, quotechar=None,
    477                escapechar=None):
--> 478         setup = h2o.parse_setup(rawkey, destination_frame, header, separator, column_names, column_types, na_strings,
    479                                 skipped_columns, custom_non_data_line_markers, partition_by, quotechar, escapechar)
    480         return self._parse_raw(setup)

~/anaconda3/lib/python3.9/site-packages/h2o/h2o.py in parse_setup(raw_frames, destination_frame, header, separator, column_names, column_types, na_strings, skipped_columns, custom_non_data_line_markers, partition_by, quotechar, escapechar)
    872                     % (len(column_names), parse_column_len))
    873         else:
--> 874             if len(column_names) != len(j[""column_types""]): raise ValueError(
    875                 ""length of col_names should be equal to the number of columns: %d vs %d""
    876                 % (len(column_names), len(j[""column_types""])))

ValueError: length of col_names should be equal to the number of columns: 23 vs 22","['import', 'h2o']",rozyang,https://stackoverflow.com/users/2669433/rozyang,619
72527070,72527070,2022-06-07T06:58:36,2022-06-07 06:58:36Z,44,"Trying out the h2o autoML option preprocessing = [""target_encoding""].The test performance did improve. How do I apply similar transformation on the unseen/ out of time data to check performance?","['python', 'target', 'h2o', 'automl', 'data-preprocessing']",Payal Sengupta,https://stackoverflow.com/users/19196718/payal-sengupta,21
72465078,72465078,2022-06-01T16:16:20,2022-06-01 22:02:04Z,107,"I ceased using h2o a few yeas back when I discovered a malware issue.


After obtaining a new Mac, I decided to give it another shot and install h2o.  The problem still persists.  I use VirusBarrier Scanner and rarely experience any inflected files of any type.


Has anyone else experienced the same problem?",['h2o'],Unknown,,N/A
72456723,72456723,2022-06-01T05:17:18,2022-06-01 14:49:21Z,145,"i have a few questions regarding H2O AI. As per my understanding, h2o AI powers Auto ML functionality. but need to integrate my own python jupyetr ML model. so my questions are,




Can we use H2O AI without Auto ML and with our own python jupyter ML algorithm?


If yes, can we integrate that own manual scripted ML with Snowflake?


If we can integrate our own scripted ml algorithm with snowflake, what are the advantages of doing it that way? instead of an own manually-created python ML algorithm?","['python', 'snowflake-cloud-data-platform', 'h2o', 'automl', 'h2o.ai']",johnson,https://stackoverflow.com/users/11409289/johnson,429
72400864,72400864,2022-05-27T05:31:47,2022-05-27 21:04:54Z,0,"I am running a loop to upload a csv file from my local machine, convert it to a h2o data frame, then run a h2o model. I then remove the h2o data frame from my r environment and the loop continues. These data frames are massive so I can only have one data frame loaded at a time (hence the reason for me removing the data frame from my environment).


My problem is that h2o creates temporary files which quickly max out my memory. I know I can restart my r session, but is there another way to flush this out in code so my loop can run happily? When I look at my task manager the all my memory is sucked up in Java(TM) Platform SE Binary.","['r', 'h2o']",chipsin,https://stackoverflow.com/users/13326190/chipsin,675
72376952,72376952,2022-05-25T11:39:23,2022-05-25 12:19:19Z,170,"I tried using the 2 best models from AutoML and used one of them as Meta learner for stacking. Named the new model stack_test. Code that I used is:


stack_test = H2OStackedEnsembleEstimator(base_models=[model1_xg, model2_xg],
metalearner_algorithm=model1_xg)

stack_test.train(x=x, y=y, training_frame=h2o_train)
stack_test.model_performance(h2o_test).auc()



Error I am getting:


NameError: name 'stack_test' is not defined



What am I doing wrong here?","['python', 'h2o']",medium-dimensional,https://stackoverflow.com/users/7789963/medium-dimensional,"2,203"
72253927,72253927,2022-05-16T03:35:18,2022-05-23 17:06:19Z,0,"I have started using 
h2o
 for aggregating large datasets and I have found peculiar behaviour when trying to aggregate the maximum value using h2o's 
h2o.group_by
 function. My dataframe often has variables which comprise some or all NA's for a given grouping. Below is an example dataframe.


df <- data.frame(""ID"" = 1:16)
df$Group<- c(1,1,1,1,2,2,2,3,3,3,4,4,5,5,5,5)
df$VarA <- c(NA_real_,1,2,3,12,12,12,12,0,14,NA_real_,14,16,16,NA_real_,16)
df$VarB <- c(NA_real_,NA_real_,NA_real_,NA_real_,10,12,14,16,10,12,14,16,10,12,14,16)
df$VarD <- c(10,12,14,16,10,12,14,16,10,12,14,16,10,12,14,16)

   ID Group VarA VarB VarD
1   1     1   NA   NA   10
2   2     1    1   NA   12
3   3     1    2   NA   14
4   4     1    3   NA   16
5   5     2   12   10   10
6   6     2   12   12   12
7   7     2   12   14   14
8   8     3   12   16   16
9   9     3    0   10   10
10 10     3   14   12   12
11 11     4   NA   14   14
12 12     4   14   16   16
13 13     5   16   10   10
14 14     5   16   12   12
15 15     5   NA   14   14
16 16     5   16   16   16



In this dataframe Group == 1 is completely missing data for VarB (but this is important information to know, so the output for aggregating for the maximum should be NA), while for Group == 1 VarA only has one missing value so the maximum should be 3.


This is a link which includes the behaviour of the behaviour of the 
na.methods
 argument (
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/groupby.html
).


If I set the 
na.methods = 'all'
 as below then the aggregated output is NA for Group 1 for both Vars A and B (which is not what I want, but I completely understand this behaviour).


h2o_agg <-  h2o.group_by(data = df_h2o, by = 'Group', max(), gb.control = list(na.methods = ""all""))

  Group max_ID max_VarA max_VarB max_VarD
1     1      4      NaN      NaN       16
2     2      7       12       14       14
3     3     10       14       16       16
4     4     12      NaN       16       16
5     5     16      NaN       16       16



If I set the 
na.methods = 'rm'
 as below then the aggregated output for Group 1 is 3 for VarA (which is the desired output and makes complete sense) but for VarB is -1.80e308 (which is not what I want, and I do not understand this behaviour).


h2o_agg <-  h2o.group_by(data = df_h2o, by = 'Group', max(), gb.control = list(na.methods = ""rm""))

  Group max_ID max_VarA  max_VarB max_VarD
  <int>  <int>    <int>     <dbl>    <int>
1     1      4        3 -1.80e308       16
2     2      7       12  1.4 e  1       14
3     3     10       14  1.6 e  1       16
4     4     12       14  1.6 e  1       16
5     5     16       16  1.6 e  1       16



Similarly I get the same output if set the 
na.methods = 'ignore'
.


h2o_agg <-  h2o.group_by(data = df_h2o, by = 'Group', max(), gb.control = list(na.methods = ""ignore""))

  Group max_ID max_VarA  max_VarB max_VarD
  <int>  <int>    <int>     <dbl>    <int>
1     1      4        3 -1.80e308       16
2     2      7       12  1.4 e  1       14
3     3     10       14  1.6 e  1       16
4     4     12       14  1.6 e  1       16
5     5     16       16  1.6 e  1       16



I am not sure why something as common as completely missing data for a given variable within a specific group is being given a value of -1.80e308? I tried the same workflow in dplyr and got results which match my expectations (but this is not a solution as I cannot process datasets of this size in dplyr, and hence my need for a solution in h2o). I realise dplyr is giving me 
-inf
 values rather than NA, and I can easily recode both 
-1.80e308
 and 
-Inf
 to NA, but I am trying to make sure that this isn't a symptom of a larger problem in 
h2o
 (or that I am not doing something fundamentally wrong in my code when attempting to aggregate in 
h2o
). I also have to aggregate normalised datasets which often have values which are approximately similar to -1.80e308, so I do not want to accidentally recode legitimate values to NA.


library(dplyr)
df %>%
  group_by(Group) %>% 
  summarise(across(everything(), ~max(.x, na.rm = TRUE)))

  Group    ID  VarA  VarB  VarD
  <dbl> <int> <dbl> <dbl> <dbl>
1     1     4     3  -Inf    16
2     2     7    12    14    14
3     3    10    14    16    16
4     4    12    14    16    16
5     5    16    16    16    16","['r', 'dplyr', 'h2o']",Unknown,,N/A
72098968,72098968,2022-05-03T11:52:42,2022-05-03 19:50:29Z,393,"I have been struggling with this error for a few hours now, but seem lost even after reading through the documentation.


I'm using H2O's Extended Isolation Forest (EIF), an unsupervised model, to detect anomalies in an unlabelled dataset. Which is working as intended, however for the project i'm working on the model explainability is extremely important. I discovered the 
explain
 function, which supposedly returns several explainablity methods for a model. I'm particularly interested in the SHAP values from this function.


The documentation states




The main functions, h2o.explain() (global explanation) and h2o.explain_row() (local explanation) work for individual 
H2O models,
 as well a list of models or an H2O AutoML object.  The h2o.explain() function generates a list of explanations.




Since the H2O models link brings me to a page which covers both supervised and unsupervised models I assume the explain function would work for both types of models.


When trying to run my code the following code works just fine.


import h2o
from h2o.estimators import H2OExtendedIsolationForestEstimator

h2o.init()
df_EIF = h2o.H2OFrame(df_EIF)
predictors = df_EIF.columns[0:37]

eif = H2OExtendedIsolationForestEstimator(ntrees = 75, sample_size =500, extension_level = (len(predictors) -1)  )

eif.train(x=predictors, training_frame = df_EIF)
eif_result = eif.predict(df_EIF)
df_EIF['anomaly_score_EIF') = eif_result['anomaly_score']



However when trying to call explain over the model (eif)


eif.explain(df_EIF)



Gives me the following KeyError


KeyError                                  Traceback (most recent call last)
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx.py in <module>
----> 1 eif.explain(df_EIF)
      2 
      3 
      4 
      5 

C:\ProgramData\Anaconda3\lib\site-packages\h2o\explanation\_explain.py in explain(models, frame, columns, top_n_features, include_explanations, exclude_explanations, plot_overrides, figsize, render, qualitative_colormap, sequential_colormap)
   2895     plt = get_matplotlib_pyplot(False, raise_if_not_available=True)
   2896     (is_aml, models_to_show, classification, multinomial_classification, multiple_models, targets,
-> 2897      tree_models_to_show, models_with_varimp) = _process_models_input(models, frame)
   2898 
   2899     if top_n_features < 0:

C:\ProgramData\Anaconda3\lib\site-packages\h2o\explanation\_explain.py in _process_models_input(models, frame)
   2802         models_with_varimp = [model for model in models if _has_varimp(model)]
   2803     tree_models_to_show = _get_tree_models(models, 1 if is_aml else float(""inf""))
-> 2804     y = _get_xy(models_to_show[0])[1]
   2805     classification = frame[y].isfactor()[0]
   2806     multinomial_classification = classification and frame[y].nlevels()[0] > 2

C:\ProgramData\Anaconda3\lib\site-packages\h2o\explanation\_explain.py in _get_xy(model)
   1790     """"""
   1791     names = model._model_json[""output""][""original_names""] or model._model_json[""output""][""names""]
-> 1792     y = model.actual_params[""response_column""]
   1793     not_x = [
   1794                 y,

KeyError: 'response_column



From my understanding this response column refers to a column that you are trying to predict. However, since i'm dealing with an unlabelled dataset this response column doesn't exist. Is there a way for me to bypass this error? Is it even possible to utilize the explain() function on unsupervised models? If, so how do I do this? If it is not possible, is there another way to extract the Shap values of each variable from the model? Since the shap.TreeExplainer also doesn't seem to work on a H2O model.


TL;DR: Is it possible to use the .explain() function from h2o on an (Extended) Isolation forest? If so how?","['pandas', 'dataframe', 'h2o', 'isolation-forest']",Sebastiaan van Dijk,https://stackoverflow.com/users/19023547/sebastiaan-van-dijk,3
72048657,72048657,2022-04-28T18:30:00,2022-04-29 18:42:43Z,156,"I have fit a GAM model in h2o with several gam variables (P-splines) using the h2o.estimators.gam package. I'd like to get a table with the factor loading for every level of each gam variable. For example, one of my variables is age, and I need a table of the coefficient for each age.","['python', 'h2o', 'gam', 'bspline', 'h2o.ai']",Carver Coleman,https://stackoverflow.com/users/18977015/carver-coleman,11
71996637,71996637,2022-04-25T08:34:59,2022-04-26 07:15:13Z,206,"I have trained a stacked ensemble model with automl() function provided by H2O (3.36.0.4)for R. Once the model is trained, i have exported it to .zip format with the download_mojo() function.


I have created a Java app following instructions, but when running the program, the model always predicts the same value.


This is the app developed in Java:


import java.io.*;
import hex.genmodel.easy.RowData;
import hex.genmodel.easy.EasyPredictModelWrapper;
import hex.genmodel.easy.prediction.*;
import hex.genmodel.MojoModel;

public class App {
    public static void main(String[] args) throws Exception {

        EasyPredictModelWrapper model = new EasyPredictModelWrapper(MojoModel.load(""StackedEnsemble_BestOfFamily_8_AutoML_1_20220407_144828.zip""));

        BufferedReader csvReader = new BufferedReader(new FileReader(""dataset.csv""));
        RowData inputrow = new RowData();
        String row;
        String[] colnames = new String[36];
        String[] data;
        for (int i = 0; i <= 10; i++) {
            row = csvReader.readLine();
            if (i == 0) {
                colnames = row.split("","");
            } else {
                data = row.split("","");
                for (int k = 0; k < colnames.length - 2; k++) {
                    inputrow.put(colnames[k], data[k]);
                }
                RegressionModelPrediction prediction = model.predictRegression(inputrow);
                System.out.println(""Prediction ""+i+"": "" + prediction.value);
            }
        }
        System.out.println("""");
    }
}



It returns this:


Prediction 1: 0.09239077248718583
Prediction 2: 0.09239077248718583
Prediction 3: 0.09239077248718583
Prediction 4: 0.09239077248718583
Prediction 5: 0.09239077248718583
Prediction 6: 0.09239077248718583
Prediction 7: 0.09239077248718583
Prediction 8: 0.09239077248718583
Prediction 9: 0.09239077248718583
Prediction 10: 0.09239077248718583



For more details, training and test datasets have the same variables and the model have been trained with following parameters:


aml <- h2o::h2o.automl(y = y,
                       training_frame = df_h2o,
                       nfolds = 10,
                       max_models = 150,
                       max_runtime_secs = NULL,
                       keep_cross_validation_predictions = TRUE,
                       stopping_metric = 'RMSE',
                       sort_metric ='RMSE',
                       verbosity = ""info"")



And I have performed the following checks, with the same dataset used in java:


modelPath <- paste0(getwd(), ""/StackedEnsemble_BestOfFamily_8_AutoML_1_20220407_144828"")
loaded_model <- h2o.loadModel(modelPath)


val_df <-  read.csv(""dataset.csv"")
input_row <- val_df[1:10 ,1:36]
new_data <- as.h2o(input_row)

h2omodel_preditions <-as.vector(h2o.predict(loaded_model, new_data, exact_quantiles=TRUE))


original_mojo_path <- paste0(getwd(), ""/StackedEnsemble_BestOfFamily_8_AutoML_1_20220407_144828.zip"")
mojo_model <- h2o.upload_mojo(original_mojo_path)
mojo_predictions  <- as.vector( h2o.predict(mojo_model, new_data))


> h2omodel_preditions
 [1] 0.27564401 0.25663341 0.17848737 0.05179671 0.02977053 0.28588998 0.29157313 0.19800770 0.06251480 0.23992213
> mojo_predictions
 [1] 0.27564401 0.25663341 0.17848737 0.05179671 0.02977053 0.28588998 0.29157313 0.19800770 0.06251480 0.23992213","['java', 'prediction', 'h2o', 'mojo']",Unknown,,N/A
71928885,71928885,2022-04-19T17:20:01,2022-12-03 17:16:48Z,220,"I am try to using 
pysparkling.ml.H2OMOJOModel
 for predict a spark dataframe using a MOJO model trained with h2o==3.32.0.2 in AWS Glue Jobs, how ever a got the error: TypeError: 'JavaPackage' object is not callable.


I opened a ticket in AWS support and they confirmed that Glue environment is ok and the problem is probably with sparkling-water (pysparkling). It seems that some dependency library is missing, but I have no idea which one.
The simple code bellow works perfectly if I run in my local computer (I only need to change the mojo path for GBM_grid__1_AutoML_20220323_233606_model_53.zip)


Could anyone ever run sparkling-water in Glue jobs successfully?


Job Details:
-Glue version 2.0
--additional-python-modules, h2o-pysparkling-2.4==3.36.0.2-1
-Worker type: G1.X
-Number of workers: 2
-Using script ""createFromMojo.py""


createFromMojo.py:


import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
import pandas as pd
from pysparkling.ml import H2OMOJOSettings
from pysparkling.ml import H2OMOJOModel
# from pysparkling.ml import *

## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, [""JOB_NAME""])

#Job setup
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

job = Job(glueContext)
job.init(args[""JOB_NAME""], args)

caminho_modelo_mojo='s3://prod-lakehouse-stream/modeling/approaches/GBM_grid__1_AutoML_20220323_233606_model_53.zip'
print(caminho_modelo_mojo)
print(dir())

settings = H2OMOJOSettings(convertUnknownCategoricalLevelsToNa = True, convertInvalidNumbersToNa = True)
model = H2OMOJOModel.createFromMojo(caminho_modelo_mojo, settings)

data = {'days_since_last_application': [3, 2, 1, 0], 'job_area': ['a', 'b', 'c', 'd']}

base_escorada = model.transform(spark.createDataFrame(pd.DataFrame.from_dict(data)))

print(base_escorada.printSchema())

print(base_escorada.show())

job.commit()","['python', 'apache-spark', 'h2o', 'sparkling-water']",MaxReis86,https://stackoverflow.com/users/14055804/maxreis86,26
71849829,71849829,2022-04-12T22:33:47,2022-04-12 22:33:47Z,0,"So I wrote a script that takes the weights of a h2o model and plants them into a keras model. I then found that the results from running the model through keras do not match up with h2o.



Here's the code I used to get this graph:


library(h2o)
library(keras)
library(tensorflow)

# create data

x <- rnorm(10000,sd=3)
y <- 2*x + rnorm(10000,sd=0.4) + 0.3*rt(10000,5) -0.1*x^2 + 2.4*sin(1.2*x)
plot(x,y, main=""h2o model vs keras model with h2o weights"" )

df <- data.frame(x=x,y=y)

# apply h2o fit to data

h2o.init()
h2odf <- as.h2o(df)


model <- h2o.deeplearning(x = 1, y = 2,
                    training_frame = h2odf,
                    hidden = c(100,10),
                    activation = ""Tanh"",        # use ""Rectifier"" for relu
                    export_weights_and_biases = TRUE,
                    export_checkpoints_dir=getwd(),
                    verbose=TRUE)

newdf <- as.h2o(data.frame(x=seq(-10,10,by=0.01)))

predict <- as.data.frame(predict(model,newdf))
lines(seq(-10,10,by=0.01),predict$predict,col=2,lwd=2)


# Get the weights from h2o and convert them into a format that tensorflow will accept

layers <- model@parameters$hidden
activation <- model@parameters$activation
weights <- list()

for(i in 1:(1+length(layers))){
    W <- as.matrix(h2o.weights(model,matrix_id=i))
    B <- as.array(as.vector(h2o.biases(model,vector_id=i)))
    colnames(W) <- NULL
    W <- t(W)
    weights[[2*i-1]] <- W
    weights[[2*i]] <- B
    
}



# set up Keras model with the same layers and activation functions

k_clear_session()
model2 <- keras_model_sequential()
model2 %>% layer_dense(units=layers[1],input_shape=1,activation=tolower(activation))

for(i in 2:(length(layers))){
    model2 %>% layer_dense(units = layers[i],
                    activation = tolower(activation))
}
                
model2 %>% layer_dense(name=""output"",units = 1,activation = ""linear"")       # Can try tanh activation here, still doesn't make it right.    


# Inject h2o weights into the keras model

set_weights(model2,weights)

newdf <- data.frame(x=seq(-10,10,by=0.01))
predict2 <- predict(model2,as.matrix(newdf))

lines(seq(-10,10,by=0.01),predict2,col=4,lwd=2)
legend(""topleft"",legend=c(""h2o"",""keras""),col=c(2,4),lwd=2)



Now one thing you might notice is that the blue line is a scaled down version of the red line.
I manually tweaked the scale until it matched up, and the scaling factor was 3 * x in the horizontal direction and 6.45 * y - 1 in the vertical direction. Weird scaling but ok.


plot(x,y, main=""h2o model vs stretched keras model with h2o weights"" )
s <- seq(-10,10,by=0.01)
lines(s,predict$predict,col=2,lwd=2)

lines(3*s,6.45*predict2-1,col=4,lwd=2)
legend(""topleft"",legend=c(""h2o"",""keras""),col=c(2,4),lwd=2)





I tried the same process on a dummy classification task. I found that typically, the two lines wouldn't match up even under scaling.




# classification task

# generate binary classification data from the nonlinear function y_gen

x <- rnorm(10000,sd=2.6)
y_gen <-   20*dnorm(x,sd=1.2)*x #2*(0.6*x +0.2*sin(2*x)+0.1-0.01*x^3)

y <-vector(length=length(x))
for(i in 1:length(x)){
    y[i] <- sample(0:1,prob=c(exp(y_gen[i]),1),replace=TRUE)
}

y <- as.logical(y)
df <- data.frame(x=x,y=y)


# generate the h2o fit

h2odf <- as.h2o(df)


model <- h2o.deeplearning(x = 1, y = 2,
                    training_frame = h2odf,
                    hidden = c(100,10),
                    activation = ""Tanh"",
                    export_weights_and_biases = TRUE,
                    export_checkpoints_dir=getwd(),
                    verbose=TRUE)

newdf <- as.h2o(data.frame(x=seq(-10,10,by=0.01)))

predict <- as.data.frame(predict(model,newdf)[,3])
names(predict) <- ""y""

plot(x,1-exp(y_gen)/(1+exp(y_gen)),main=c(""h2o model vs keras model with h2o weights"",""classification test""))
lines(seq(-10,10,by=0.01),predict$y,col=2,lwd=2)


layers <- model@parameters$hidden
activation <- model@parameters$activation
weights <- list()

for(i in 1:(1+length(layers))){
    W <- as.matrix(h2o.weights(model,matrix_id=i))
    B <- as.array(as.vector(h2o.biases(model,vector_id=i)))
    colnames(W) <- NULL
    W <- t(W)

    
    if(i == 1+length(layers)){
        weights[[2*i-1]] <- as.matrix(W[,2])    # h2o output for a classification task is of length 2. value 1 is the negative probability, value 2 is the positive probability.
        weights[[2*i]] <- as.array(B[2])        # so here we only need one of them so we take only one of them
    } else {
        weights[[2*i-1]] <- W
        weights[[2*i]] <- B
    }   
}



# set up tensorflow

k_clear_session()
model2 <- keras_model_sequential()
model2 %>% layer_dense(units=layers[1],input_shape=1,activation=tolower(activation))

for(i in 2:(length(layers))){
    model2 %>% layer_dense(units = layers[i],
                    activation = tolower(activation))
}
                
model2 %>% layer_dense(name=""output"",units = 1,activation = ""sigmoid"")

set_weights(model2,weights)

predict2 <- predict(model2,as.matrix(newdf))

lines(seq(-10,10,by=0.01),predict2,col=4,lwd=2)
legend(""topleft"",legend=c(""h2o"",""keras""),col=c(2,4),lwd=2)



I suspect there is something funny with the way h2o sets up its activation layers
. H2o doesn't allow nearly as much 'under the hood' access as keras, so I've been so far unable to look at the inner goings on in the h2o model. For instance, how is h2o able to do regression on data that extends beyond (-1,1) with only tanh activation functions?


I've tried the above code with ReLu as well as Tanh, and I've tried altering the final layer of the keras model from tanh tanh to linear, to no avail.


My questions are:


How does the h2o model give different results to the Keras model even when the same weights are used? What goes on under the hood when h2o is performing a prediction?


and


is there any other basic mistake I made when attempting to convert the weights?","['r', 'tensorflow', 'keras', 'h2o']",Ingolifs,https://stackoverflow.com/users/8968617/ingolifs,311
71777142,71777142,2022-04-07T06:08:55,2022-04-26 12:33:26Z,157,"I would like to plot a loss vs epoch graph from a deep quantile regression model in H2O. I'm using the H2ODeepLearningEstimator but can't seem to find a way to retrieve the loss like in Keras.


https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/


Could somebody please point me in the right direction?","['python', 'h2o', 'h2o.ai']",mj_whales,https://stackoverflow.com/users/14039621/mj-whales,122
71741263,71741263,2022-04-04T17:19:22,2022-04-04 18:08:54Z,485,"I used H2O's automl on my dataset, and the top model is a stackedensemble model. I like to get all basemodels with it's parameters of this stacked model. How do I get this models?","['python', 'h2o']",seb2704,https://stackoverflow.com/users/5269959/seb2704,540
71693880,71693880,2022-03-31T14:00:33,2022-06-30 14:29:18Z,0,"I would like to use h2o in 
R
 for glm regression but with random effects (HGLM, seems possible from 
this page
 ). I do not manage to make it work yet, and get errors I do not understand.


Is here my working example: I define a dataset with Simpson paradox: a global increasing trend, but a decreasing trend in each group


library(tidyverse)
library(ggplot2)
library(h2o)
library(data.table)

global_slope <- 1
global_int <- 1

Npoints_per_group <- 50
N_groups <- 10
pentes <- rnorm(N_groups,-1,.5)

centers_x <- seq(0,10,length = N_groups)
center_y <- global_slope*centers_x + global_int

group_spread <- 2

group_names <- sample(LETTERS,N_groups)

df <- lapply(1:N_groups,function(i){
  x <- seq(centers_x[i]-group_spread/2,centers_x[i]+group_spread/2,length = Npoints_per_group)
  y <- pentes[i]*(x- centers_x[i])+center_y[i]+rnorm(Npoints_per_group)
  data.table(x = x,y = y,ID = group_names[i])
}) %>% rbindlist()



You can recognize something similar to the example of the 
wiki page of Simpson paradox
:


ggplot(df,aes(x,y,color = as.factor(ID)))+
  geom_point()





The linear regression without random effect sees the increasing trend:


lm(y~x,data = df) %>% 
summary()

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.28187    0.13077   9.803   <2e-16 ***
x            0.94147    0.02194  42.917   <2e-16 ***



A standard multilevel regression would look like that:


library(lme4)
library(lmerTest)

lmer( y ~ x + (1+x|ID) ,data = df) %>% 
  summary()



And would estimate properly a decreasing trend:


Fixed effects:
            Estimate Std. Error      df t value Pr(>|t|)    
(Intercept)  11.7192     2.6218  8.8220   4.470 0.001634 ** 
x            -1.0418     0.1959  8.9808  -5.318 0.000486 ***



Now I test with 
h2o
:


library(h2o)
h2o.init()

df2 <- as.h2o(df)
test_glm <- h2o.glm(family = ""gaussian"",
                        x = ""x"",
                        y = ""y"",
                        training_frame = df2,
                        lambda = 0,
                        compute_p_values = TRUE)
test_glm



And it works well, similar to the linear model above:


Coefficients: glm coefficients
      names coefficients std_error   z_value  p_value standardized_coefficients
1 Intercept     1.281868  0.130766  9.802785 0.000000                  5.989232
2         x     0.941473  0.021937 42.916536 0.000000                  3.058444



But when I want to use random effects:


test_glm2 <- h2o.glm(family = ""gaussian"",
                     x = ""x"",
                     y = ""y"",
                     training_frame = df2,
                     random_columns = ""ID"",
                     lambda = 0,
                     compute_p_values = TRUE)




I got




Error in .h2o.checkAndUnifyModelParameters(algo = algo, allParams = ALL_PARAMS, : vector of random_columns must be of type numeric, but got character.




Even if I force 
df2$ID  <- as.numeric(df2$ID)
.


What Am I doing wrong? What is the proper way to find something similar to the mixed effect model with 
lmer
 (i.e. random slope and intercept)?




EDIT


I changed to use, as suggested by Erin LeDell, the column number. I now get a different error, that I do not understand either:


df2$ID  <- as.factor(df2$ID)

test_glm2 <- h2o.glm(family = ""gaussian"",
                     x = ""x"",
                     y = ""y"",
                     training_frame = df2,
                     random_columns = c(3),
                     HGLM = TRUE,
                     lambda = 0,
                     compute_p_values = TRUE)

DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.NullPointerException

DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.NullPointerException
    at water.MRTask.getResult(MRTask.java:660)
    at water.MRTask.getResult(MRTask.java:670)
    at water.MRTask.doAll(MRTask.java:530)
    at water.MRTask.doAll(MRTask.java:482)
    at hex.glm.GLM$GLMDriver.fitCoeffs(GLM.java:1334)
    at hex.glm.GLM$GLMDriver.fitHGLM(GLM.java:1505)
    at hex.glm.GLM$GLMDriver.fitModel(GLM.java:2060)
    at hex.glm.GLM$GLMDriver.computeSubmodel(GLM.java:2526)
    at hex.glm.GLM$GLMDriver.doCompute(GLM.java:2664)
    at hex.glm.GLM$GLMDriver.computeImpl(GLM.java:2561)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:247)
    at hex.glm.GLM$GLMDriver.compute2(GLM.java:1188)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1658)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:976)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)





Edit 2:


I actually found a way to remove the above error, by adding


rand_link = c(""identity""),
rand_family = c(""gaussian""),



to the 
h2o.glm
 arguments:


h2o.glm(family = ""gaussian"",
                     rand_link = c(""identity""),
                     rand_family = c(""gaussian""),
                     # compute_p_values = TRUE,
                     x = ""x"",
                     y = ""y"",
                     training_frame = df2,
                     random_columns = c(3),
                     HGLM = TRUE,
                     lambda = 0)



Works. But when I set 
compute_p_values = TRUE
, and then find a new error:



Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  

ERROR MESSAGE:

degrees of freedom (0)","['r', 'h2o', 'glm']",Unknown,,N/A
71616995,71616995,2022-03-25T12:26:18,2022-03-25 20:41:20Z,0,"I am running Spark 2.4.4 using Yarn and interfacing using RSparkling and Sparklyr


As per 
these
 instructions I've




Installed Sparklyr


Called the library for Sparklyr


Removed any prior installs of H2O


Installed the latest version of H2O (rel-zorn)


Installed rsparkling 3.36.0.3-1-2.4


Called the library for rsparkling


Specified my spark_config()


Successfully made a connection to Spark using Yarn


Ran h2oConf <- H2OConf()




When I try to make a H2O context using the h2oConf above I get the following error:


Error in h2o.init(strict_version_check = FALSE, https = https, insecure = insecure,  : 
  unused argument (cacert = conf$sslCACert())



I've tried multiple different versions of RSparkling and H2O and have been unsuccessful connecting.


Is there some obvious step that I'm missing? Any suggestions would be greatly appreciated.


Solution:
Based on feedback from @Marek Novotny below I discovered that I had a reference to an old version of H2O in my namespace. Once I unloaded the package I was able to resolve this issue and moved on to the 
next","['r', 'rstudio', 'h2o', 'sparklyr', 'h2o.ai']",Unknown,,N/A
71609592,71609592,2022-03-24T21:23:05,2022-03-25 12:19:19Z,0,"I am trying to get H2O working with Sparklyr on my spark cluster (yarn)


spark_version(sc) = 2.4.4

My spark cluster is running V2.4.4


According to 
this
 page the compatible version with my spark is 2.4.5 for Sparkling Water and the H2O release is rel-xu patch version 3. However when I install this version I am prompted to update my H2O install to the next release (REL-ZORN). Between the H2O guides and the sparklyr guides it's very confusing and contradictory at times.




Since this is a yarn deployment and not local, unfortunately I can't provide a repex to help with trobleshooting.


url <- ""http://h2o-release.s3.amazonaws.com/sparkling-water/rel-2.4/5/sparkling-water-2.4.5.zip""

download.file(url = url,""sparkling-water-2.4.5.zip"")

unzip(""sparkling-water-2.4.5.zip"")

# RUN THESE CMDs FROM THE TERMINAL
cd sparkling-water-2.4.5
bin/sparkling-shell --conf ""spark.executor.memory=1g""

# RUN THESE FROM WITHIN RSTUDIO
install.packages(""sparklyr"")
library(sparklyr)

# REMOVE PRIOR INSTALLS OF H2O
detach(""package:rsparkling"", unload = TRUE)
if (""package:h2o"" %in% search()) { detach(""package:h2o"", unload = TRUE) }
if (isNamespaceLoaded(""h2o"")){ unloadNamespace(""h2o"") }
remove.packages(""h2o"")

# INSTALLING REL-ZORN (3.36.0.3) WHICH IS REQUIRED FOR SPARKLING WATER 3.36.0.3
install.packages(""h2o"", type = ""source"", repos = ""https://h2o-release.s3.amazonaws.com/h2o/rel-zorn/3/R"")

# INSTALLING FROM S3 SINCE CRAN NO LONGER SUPPORTED
install.packages(""rsparkling"", type = ""source"", repos = ""http://h2o-release.s3.amazonaws.com/sparkling-water/spark-2.4/3.36.0.3-1-2.4/R"")

# AS PER THE GUIDE
options(rsparkling.sparklingwater.version = ""2.4.5"")
library(rsparkling)

# SPECIFY THE CONFIGURATION
config <- sparklyr::spark_config()
config[[""spark.yarn.queue""]] <- ""my_data_science_queue""
config[[""sparklyr.backend.timeout""]] <- 36000
config[[""spark.executor.cores""]] <- 32
config[[""spark.driver.cores""]] <- 32
config[[""spark.executor.memory""]] <- ""40g""
config[[""spark.executor.instances""]] <- 8
config[[""sparklyr.shell.driver-memory""]] <- ""16g""
config[[""spark.default.parallelism""]] <- ""8""
config[[""spark.rpc.message.maxSize""]] <- ""256""

# MAKE A SPARK CONNECTION
sc <- sparklyr::spark_connect(
  master = ""yarn"",
  spark_home = ""/opt/mapr/spark/spark"",
  config = config,
  log = ""console"",
  version = ""2.4.4""
)



When I try to establish a H2O context using the next chunk I get the following error


h2o_context(sc)

Error in h2o_context(sc) : could not find function ""h2o_context""



Any pointers as to where I'm going wrong would be greatly appreciated.","['r', 'apache-spark', 'h2o', 'sparklyr']",Phil,https://stackoverflow.com/users/5221626/phil,"8,077"
71587437,71587437,2022-03-23T12:42:50,2022-03-23 12:42:50Z,0,"when running h2o.init() (using the latest package version) from r, it is creating libxgboost4j_gpu*.so files within temp directory which take significant amount of space and pile up (when run several times). Does anyone know how to stop this?","['r', 'h2o', 'temp']",martin_hulin,https://stackoverflow.com/users/14849337/martin-hulin,43
71529386,71529386,2022-03-18T15:26:31,2022-09-09 01:15:54Z,665,"I am currently trying to use H2O from Python, and I encounter some problems on my Mac OS with XGBoost.
It seems like H2O does not find it anywhere.


More precisely, the next simple snippet


import pandas as pd
import h2o

data = [['2015-01-01', '2490.925806' , '-0.41'],
        ['2015-01-02', '2412.623113' , '-0.48'],
        ['2015-01-03', '2365.611276' , '-0.55']]
df = pd.DataFrame(data, columns=[""time"", ""base"", ""target""]).set_index(""time"", drop=True)

h2o.init(nthreads=-1)
estimator = h2o.estimators.H2OXGBoostEstimator()
training_frame = h2o.H2OFrame(df) 
estimator.train([""base""], ""target"", training_frame)



gives me the error :


H2OResponseError: Server error water.exceptions.H2ONotFoundArgumentException:
  Error: POST /3/ModelBuilders/xgboost not found
  Request: POST /3/ModelBuilders/xgboost
    data: {'training_frame': 'Key_Frame__upload_893634781f588299bbd20d51c98d43a9.hex', 'nfolds': '0', 'keep_cross_validation_models': 'True', 'keep_cross_validation_predictions': 'False', 'keep_cross_validation_fold_assignment': 'False', 'score_each_iteration': 'False', 'fold_assignment': 'auto', 'response_column': 'target', 'ignore_const_cols': 'True', 'stopping_rounds': '0', 'stopping_metric': 'auto', 'stopping_tolerance': '0.001', 'max_runtime_secs': '0.0', 'seed': '-1', 'distribution': 'auto', 'tweedie_power': '1.5', 'categorical_encoding': 'auto', 'quiet_mode': 'True', 'ntrees': '50', 'max_depth': '6', 'min_rows': '1.0', 'min_child_weight': '1.0', 'learn_rate': '0.3', 'eta': '0.3', 'sample_rate': '1.0', 'subsample': '1.0', 'col_sample_rate': '1.0', 'colsample_bylevel': '1.0', 'col_sample_rate_per_tree': '1.0', 'colsample_bytree': '1.0', 'colsample_bynode': '1.0', 'max_abs_leafnode_pred': '0.0', 'max_delta_step': '0.0', 'score_tree_interval': '0', 'min_split_improvement': '0.0', 'gamma': '0.0', 'nthread': '-1', 'build_tree_one_node': 'False', 'calibrate_model': 'False', 'max_bins': '256', 'max_leaves': '0', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': '0.0', 'one_drop': 'False', 'skip_drop': '0.0', 'tree_method': 'auto', 'grow_policy': 'depthwise', 'booster': 'gbtree', 'reg_lambda': '1.0', 'reg_alpha': '0.0', 'dmatrix_type': 'auto', 'backend': 'auto', 'gainslift_bins': '-1', 'auc_type': 'auto', 'scale_pos_weight': '1.0'}



For more information about my distribution:




OS: Monterey 12.3


Processor: Apple M1


Python: 3.9.10


H2O: 3.36.0.3




I suspect Apple M1 to be the cause of the error, but is that really the case ?","['xgboost', 'h2o', 'apple-m1']",Clej,https://stackoverflow.com/users/11963167/clej,466
71412682,71412682,2022-03-09T16:24:41,2022-03-09 16:24:41Z,0,"I have been trying to replicate the results from cross-validation from 
h2o
 by hand, but I'm puzzled on why the results are different. In my understanding, if a 
fold_column
 is specified, then cross-validation happens according to the groups defined within that column. Therefore, for fold 1, the model should train on folds 2, 3, 4 and 5.


set.seed(1)
iris$fold <- sample(1:5, nrow(iris), replace = TRUE)

fit_cv <- h2o.glm(x = c(""Species"", ""Petal.Width"", ""Petal.Length""),
                  y = c(""Sepal.Length""),
                  training_frame = as.h2o(iris),
                  fold_column = ""fold"", 
                  alpha = 0.2, lambda = 0.1, seed=1,
                  keep_cross_validation_models = TRUE)



This outputs for fold 1 a training MAE of:


fit_fold1 <-  h2o.cross_validation_models(fit_cv)[[1]]

> fit_fold1@model$training_metrics@metrics$mae
[1] 0.3035526



However, if fitted by hand, the training MAE is (slightly) different:


dd_fold1_fit <- iris[iris$fold != 1,]

fit_fold1_direct <- h2o.glm(x = c(""Species"", ""Petal.Width"", ""Petal.Length""),
                                y = c(""Sepal.Length""),
                                training_frame = as.h2o(dd_fold1_fit),
                                alpha = 0.2, lambda = 0.1, 
                                seed=1)
    
fit_fold1_direct@model$training_metrics@metrics$mae
[1] 0.3004132



This is a minimal example, the issue is that with real data that I am working with the difference is larger, and I can't replicate the cross-validation results.


Any idea on what the explanation could be?","['r', 'h2o']",Theodor,https://stackoverflow.com/users/2236837/theodor,"1,016"
71172747,71172747,2022-02-18T11:30:19,2022-02-23 13:57:32Z,96,"I configured h2o application to use openLDAP configuration referring to the link:

https://docs.h2o.ai/sparkling-water/2.3/latest-stable/doc/tutorials/ldap.html

Added userNameAttribute=uid to ovreride 'cn' to use 'uid' field instead.


However it is not working. The query getting passed on LDAP still has cn=user_id instead of uid=user_id.


Am I doing something wrong or is a bug?


ldap.conf:


    ai.h2o.org.eclipse.jetty.plus.jaas.spi.LdapLoginModule required
    debug=""true""
    useLdaps=""true""
    contextFactory=""com.sun.jndi.ldap.LdapCtxFactory""
    hostname=""ldap.h2o.ai""
    port=""389""
    bindDn=""cn=admin,dc=h2o,dc=ai""
    bindPassword=""h2o""
    authenticationMethod=""simple""
    forceBindingLogin=""true""
    userBaseDn=""ou=users,dc=h2o,dc=ai""
    userNameAttribute=""uid"";
};```","['ldap', 'h2o', 'openldap']",Pankaj Mahadik,https://stackoverflow.com/users/16570951/pankaj-mahadik,31
71146238,71146238,2022-02-16T17:07:49,2022-02-17 00:02:39Z,156,"We use XGBoost model for regression prediction model, We use XGBoost as grid search hyper parameter tuning process,


We run this model on 90GB h2o cluster. This process now running over 1.2 years, but suddenly this process stop due to ""Closing connection _sid_af1c at exit""


Training data set is 800 000, due to this error we decreased it to 500 000 but same error occurred.


ntrees - 300,400


depth - 8.10


variables - 382


I have attached H2o memory log and our application error log. Could you please support to fixed this issue.


----------------------------------------H2o Log [Start]----------------------

**We start H2o as 2 node cluster, but h2o log crated on one node.** 

INFO water.default: ----- H2O started  -----
INFO water.default: Build git branch: master
INFO water.default: Build git hash: 0588cccd72a7dc1274a83c30c4ae4161b92d9911
INFO water.default: Build git describe: jenkins-master-5236-4-g0588ccc
INFO water.default: Build project version: 3.33.0.5237
INFO water.default: Build age: 1 year, 3 months and 17 days
INFO water.default: Built by: 'jenkins'
INFO water.default: Built on: '2020-10-27 19:21:29'
WARN water.default: 
WARN water.default: *** Your H2O version is too old! Please download the latest version from http://h2o.ai/download/ ***
WARN water.default: 
INFO water.default: Found H2O Core extensions: [XGBoost, KrbStandalone]
INFO water.default: Processed H2O arguments: [-flatfile, /usr/local/h2o/flatfile.txt, -port, 54321]
INFO water.default: Java availableProcessors: 20
INFO water.default: Java heap totalMemory: 962.5 MB
INFO water.default: Java heap maxMemory: 42.67 GB
INFO water.default: Java version: Java 1.8.0_262 (from Oracle Corporation)
INFO water.default: JVM launch parameters: [-Xmx48g]
INFO water.default: JVM process id: 
[email protected]

INFO water.default: OS version: Linux 3.10.0-1127.10.1.el7.x86_64 (amd64)
INFO water.default: Machine physical memory: 62.74 GB
INFO water.default: Machine locale: en_US
INFO water.default: X-h2o-cluster-id: 1644769990156
INFO water.default: User name: 'root'
INFO water.default: IPv6 stack selected: false
INFO water.default: Possible IP Address: ens192 (ens192), xxxxxxxxxxxxxxxxxxxx
INFO water.default: Possible IP Address: ens192 (ens192), xxxxxxxxxxx
INFO water.default: Possible IP Address: lo (lo), 0:0:0:0:0:0:0:1%lo
INFO water.default: Possible IP Address: lo (lo), 127.0.0.1
INFO water.default: H2O node running in unencrypted mode.
INFO water.default: Internal communication uses port: 54322
INFO water.default: Listening for HTTP and REST traffic on http://xxxxxxxxxxxx:54321/
INFO water.default: H2O cloud name: 'root' on /xxxxxxxxxxxx:54321, discovery address /xxxxxxxxxxxx:57653
INFO water.default: If you have trouble connecting, try SSH tunneling from your local machine (e.g., via port 55555):
INFO water.default:   1. Open a terminal and run 'ssh -L 55555:localhost:54321 root@xxxxxxxxxxxx'
INFO water.default:   2. Point your browser to http://localhost:55555
INFO water.default: Log dir: '/tmp/h2o-root/h2ologs'
INFO water.default: Cur dir: '/usr/local/h2o/h2o-3.33.0.5237'
INFO water.default: Subsystem for distributed import from HTTP/HTTPS successfully initialized
INFO water.default: HDFS subsystem successfully initialized
INFO water.default: S3 subsystem successfully initialized
INFO water.default: GCS subsystem successfully initialized
INFO water.default: Flow dir: '/root/h2oflows'
INFO water.default: Cloud of size 1 formed [/xxxxxxxxxxxx:54321]
INFO water.default: Registered parsers: [GUESS, ARFF, XLS, SVMLight, AVRO, PARQUET, CSV]
INFO water.default: XGBoost extension initialized
INFO water.default: KrbStandalone extension initialized
INFO water.default: Registered 2 core extensions in: 2632ms
INFO water.default: Registered H2O core extensions: [XGBoost, KrbStandalone]
INFO hex.tree.xgboost.XGBoostExtension: Found XGBoost backend with library: xgboost4j_gpu
INFO hex.tree.xgboost.XGBoostExtension: XGBoost supported backends: [WITH_GPU, WITH_OMP]
INFO water.default: Registered: 217 REST APIs in: 353ms
INFO water.default: Registered REST API extensions: [Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4]
INFO water.default: Registered: 291 schemas in 112ms
INFO water.default: H2O started in 4612ms
INFO water.default: 
INFO water.default: Open H2O Flow in your web browser: http://xxxxxxxxxxxx:54321
INFO water.default: 
INFO water.default: Cloud of size 2 formed [mastera.xxxxxxxxxxxx.com/xxxxxxxxxxxx:54321, masterb.xxxxxxxxxxxx.com/xxxxxxxxxxxx:54321]
INFO water.default: Locking cloud to new members, because water.rapids.Session$1
INFO hex.tree.xgboost.task.XGBoostUpdater: Initial Booster created, size=448
ERROR water.default: Got IO error when sending a batch of bytes: 
java.io.IOException: Connection reset by peer
    at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
    at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
    at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
    at sun.nio.ch.IOUtil.write(IOUtil.java:51)
    at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:468)
    at water.H2ONode$SmallMessagesSendThread.sendBuffer(H2ONode.java:605)
    at water.H2ONode$SmallMessagesSendThread.run(H2ONode.java:588)
----------------------------------------H2o Log [End]--------------------------------

----------------------------------------Application Log [Start]----------------------
Checking whether there is an H2O instance running at http://localhost:54321 . connected.
Warning: Your H2O cluster version is too old (1 year, 3 months and 17 days)! Please download and install the latest version from http://h2o.ai/download/
--------------------------  ------------------------------------------------------------------
H2O_cluster_uptime:         19 mins 49 secs
H2O_cluster_timezone:       Asia/Colombo
H2O_data_parsing_timezone:  UTC
H2O_cluster_version:        3.33.0.5237
H2O_cluster_version_age:    1 year, 3 months and 17 days !!!
H2O_cluster_name:           root
H2O_cluster_total_nodes:    2
H2O_cluster_free_memory:    84.1 Gb
H2O_cluster_total_cores:    40
H2O_cluster_allowed_cores:  40
H2O_cluster_status:         locked, healthy
H2O_connection_url:         http://localhost:54321
H2O_connection_proxy:       {""http"": null, ""https"": null}
H2O_internal_security:      False
H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4
Python_version:             3.7.0 final
--------------------------  ------------------------------------------------------------------
--------------------------  ------------------------------------------------------------------
H2O_cluster_uptime:         19 mins 49 secs
H2O_cluster_timezone:       Asia/Colombo
H2O_data_parsing_timezone:  UTC
H2O_cluster_version:        3.33.0.5237
H2O_cluster_version_age:    1 year, 3 months and 17 days !!!
H2O_cluster_name:           root
H2O_cluster_total_nodes:    2
H2O_cluster_free_memory:    84.1 Gb
H2O_cluster_total_cores:    40
H2O_cluster_allowed_cores:  40
H2O_cluster_status:         locked, healthy
H2O_connection_url:         http://localhost:54321
H2O_connection_proxy:       {""http"": null, ""https"": null}
H2O_internal_security:      False
H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4
Python_version:             3.7.0 final
--------------------------  ------------------------------------------------------------------
release memory here...
Checking whether there is an H2O instance running at http://localhost:54321 . connected.
Warning: Your H2O cluster version is too old (1 year, 3 months and 17 days)! Please download and install the latest version from http://h2o.ai/download/
--------------------------  ------------------------------------------------------------------
H2O_cluster_uptime:         19 mins 49 secs
H2O_cluster_timezone:       Asia/Colombo
H2O_data_parsing_timezone:  UTC
H2O_cluster_version:        3.33.0.5237
H2O_cluster_version_age:    1 year, 3 months and 17 days !!!
H2O_cluster_name:           root
H2O_cluster_total_nodes:    2
H2O_cluster_free_memory:    84.1 Gb
H2O_cluster_total_cores:    40
H2O_cluster_allowed_cores:  40
H2O_cluster_status:         locked, healthy
H2O_connection_url:         http://localhost:54321
H2O_connection_proxy:       {""http"": null, ""https"": null}
H2O_internal_security:      False
H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4
Python_version:             3.7.0 final
--------------------------  ------------------------------------------------------------------
Parse progress: |█████████████████████████████████████████████████████████| 100%
xgboost Grid Build progress: |████████Closing connection _sid_af1c at exit
H2O session _sid_af1c was not closed properly.
Closing connection _sid_9313 at exit
H2O session _sid_9313 was not closed properly.
----------------------------------------Application Log [End]----------------------","['machine-learning', 'artificial-intelligence', 'xgboost', 'h2o']",James Z,https://stackoverflow.com/users/4420967/james-z,12.3k
71138526,71138526,2022-02-16T08:38:17,2022-02-16 23:54:28Z,280,"I am using h2o autoML on python.


I used the autoML part to find the best model possible: it is a StackedEnsemble.


Now I would like to take the model and retrain it on a bigger dataset (which was not possible before because I would explode the google colab free RAM capacity).


But AutoML does some preprocessing to my data and I don't know which one.


How can I get the preprocessing steps to re-apply it to my bigger data before feeding it to the model ?


Thanks in advance,


Gab","['python', 'google-colaboratory', 'h2o', 'automl', 'data-preprocessing']",Gabiboubibel,https://stackoverflow.com/users/13354125/gabiboubibel,75
71093974,71093974,2022-02-12T16:58:15,2022-02-17 00:06:57Z,0,"I have just computed a gamma GLM with the h2o package in R.


When I'm trying to predict on the test set I get this error:




Illegal argument(s) for GLM model: GLM_model_R_1644680218230_95. Details: ERRR on field: _family: Response value for gamma distribution must be greater than 0.




I understand that a gamma model cannot be trained on data with zero response, but one should be able to predict on data with a true value 0 (this is used a a lot in actuaries).


Does any one know a h2o solution? I know I can simply make the model with glm() or something similar, but I'm relying on mean encoded categorical variables (which is really convenient in h2o).


Thanks!","['r', 'h2o', 'glm', 'glmnet']",marc_s,https://stackoverflow.com/users/13302/marc-s,752k
71081912,71081912,2022-02-11T14:41:22,2022-02-16 23:46:43Z,0,"I am running a randomForest model using h2o in R. The exercise is a binary classification problem in which I have approx. 5x as many ‘1’s as ‘0’s.


Because the dataset is a time series (approx. 50k observations), I am using a growing window validation scheme as opposed to the typical CV schemes used in most ML cases. For each step I train the model on 60% of the data available up to that time point and then hold out the remaining 40% to be split equally between the validation frame and the testing data.


I am getting extremely poor performance in both my hold out validation data and test data which suggests that I am overfitting to the training dataset. If anyone can spot any obvious, glaring errors in how I am setting up my model and the hyper parameter search I’d really appreciate some pointers. It may be that I simply have in sufficient number of features (n = 27) to adequately capture the response, but I would like to rule out incorrect model specification first.


Here is the specification of my model and the hyper-parameter grid search.


# create feature names
y <- ""Response""
x <- setdiff(names(gwWF1_train[, -c(1:3)]), y)

# turn training set into h2o object
gwWF1_train.h2o <- as.h2o(gwWF1_train[, -c(1:3)])

# turn validation set into h2o object
gwWF1_valid.h2o <- as.h2o(gwWF1_valid[, -c(1:3)])

# hyperparameter grid
hyper_grid.h2o <- list(
  ntrees      = seq(50, 1000, by = 50),
  mtries      = seq(2, 10, by = 1),
  max_depth   = seq(2, 10, by = 1),
  min_rows    = seq(5, 15, by = 1),
  nbins       = seq(5, 40, by = 5),
  sample_rate = c(.55, .632, .75)
)

# random grid search criteria
search_criteria <- list(
  strategy = ""RandomDiscrete"",
  stopping_metric = ""auc"",
  stopping_tolerance = 0.005,
  stopping_rounds = 20,
  max_runtime_secs = 5*60
  )

# build grid search 
random_grid <- h2o.grid(
  algorithm = ""randomForest"",
  grid_id = ""rf_grid"",
  x = x, 
  y = y, 
  seed = 29,
  balance_classes = TRUE,
  training_frame = gwWF1_train.h2o,
  validation_frame = gwWF1_valid.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria
  )
  
# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
  grid_id = ""rf_grid"", 
  sort_by = ""auc"", 
  decreasing = TRUE
  )
print(grid_perf)



Here is the performance on the training and validation frames


Model Details:
==============

H2OBinomialModel: drf
Model ID:  rf_grid_model_14 
Model Summary: 
  number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves
1             100                      100              224183        10        10   10.00000         99        272   173.86000


H2OBinomialMetrics: drf
** Reported on training data. **
** Metrics reported on Out-Of-Bag training samples **

MSE:  0.1154841
RMSE:  0.3398295
LogLoss:  0.3836627
Mean Per-Class Error:  0.3434005
AUC:  0.716039
AUCPR:  0.3877677
Gini:  0.432078
R^2:  0.08126146

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
           0    1    Error         Rate
0      12089 2009 0.142502  =2009/14098
1       1327 1111 0.544299   =1327/2438
Totals 13416 3120 0.201742  =3336/16536

Maximum Metrics: Maximum metrics at their respective thresholds
                        metric threshold        value idx
1                       max f1  0.182031     0.399784 175
2                       max f2  0.133141     0.498254 264
3                 max f0point5  0.229674     0.437443 119
4                 max accuracy  0.263718     0.863147  94
5                max precision  0.705162     1.000000   0
6                   max recall  0.019641     1.000000 396
7              max specificity  0.705162     1.000000   0
8             max absolute_mcc  0.209525     0.298843 139
9   max min_per_class_accuracy  0.153280     0.651354 225
10 max mean_per_class_accuracy  0.174269     0.661033 188
11                     max tns  0.705162 14098.000000   0
12                     max fns  0.705162  2437.000000   0
13                     max fps  0.010717 14098.000000 399
14                     max tps  0.019641  2438.000000 396
15                     max tnr  0.705162     1.000000   0
16                     max fnr  0.705162     0.999590   0
17                     max fpr  0.010717     1.000000 399
18                     max tpr  0.019641     1.000000 396

Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`
H2OBinomialMetrics: drf
** Reported on validation data. **

MSE:  0.1340771
RMSE:  0.3661654
LogLoss:  0.4422115
Mean Per-Class Error:  0.5
AUC:  0.5295048
AUCPR:  0.1770512
Gini:  0.05900952
R^2:  -0.006636504

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
       0     1    Error          Rate
0      0 13220 1.000000  =13220/13220
1      0  2485 0.000000       =0/2485
Totals 0 15705 0.841770  =13220/15705

Maximum Metrics: Maximum metrics at their respective thresholds
                        metric threshold        value idx
1                       max f1  0.025167     0.273227 399
2                       max f2  0.025167     0.484500 399
3                 max f0point5  0.161209     0.203459 140
4                 max accuracy  0.291947     0.841770   1
5                max precision  0.291947     0.500000   1
6                   max recall  0.025167     1.000000 399
7              max specificity  0.302964     0.999924   0
8             max absolute_mcc  0.178262     0.056716 103
9   max min_per_class_accuracy  0.133292     0.525553 215
10 max mean_per_class_accuracy  0.140828     0.529093 195
11                     max tns  0.302964 13219.000000   0
12                     max fns  0.302964  2485.000000   0
13                     max fps  0.030503 13220.000000 398
14                     max tps  0.025167  2485.000000 399
15                     max tnr  0.302964     0.999924   0
16                     max fnr  0.302964     1.000000   0
17                     max fpr  0.030503     1.000000 398
18                     max tpr  0.025167     1.000000 399

Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`



And here is the performance on the testing data


H2OBinomialMetrics: drf

MSE:  0.1293883
RMSE:  0.3597059
LogLoss:  0.4274263
Mean Per-Class Error:  0.4719334
AUC:  0.530868
AUCPR:  0.1588326
Gini:  0.06173602
R^2:  -0.001929629

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
          0     1    Error          Rate
0      4321 12888 0.748910  =12888/17209
1       603  2490 0.194956     =603/3093
Totals 4924 15378 0.664516  =13491/20302

Maximum Metrics: Maximum metrics at their respective thresholds
                        metric threshold        value idx
1                       max f1  0.128344     0.269612 283
2                       max f2  0.084278     0.473644 380
3                 max f0point5  0.140284     0.194700 248
4                 max accuracy  0.313605     0.847601   1
5                max precision  0.313605     0.333333   1
6                   max recall  0.050387     1.000000 399
7              max specificity  0.316369     0.999884   0
8             max absolute_mcc  0.128344     0.047063 283
9   max min_per_class_accuracy  0.152158     0.522793 210
10 max mean_per_class_accuracy  0.140284     0.531315 248
11                     max tns  0.316369 17207.000000   0
12                     max fns  0.316369  3093.000000   0
13                     max fps  0.062918 17209.000000 398
14                     max tps  0.050387  3093.000000 399
15                     max tnr  0.316369     0.999884   0
16                     max fnr  0.316369     1.000000   0
17                     max fpr  0.062918     1.000000 398
18                     max tpr  0.050387     1.000000 399

Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`","['r', 'machine-learning', 'random-forest', 'h2o']",DaveTheRave,https://stackoverflow.com/users/12774843/davetherave,77
71068729,71068729,2022-02-10T16:27:26,2022-02-10 17:39:03Z,0,"I want to make sure the weights_column arguments in h2o.glm() is the same as the weights argument in glm(). To compare, I am looking at the rmse of both models using the Seatbelts dataset in R. I don't think a weight is needed in this model, but for the sake of demonstration I added one.


head(Seatbelts)

Seatbelts<-Seatbelts[complete.cases(Seatbelts),]

## 75% of the sample size
smp_size <- floor(0.75 * nrow(Seatbelts))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(Seatbelts)), size = smp_size)

train <- Seatbelts[train_ind, ]
test <- Seatbelts[-train_ind, ]

# glm()
m1 <- glm(DriversKilled  ~  front + rear + kms + PetrolPrice + VanKilled + law,
          family=poisson(link = ""log""),
          weights = drivers,
          data=train)

pred <- predict(m1, test)
RMSE(pred = pred, obs = test$DriversKilled)



The rmse is 120.5797.


# h2o.glm()
library(h2o)
h2o.init()

train <- as.h2o(train)
test <- as.h2o(test)
m2 <- h2o.glm(x = c(""front"", ""rear"", ""kms"", ""PetrolPrice"", ""VanKilled"", ""law""),
                 y = ""DriversKilled"",
                 training_frame = train,
                 family = 'poisson',
                 link = 'log',
                 lambda = 0,
                 weights_column = ""drivers"")

# performance metrics on test data
h2o.performance(m2, test)



The rmse is 18.65627. Why do these models have such different rmse? Am I using the weights_column argument in h2o.glm() incorrectly?","['r', 'h2o', 'glm']",Unknown,,N/A
71063715,71063715,2022-02-10T10:46:21,2022-02-16 23:34:54Z,0,"So I'm about to write my master thesis, and doing some data modelling/scoring in R, where I have some issues. I have been given a ReadMe file to follow with older versions of R, but did not seem to work, and therefore I have tried to update R and the code for it. But I have never modelled with h2o in R, and it makes an error when I try to loadmodel from the given code of the author. Can anyone help me with a solution?


I run this code:
w2v_model <- h2o.loadModel(w2v_fname)


which results in this error:
ERROR: Unexpected HTTP Status code: 400 Bad Request (url = http://localhost:54321/99/Models.bin/)


java.lang.IllegalArgumentException
[1] ""java.lang.IllegalArgumentException: Found version 3.10.4.6, but running version 3.36.0.2\n\nFor more information visit:\n  
http://jira.h2o.ai/browse/TN-14""

[2] ""    water.AutoBuffer.checkVersion(AutoBuffer.java:296)""

[3] ""    water.AutoBuffer.(AutoBuffer.java:277)""

[4] ""    water.AutoBuffer.(AutoBuffer.java:257)""

[5] ""    hex.Model.importBinaryModel(Model.java:3192)""

[6] ""    water.api.ModelsHandler.importModel(ModelsHandler.java:250)""

[7] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""

[8] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)""

[9] ""    java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""

[10] ""    java.base/java.lang.reflect.Method.invoke(Method.java:568)""

[11] ""    water.api.Handler.handle(Handler.java:60)""

[12] ""    water.api.RequestServer.serve(RequestServer.java:470)""

[13] ""    water.api.RequestServer.doGeneric(RequestServer.java:301)""

[14] ""    water.api.RequestServer.doPost(RequestServer.java:227)""

[15] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:707)""

[16] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:790)""

[17] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)""

[18] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)""

[19] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)""

[20] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)""

[21] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)""

[22] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)""

[23] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)""

[24] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)""

[25] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)""

[26] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""

[27] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""

[28] ""    water.webserver.jetty9.Jetty9ServerAdapter$LoginHandler.handle(Jetty9ServerAdapter.java:130)""

[29] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""

[30] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""

[31] ""    org.eclipse.jetty.server.Server.handle(Server.java:531)""

[32] ""    org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)""

[33] ""    org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)""

[34] ""    org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)""

[35] ""    org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)""

[36] ""    org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)""

[37] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)""

[38] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)""

[39] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)""

[40] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)""

[41] ""    org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)""

[42] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)""

[43] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)""

[44] ""    java.base/java.lang.Thread.run(Thread.java:833)""


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  :


ERROR MESSAGE:


Found version 3.10.4.6, but running version 3.36.0.2


For more information visit:

http://jira.h2o.ai/browse/TN-14


Thanks in advance.","['r', 'h2o']",Super Kai - Kazuya Ito,https://stackoverflow.com/users/3247006/super-kai-kazuya-ito,1
70759860,70759860,2022-01-18T17:26:35,2022-02-17 00:05:26Z,107,"I have a requirement to add new algorithms built in Python/Java to H2O and making it available in Flow UI. I have not found much information other than these old posts




https://groups.google.com/g/h2ostream/c/lFXdizcgemE?pli=1


https://www.h2o.ai/blog/hacking-algorithms-into-h2o-quantiles/




Please let me know if there is a way to do it. I don't think these articles are still relevant. Thanks in advance!",['h2o'],python dev,https://stackoverflow.com/users/2020301/python-dev,219
70721119,70721119,2022-01-15T11:47:31,2022-01-17 06:22:36Z,190,"Is the H2OModelSelectionEstimator deprecated? When I run the code


from h2o.estimators import H2OModelSelectionEstimator



I get the message: ImportError: cannot import name 'H2OModelSelectionEstimator' from 'h2o.estimators'","['python', 'h2o', 'glm']",lostwanderer,https://stackoverflow.com/users/12029329/lostwanderer,163
70637288,70637288,2022-01-08T23:33:47,2023-07-13 18:03:24Z,112,"I see Snowflake has a partner connect through which I could activate the H2O Driverless AI and access Snowflake from there.
I also see that H2O Driverless AI can be independently deployed on any cloud cluster, by we managing our own cluster instances.


How are both the clusters in above different?
In the H2O Driverless AI activated through the partner connect from Snowflake, don't we not require to manage the instances of H2O Driverless AI, so are we charged accordingly for that?


In the H2O Driverless AI deployed on our own Cloud cluster instances, is it the licensed version of H2O Driverless AI that we deploy and manage? Also, can we deploy the H2O-3(h2o flow) on these instances for building using h20 python packages, since i don't see any notebooks on Driverless AI for developing from ground-up?","['h2o', 'driverless-ai', 'h2o.ai']",topchef,https://stackoverflow.com/users/59470/topchef,19.8k
70622044,70622044,2022-01-07T13:35:43,2022-01-12 08:48:55Z,72,"h2o version: h2o-3.34.0.3 (rel-zizler)


Java version: openjdk version ""15.0.2"" 2021-01-19
(installed  with: 
FROM adoptopenjdk:15-jre-openj9-focal
)


I want to build an XGBoost model using Java 15, but the same code with the same data which runs without issues on Java 14 (openjdk version ""14.0.2"" 2020-07-14) fails on Java 15, producing the following error messages:


water.exceptions.H2OIllegalArgumentException: Illegal argument: o of function: IcedWrapper: 
    at water.IcedWrapper.<init>(IcedWrapper.java:152) ~[h2o.jar:?]
    at water.util.TwoDimTable.set(TwoDimTable.java:254) ~[h2o.jar:?]
    at water.util.ReproducibilityInformationUtils.createNodeInformationTable(ReproducibilityInformationUtils.java:72) ~[h2o.jar:?]
    at hex.Model$Output.createReproducibilityInformationTable(Model.java:1199) ~[h2o.jar:?]
    at hex.Model$Output.<init>(Model.java:991) ~[h2o.jar:?]
    at hex.Model$Output.<init>(Model.java:973) ~[h2o.jar:?]
    at hex.tree.xgboost.XGBoostOutput.<init>(XGBoostOutput.java:16) ~[h2o.jar:?]
    at hex.tree.xgboost.XGBoost$XGBoostDriver.buildModelImpl(XGBoost.java:419) ~[h2o.jar:?]
    at hex.tree.xgboost.XGBoost$XGBoostDriver.buildModel(XGBoost.java:393) ~[h2o.jar:?]
    at hex.tree.xgboost.XGBoost$XGBoostDriver.computeImpl(XGBoost.java:379) ~[h2o.jar:?]
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:246) ~[h2o.jar:?]
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1652) ~[h2o.jar:?]
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468) ~[h2o.jar:?]
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263) [h2o.jar:?]
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974) [h2o.jar:?]
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477) [h2o.jar:?]
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) [h2o.jar:?] 



I launch the h2o server with the following command:


ENTRYPOINT /bin/bash -c ""cd h2o && java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=1 -XshowSettings:vm -jar h2o.jar


Has anyone came across with similar issues? It looks like a version incompatibility to me, but based on a comment from this, h2o should support Java 15 from version 3.32.1.1 and up.

Running H2O with Java 16 on R","['java', 'xgboost', 'h2o']",Unknown,,N/A
70597370,70597370,2022-01-05T17:49:20,2022-01-06 14:54:28Z,0,"Using the h2o package for R, I created a set of base models using AutoML with StackedEnsemble's disabled. Thus, the set of models only contains the base models that AutoML generates by default (GLM, GBM, XGBoost, DeepLearning, and DRF). Using these base models I was able to successfully train a default stacked ensemble manually using the h2o.stackedEnsemble function (i.e., a GLM with default params). I exported the model as a MOJO, shutdown the H2O cluster, restarted R, initialized a new H2O cluster, imported the stacked ensemble MOJO, and successfully generated predictions on a new validation set.


So far so good.


Next, I did the exact same thing following the exact same process, but this time I made one change: I trained the stacked ensemble 
with all pairwise interactions between the base models
. The interactions were created automatically by feeding a list of the base model Ids to the interaction metalearner_parameter. The model appeared to train without issue and (as I described above) was able to export it as a MOJO, restart the h2o cluster, restart R, and import the MOJO. However, when I attempt to generate predictions on the same validation set I used above I get the following error:


DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.ArrayIndexOutOfBoundsException

DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.ArrayIndexOutOfBoundsException
    at water.MRTask.getResult(MRTask.java:660)
    at water.MRTask.getResult(MRTask.java:670)
    at water.MRTask.doAll(MRTask.java:530)
    at water.MRTask.doAll(MRTask.java:549)
    at hex.Model.predictScoreImpl(Model.java:2057)
    at hex.generic.GenericModel.predictScoreImpl(GenericModel.java:127)
    at hex.Model.score(Model.java:1896)
    at water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:491)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1658)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:976)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
Caused by: java.lang.ArrayIndexOutOfBoundsException

Error: DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.ArrayIndexOutOfBoundsException



When I exported the stacked ensemble with interactions as a MOJO I also exported it as a binary. When I instead import the binary for the stacked ensemble with interactions it is able to generate predictions on the validation set without error.


I'm running R 4.1.2 and this was all done using h2o_3.36.0.1


Does anyone have any suggestions for resolving this issue?


EDIT (more info): The datasets used to train and validate the model all contain continuous predictors and targets, so I do not believe this is related to one-hot encoding as might be the case for others getting this error.","['r', 'h2o', 'glm']",Unknown,,N/A
70471661,70471661,2021-12-24T09:57:18,2022-01-27 18:28:01Z,71,"I am implementing my own algorithm in H2O's Java source code (under package 
h2o-algos
).


How can I join two frames' rows (i.e. vectors) in H2O given H2O Java methods?


For instance, given two Frame A and B


Frame A:

| Id       | Name           |
| -------- | -------------- |
| 123      | John           |
| 456      | Bob            |
Frame B:
| Id       | Name           |
| -------- | -------------- |
| 789      | Alice          |



I want the resultant Frame C to be:


| Id       | Name           |
| -------- | -------------- |
| 123      | John           |
| 456      | Bob            |
| 789      | Alice          |



Is there a way to do this faster then: making new vectors, than create a new frame from the new vectors? I have read the documentation and found that the 
Frame::append()
 method would create new columns, not joining rows.","['java', 'h2o']",Huy Ngo,https://stackoverflow.com/users/3972907/huy-ngo,194
70440726,70440726,2021-12-21T19:32:36,2021-12-21 22:39:00Z,417,"In a Databricks notebook, I am trying to load an H2O model that was trained for H2O version 3.30.1.3.


I have installed the version of Sparkling Water which corresponds to the Spark version used for the model training (3.0), 
h2o-pysparkling-3.0
, which I pulled from PyPI.


The Sparkling Water server is using the latest version of H2O rather than the version I need. Maybe there is a way to specify the H2O version when I initiate the Sparkling Water context?  Something like this:


import h2o
from pysparkling import H2OContext
from pysparkling.ml import H2OBinaryModel

hc = H2OContext.getOrCreate(h2o_version='3.30.1.3')
model = H2OBinaryModel.read('s3://bucket/model_file')



I run the above code without an argument to 
H2OContext.getOrCreate()
 and I get this error:


IllegalArgumentException: 
 The binary model has been trained in H2O of version
 3.30.1.3 but you are currently running H2O version of 3.34.0.6.
 Please make sure that running Sparkling Water/H2O-3 cluster and the loaded binary
 model correspond to the same H2O-3 version.



Where is the Python API for Sparkling Water? If I could find that I might be able to determine if there's an H2O version argument for the context initializer but surprisingly it's been impossible for me to find so far with Google and poking around in the docs.


Or is this something that's instead handled by installing an H2O version-specific build of Sparkling Water? Or perhaps there's another relevant configuration setting someplace?","['h2o', 'sparkling-water']",James Adams,https://stackoverflow.com/users/85248/james-adams,"8,687"
70408222,70408222,2021-12-19T00:35:45,2021-12-23 21:49:01Z,90,"My question is if Open Source H2O-3, Open Source Sparkling Water and Driverless AI are affected by CVE-2021-44228 and CVE-2021-45046.","['h2o', 'sparkling-water', 'driverless-ai', 'h2o.ai']",Michal,https://stackoverflow.com/users/5089773/michal,437
70366277,70366277,2021-12-15T15:29:29,2021-12-15 15:47:13Z,0,"A vulnerability of 
log4j
 became public.
Amongst other packages, I am using R 
shiny
 and 
h2o
 packages.
I already found out, that 
shiny
 is not affected by the vulnerability, since it uses 
log4js
(see 
https://github.com/log4js-node/log4js-node/issues/1105
), which is an implementation in Javascript.


Now we come to 
h2o
. I know that this package provides an API to the 
h2o
-framework in Java. In the documentation of building 
h2o
 from source from github (see 
https://h2o-release.s3.amazonaws.com/h2o/rel-noether/4/docs-website/developuser/quickstart_git.html
), i found along the lines


javac -source 1.6 -target 1.6 -sourcepath src/main/java -classpath
""../lib/log4j/log4j-1.2.15.jar:../target/h2o.jar:../lib/hadoop/mapr2.1.3/hadoop-0.20.2-dev-core.jar""
-d classes/mapr2.1.3 src/main/java/water/hadoop/*.java
warning: [options] bootstrap class path not set in conjunction with -source 1.6
1 warning
jar cf target/h2odriver_mapr2.1.3.jar -C classes/mapr2.1.3 .
make build_inner HADOOP_VERSION=cdh3
mkdir classes/cdh3
javac -source 1.6 -target 1.6 -sourcepath src/main/java -classpath
""../lib/log4j/log4j-1.2.15.jar:../target/h2o.jar:../lib/hadoop/cdh3/hadoop-core-0.20.2-cdh3u6.jar"" -d
classes/cdh3 src/main/java/water/hadoop/*.java
warning: [options] bootstrap class path not set in conjunction with -source 1.6
1 warning
jar cf target/h2odriver_cdh3.jar -C classes/cdh3 .
make build_inner HADOOP_VERSION=cdh4
mkdir classes/cdh4
javac -source 1.6 -target 1.6 -sourcepath src/main/java -classpath
""../lib/log4j/log4j-1.2.15.jar:../target/h2o.jar:../lib/hadoop/cdh4/hadoop-common.jar:../



It seems like 
h2o
 is using 
log4j
, but I don't know much about Java and its dependencies.


Can anyone with more knowledge clearify whether the 
h2o
-package is affected by the 
log4j
 vulnerability? And if so, how to solve or workaround this?


Thank you very much in advance.","['java', 'r', 'security', 'log4j', 'h2o']",Jonas,https://stackoverflow.com/users/14316488/jonas,"1,788"
70311395,70311395,2021-12-10T23:22:57,2022-02-17 00:15:38Z,331,"As my plan was to exclusively use H2O's web GUI, I installed H2O following these steps: 
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html#download-and-run-from-the-command-line
. I got a Windows Firewall popup asking me to allow h2o to communicate on public networks (I was connected to a public network at that moment). I accepted.


Now I would like to uninstall it, but I have not been able to find info about it. Also, it does not appear in the list of installed applications in Windows 10 settings. I have not found any rules related to H2O or port 54321 in Windows firewall either.


Please note:




This is the first time I install programs this way.


I do not have Python or R installed in my computer, as it was not necessary for using H2O's web GUI.","['uninstallation', 'h2o']",Unknown,,N/A
70294943,70294943,2021-12-09T18:27:46,2022-02-17 00:18:45Z,108,"With the latest version of 
hex.genmodel
 java library, we can get the MSE value. But I need to define the threshold value to identify the anomaly from generated H2O Poco class itself. Even a calculation would be helpful.


Thanks for the help.","['java', 'h2o', 'autoencoder', 'threshold', 'anomaly-detection']",Niroshan K,https://stackoverflow.com/users/10318195/niroshan-k,88
70279189,70279189,2021-12-08T17:22:34,2022-04-15 12:24:11Z,73,"I'm new to machine learning and H2O tools, and I'd like to know if there is a high-level H2O interface that allows us to implement new methods into a pipeline.


I know we can build models thanks to Flow interface and export them as POJO/MOJO. But how can I, for example, decide to use kNN method as an imputation method for my data, when Flow only allows simple imputation like mean/mode ?","['methods', 'h2o', 'imputation']",General Grievance,https://stackoverflow.com/users/4294399/general-grievance,"4,967"
70274744,70274744,2021-12-08T12:06:17,2022-02-17 00:08:37Z,172,"plz help~


i create h2o-stateful-set which set replicas: 3, then i run a h2o automl job, it works well. but suddenly one of pod breakdown, i use 
kubectl delete pod h2o-k8s-1
 to delete this pod. the statefulset create a new pod has same name h2o-k8s-1.
But here's the problem, the new pod can't join h2o cluster, and  job stuck, logs as follows


FJ-126-3  WARN water.default: Killing h2o-stateful-set-1.h2o-service.dhr-h2o.svc.cluster.local/10.177.5.212:54321 because the cloud is no longer accepting n
ew H2O nodes.



i know New H2O nodes join to form a cluster during launch. After a job has started on the cluster, it prevents new members from joining. but what should i do if cluster pod breakdown during training?","['h2o', 'h2o.ai']",dhr,https://stackoverflow.com/users/11118784/dhr,1
70205550,70205550,2021-12-02T19:43:29,2022-01-15 06:58:37Z,383,"I have recently started learning about H2O AutoML. I am wondering which one of the following options works better. Single node with 6GB of memory or a cluster of three nodes with 2GB memory each.




java -Xmx6g -jar h2o.jar -name MyCluster


java -Xmx2g -jar h2o.jar &   java -Xmx2g -jar h2o.jar &   java -Xmx2g -jar h2o.jar &




If there are drawbacks with single node deployment, can you recommend any methods to optimize the performance?
Thanks in advance!","['h2o', 'h2o.ai']",python dev,https://stackoverflow.com/users/2020301/python-dev,219
70178457,70178457,2021-12-01T02:29:58,2021-12-06 07:33:54Z,183,"In my algorithm, master node needs more memory (say 20GB) while the worker nodes need much less memory (say 3GB). However, as far as I know, in H2O it is only possibly to set the master node the same memory as worker nodes using 
-mapperXmx
.


In Apache Spark, it is possible to specify the driver memory with 
--driver-memory
 argument. However, I have 
not
 been able to find a equivalent way to set ""master/driver"" node's memory in H2O.


I am running H2O (
not
 Sparkling Water) on a Hadoop cluster (essentially on a YARN cluster) using this command:

hadoop jar h2o-hadoop-3/h2o-cdh6.3-assembly/build/libs/h2odriver-3.33.1.jar -nodes 5 -mapperXmx 3g -output my/output/dir/on/hdfs
. This way I am able to specify worker nodes' memory as 3GB. However, I could not find the argument to specify the master node's memory.  Is it possible to set the master node 20GB?","['java', 'hadoop', 'hadoop-yarn', 'h2o']",Unknown,,N/A
70071481,70071481,2021-11-22T19:39:50,2021-11-22 19:39:50Z,0,"I am trying to convert some R code to Python. The code builds h2o models and uses Lime for explanations. The Lime modules in each language seem to be quite different.


What would be the equiavlent python functions to this R code?


explainer <- lime(x=observations, model=mymodel)
explainations <- lime::exaplin(x=observations, explainer=explainer,n_permutations=5000, feature_select=""highest_weights"", n_labels=5, n_features=10)



This is a multiclass classification problem with 3 classes. I was thinking the best translation in Python would be to use the LimeTabularExplainer.


explainer = lime.lime_tabular.LimeTabularExplainer(observations, mode='classification', class_names=myclasses,feature_names=observationCols)
explainations = explainer.explain_instance(observations, mymodel, num_samples=5000, num_features=8)



This doesn't seem to be working through so I am not confident. Is there a better way to translate this R code, or better Lime documentation somewhere for explaining models.","['python', 'r', 'model', 'h2o', 'lime']",Jacqueline Schafer,https://stackoverflow.com/users/17482226/jacqueline-schafer,11
69939756,69939756,2021-11-12T08:14:53,2021-11-16 17:23:51Z,532,"I'm using function 
relevel()
 to set base level in column of my dataframe and getting this error in case column's values consists '+', '-' or ' ' symbols (mostly there are more, but I've tried this ones).


import pandas as pd 
from h2o.estimators.glm import H2OGeneralizedLinearEstimator 
import h2o 
import re 

h2o.init() 

data = { 
     'name': ['Xavier', 'Ann', 'Jana', 'Yi', 'Robin', 'Amal', 'Nori'], 
     'city': ['Mexico City', 'Toronto', 'Prague', 'Shanghai', 
              'Manchester', 'Cairo', 'Osaka'], 
     'age': [41, 28, 33, 34, 38, 31, 37], 
     'py-score': [88.0, 79.0, 81.0, 80.0, 68.0, 61.0, 84.0] 
 } 
  
df = pd.DataFrame(data=data) 
hf = h2o.H2OFrame(df) 

hf['city'].asfactor().relevel('Mexico City')



This will cause H2OResponseError:


---------------------------------------------------------------------------
H2OResponseError                          Traceback (most recent call last)
~\PycharmProjects\h2o\venv\lib\site-packages\IPython\core\formatters.py in __call__(self, obj)
    700                 type_pprinters=self.type_printers,
    701                 deferred_pprinters=self.deferred_printers)
--> 702             printer.pretty(obj)
    703             printer.flush()
    704             return stream.getvalue()

~\PycharmProjects\h2o\venv\lib\site-packages\IPython\lib\pretty.py in pretty(self, obj)
    392                         if cls is not object \
    393                                 and callable(cls.__dict__.get('__repr__')):
--> 394                             return _repr_pprint(obj, self, cycle)
    395 
    396             return _default_pprint(obj, self, cycle)

~\PycharmProjects\h2o\venv\lib\site-packages\IPython\lib\pretty.py in _repr_pprint(obj, p, cycle)
    698     """"""A pprint that just redirects to the normal repr function.""""""
    699     # Find newlines and replace them with p.break_()
--> 700     output = repr(obj)
    701     lines = output.splitlines()
    702     with p.group():

~\PycharmProjects\h2o\venv\lib\site-packages\h2o\frame.py in __repr__(self)
    579             stk = traceback.extract_stack()
    580             if not (""IPython"" in stk[-2][0] and ""info"" == stk[-2][2]):
--> 581                 self.show()
    582         return """"
    583 

~\PycharmProjects\h2o\venv\lib\site-packages\h2o\frame.py in show(self, use_pandas, rows, cols)
    610             print(""This H2OFrame is empty and not initialized."")
    611             return
--> 612         if self.nrows == 0:
    613             print(""This H2OFrame is empty."")
    614             return

~\PycharmProjects\h2o\venv\lib\site-packages\h2o\frame.py in nrows(self)
    319         if not self._ex._cache.nrows_valid():
    320             self._ex._cache.flush()
--> 321             self._frame(fill_cache=True)
    322         return self._ex._cache.nrows
    323 

~\PycharmProjects\h2o\venv\lib\site-packages\h2o\frame.py in _frame(self, rows, rows_offset, cols, cols_offset, fill_cache)
    729 
    730     def _frame(self, rows=10, rows_offset=0, cols=-1, cols_offset=0, fill_cache=False):
--> 731         self._ex._eager_frame()
    732         if fill_cache:
    733             self._ex._cache.fill(rows=rows, rows_offset=rows_offset, cols=cols, cols_offset=cols_offset)

~\PycharmProjects\h2o\venv\lib\site-packages\h2o\expr.py in _eager_frame(self)
     88         if not self._cache.is_empty(): return
     89         if self._cache._id is not None: return  # Data already computed under ID, but not cached locally
---> 90         self._eval_driver('frame')
     91 
     92     def _eager_scalar(self):  # returns a scalar (or a list of scalars)

~\PycharmProjects\h2o\venv\lib\site-packages\h2o\expr.py in _eval_driver(self, top)
    112         """"""
    113         exec_str = self._get_ast_str(top)
--> 114         res = ExprNode.rapids(exec_str)
    115         if 'scalar' in res:
    116             if isinstance(res['scalar'], list):

~\PycharmProjects\h2o\venv\lib\site-packages\h2o\expr.py in rapids(expr)
    256         :returns: The JSON response (as a python dictionary) of the Rapids execution
    257         """"""
--> 258         return h2o.api(""POST /99/Rapids"", data={""ast"": expr, ""session_id"": h2o.connection().session_id})
    259 
    260 

~\PycharmProjects\h2o\venv\lib\site-packages\h2o\h2o.py in api(endpoint, data, json, filename, save_to)
    111     # type checks are performed in H2OConnection class
    112     _check_connection()
--> 113     return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
    114 
    115 

~\PycharmProjects\h2o\venv\lib\site-packages\h2o\backend\connection.py in request(self, endpoint, data, json, filename, save_to)
    479                 save_to = save_to(resp)
    480             self._log_end_transaction(start_time, resp)
--> 481             return self._process_response(resp, save_to)
    482 
    483         except (requests.exceptions.ConnectionError, requests.exceptions.HTTPError) as e:

~\PycharmProjects\h2o\venv\lib\site-packages\h2o\backend\connection.py in _process_response(response, save_to)
    817         if status_code in {400, 404, 412} and isinstance(data, H2OErrorV3):
    818             data.show_stacktrace = False
--> 819             raise H2OResponseError(data)
    820 
    821         # Server errors (notably 500 = ""Server Error"")

H2OResponseError: Server error java.lang.IllegalArgumentException:
  Error: Did not find level `Mexico%20City` in the column.
  Request: POST /99/Rapids
    data: {'ast': ""(tmp= py_140_sid_b771 (relevel (as.factor (cols_py Key_Frame__upload_bda4861347c26f55bb24425d8760491c.hex 'city')) 'Mexico%20City'))"", 'session_id': '_sid_b771'}



Any ideas how can I relevel data with whitespaces and other symbols?


Thanks.","['python', 'encoding', 'jvm', 'h2o']",Grigory Skvortsov,https://stackoverflow.com/users/7101708/grigory-skvortsov,463
69930234,69930234,2021-11-11T14:43:52,2022-02-17 06:45:55Z,294,"I am unable to reproduce the only example I can find of using h2o with iml (
https://www.r-bloggers.com/2018/08/iml-and-h2o-machine-learning-model-interpretability-and-feature-explanation/
) as detailed here (
Error when extracting variable importance with FeatureImp$new and H2O
). Can anyone point to a workaround or other examples of using iml with h2o?




Reproducible example:


library(rsample)   # data splitting
library(ggplot2)   # allows extension of visualizations
library(dplyr)     # basic data transformation
library(h2o)       # machine learning modeling
library(iml)       # ML interprtation
library(modeldata) #attrition data 


# initialize h2o session
h2o.no_progress()
h2o.init()

# classification data
data(""attrition"", package = ""modeldata"")
df <- rsample::attrition %>% 
  mutate_if(is.ordered, factor, ordered = FALSE) %>%
  mutate(Attrition = recode(Attrition, ""Yes"" = ""1"", ""No"" = ""0"") %>% factor(levels = c(""1"", ""0"")))

# convert to h2o object
df.h2o <- as.h2o(df)

# create train, validation, and test splits
set.seed(123)
splits <- h2o.splitFrame(df.h2o, ratios = c(.7, .15), destination_frames = 
    c(""train"",""valid"",""test""))
names(splits) <- c(""train"",""valid"",""test"")

# variable names for resonse & features
y <- ""Attrition""
x <- setdiff(names(df), y) 

# elastic net model 
glm <- h2o.glm(
  x = x, 
  y = y, 
  training_frame = splits$train,
  validation_frame = splits$valid,
  family = ""binomial"",
  seed = 123
  )

# 1. create a data frame with just the features
features <- as.data.frame(splits$valid) %>% select(-Attrition)

# 2. Create a vector with the actual responses
response <- as.numeric(as.vector(splits$valid$Attrition))

# 3. Create custom predict function that returns the predicted values as a
#    vector (probability of purchasing in our example)
pred <- function(model, newdata)  {
  results <- as.data.frame(h2o.predict(model, as.h2o(newdata)))
  return(results[[3L]])
}

# create predictor object to pass to explainer functions
predictor.glm <- Predictor$new(
  model = glm, 
  data = features, 
  y = response, 
  predict.fun = pred,
  class = ""classification""
  )

imp.glm <- FeatureImp$new(predictor.glm, loss = ""mse"")



Error obtained:


Error in `[.data.frame`(prediction, , self$class, drop = FALSE): undefined columns 
selected

traceback()

1. FeatureImp$new(predictor.glm, loss = ""mse"")

2. .subset2(public_bind_env, ""initialize"")(...)

3. private$run.prediction(private$sampler$X)

4. self$predictor$predict(data.frame(dataDesign))

5. prediction[, self$class, drop = FALSE]

6. `[.data.frame`(prediction, , self$class, drop = FALSE)

7. stop(""undefined columns selected"")","['machine-learning', 'h2o', 'iml', 'dalex']",Zoe - Save the data dump,https://stackoverflow.com/users/6296561/zoe-save-the-data-dump,28.1k
69910275,69910275,2021-11-10T08:55:23,2024-10-29 23:26:28Z,310,"Closed
. This question needs 
details or clarity
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Add details and clarify the problem by 
editing this post
.






Closed 
3 years ago
.















                        Improve this question
                    








I am working through this lab, where a H2O Driverless AI model is trained, and deployed to Snowflake. I used Snowflake Partner Connect to set up a 7 day trial of H2O Driverless AI.


https://quickstarts.snowflake.com/guide/automl_with_snowflake_and_h2o/index.html?index=..%2F..index#0


Everything went smoothly until the last step, to deploy the model to Snowflake, where a license.sig file from H20 is needed. All it says is:




Last, you will need your Driverless AI license file license.sig




However I cannot find this file, nor any instructions about where to find it or download it.","['snowflake-cloud-data-platform', 'h2o', 'snowflake-schema']",Robert Long,https://stackoverflow.com/users/1009823/robert-long,"6,706"
69883147,69883147,2021-11-08T12:02:49,2021-11-08 12:02:49Z,175,"I am working on pysparkling in Databricks. I have built a model with pyspark transformers and h2o pysparkling algorithm. When I log the model on to mlflow and deploy in from a job cluster, I get the following error in the job cluster logs. I have mentioned pysparkling as a dependancy in _mlflow_conda_env. Any ideas about the workaround for this?! Thanks in advance!


ERROR Instrumentation: java.lang.ClassNotFoundException: ai.h2o.sparkling.ml.models.H2OGBMMOJOModel
at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at org.apache.spark.util.Utils$.classForName(Utils.scala:227)
at org.apache.spark.ml.util.DefaultParamsReader$.loadParamsInstanceReader(ReadWrite.scala:630)
at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:276)
at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
at scala.collection.TraversableLike.map(TraversableLike.scala:238)
at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)
at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)
at scala.util.Try$.apply(Try.scala:213)
at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)
at org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)
at org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)
at org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)
at org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)
at org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)
at org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)
at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)
at scala.util.Try$.apply(Try.scala:213)
at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)
at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
at py4j.Gateway.invoke(Gateway.java:295)
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
at py4j.commands.CallCommand.execute(CallCommand.java:79)
at py4j.GatewayConnection.run(GatewayConnection.java:251)
at java.lang.Thread.run(Thread.java:748)enter code here","['apache-spark', 'pyspark', 'databricks', 'h2o', 'sparkling-water']",Pradeep Ganesan,https://stackoverflow.com/users/17356884/pradeep-ganesan,11
69851819,69851819,2021-11-05T10:27:33,2021-11-05 10:33:23Z,0,"I am using R 4.1.1, and when I wanted to use 
rsparkling
 package, I encountered an error saying :


Error in as_h2o_frame(sc, partitions$training, strict_version_check = FALSE) : 
  could not find function ""as_h2o_frame""



It seems I cannot load any function from 
rsparkling
.


> ls(""package:rsparkling"")
character(0)
> getAnywhere(as_h2o_frame)
no object named ‘as_h2o_frame’ was found



A minimal reproducible example can be (It's copied from 
here
):


install.packages(""rsparkling"")
library(rsparkling)
library(sparklyr)
library(h2o)
library(dplyr)

sc <- spark_connect(""local"")

mtcars_tbl <- copy_to(sc, mtcars, ""mtcars"")
partitions <- mtcars_tbl %>%
  filter(hp >= 100) %>%
  mutate(cyl8 = cyl == 8) %>%
  sdf_partition(training = 0.5, test = 0.5, seed = 1099)
training <- as_h2o_frame(sc, partitions$training, strict_version_check = FALSE)
test <- as_h2o_frame(sc, partitions$test, strict_version_check = FALSE)","['r', 'apache-spark', 'h2o', 'sparklyr']",Unknown,,N/A
69496937,69496937,2021-10-08T13:49:05,2021-10-26 12:40:33Z,393,"I am attempting to read a multi-file Parquet dataset into an H2OFrame and it results in a column mismatch error:


H2OResponseError: Server error water.exceptions.H2OIllegalArgumentException:
  Error: Column separator mismatch. One file seems to use """" and the other uses "" "".



The dataset is initially converted from Delta to Parquet since H2O doesn't support Delta tables as data sources:


# convert from Delta to Parquet
delta_uri = 's3://my_bucket/path/to/delta/folder/'
df = spark.read.format('delta').load(delta_uri)
parquet_uri = 's3://my_bucket/path/to/parquet/folder/'
df.write.parquet(parquet_uri)

# extract Parquet into H2OFrame (this line is where the error happens)
data = h2o.import_file(path=parquet_uri)



Is there a way to enforce a single column separator across all Parquet files when making the conversion from Delta to Parquet?


The H2O cluster is running version 3.34.0.3 of H2O. The code above is being run within a Databricks notebook.","['pyspark', 'parquet', 'h2o', 'delta-lake']",Unknown,,N/A
69485936,69485936,2021-10-07T18:12:04,2022-01-27 19:14:55Z,0,"I am experiencing a persistent error while trying to use H2O's 
h2o.automl
 function. I am trying to repeatedly run this model. It seems to completely fail after 5 or 10 runs.


Error in .h2o.__checkConnectionHealth() : 
  H2O connection has been severed. Cannot connect to instance at http://localhost:54321/
getaddrinfo() thread failed to start

In addition: There were 13 warnings (use warnings() to see them)
Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  : 
  Unexpected CURL error: getaddrinfo() thread failed to start



I have updated java in response to: 
https://h2o-release.s3.amazonaws.com/h2o/rel-wolpert/4/docs-website/h2o-docs/faq/r.html
 (even though I am using a linux virtual machine).
I have added a 
h2o.removeall()
 and 
gc()
 in response to 
R h2o server CURL error, kind of repeatable

I have not attempted any changes regarding memory because my cluster has 16+ GB and the highest reading I have seen is 1.6 GiB in RStudio.


H2O is running in R/Rstudio Server on an Ubuntu 20.04 virtual machine. Could the virtual box software be blocking something?


The details on my H2O cluster are below:


openjdk version ""11.0.11"" 2021-04-20
OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.20.04)
OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.20.04, mixed mode, sharing)

Starting H2O JVM and connecting: ... Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         1 seconds 896 milliseconds 
    H2O cluster timezone:       America/Chicago 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.35.0.2 
    H2O cluster version age:    19 hours and 24 minutes  
    H2O cluster name:           H2O_started_from_R_jholderieath_glq667 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   19.84 GB 
    H2O cluster total cores:    12 
    H2O cluster allowed cores:  12 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4 
    R Version:                  R version 4.1.1 (2021-08-10)","['r', 'virtualbox', 'h2o', 'rstudio-server']",Phil,https://stackoverflow.com/users/5221626/phil,"8,077"
69477779,69477779,2021-10-07T08:23:49,2021-10-07 08:23:49Z,75,"can someone help with h2o.importfolder error for h2o 3.29 and above ,it works with h2o3.28 and below


dataingest.hex <- h2o.importFolder(path = 's3://h2o_test/', pattern="".*\.snappy\.parquet$"")


ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http:/xxxxxxxxx:45820/3/ParseSetup) water.exceptions.H2OIllegalArgumentException [1] ""water.exceptions.H2OIllegalArgumentException: Column separator mismatch. One file seems to use "" "" and the other uses ""\001""."" [2] ""


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page, : ERROR MESSAGE: Column separator mismatch. One file seems to use "" "" and the other uses "" "".",['h2o'],Abhay,https://stackoverflow.com/users/12215233/abhay,11
69397317,69397317,2021-09-30T18:34:49,2021-09-30 18:34:49Z,81,"I am using H2O to fit a Generalized Low Rank Model (GLRM) in python. How do I extract X from the fitted model?


from h2o.estimators import H2OGeneralizedLowRankEstimator
import numpy as np

# Import the USArrests dataset into H2O:
arrestsH2O = h2o.import_file(""https://s3.amazonaws.com/h2o-public-test-data/smalldata/pca_test/USArrests.csv"")

# Split the dataset into a train and valid set:
train, valid = arrestsH2O.split_frame(ratios=[.8], seed=1234)

# Build and train the model:
glrm_model = H2OGeneralizedLowRankEstimator(k=2,
                                            loss=""quadratic"",
                                            gamma_x=0.5,
                                            gamma_y=0.5,
                                            max_iterations=700,
                                            recover_svd=True,
                                            init=""SVD"",
                                            transform=""standardize"")
glrm_model.train(training_frame=train)



I seem to be able to retrieve Y with


Y = glrm_model.archetypes()
np.shape(Y)



but I cannot find the method to retrieve X.","['python', 'h2o']",Whitebeard,https://stackoverflow.com/users/1683061/whitebeard,"6,173"
69377034,69377034,2021-09-29T13:08:21,2021-11-18 21:00:40Z,431,"I'm brand new in the use of h2o (trough R) and have now encountered a problem when using h2o.automl. The problem is very much similar to that of another thread (
R - H20 - ERROR: Unexpected HTTP Status code: 500 Server Error
), however the error message thrown is different - I get a javaNullPointerException.


My R version is 4.1.1 and h2o package is the latest one (3.34.0.1)


I list below a snippet of my code, the error message, and the output after the h2o.init command.


Any help would be greatly appreciated!


Thanks heaps,
Christian


library(h2o)
h2o.init()

url <- ""http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris_wheader.csv""
iris <- h2o.importFile(url)

parts <- h2o.splitFrame(iris, 0.8)
train <- parts[[1]]
test <- parts[[2]]

summary(train)
nrow(train)
nrow(test)

mA <- h2o.automl(1:4, 5, train,
                 max_runtime_secs = 30,
                 max_models=1)



The complete error message is as follows:


ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://localhost:54321/99/AutoMLBuilder)

java.lang.NullPointerException
 [1] ""java.lang.NullPointerException""                                                                              
 [2] ""    water.nbhm.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:369)""                                   
 [3] ""    water.nbhm.NonBlockingHashMap.put(NonBlockingHashMap.java:320)""                                          
 [4] ""    ai.h2o.automl.AutoMLSession.getModelingSteps(AutoMLSession.java:76)""                                     
 [5] ""    ai.h2o.automl.ModelingStepsRegistry.getOrderedSteps(ModelingStepsRegistry.java:54)""                      
 [6] ""    ai.h2o.automl.AutoML.getExecutionPlan(AutoML.java:330)""                                                  
 [7] ""    ai.h2o.automl.AutoML.planWork(AutoML.java:359)""                                                          
 [8] ""    ai.h2o.automl.AutoML.submit(AutoML.java:395)""                                                            
 [9] ""    ai.h2o.automl.AutoML.startAutoML(AutoML.java:80)""                                                        
[10] ""    water.automl.api.AutoMLBuilderHandler.build(AutoMLBuilderHandler.java:15)""                               
[11] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                          
[12] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)""        
[13] ""    java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""
[14] ""    java.base/java.lang.reflect.Method.invoke(Method.java:568)""                                              
[15] ""    water.api.Handler.handle(Handler.java:60)""                                                               
[16] ""    water.api.RequestServer.serve(RequestServer.java:470)""                                                   
[17] ""    water.api.RequestServer.doGeneric(RequestServer.java:301)""                                               
[18] ""    water.api.RequestServer.doPost(RequestServer.java:227)""                                                  
[19] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:707)""                                            
[20] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:790)""                                            
[21] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)""                                  
[22] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)""                              
[23] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)""                       
[24] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)""                      
[25] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)""                        
[26] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)""                               
[27] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)""                        
[28] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)""                       
[29] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)""                           
[30] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                   
[31] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                         
[32] ""    water.webserver.jetty9.Jetty9ServerAdapter$LoginHandler.handle(Jetty9ServerAdapter.java:130)""            
[33] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                   
[34] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                         
[35] ""    org.eclipse.jetty.server.Server.handle(Server.java:531)""                                                 
[36] ""    org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)""                                       
[37] ""    org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)""                             
[38] ""    org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)""             
[39] ""    org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)""                                       
[40] ""    org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)""                                    
[41] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)""                  
[42] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)""                
[43] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)""               
[44] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)""                      
[45] ""    org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)""
[46] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)""                        
[47] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)""                         
[48] ""    java.base/java.lang.Thread.run(Thread.java:833)""                                                         

Fejl i .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  

ERROR MESSAGE:

Caught exception: java.lang.NullPointerException



Should the problem be pointing towards a connection problem, here's what I get when running the h2o.init() command:


R is connected to the H2O cluster: 
    H2O cluster uptime:         43 minutes 18 seconds 
    H2O cluster timezone:       Europe/Copenhagen 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.34.0.1 
    H2O cluster version age:    14 days, 20 hours and 2 minutes  
    H2O cluster name:           H2O_started_from_R_csoe0352_kcc786 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   3.92 GB 
    H2O cluster total cores:    12 
    H2O cluster allowed cores:  12 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4 
    R Version:                  R version 4.1.1 (2021-08-10)","['h2o', 'automl']",cmiso84,https://stackoverflow.com/users/17034413/cmiso84,11
69376740,69376740,2021-09-29T12:50:16,2021-09-29 12:50:16Z,29,"I would like to do some kind of ""custom GBM"" with the H2O format, so it can be exported in MOJO.
In particular, I would like to edit trees (by changing the split rules and the leafs predictions).


I don't see anything providing access to the H2OGradientBoostingEstimator trees, and even less possibilities to modify them.
Do you know if it is it possible to do that ? (or would it require to basically re-write the h2o.estimators.gbm package...)


Many thanks !","['python', 'h2o']",user2485450,https://stackoverflow.com/users/2485450/user2485450,21
69366490,69366490,2021-09-28T18:01:58,2022-02-27 09:13:30Z,0,"I have made a simple example of something I'm trying to do that I intend to be more flexible.


I want to be able to subset an h2o.frame on its rows, do some calculation on those, then assign the result to those same rows. In this example I calculate the relative ""mpg"" within each group of ""cyl"".


library(h2o)
packageVersion(""h2o"")
[1] ‘3.32.1.3’

version
               _                           
platform       x86_64-w64-mingw32          
arch           x86_64                      
os             mingw32                     
system         x86_64, mingw32             
status                                     
major          4                           
minor          1.1                         
year           2021                        
month          08                          
day            10                          
svn rev        80725                       
language       R                           
version.string R version 4.1.1 (2021-08-10)
nickname       Kick Things  

h2o.init()

mtcars <- as.h2o(mtcars)

mtcars$mpg_rel <- NA
mtcars
   mpg cyl disp  hp drat    wt  qsec vs am gear carb mpg_rel
1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     NaN
2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     NaN
3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1     NaN
4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1     NaN
5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2     NaN
6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1     NaN

[32 rows x 12 columns]

mtcars[mtcars[[""cyl""]] == 4, ""mpg""] / h2o.mean(mtcars[mtcars[[""cyl""]] == 4, ""mpg""])
        mpg
1 0.8550972
2 0.9151040
3 0.8550972
4 1.2151381
5 1.1401296
6 1.2713945

[11 rows x 1 column] 

# however, the assignment throws an error and corrupts `mtcars`
mtcars[mtcars[[""cyl""]] == 4, ""mpg_rel""] <- mtcars[mtcars[[""cyl""]] == 4, ""mpg""] / h2o.mean(mtcars[mtcars[[""cyl""]] == 4, ""mpg""])
ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/99/Rapids)

water.exceptions.H2OIllegalArgumentException
 [1] ""water.exceptions.H2OIllegalArgumentException: unimplemented""                                                 
 [2] ""    water.H2O.unimpl(H2O.java:1310)""                                                                         
 [3] ""    water.rapids.ast.prims.assign.AstRectangleAssign.apply(AstRectangleAssign.java:93)""                      
 [4] ""    water.rapids.ast.prims.assign.AstRectangleAssign.apply(AstRectangleAssign.java:30)""                      
 [5] ""    water.rapids.ast.AstExec.exec(AstExec.java:63)""                                                          
 [6] ""    water.rapids.ast.prims.assign.AstTmpAssign.apply(AstTmpAssign.java:48)""                                  
 [7] ""    water.rapids.ast.prims.assign.AstTmpAssign.apply(AstTmpAssign.java:17)""                                  
 [8] ""    water.rapids.ast.AstExec.exec(AstExec.java:63)""                                                          
 [9] ""    water.rapids.Session.exec(Session.java:85)""                                                              
[10] ""    water.rapids.Rapids.exec(Rapids.java:94)""                                                                
[11] ""    water.api.RapidsHandler.exec(RapidsHandler.java:38)""                                                     
[12] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                          
[13] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""        
[14] ""    java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""
[15] ""    java.base/java.lang.reflect.Method.invoke(Method.java:566)""                                              
[16] ""    water.api.Handler.handle(Handler.java:60)""                                                               
[17] ""    water.api.RequestServer.serve(RequestServer.java:470)""                                                   
[18] ""    water.api.RequestServer.doGeneric(RequestServer.java:301)""                                               
[19] ""    water.api.RequestServer.doPost(RequestServer.java:227)""                                                  
[20] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:707)""                                            
[21] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:790)""                                            
[22] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)""                                  
[23] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)""                              
[24] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)""                       
[25] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)""                      
[26] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)""                        
[27] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)""                               
[28] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)""                        
[29] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)""                       
[30] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)""                           
[31] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                   
[32] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                         
[33] ""    water.webserver.jetty9.Jetty9ServerAdapter$LoginHandler.handle(Jetty9ServerAdapter.java:130)""            
[34] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                   
[35] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                         
[36] ""    org.eclipse.jetty.server.Server.handle(Server.java:531)""                                                 
[37] ""    org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)""                                       
[38] ""    org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)""                             
[39] ""    org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)""             
[40] ""    org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)""                                       
[41] ""    org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)""                                    
[42] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)""                  
[43] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)""                
[44] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)""               
[45] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)""                      
[46] ""    org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)""
[47] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)""                        
[48] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)""                         
[49] ""    java.base/java.lang.Thread.run(Thread.java:834)""                                                         

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  

ERROR MESSAGE:

unimplemented



One solution I found was to use a 
vector
 of the row numbers to do the subsetting. This works, however, when using big data the 
as.vector
 conversion is very inefficient. It would be great if a method similar to the above approach was possible.


which_rows <- as.vector(h2o.which(mtcars[[""cyl""]] == 4))
mtcars[which_rows, ""mpg_rel""] <- mtcars[which_rows, ""mpg""] / h2o.mean(mtcars[which_rows, ""mpg""])
mtcars
   mpg cyl disp  hp drat    wt  qsec vs am gear carb   mpg_rel
1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4       NaN
2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4       NaN
3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 0.8550972
4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1       NaN
5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2       NaN
6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1       NaN

[32 rows x 12 columns]","['r', 'h2o']",Hutch3232,https://stackoverflow.com/users/9244371/hutch3232,428
69351124,69351124,2021-09-27T17:46:52,2021-09-28 00:21:04Z,234,"How to run H2O.ai python in background?


I tried putting


import h2o
h2o.init()
input() # to stop closing the python program



in a python script and run with 
nohup python3 script.py &
. But the script exits if I enter the return key. Is there a way to avoid this behavior and run h2o.ai in background?","['python', 'h2o']",Rumesh Madhusanka,https://stackoverflow.com/users/7205625/rumesh-madhusanka,"1,365"
69263794,69263794,2021-09-21T05:31:38,2021-09-21 08:13:38Z,534,"I trained a H2O GBM model (H2O version 3.22.0.1) using many categorical columns but when i try to predict on test data it is giving me warnings for unknown categorical levels in test. Does H2O GBM not handle then unseen categorical levels automatically? or is there a specific version where this capability is added?


Kindly suggest any tips to resolve this problem.","['machine-learning', 'h2o']",Frank001,https://stackoverflow.com/users/11289178/frank001,63
69234002,69234002,2021-09-18T10:56:28,2021-10-01 17:46:35Z,512,"I receive following error while training an auto ml model from h2o.


H2OServerError: HTTP 500 Server Error:
Server error java.lang.NullPointerException:
Error: Caught exception: java.lang.NullPointerException
Request: None



This is how I initialized h2o: 
h2o.init(nthreads = -1)

And this is what I received after h2o initialization:


Checking whether there is an H2O instance running at http://localhost:54321 . connected.
H2O_cluster_uptime: 31 mins 51 secs
H2O_cluster_timezone:   Asia/Kolkata
H2O_data_parsing_timezone:  UTC
H2O_cluster_version:    3.34.0.1
H2O_cluster_version_age:    3 days
H2O_cluster_name:   H2O_from_python_rosha_vhyf5v
H2O_cluster_total_nodes:    1
H2O_cluster_free_memory:    2.959 Gb
H2O_cluster_total_cores:    4
H2O_cluster_allowed_cores:  4
H2O_cluster_status: locked, healthy
H2O_connection_url: http://localhost:54321
H2O_connection_proxy:   {""http"": null, ""https"": null}
H2O_internal_security:  False
H2O_API_Extensions: Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4
Python_version: 3.8.8 final



And this is how I'm training automl model:


aml = H2OAutoML(max_models = 10, max_runtime_secs = 500, stopping_rounds = 5)
aml.train(x = aml_x, y = aml_y, training_frame = train)



I exported and processed data as pandas data frame. So, I changed it using


train = h2o.H2OFrame(df)



I tried the solutions on stack overflow which tries to find error in data by splitting it various halves. I get same error no matter how I split.
And solution asssumes the error pop ups instantly after running the code but in my case it takes about 4-5 second before giving a error.


Please note I have a large dataset of about 200,000 columns and 20 rows.","['machine-learning', 'h2o', 'automl']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
69195292,69195292,2021-09-15T14:44:20,2021-09-20 18:17:18Z,310,"I have 2 docker containers running my webapp and my machine learning application, both using h2o. Initially, I had both calling h2o.init() and pointing to the same IP:PORT, consequently a single h2o cluster with one node was initialized.


Consider that I have a model already trained and now I'm training a second one. During this training process, if the webapp made a call to the h2o cluster (e.g., requesting a predict from the first model), it would kill the training process (error message bellow), which was unintended. I tried setting a different port for each app but the same situation kept ocurring. I don't understand why since I thought that by setting two different ports, two indepentent clusters would be initialized, and, therefore, two jobs could run simultaneously.


Error message


Job request failed Server error java.lang.IllegalArgumentException:
      Error: Job is missing
      Request: GET /3/Jobs/$0301c0a8f00232d4ffffffff$_911222b9c2e4404c31191c0d3ffd44c6, will retry after 3s.



Alternatively, I moved H2O to a container of its own and I'm trying to set a multi-node cluster so that each app run on a node. Bellow is the Dockerfile and entrypoint.sh file used to start the cluster:


Dockerfile


########################################################################
# Dockerfile for Oracle JDK 8 on Ubuntu 16.04
########################################################################

# pull base image
FROM ubuntu:16.04

RUN \
    echo 'DPkg::Post-Invoke {""/bin/rm -f /var/cache/apt/archives/*.deb || true"";};' | tee /etc/apt/apt.conf.d/no-cache && \
    echo ""deb http://mirror.math.princeton.edu/pub/ubuntu xenial main universe"" >> /etc/apt/sources.list && \
    apt-get update -q -y && \
    apt-get dist-upgrade -y && \
    apt-get clean && \
    rm -rf /var/cache/apt/* && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y wget unzip openjdk-8-jdk python-pip python-sklearn python-pandas python-numpy python-matplotlib software-properties-common python-software-properties && \
    apt-get clean

# Fetch h2o
ENV H2O_RELEASE rel-zipf
ENV H2O_VERSION 3.32.1.7
RUN \
    wget http://h2o-release.s3.amazonaws.com/h2o/${H2O_RELEASE}/$(echo $H2O_VERSION | cut -d ""."" -f4)/h2o-${H2O_VERSION}.zip -O /opt/h2o.zip && \
    unzip -d /opt /opt/h2o.zip && \
    rm /opt/h2o.zip && \
    cd /opt && \
    cd `find . -name 'h2o.jar' | sed 's/.\///;s/\/h2o.jar//g'` && \
    cp h2o.jar /opt && \
    /usr/bin/pip install `find . -name ""*.whl""`

# Define the working directory
WORKDIR \
    /home/h2o

EXPOSE 54321-54326

# Define entrypoint
COPY ./bin/entrypoint.sh ./entrypoint.sh
RUN chmod +x entrypoint.sh
ENTRYPOINT [""./entrypoint.sh""]



entrypoint.sh


#!/bin/bash
# Entrypoint script.

set -e

d=`dirname $0`

# Use 90% of RAM for H2O, 30% for each node.
memTotalKb=`cat /proc/meminfo | grep MemTotal | sed 's/MemTotal:[ \t]*//' | sed 's/ kB//'`
memTotalMb=$[ $memTotalKb / 1024 ]
tmp=$[ $memTotalMb * 30 ]
xmxMb=$[ $tmp / 100 ]

# Use all 36 cores for H2O, 12 for each node.
totalCores=`lscpu | grep ""^CPU(s)"" | sed 's/CPU(s):[ \t]*//'`
nthreads=$[ $totalCores / 3 ]

# First try running java.
java -version

# Start 2 H2O nodes in the background
nohup java -Xmx${xmxMb}m -jar /opt/h2o.jar -nthreads ${nthreads} -name ${H2O_CLUSTER_NAME} -port ${H2O_NODE_2_PORT} &
nohup java -Xmx${xmxMb}m -jar /opt/h2o.jar -nthreads ${nthreads} -name ${H2O_CLUSTER_NAME} -port ${H2O_NODE_3_PORT} & 

# Start the 3rd node.
java -Xmx${xmxMb}m -jar /opt/h2o.jar -nthreads ${nthreads} -name ${H2O_CLUSTER_NAME} -port ${H2O_NODE_1_PORT}



As can be seen, I start a total of 3 nodes (the webapp can request 2 operations at once), each on a different port (ports 54321, 54323, and 54325. The IP is the same), I setted the memory to 30% of the total memory for each node and the nthreads to a third of the available cores (36 total, 12 for each node). The cluster start fine with 3 nodes, however, contrary to what I expected, each node has all the 36 cores instead of 12 (giving a total of 108), as shown in the image bellow, leading to the same error I had before.


H2O 3-node cluster


I looked other stackoverflow post as well as H2O documentation but couldn't find anything that works for me. How can I configure H2O so that I can have multiple jobs running simultaneously from different applications?","['python', 'h2o']",Arthur Matta,https://stackoverflow.com/users/16919489/arthur-matta,13
69113860,69113860,2021-09-09T07:19:49,2021-09-09 08:01:08Z,809,"When I am trying to convert pandas dataframe to h2o.H2OFrame, I am getting this error.","['pandas', 'dataframe', 'h2o']",Divyansh Sharma,https://stackoverflow.com/users/16231567/divyansh-sharma,21
68888281,68888281,2021-08-23T06:42:15,2021-08-24 03:01:22Z,479,"I am trying to get an Apache Airflow image to install 
h2o
, usually I would just need to run 
pip install h2o
 and it works.


Based on 
this
 answer I would need to extend the image and I did so.


airflow/Dockerfile
:


FROM apache/airflow:2.1.2
USER root
RUN apt-get update \
  && apt-get install -y --no-install-recommends \
         build-essential h2o \
  && apt-get autoremove -yqq --purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*
USER airflow
RUN pip install --no-cache-dir --user h2o



docker-compose.yaml
:


---
version: ""3""
x-airflow-common:
  build: ./airflow
  environment: &ref_0
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: """"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ""true""
    AIRFLOW__CORE__LOAD_EXAMPLES: ""true""
    AIRFLOW__API__AUTH_BACKEND: airflow.api.auth.backend.basic_auth
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
    AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    AWS_SESSION_TOKEN: ${AWS_SESSION_TOKEN}
    AWS_ROLE_ARN: ${AWS_ROLE_ARN}
    REGION_NAME: ${REGION_NAME}
  volumes: &ref_1
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
  user: ${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}
  depends_on: &ref_2
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy
services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test:
        - CMD
        - pg_isready
        - -U
        - airflow
      interval: 5s
      retries: 5
    restart: always
  redis:
    image: redis:latest
    ports:
      - 6379:6379
    healthcheck:
      test:
        - CMD
        - redis-cli
        - ping
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always
  airflow-webserver:
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.2}
    environment: *ref_0
    volumes: *ref_1
    user: ${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}
    depends_on: *ref_2
    command: webserver
    ports:
      - 8080:8080
    healthcheck:
      test:
        - CMD
        - curl
        - --fail
        - http://localhost:8080/health
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
  airflow-scheduler:
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.2}
    environment: *ref_0
    volumes: *ref_1
    user: ${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}
    depends_on: *ref_2
    command: scheduler
    healthcheck:
      test:
        - CMD-SHELL
        - airflow jobs check --job-type SchedulerJob --hostname ""$${HOSTNAME}""
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
  airflow-worker:
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.2}
    environment: *ref_0
    volumes: *ref_1
    user: ${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}
    depends_on: *ref_2
    command: celery worker
    healthcheck:
      test:
        - CMD-SHELL
        - >-
          celery --app airflow.executors.celery_executor.app inspect ping -d
          ""celery@$${HOSTNAME}""
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
  airflow-init:
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.2}
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: """"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ""true""
      AIRFLOW__CORE__LOAD_EXAMPLES: ""true""
      AIRFLOW__API__AUTH_BACKEND: airflow.api.auth.backend.basic_auth
      _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_SESSION_TOKEN: ${AWS_SESSION_TOKEN}
      REGION_NAME: ${REGION_NAME}
      _AIRFLOW_DB_UPGRADE: ""true""
      _AIRFLOW_WWW_USER_CREATE: ""true""
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
    volumes: *ref_1
    user: ${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}
    depends_on: *ref_2
    command: version
  flower:
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.2}
    environment: *ref_0
    volumes: *ref_1
    user: ${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}
    depends_on: *ref_2
    command: celery flower
    ports:
      - 5555:5555
    healthcheck:
      test:
        - CMD
        - curl
        - --fail
        - http://localhost:5555/
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
volumes:
  postgres-db-volume:



When I check the packages installed using 
docker exec -it <CONTAINER_ID> pip list
,  
h2o
 cannot be found and I already have the package included in the 
requirements.txt
.


Directory structure:


├── airflow
│   └── Dockerfile
├── dags
│   ├── 01_lasic_retraining_overview.py
│   ├── 02_lasic_retraining_sagemaker_autopilot.py
│   ├── 03_lasic_retraining_h20_automl.py
│   ├── __init__.py
│   └── common
│       ├── __init__.py
│       └── helper.py
├── docker-compose.yaml
├── requirements.txt



Error confirmation inside of airflow:




EDIT:


I tried 
this answer
 as well but still the same issue.","['python', 'docker', 'airflow', 'h2o']",Unknown,,N/A
68874158,68874158,2021-08-21T14:59:37,2021-09-03 22:22:43Z,623,"Here is my example with 
Default
 date set from ISLR package. The data is imbalanced so I rebalance it and run H2O AutoML with GBMs only.


library(ISLR)
library(h2o)
library(magrittr)
library(dplyr)
core_count <- detectCores()
h2o.init(nthreads = (core_count -1))

my_df <- Default
x <- setdiff(colnames(df_train), 'default')
y <- 'default'
    
    my_df %<>% mutate(weights = if_else(default =='No',
0.6/table(my_df$default)[[1]],0.4/table(my_df$default)[[2]]))

aml_test <- h2o.automl(x = x, y = y,
                  training_frame = as.h2o(my_df[1:8000, ]),
                  validation_frame = as.h2o(my_df[8001:10000, ]),
                  nfolds = 0, 
                  weights_column = ""weights"",
                  include_algos = c('GBM'),
                  seed = 12345,
                  max_runtime_secs = 1200)



It generates the following errors:


        09:46:49.611: Skipping training of model GBM_1_AutoML_20210821_094649 due to exception:
     water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model:
     GBM_1_AutoML_20210821_094649.  Details: ERRR on field: _min_rows: The dataset size is too 
    small to split for min_rows=1.0: must have at least 2.0 (weighted) rows, but have only 
    0.7172904568994339.
    
    09:46:49.622: Skipping training of model GBM_2_AutoML_20210821_094649 due to exception:
 water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: 
GBM_2_AutoML_20210821_094649.  Details: ERRR on field: _min_rows: The dataset size is too 
small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 
0.7172904568994339.
    
    09:46:49.630: Skipping training of model GBM_3_AutoML_20210821_094649 due to exception: 
water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: 
GBM_3_AutoML_20210821_094649.  Details: ERRR on field: _min_rows: The dataset size is too 
small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 
0.7172904568994339.

    
    09:46:49.637: Skipping training of model GBM_4_AutoML_20210821_094649 due to exception: 
water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: 
GBM_4_AutoML_20210821_094649.  Details: ERRR on field: _min_rows: The dataset size is too 
small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 
0.7172904568994339.

    
    09:46:49.644: Skipping training of model GBM_5_AutoML_20210821_094649 due to exception: 
water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: 
GBM_5_AutoML_20210821_094649.  Details: ERRR on field: _min_rows: The dataset size is too 
small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 
0.7172904568994339.

      |===================================================================================| 100%
    



       09:49:50.241: Empty leaderboard.

    AutoML was not able to build any model within a max runtime constraint of 1200 seconds,
 you may want to increase this value before retrying.The leaderboard contains zero models: 
try running AutoML for longer (the default is 1 hour).



Essentially it does not work with GBM whenever weights for classes are provided. It works fine without weights. It even did not run for full 20 minutes. No models are generated.","['h2o', 'weighted', 'automl', 'gbm']",user1700890,https://stackoverflow.com/users/1700890/user1700890,"7,672"
68695428,68695428,2021-08-07T19:10:28,2021-08-30 06:37:33Z,201,"I am trying to run 
H2OAutoML
 in a specific way. I am only running XGBOOST, and I only want stratified folds, along with only using 
booster = 'gbtree'
 (i.e. I don't want 
booster = 'dart'
 in the hyper-parameter search)


I have tried:


    aml = H2OAutoML(
        nfolds=5, 
        include_algos=['XGBoost'], 
        sort_metric='auc', 
        seed=1, 
        max_runtime_secs=10,
        algo_parameters=dict(booster = 'gbtree')
    )



But get the error:


  Error: Illegal value for field: algo_parameters: booster
  Request: POST /99/AutoMLBuilder
    json: {'build_control': {'stopping_criteria': {'max_runtime_secs': 10, 
    'stopping_metric': 'AUTO', 'stopping_rounds': 3, 'seed': 1}, 'nfolds': 5, 
    'balance_classes': False, 'max_after_balance_size': 5.0, 
   'keep_cross_validation_models': False, 'keep_cross_validation_fold_assignment': 
   False, 'keep_cross_validation_predictions': False}, 'build_models': 
   {'include_algos': ['XGBoost'], 'exploitation_ratio': 0, 'algo_parameters': 
   [{'scope': 'any', 'name': 'booster', 'value': 'gbtree'}]}, 'input_spec': 
   {'sort_metric': 'auc', 'training_frame': 'py_1_sid_928f', 'response_column': 
   'successful', 'weights_column': 'weight', 'ignored_columns': ['C1']}}","['python', 'h2o']",James Ellis,https://stackoverflow.com/users/16537296/james-ellis,101
68672166,68672166,2021-08-05T18:58:42,2021-08-06 11:18:52Z,92,"Training the following GBM model on 2 cores vs 96 cores (on EC2 c5.large and c5.metal) results in faster training times when using 
less
 cores.  I checked the water meter to verify all cores were running.


Training times:
c5.large (2 cores): ~1min
c5.metal (96 cores): ~2min
Training details:


training set size     6840 rows x 95 cols

seed                  1
ntrees                1000
max_depth             50
min_rows              10
learn_rate            0.005
sample_rate           0.5
col_sample_rate       0.5
stopping_rounds       2
stopping_metric       ""MSE""
stopping_tolerance    1.0E-5
score_tree_interval   500
histogram_type        ""UniformAdaptive""
nbins                 800
nbins_top_level       1024



Any thoughts on why this is happening?",['h2o'],Shaun Lebron,https://stackoverflow.com/users/142317/shaun-lebron,"2,531"
68669460,68669460,2021-08-05T15:28:57,2021-08-19 15:44:22Z,182,"How do you set the value for 
scale_pos_weight
 in h2o for XGBOOST?


I have thought about setting weights_column but I am not sure if they are the same thing or not?",['h2o'],James Ellis,https://stackoverflow.com/users/16537296/james-ellis,101
68640155,68640155,2021-08-03T17:05:38,2021-08-04 20:57:00Z,126,"When fitting a GLM in H2O_cluster_version: 3.32.0.5 with


lamdba_search = True, nlambdas = 20, and lambda_min_ratio = .0001



My team and I receive 24 lambdas in our regularization path. The last 4 lambdas in the path are repeats of the first 4, the largest values.


Here is a reproducible example:


import pandas as pd
import numpy as np
import tweedie
import scipy

import os
import sys
import time

import h2o
from h2o.estimators.glm import H2OGeneralizedLinearEstimator
from h2o.grid.grid_search import H2OGridSearch
sys.path.append(h2odir)
from h2o_auto_init import h2o_auto_init

os.system(h2oshellscript)
    
time.sleep(10)
    
h2o_auto_init()

#sample data
resp = np.random.choice(range(0,100),size=1000)
pred1 = np.random.choice(range(40,50),size=1000)
pred2 = np.random.choice(range(20,30),size=1000)
pred3 = np.random.choice([1,2,3,4,5],size=1000)
weight = np.random.choice([1,1,1,.9,.37],size=1000)
folds = np.random.choice([1,2,3,4,5],size=1000)

data = pd.DataFrame({'resp': resp, 'pred1':pred1,'pred2':pred2,'pred3':pred3,'weight':weight,'fold_column':folds})

predictors = ['pred1','pred2','pred3']

    # convert pandas df to h2oframe
H2Odata = h2o.H2OFrame(data, column_names=data.columns.tolist())


  
    # set up model
model = H2OGeneralizedLinearEstimator(
        family=""tweedie"",
        tweedie_link_power = 0,
        tweedie_variance_power = 1.7,
        lambda_search=True,
        early_stopping = False,
        lambda_min_ratio = 0.0001,
        nlambdas=20,
        alpha=.5,
        standardize = True,
        weights_column='weight',
        solver = 'IRLSM',
        #beta_constraints = constraints,
        keep_cross_validation_models = True,
        keep_cross_validation_predictions = True,
        keep_cross_validation_fold_assignment=True
)
    # Train the model with training and validation data
model.train(
        x=predictors, 
        y='resp',
        training_frame=H2Odata,
        fold_column = 'fold_column'
)


# get full regularization paths
#list of cross validation model objects
regpath_h2o_cv=[]
for i in range(0,len(model.cross_validation_models())):
        regpath_h2o_cv.append(H2OGeneralizedLinearEstimator.getGLMRegularizationPath(model.cross_validation_models()[i]))

H2OGeneralizedLinearEstimator.getGLMRegularizationPath(model.cross_validation_models()[0])['lambdas']



When I run this, there is an extra lambda, a repeat of the first lambda.
Can anyone provide guidance on why H2O is providing more lambdas than requested, and especially repeated lambdas?
Does this mean it is fitting unnecessary models?


Our real use case is on very large data, and any time we can save
avoiding unnecessary modeling will be helpful.","['python', 'h2o', 'glm']",Unknown,,N/A
68577442,68577442,2021-07-29T14:06:18,2021-08-06 14:07:16Z,0,"I want to load a large dataset in .sav format. I used h2o package. I started a connection using h2o.init and after the h2o.importFile to load the data in .sav format:


> h2o.init(ip = ""localhost"",nthreads = -1,max_mem_size = ""3g"")

H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    C:\Users\User\AppData\Local\Temp\RtmpW0VBWa\file232c4ba961b9/h2o_user_started_from_r.out
    C:\Users\User\AppData\Local\Temp\RtmpW0VBWa\file232c7e0c26fc/h2o_user_started_from_r.err

java version ""15.0.2"" 2021-01-19
Java(TM) SE Runtime Environment (build 15.0.2+7-27)
Java HotSpot(TM) 64-Bit Server VM (build 15.0.2+7-27, mixed mode, sharing)

Starting H2O JVM and connecting: . Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         3 seconds 81 milliseconds 
    H2O cluster timezone:       America/Costa_Rica 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.32.1.3 
    H2O cluster version age:    2 months and 17 days  
    H2O cluster name:           H2O_started_from_R_user_gqo645 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   3.00 GB 
    H2O cluster total cores:    8 
    H2O cluster allowed cores:  8 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4 
    R Version:                  R version 4.1.0 (2021-05-18) 

> datos_1 = h2o.importFile(path   = ""C:/Users/User/Desktop/Data/data.sav"")

ERROR: Unexpected HTTP Status code: 400 Unable to parse URI query (url = http://localhost:54321/3/ImportFiles?path=C%3A%2FUsers%2FUser%2FDesktop%2FDataFdata.sav&pattern=)

Error: lexical error: invalid char in json text.
                                       <html> <head> <meta http-equiv=
                     (right here) ------^



When I try to load the data, I get this error. I want to known how to fix it and how to import .sav files properly.","['r', 'bigdata', 'h2o']",Unknown,,N/A
68461155,68461155,2021-07-20T20:47:39,2021-08-01 12:02:46Z,963,"I'm working on detect anomalies from the following data:




It comes from a processed signal of and hydraulic system, from there I know that the dots in the red boxes are anomalies happen when the system fails.


I'm using the first 3k records to train a model, both in pycaret and H20. These 3k records covers 5 cycles of data, as shown in the image bellow:


To train the model in pycaret I'm using the following code:




from pycaret.anomaly import *
from pycaret.datasets import get_data
import pandas as pd
exp_ano101 = setup(df[[""Pressure_median_mw_2500_ac""]][0:3000], normalize = True, 
                   session_id = 123)

iforest = create_model('iforest')
unseen_predictions = predict_model(iforest, data=df[[""Pressure_median_mw_2500_ac""]])
unseen_predictions = unseen_predictions.reset_index()



The results I get from pycaret are pretty good:




And with a bit of post processing I can get the follwing, which is quite close to the ideal:




On the other hand, using H20, with the following code:


import pandas as pd
from h2o.estimators import H2OIsolationForestEstimator, H2OGenericEstimator
import tempfile
ifr = H2OIsolationForestEstimator()
ifr.train(x=""Pressure_median_mw_2500_ac"",training_frame=hf)
th = df[""mean_length""][0:3000].quantile(0.05)
df[""anomaly""] = df[""mean_length""].apply(lambda x: ""1"" if x> th  else ""0"")



I get this:




Which is a huge difference, since it is not detecting as anomalies this block:




My doubt is, how can I get similar results that the ones I get from pycaret given that I'm using the same algorithm, which is Isolation Forest. And even using SVM in Pycaret I get closer results than using isolation forest in H2O","['python', 'h2o', 'anomaly-detection', 'pycaret']",Luis Ramon Ramirez Rodriguez,https://stackoverflow.com/users/4544413/luis-ramon-ramirez-rodriguez,10.3k
68455131,68455131,2021-07-20T12:46:12,2021-07-20 12:46:12Z,0,"Several of the codes I wrote last month have become extremely large (2MB - 11MB) and are no longer readable. Does anybody know what happened?


All of the codes used the iml package and possibly h2o


This is what left of them:","['rstudio', 'h2o', 'iml']",Unknown,,N/A
68404982,68404982,2021-07-16T07:18:47,2021-07-20 11:35:20Z,93,"I have a h20 frame, which I need to pass to sklearn kneighbors (NearestNeighbors), If i'm not wrong ""from sklearn.neighbors import NearestNeighbors"" accepts only arrays, , I tried for one single row, it's working. But, How can I pass the who h20 daframe to that function? I guess I can use a for loop, but wondering is there any other efficient way. FYI -I'm using pyspark for my implementation


from sklearn.neighbors import NearestNeighbors

h20_df_mod_output = model_name(input_Dataset)
neigh = NearestNeighbors(n_neighbors=1)
neigh.fit(centroid_values['centroids'])
distance, indices = neigh.kneighbors([h20_df_mod_output[1,:]]) # How can I pass the entire dataset here?","['python', 'pandas', 'numpy', 'pyspark', 'h2o']",user7343922,https://stackoverflow.com/users/7343922/user7343922,306
68401118,68401118,2021-07-15T21:47:38,2021-07-18 05:46:08Z,128,"I am using a H2ORandomForestEsimator. What is the default target metric that H2O models use for their 
predict()
 method?

https://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html#h2o.automl.H2OAutoML.predict


Is there a way to set this? (Eg. to use one of the other metric maximizing thresholds that can be seen when looking at the results of 
get_params()
 method)


Currently am doing something like...


df_preds = mymodel.predict(df)
activation_threshold = mymodel.find_threshold_by_max_metric('f1', valid=True)
# adjust the predicted label for the desired metric's maximizing threshold
df_preds['predict'] = df_preds['my_positive_class'].apply(lambda probability: 'my_positive_class' if probability >= activation_threshold else 'my_negative_class')



see




https://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/model_categories.html?highlight=find_threshold#h2o.model.binomial.H2OBinomialModel.find_threshold_by_max_metric


https://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html?highlight=apply#h2o.H2OFrame.apply","['h2o', 'h2o.ai']",Unknown,,N/A
68310117,68310117,2021-07-09T01:08:54,2021-07-09 12:39:57Z,0,"For my first use of Package H2o on Rstudio, I received the following message for Ridge model


Error: water.exceptions.H2OConcurrentModificationException: Rollups not possible, because Vec was deleted 



Here is the code I used and unfortunately I can't share my database because it is confidential


library(h2o)
h2o.init(nthreads = -1) 
h2o.no_progress()                    
learn.h2o<-as.h2o(learn_preppeds)   
test.h2o<-as.h2o(test_preppeds)
Lambda<- 10^seq(-3, 3, length = 100)
x <- setdiff(colnames(learn.h2o), c(""NBCLAIM"", ""Offset"")) 
y <- ""NBCLAIM""      # Target variable
offset <- ""Offset""  # log(exposure)
glm_fit_ridge<-h2o.glm(  
x = x,   
y = y,                                            
offset_column = offset,  
training_frame = learn.h2o,  
validation_frame = test.h2o,  
family = ""poisson"",  
link = 'log',
nfolds= 10, # 10 fold cross-validation  
alpha = 0,  
lambda =Lambda,  
interactions= interactions_list, 
keep_cross_validation_predictions = TRUE,  
seed = 2    # For reproducibility)","['r', 'machine-learning', 'h2o', 'h2o.ai']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
68295229,68295229,2021-07-08T03:24:30,2021-07-14 01:09:23Z,870,"I have an H2O AutoML generated GBM model using python. I wonder if we can convert this into a standard sklearn model so that I can fit it into my ecosystem of other sklearn models.
I can see the model properties as below when I print the model.




If direct conversion from H2O to sklearn is not feasible, is there a way we can use the above properties to recreate GBM in sklearn? These terminologies look slightly different from the standard sklearn GBM parameters.


Thanks in advance.","['python', 'scikit-learn', 'h2o', 'automl', 'boosting']",Math Lover,https://stackoverflow.com/users/5428238/math-lover,177
68152594,68152594,2021-06-27T15:22:43,2021-06-27 15:22:43Z,438,"I'm searching for a python package of GAM model with the functions of tensor product and thin plate spline like mgcv in R. Specifically, like ti(a, b, bs = c('tp','tp')) by mgcv.
PyGAM has tensor product function te(), however it doesn't seem to have thin plate spline. Meanwhile H2O has functions for thin plate spline but doesn't seems to have functions for tensor product.
I'm a green hand of statistics and have no experiences in R programming. As I'm relatively familiar with python, I hope to use python to solve this problem. Is there any package that both of these two functions are available? Or is there any method to make a term like ti(a, b, bs = c('tp','tp')) in python?","['python', 'h2o', 'gam', 'pygam']",Pablo34,https://stackoverflow.com/users/16328135/pablo34,21
68095583,68095583,2021-06-23T07:41:07,2021-06-23 11:48:56Z,0,"I am having difficulties when running R script with 
h2o
 library via 
cron
 in linux.


The script runs perfectly fine in interactive mode, but when scheduled in 
cron
 the script fails.


Part of the code causing the error:


automl_h2o_models <- h2o.automl(
    x = predictors, 
    y = target,
    training_frame = train_conv_h2o,
    leaderboard_frame = valid_conv_h2o,
    max_runtime_secs = 3600,
    seed = 1234
)



When 
max_runtime_secs
 is set to 
1800
 there is no issue, but anything beyond this value will result in error below.


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  Unexpected CURL error: getaddrinfo() thread failed to start



I am on Ubuntu 20.04, R Version 3.6.3, h2o version 3.32.1.3","['r', 'cron', 'h2o', 'automl']",Tomas,https://stackoverflow.com/users/11533623/tomas,91
68047717,68047717,2021-06-19T14:27:04,2021-06-21 23:26:02Z,0,"I have training, validation and test data frames. Since, these data frames are big , I can't share here.


I want to tune parameters of deep learning procedure from package h2o.


Main body of the code is as below:


train<-df2_input
 valid<-df3_input
 test<-df4_input
 
 response<-""realized1""
 features<-setdiff(colnames(train),""realized1"") 

 h2o.init()
 train.h2o <- as.h2o(train) 
 valid.h2o <- as.h2o(valid) 
 test.h2o <- as.h2o(test) 
 
 
 activation_opt <- c(""Rectifier"",""RectifierWithDropout"", ""Maxout"",""MaxoutWithDropout"")
 hidden_opt <- list(c(10,10),c(20,15),c(50,50,50))
 
 l1_opt <- c(0,1e-3,1e-5)
 l2_opt <- c(0,1e-3,1e-5)
 
 hyper_params <- list( activation=activation_opt,
                       hidden=hidden_opt,
                       l1=l1_opt,
                       l2=l2_opt )
 
 #set search criteria
 search_criteria <- list(strategy = ""RandomDiscrete"", max_models=10)
 
 #train model
 
 dl_grid <- h2o.grid(""deeplearning""
                     ,grid_id = ""deep_learn""
                     ,hyper_params = hyper_params
                     ,search_criteria = search_criteria
                     ,training_frame = train.h2o
                     ,validation_frame = valid.h2o
                     ,x=features
                     ,y=response
                     ,epochs = 100)
 



However, I get the below error:


ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/99/Grid/deeplearning)

water.exceptions.H2OIllegalArgumentException
 [1] ""water.exceptions.H2OIllegalArgumentException: Illegal argument: training_frame of function: grid: Cannot append new models to a grid with different training input""
 [2] ""    hex.grid.GridSearch.loadFromDKV(GridSearch.java:171)""                                                                                                          
 [3] ""    hex.grid.GridSearch.getOrCreateGrid(GridSearch.java:153)""                                                                                                      
 [4] ""    hex.grid.GridSearch.start(GridSearch.java:104)""                                                                                                                
 [5] ""    hex.grid.GridSearch.startGridSearch(GridSearch.java:769)""                                                                                                      
 [6] ""    hex.grid.GridSearch.startGridSearch(GridSearch.java:648)""                                                                                                      
 [7] ""    water.api.GridSearchHandler.trainGrid(GridSearchHandler.java:137)""                                                                                             
 [8] ""    water.api.GridSearchHandler.handle(GridSearchHandler.java:53)""                                                                                                 
 [9] ""    water.api.GridSearchHandler.handle(GridSearchHandler.java:39)""                                                                                                 
[10] ""    water.api.RequestServer.serve(RequestServer.java:470)""                                                                                                         
[11] ""    water.api.RequestServer.doGeneric(RequestServer.java:301)""                                                                                                     
[12] ""    water.api.RequestServer.doPost(RequestServer.java:227)""                                                                                                        
[13] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:707)""                                                                                                  
[14] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:790)""                                                                                                  
[15] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)""                                                                                        
[16] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)""                                                                                    
[17] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)""                                                                             
[18] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)""                                                                            
[19] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)""                                                                              
[20] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)""                                                                                     
[21] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)""                                                                              
[22] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)""                                                                             
[23] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)""                                                                                 
[24] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                                                                         
[25] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                                                                               
[26] ""    water.webserver.jetty9.Jetty9ServerAdapter$LoginHandler.handle(Jetty9ServerAdapter.java:130)""                                                                  
[27] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                                                                         
[28] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                                                                               
[29] ""    org.eclipse.jetty.server.Server.handle(Server.java:531)""                                                                                                       
[30] ""    org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)""                                                                                             
[31] ""    org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)""                                                                                   
[32] ""    org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)""                                                                   
[33] ""    org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)""                                                                                             
[34] ""    org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)""                                                                                          
[35] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)""                                                                        
[36] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)""                                                                      
[37] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)""                                                                     
[38] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)""                                                                            
[39] ""    org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)""                                                      
[40] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)""                                                                              
[41] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)""                                                                               
[42] ""    java.lang.Thread.run(Unknown Source)""                                                                                                                          

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  

ERROR MESSAGE:

Illegal argument: training_frame of function: grid: Cannot append new models to a grid with different training input



I found the below topic probaby solution of similar error:


H2O ""grid: Cannot append new models to a grid with different training input"" error when parallelizing the execution of autoML in a foreach loop


But, I couldn't solve the problem with that topic.","['r', 'deep-learning', 'h2o']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
68026366,68026366,2021-06-17T20:47:29,2024-05-30 13:30:15Z,353,"I am using both H2O and Sparkling Water on Amazon Clusters. I have been using Qubole and have been able to access the Flow UI on that platform. I am currently testing Databricks and Sagemaker, but I am unable to access the Flow UI using either platform (using port 54321). I am using H2O_cluster_version: 3.32.1.3. Do I need to use another port?","['h2o', 'sparkling-water']",David Comfort,https://stackoverflow.com/users/6287730/david-comfort,153
67940893,67940893,2021-06-11T17:00:08,2021-09-08 23:28:36Z,0,"Whenever I import 
h2o
 from R on my Mac, I see a warning message


Your H2O cluster version is too old (8 months and 2 days)!
Please download and install the latest version from http://h2o.ai/download/



So I followed the installation guide from h2o homepage(
http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/3/index.html
) by running


# The following two commands remove any previously installed H2O packages for R.
if (""package:h2o"" %in% search()) { detach(""package:h2o"", unload=TRUE) }
if (""h2o"" %in% rownames(installed.packages())) { remove.packages(""h2o"") }

# Next, we download packages that H2O depends on.
pkgs <- c(""RCurl"",""jsonlite"")
for (pkg in pkgs) {
if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}

# Now we download, install and initialize the H2O package for R.
install.packages(""h2o"", type=""source"", repos=""http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/3/R"")



But this approach does not solve the problem but generates error.


R is connected to the H2O cluster: 
    H2O cluster uptime:         10 days 3 hours 
    H2O cluster timezone:       America/New_York 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.32.0.1 
    H2O cluster version age:    8 months and 2 days !!! 
    H2O cluster name:           H2O_started_from_R_matthewson_gtl621 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   25.55 GB 
    H2O cluster total cores:    8 
    H2O cluster allowed cores:  8 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4 
    R Version:                  R version 4.0.3 (2020-10-10) 

Error in h2o.init() : 
  Version mismatch! H2O is running version 3.32.0.1 but h2o-R package is version 3.32.1.3.
         Install the matching h2o-R version from - https://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/1/index.html
In addition: Warning message:
In h2o.clusterInfo() : 
Your H2O cluster version is too old (8 months and 2 days)!
Please download and install the latest version from http://h2o.ai/download/



Is there a way to update cluster's h2o version? I'm not experiencing this issue on windows machine though.","['r', 'h2o']",Matthew Son,https://stackoverflow.com/users/10484383/matthew-son,"1,385"
67823594,67823594,2021-06-03T14:43:24,2021-06-04 00:45:14Z,0,"I am having a problem reproducing the 
AutoML tutorial in H2O documentation
. After initatiing my h2o local server (
h2o.init()
) I get the following output, which sounds correct:


Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.
Attempting to start a local H2O server...
  Java Version: java version ""1.8.0_181""; Java(TM) SE Runtime Environment (build 1.8.0_181-b13); Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)
  Starting server from /home/cdsw/.local/lib/python3.8/site-packages/h2o/backend/bin/h2o.jar
  Ice root: /tmp/tmp3nh32di4
  JVM stdout: /tmp/tmp3nh32di4/h2o_cdsw_started_from_python.out
  JVM stderr: /tmp/tmp3nh32di4/h2o_cdsw_started_from_python.err
  Server is running at http://127.0.0.1:54321
Connecting to H2O server at http://127.0.0.1:54321 ... successful.
H2O_cluster_uptime: 01 secs
H2O_cluster_timezone:   Etc/UTC
H2O_data_parsing_timezone:  UTC
H2O_cluster_version:    3.32.1.3
H2O_cluster_version_age:    14 days, 20 hours and 29 minutes
H2O_cluster_name:   H2O_from_python_cdsw_cpcrap
H2O_cluster_total_nodes:    1
H2O_cluster_free_memory:    13.98 Gb
H2O_cluster_total_cores:    32
H2O_cluster_allowed_cores:  32
H2O_cluster_status: accepting new members, healthy
H2O_connection_url: http://127.0.0.1:54321
H2O_connection_proxy:   {""http"": null, ""https"": null}
H2O_internal_security:  False
H2O_API_Extensions: Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4
Python_version: 3.8.5 final



Next, I import the datasets as specified by the tutorial:


# Identify predictors and response
x = train.columns
y = ""response""
x.remove(y)

# For binary classification, response should be a factor
train[y] = train[y].asfactor()
test[y] = test[y].asfactor()



Finally, I train my AutoML model:


# Run AutoML for 20 base models (limited to 1 hour max runtime by default)
aml = H2OAutoML(max_models=20, seed=1)
aml.train(x=x, y=y, training_frame=train)



That is when it crashes with following message:


AutoML progress: |██Failed polling AutoML progress log: Local server has died unexpectedly. RIP.
Job request failed Local server has died unexpectedly. RIP., will retry after 3s.
Job request failed Local server has died unexpectedly. RIP., will retry after 3s.



Have tried with different datasets, including some sample in case it was a memory issue but with no avail. The error prevails.


Anyone knows what should I do to fix this?


Much appreciated!


Regards.","['python', 'h2o', 'automl', 'h2o.ai']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
67814769,67814769,2021-06-03T02:48:00,2021-06-03 10:15:21Z,0,"What is the difference between these two arguments 
nfolds
, and 
train_samples_per_iteration
, and is one of the more important to determining the optimum hyperparameters than the other?


Also, is it necessary to scale the training and testing sets before training the model?
Would it be important to transfer the response variable to a 
factor
 form?","['r', 'deep-learning', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
67727907,67727907,2021-05-27T18:24:12,2021-05-30 10:12:35Z,70,"Can you please let me know if there's support for h2o4gpu on AWS EC2 instance to generate deep learning models? If yes, then are there any references for installation and examples?


Thanks","['python', 'h2o']",Mr R,https://stackoverflow.com/users/15310387/mr-r,784
67725019,67725019,2021-05-27T15:09:03,2021-05-27 15:33:05Z,0,"I am trying to tune the hyperparameters in 
mlr
 using the 
tuneParams
 function. However, I can't make sense of the results it is giving me (or else Im using it incorrectly).


For example, if I create some data with a binary response and then create an 
mlr
 
h2o
 classification model and then check the accuracy and AUC I will get some values.
Then, if I use 
tuneParams
 on some parameters and find a better accuracy and AUC and then plug them into my model. The resulting accuracy and AUC (for the model) does not match that found by using 
tuneParams
.


Hopefully the code below will illustrate my issue:


library(mlr)

# Create data
set.seed(1234)
Species <- sample(c(""yes"", ""no""), size = 150, replace = T)

dat <- data.frame(
  x1 = (Species == ""yes"") + rnorm(150),
  x2 = (Species == ""no"") + rnorm(150), Species
)

# split into training and test
train <- sample(nrow(dat), round(.7*nrow(dat))) # split 70-30
datTrain <- dat[train, ]
datTest <- dat[-train, ]

# create mlr h2o model
task <- makeClassifTask(data = dat, target = ""Species"")
learner <- makeLearner(""classif.h2o.deeplearning"", predict.type = ""prob"", 
                       par.vals = list(reproducible = TRUE,
                                       seed = 1))
Mod <- train(learner, task)

# Test predictions
pred <- predict(Mod, newdata = datTest)
# Evaluate performance accuracy & area under curve 
performance(pred, measures = list(acc, auc)) 



The result of the above performance check is:


acc       auc 
0.7111111 0.7813765 



Now, if I tune just one of the parameters (e.g., epochs):


set.seed(1234)
# Tune epoch parameter
param_set <- makeParamSet(
  makeNumericParam(""epochs"", lower = 1, upper = 10))
rdesc <- makeResampleDesc(""CV"", iters = 3L, predict = ""both"") 
ctrl <- makeTuneControlRandom(maxit = 3)

res <- tuneParams(
  learner = learner, task = task, resampling = rdesc, measures = list(auc, acc),
  par.set = param_set, control = ctrl
)



the result I get from tuning epochs is:


Tune result:
Op. pars: epochs=1.95
auc.test.mean=0.8526496,acc.test.mean=0.7466667



Now, if I plug that value for the epochs into the learner and run the model again and check the performance:


set.seed(1234)
# plugging the tuned value into model and checking performance again:
learner <- makeLearner(""classif.h2o.deeplearning"", predict.type = ""prob"", 
                       par.vals = list(epochs = 1.95,
                                       reproducible = TRUE,
                                       seed = 1))
Mod <- train(learner, task)

# Test predictions
pred1 <- predict(Mod, newdata = datTest)
# Evaluate performance accuracy & area under curve 
performance(pred1, measures = list(acc, auc))



The resulting accuracy and AUC I get is now:


   acc       auc 
0.6666667 0.8036437 



My question is, why is there such a difference between the accuracy and AUC of the results of using 
tuneParams
 and when I plug the tuned values into the learner?
Or am I using or interpreting 
tuneParams
 incorrectly?","['r', 'h2o', 'mlr']",Electrino,https://stackoverflow.com/users/3447708/electrino,"2,870"
67638484,67638484,2021-05-21T14:07:20,2021-08-18 21:07:35Z,159,"If I train a weighted H2O GAM regression model, I can not predict with it. Weighted regression is done using the parameter weights_column


I am running python=3.6.13, h2o=3.32.1.3, pandas=0.25.3, numpy=1.19.5, sklearn=0.24.2. Java Version: openjdk version ""14.0.2"".


Prediction works with :




Unweighted H2O GAM


Weighted H2O GLM


Weighted H2O GAM when downgrading to h2o=3.32.0.5




I have registrered this as a bug on 
http://jira.h2o.ai
, but I would still be interested if anyone has a way to make it work, without downgrading h2o.


import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
import h2o
from h2o.estimators.gam import H2OGeneralizedAdditiveEstimator

h2o.no_progress()
h2o.init()

np.random.seed(42)
boston = load_boston()
y = pd.Series(boston[""target""], name=""y"")
X = pd.DataFrame(boston[""data""], columns=boston[""feature_names""])  # shape: (506, 13)
myweight = pd.Series(np.random.random_sample((len(y),)), name=""myweight2"")

predictors = ['CRIM', 'AGE']
gam_columns = ['CRIM']

params = {
    ""family"": ""gaussian"",
    ""gam_columns"": gam_columns,
    'bs': len(gam_columns) * [0],
}

df0 = pd.concat([y, X, myweight], axis=1)
df = h2o.H2OFrame(python_obj=df0)

model = H2OGeneralizedAdditiveEstimator(**params)
model.train(
    x=predictors,
    y=""y"",
    weights_column=""myweight2"",
    training_frame=df,
)

print('df.shape', df.shape)
y_pred = model.predict(df)
print('y_pred:', y_pred.as_data_frame()[""predict""].values[0:5])



I get this output. It complains about 
myweight2
:


Checking whether there is an H2O instance running at http://localhost:54321 . connected.
--------------------------  ------------------------------------------

df.shape (506, 15)
Traceback (most recent call last):
  File ""/Users/g009655/tmp7/h2otest/test_gam_predict.py"", line 37, in <module>
    y_pred = model.predict(df)
  File ""/Users/g009655/Library/Caches/pypoetry/virtualenvs/h2otest-S7Xak4Mg-py3.6/lib/python3.6/site-packages/h2o/model/model_base.py"", line 237, in predict
    j.poll()
  File ""/Users/g009655/Library/Caches/pypoetry/virtualenvs/h2otest-S7Xak4Mg-py3.6/lib/python3.6/site-packages/h2o/job.py"", line 80, in poll
    ""\n{}"".format(self.job_key, self.exception, self.job[""stacktrace""]))
OSError: Job with key $03017f00000132d4ffffffff$_9242dd1b28497090cf9ccad52bd54b9f failed with an exception: java.lang.AssertionError:  null vec: $04ff0f000000ffffffff$_b0f0839f8f1a041e8bf5254b552e4dd3; 

name: myweight2

stacktrace: 
java.lang.AssertionError:  null vec: $04ff0f000000ffffffff$_b0f0839f8f1a041e8bf5254b552e4dd3; 

name: myweight2

    at water.fvec.Frame.<init>(Frame.java:161)
    at hex.gam.GAMModel.cleanUpInputFrame(GAMModel.java:505)
    at hex.gam.GAMModel.adaptTestForTrain(GAMModel.java:492)
    at hex.Model.score(Model.java:1697)
    at water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:422)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1637)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

Closing connection _sid_ad95 at exit
H2O session _sid_ad95 closed.

Process finished with exit code 1","['python', 'h2o', 'predict', 'gam']",Unknown,,N/A
67636662,67636662,2021-05-21T12:16:32,2021-05-21 22:58:59Z,185,"I can not make the parameter ""fold_column"" work with the H2OGeneralizedAdditiveEstimator, using Python.


I need to create folds outside H2O, and read the finished Pandas DataFrame into a H2OFrame. In the H2OFrame there is a column ""fold_number"". I can loop through the folds and train models for each fold. But when running GAM training with 
fold_column=""fold_number""
 it fails, ""Not enough data to create 2 random cross-validation splits"". But I just made those two models! Even if I enhance the data set a lot, by adding modified copies of the original, it fails. Everything works fine with H2OGeneralizedLinearEstimator.


Any tips on this - or is this bug?


I am running python=3.6.13, h2o=3.32.1.3, pandas=0.25.3, numpy=1.19.5, sklearn=0.24.2. Java Version: openjdk version ""14.0.2"".


import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
import h2o
from h2o.estimators.gam import H2OGeneralizedAdditiveEstimator

h2o.no_progress()
h2o.init()

np.random.seed(42)
boston = load_boston()
y = pd.Series(boston[""target""], name=""y"")
X = pd.DataFrame(boston[""data""], columns=boston[""feature_names""])  # shape: (506, 13)
myweight = pd.Series(np.random.random_sample((len(y),)), name=""myweight2"")

predictors = ['CRIM', 'AGE']
gam_columns = ['CRIM']

params = {
    ""family"": ""gaussian"",
    ""gam_columns"": gam_columns,
    'bs': len(gam_columns) * [0],
}

fold = pd.Series(np.append(np.zeros(253), np.ones(253)), dtype=int, index=y.index, name=""fold_number"")
df0 = pd.concat([y, X, myweight, fold], axis=1)
df = h2o.H2OFrame(python_obj=df0)

# df[""fold_number""] = df[""fold_number""].asfactor()

for i in [0, 1]:
    mask = df[""fold_number""] == i
    df_train = df[~mask, :]
    df_val = df[mask, :]

    model = H2OGeneralizedAdditiveEstimator(**params)
    model.train(
        x=predictors,
        y=""y"",
        weights_column=""myweight2"",
        training_frame=df_train,
    )

    print(""Finished training for fold_number="", i, "", with validation-RMSE="", model.rmse(df_val))

print(""\nStarting training with API option fold_column="")
model2 = H2OGeneralizedAdditiveEstimator(**params)
model2.train(
    x=predictors,
    y=""y"",
    weights_column=""myweight2"",
    training_frame=df,
    fold_column=""fold_number""
)
print(""Finished training with API option fold_column="")



The output I get is:


Checking whether there is an H2O instance running at http://localhost:54321 . connected.
--------------------------  ------------------------------------------------

Finished training for fold_number= 0 , with validation-RMSE= 7.33788975630292
Finished training for fold_number= 1 , with validation-RMSE= 7.912477133985602

Starting training with API option fold_column=
Traceback (most recent call last):
  File ""/Users/g009655/tmp7/h2otest/test_gam_cv.py"", line 55, in <module>
    fold_column=""fold_number""
  File ""/Users/g009655/Library/Caches/pypoetry/virtualenvs/h2otest-S7Xak4Mg-py3.6/lib/python3.6/site-packages/h2o/estimators/estimator_base.py"", line 115, in train
    self._train(parms, verbose=verbose)
  File ""/Users/g009655/Library/Caches/pypoetry/virtualenvs/h2otest-S7Xak4Mg-py3.6/lib/python3.6/site-packages/h2o/estimators/estimator_base.py"", line 207, in _train
    job.poll(poll_updates=self._print_model_scoring_history if verbose else None)
  File ""/Users/g009655/Library/Caches/pypoetry/virtualenvs/h2otest-S7Xak4Mg-py3.6/lib/python3.6/site-packages/h2o/job.py"", line 80, in poll
    ""\n{}"".format(self.job_key, self.exception, self.job[""stacktrace""]))
OSError: Job with key $03017f00000132d4ffffffff$_af1219b23ff0642a316d9f092b214dc6 failed with an exception: water.exceptions.H2OIllegalArgumentException: 

Not enough data to create 2 random cross-validation splits. 
Either reduce nfolds, specify a larger dataset (or specify another random number seed, 
if applicable).

stacktrace: 
water.exceptions.H2OIllegalArgumentException: 

Not enough data to create 2 random cross-validation splits. 
Either reduce nfolds, specify a larger dataset (or specify another random number seed, 
if applicable).

    at hex.ModelBuilder.cv_makeWeights(ModelBuilder.java:726)
    at hex.ModelBuilder.computeCrossValidation(ModelBuilder.java:604)
    at hex.glm.GLM.computeCrossValidation(GLM.java:136)
    at hex.ModelBuilder$1.compute2(ModelBuilder.java:379)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1637)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

Closing connection _sid_b4d8 at exit
H2O session _sid_b4d8 closed.

Process finished with exit code 1","['python', 'h2o', 'gam']",Geir Inge,https://stackoverflow.com/users/8771368/geir-inge,189
67633018,67633018,2021-05-21T08:05:54,2021-05-23 01:00:12Z,0,"I'm trying to run H2O xgboost on r4.8x large. But it's taking too long to run (15+ hrs as opposed to 4 hours for GBM with same hyperparameter grid size).


Knowing that XGBoost uses cache optimization, is there any particular instance type that works best for H2O's XGBoost implementation?


My training data has 28K rows with 150 binary columns. And I'm running a grid search.","['amazon-web-services', 'amazon-ec2', 'xgboost', 'h2o', 'h2o.ai']",Unknown,,N/A
67602679,67602679,2021-05-19T11:49:56,2021-05-21 06:03:58Z,377,"From what I can see in the 
docs
, H2O supports calibration for GBM, DRF, XGBoost models only and has to be specified prior to the training phase.


I find it confusing. If calibration is a post-processing step and is model agnostic, shouldn't it be possible to calibrate any model trained using H2O, even after the training process is finished?


Currently, I'm dealing with a model that I've trained using 
AutoML
. Even though it is a GBM model, I'm not able to easily calibrate it by providing a 
calibrate_model
 parameter as it is not supported by AutoML. I don't see any option to calibrate it after it's trained either.


Does anyone know an easy way to calibrate already-trained H2O models? Is it necessary to ""manually"" calibrate them using algorithms such as Platt scaling or is there a way to do it without using any extra libraries?
Thanks","['python', 'machine-learning', 'h2o']",Wojciech Blachowski,https://stackoverflow.com/users/15852149/wojciech-blachowski,21
67584018,67584018,2021-05-18T10:00:32,2021-05-18 15:18:24Z,0,"I use 
targets
 as a pipelining tool for an ML project with 
H2O
.
The main uniqueness of using H2O here is that it creates a new ""cluster"" (basically a new local process/server which communicates via Rest APIs as far as I understand).


The issue I am having is two-fold.




How can I stop/operate the cluster within the targets framework in a smart way


How can I save & load the data/models within the targets framework




MWE


A minimum working example I came up with looks like this (being the 
_targets.R
 file):


library(targets)
library(h2o)

# start h20 cluster once _targets.R gets evaluated
h2o.init(nthreads = 2, max_mem_size = ""2G"", port = 54322, name = ""TESTCLUSTER"")

create_dataset_h2o <- function() {
  # connect to the h2o cluster
  h2o.init(ip = ""localhost"", port = 54322, name = ""TESTCLUSTER"", startH2O = FALSE)
  # convert the data to h2o dataframe
  as.h2o(iris)
}
train_model <- function(hex_data) {
  # connect to the h2o cluster
  h2o.init(ip = ""localhost"", port = 54322, name = ""TESTCLUSTER"", startH2O = FALSE)

  h2o.randomForest(x = c(""Sepal.Length"", ""Sepal.Width"", ""Petal.Length"", ""Petal.Width""),
                   y = c(""Species""),
                   training_frame = hex_data,
                   model_id = ""our.rf"",
                   seed = 1234)
}
predict_model <- function(model, hex_data) {
  # connect to the h2o cluster
  h2o.init(ip = ""localhost"", port = 54322, name = ""TESTCLUSTER"", startH2O = FALSE)
  h2o.predict(model, newdata = hex_data)
}

list(
  tar_target(data, create_dataset_h2o()),
  tar_target(model, train_model(data), format = ""qs""),
  tar_target(predict, predict_model(model, data), format = ""qs"")
)



This kinda works but faces the two issues I was outlying above and below...


Ad 1 - stopping the cluster


Usually I would out a 
h2o::h2o.shutdown(prompt = FALSE)
 at the end of my script, but this does not work in this case.
Alternatively, I came up with a new target that is always run.


# in _targets.R in the final list
  tar_target(END, h2o.shutdown(prompt = FALSE), cue = tar_cue(mode = ""always""))



This works when I run 
tar_make()
 but not when I use 
tar_visnetwork()
.


Another option is to use.


# after the h2o.init(...) call inside _targets.R
on.exit(h2o.shutdown(prompt = FALSE), add = TRUE)



Another alternative that I came up with is to handle the server outside of targets and only connect to it. But I feel that this might break the targets workflow...


Do you have any other idea how to handle this?


Ad 2 - saving the dataset and model


The code in the MWE does not save the data for the targets 
model
 and 
predict
 in the correct format (
format = ""qs""
). Sometimes (I think when the cluster gets restarted or so), the data gets ""invalidated"" and h2o throws an error. The data in h2o format in the R session is a pointer to the h2o dataframe (see also 
docs
).


For keras, which similarly stores the models outside of R, there is the option 
format = ""keras""
, which calls 
keras::save_model_hdf5()
 behind the scenes. Similarly, H2O would require 
h2o::h2o.exportFile()
 and 
h2o::h2o.importFile()
 for the dataset and 
h2o::h2o.saveModel()
 and 
h2o::h2o.loadModel()
 for models (see also 
docs
).


Is there a way to create additional formats for 
tar_targets
 or do I need to write the data to file, and return the file? The downside to this is that this file is outside of the 
_targets
 folder system, if I am not mistaken.","['r', 'h2o', 'targets-r-package']",David,https://stackoverflow.com/users/3048453/david,10k
67559435,67559435,2021-05-16T17:04:46,2021-05-16 17:04:46Z,0,"I am trying to change some of the parameters for a 
h2o
 deep learner using 
mlr
. Similar questions have been asked 
here
 and 
here
. However, I'm still confused as to how to change some specific parameters. I have provided a simple example below to illustrate what Im trying to achieve.


If I have some data:


data <- data.frame(
  x = rnorm(144),
  y = rnorm(144),
  z = rnorm(144),
  Factor1 = as.factor(rep(c(""A"", ""B""), each = 36)),
  Factor2 = as.factor(rep(c(rep(""Red"", 18), rep(""Blue"", 18)), 4)),
  Response = as.factor(rep(c(rep(1, 11), rep(0, 7), rep(0, 18)), 4))
)



And I set up a 
h2o
 deep learner model, I can change parameters (such as epochs) like so:


library(h2o)
y <- ""Response""
x <- names(data[, -6])


h2o.init()
h2o.no_progress()

set.seed(1234)
h2oDL <- h2o.deeplearning(x,
  y,
  as.h2o(data),
  epochs = 50,
  nfolds = 3, 
  score_interval = 1, 
  stopping_rounds = 5, 
  stopping_metric = ""misclassification"",
  stopping_tolerance = 1e-3,
  
)



But what Im trying to do is alter those specific parameters using 
mlr
. Normally, you could create an 
mlr
 model like so:


library(mlr)

Task <- makeClassifTask(data = data, target = ""Response"")
Lrn <- makeLearner(""classif.h2o.deeplearning"", predict.type = ""prob"")
Mod <-train(Lrn, Task)



I was trying to do something like this:


param_set <- makeParamSet(
  makeNumericParam(""epochs"", default = 50)

)



and then add this when creating the learner, like so:


Task <- makeClassifTask(data = data, target = ""Response"")
Lrn <- makeLearner(""classif.h2o.deeplearning"", predict.type = ""prob"", par.vals = param_set)
Mod <-train(Lrn, Task)



But this throws back an error. Any suggestions as to how I could change the specific parameters (i.e., the ones I'm  altering in the 
h2o.deeplearning
 function example above) in 
mlr
?","['r', 'h2o', 'mlr']",Electrino,https://stackoverflow.com/users/3447708/electrino,"2,870"
67509897,67509897,2021-05-12T19:27:40,2021-05-14 04:32:47Z,0,"I've been trying to run a saved H2O models on Google H2O Cluster for the past few days.


I was able to deploy and connect to the cluster using this guide

http://docs.h2o.ai/h2o/latest-stable/h2o-docs/cloud-integration/google-compute.html


h2o.cluster().show_status()





H2O_cluster_uptime:    4 hours 38 mins

H2O_cluster_timezone:  Etc/UTC

H2O_data_parsing_timezone: UTC

H2O_cluster_version:   3.32.1.2

H2O_cluster_version_age:   12 days

H2O_cluster_name:  root

H2O_cluster_total_nodes:   1

H2O_cluster_free_memory:   6.220 Gb

H2O_cluster_total_cores:   2

H2O_cluster_allowed_cores: 2

H2O_cluster_status:    locked, healthy




I uploaded saved model on to Google Cloud Storage and fuse to the VM using Cloud Storage FUSE to this folder




/tmp/gcsModels/




Now, whenever I try to load the model using .load_model:


models_path = ""/tmp/gcsModels/serverless/v1/""
pca_model = h2o.load_model(os.path.join(models_path, ""cust_PCA_DEMO_v1""))



I encounter this error:


H2OResponseError: Server error water.exceptions.H2OIllegalArgumentException:
  Error: Illegal argument: dir of function: importModel: water.api.FSIOException: FS IO Failure: 
 accessed path : file:/tmp/gcsModels/serverless/v1/cust_PCA_DEMO_v1 msg: File not found
  Request: POST /99/Models.bin/
    data: {'dir': '/tmp/gcsModels/serverless/v1/cust_PCA_DEMO_v1'}



Upon checking, the models file are all in the /tmp/gcsModels folder


ls /tmp/gcsModels/serverless/v1/





cust_GBM_DEMO_LIKELIHOOD_v2

cust_GBM_DEMO_LIKELIHOOD_v2_cv5

cust_GBM_DEMO_LOGAMOUNT_v1_cv5

cust_PCA_DEMO_v1




I have no idea what I did wrong. Any ideas would be greatly appreciated.","['google-cloud-platform', 'google-cloud-storage', 'h2o', 'gcsfuse']",V Nguyn,https://stackoverflow.com/users/4606796/v-nguyn,1
67509322,67509322,2021-05-12T18:39:59,2021-05-14 16:27:39Z,0,"As the title says. I cannot run 
h20.init
.


I have already downloaded the 64 bit version of the 
Java SE Development Kit 8u291
. I also downloaded the xgboost library in R (
install.packages(""xgboost"")
 ). Finally, I have updated all my NVIDIA drivers and downloaded the latest CUDA (although, tbh I don't even know what that does). I followed the steps described 
in the NVIDIA forums
 to avoid the crash I had when installing (i.e. remove integration with visual studio). FWIW I'm using a 
DELL Inspiron 15 Gaming
 and it has a NVIDIA GTX 1050 with 4GB.


Here's the full code I'm using (straight from the 
h2o download instructions
 except for the first line):


library(xgboost)
library(h2o)
localH2O = h2o.init()
demo(h2o.kmeans)



Any help would be much appreciated.


The full message I get when running the above code chunk:


H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    C:\Users\<my username>\AppData\Local\Temp\RtmpcdvCce\file1a106074110b/h2o_<my username>_started_from_r.out
    C:\Users\<my username>\AppData\Local\Temp\RtmpcdvCce\file1a10253139db/h2o_<my username>_started_from_r.err

java version ""15.0.2"" 2021-01-19
Java(TM) SE Runtime Environment (build 15.0.2+7-27)
Java HotSpot(TM) 64-Bit Server VM (build 15.0.2+7-27, mixed mode, sharing)

Starting H2O JVM and connecting: ............................................................Diagnostic HTTP Request:
   HTTP Status Code: -1
HTTP Error Message: Failed to connect to localhost port 54321: Connection refused
Cannot load library from path lib/windows_64/xgboost4j_gpu.dll
Cannot load library from path lib/xgboost4j_gpu.dll
Failed to load library from both native path and jar!
Cannot load library from path lib/windows_64/xgboost4j_omp.dll
Cannot load library from path lib/xgboost4j_omp.dll
Failed to load library from both native path and jar!
Cannot load library from path lib/windows_64/xgboost4j_minimal.dll
Cannot load library from path lib/xgboost4j_minimal.dll
Failed to load library from both native path and jar!
Failed to add native path to the classpath at runtime
java.io.IOException: Failed to get field handle to set library path
    at ai.h2o.xgboost4j.java.NativeLibLoader.addNativeDir(NativeLibLoader.java:229)
    at ai.h2o.xgboost4j.java.NativeLibLoader.initXGBoost(NativeLibLoader.java:43)
    at ai.h2o.xgboost4j.java.NativeLibLoader.getLoader(NativeLibLoader.java:66)
    at hex.tree.xgboost.XGBoostExtension.initXgboost(XGBoostExtension.java:70)
    at hex.tree.xgboost.XGBoostExtension.isEnabled(XGBoostExtension.java:51)
    at water.ExtensionManager.isEnabled(ExtensionManager.java:189)
    at water.ExtensionManager.registerCoreExtensions(ExtensionManager.java:103)
    at water.H2O.main(H2O.java:2203)
    at water.H2OStarter.start(H2OStarter.java:22)
    at water.H2OStarter.start(H2OStarter.java:48)
    at water.H2OApp.main(H2OApp.java:12)
Cannot initialize XGBoost backend! Xgboost (enabled GPUs) needs: 
  - CUDA 8.0
XGboost (minimal version) needs: 
  - GCC 4.7+
For more details, run in debug mode: `java -Dlog4j.configuration=file:///tmp/log4j.properties -jar h2o.jar`


ERROR: Unknown argument (<my username>/AppData/Local/Temp/RtmpcdvCce)


Usage:  java [-Xmx<size>] -jar h2o.jar [options]
        (Note that every option has a default and is optional.)

    -h | -help
          Print this help.

    -version
          Print version info and exit.

    -name <h2oCloudName>
          Cloud name used for discovery of other nodes.
          Nodes with the same cloud name will form an H2O cloud
          (also known as an H2O cluster).

    -flatfile <flatFileName>
          Configuration file explicitly listing H2O cloud node members.

    -ip <ipAddressOfNode>
          IP address of this node.

    -port <port>
          Port number for this node (note: port+1 is also used by default).
          (The default port is 54321.)

    -network <IPv4network1Specification>[,<IPv4network2Specification> ...]
          The IP address discovery code will bind to the first interface
          that matches one of the networks in the comma-separated list.
          Use instead of -ip when a broad range of addresses is legal.
          (Example network specification: '10.1.2.0/24' allows 256 legal
          possibilities.)

    -ice_root <fileSystemPath>
          The directory where H2O spills temporary data to disk.

    -log_dir <fileSystemPath>
          The directory where H2O writes logs to disk.
          (This usually has a good default that you need not change.)

    -log_level <TRACE,DEBUG,INFO,WARN,ERRR,FATAL>
          Write messages at this logging level, or above.  Default is INFO.

    -max_log_file_size
          Maximum size of INFO and DEBUG log files. The file is rolled over after a specified size has been reached.
          (The default is 3MB. Minimum is 1MB and maximum is 99999MB)

    -flow_dir <server side directory or HDFS directory>
          The directory where H2O stores saved flows.
          (The default is 'C:\Users\<my username>\h2oflows'.)

    -nthreads <#threads>
          Maximum number of threads in the low priority batch-work queue.
          (The default is.)

    -client
          Launch H2O node in client mode.

    -notify_local <fileSystemPath>
          Specifies a file to write when the node is up. The file contains one line with the IP and
          port of the embedded web server. e.g. 192.168.1.100:54321

    -context_path <context_path>
          The context path for jetty.

Authentication options:

    -jks <filename>
          Java keystore file

    -jks_pass <password>
          (Default is 'h2oh2o')

    -jks_alias <alias>
          (Optional, use if the keystore has multiple certificates and you want to use a specific one.)

    -hostname_as_jks_alias
          (Optional, use if you want to use the machine hostname as your certificate alias.)

    -hash_login
          Use Jetty HashLoginService

    -ldap_login
          Use Jetty Ldap login module

    -kerberos_login
          Use Jetty Kerberos login module

    -spnego_login
          Use Jetty SPNEGO login service

    -pam_login
          Use Jetty PAM login module

    -login_conf <filename>
          LoginService configuration file

    -spnego_properties <filename>
          SPNEGO login module configuration file

    -form_auth
          Enables Form-based authentication for Flow (default is Basic authentication)

    -session_timeout <minutes>
          Specifies the number of minutes that a session can remain idle before the server invalidates
          the session and requests a new login. Requires '-form_auth'. Default is no timeout

    -internal_security_conf <filename>
          Path (absolute or relative) to a file containing all internal security related configurations

Cloud formation behavior:

    New H2O nodes join together to form a cloud at startup time.
    Once a cloud is given work to perform, it locks out new members
    from joining.

Examples:

    Start an H2O node with 4GB of memory and a default cloud name:
        $ java -Xmx4g -jar h2o.jar

    Start an H2O node with 6GB of memory and a specify the cloud name:
        $ java -Xmx6g -jar h2o.jar -name MyCloud

    Start an H2O cloud with three 2GB nodes and a default cloud name:
        $ java -Xmx2g -jar h2o.jar &
        $ java -Xmx2g -jar h2o.jar &
        $ java -Xmx2g -jar h2o.jar &","['r', 'h2o']",rrodriguezbarron,https://stackoverflow.com/users/11689518/rrodriguezbarron,29
67452392,67452392,2021-05-08T21:38:34,2021-05-08 21:55:56Z,0,"I have a H2OFrame with two columns and I want to create new column, which is calculated from existing columns (sum of existing columns). How can I create new column in H2OFrame (like mutate() in dplyr) without converting H2OFrame to another frame? Is there any H2O R function doing this?


data <- data.frame(X = c(10, 20),
                   Y = c(30, 40))

library(h2o)
h2o.init()

data.hex <- as.h2o(data)
data.hex



How could I create output (Z = X + Y)?


   X  Y  Z
1 10 30 40
2 20 40 60","['r', 'h2o']",Jānis,https://stackoverflow.com/users/9579974/j%c4%81nis,63
67404805,67404805,2021-05-05T16:01:11,2021-05-08 09:40:33Z,0,"I am getting this warning when running h2o AutoML. I have version 3.32.1.2 installed, and running it on python 3.8.


AutoML progress: |
11:30:52.773: AutoML: XGBoost is not available; skipping it.



CODE:


import h2o
h2o.init()

h2o_df = h2o.H2OFrame(df)

train, test = h2o_df.split_frame(ratios=[.75])

# Identify predictors and response
x = train.columns
y = ""TERM_DEPOSIT""
x.remove(y)

from h2o.automl import H2OAutoML

aml = H2OAutoML(max_runtime_secs=600,
                #exclude_algos=['DeepLearning'],
                seed=1,
                #stopping_metric='logloss',
                #sort_metric='logloss',
                balance_classes=False,
                project_name='Completed'
)
%time aml.train(x=x, y=y, training_frame=train)","['python-3.x', 'xgboost', 'h2o', 'automl']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
67376720,67376720,2021-05-03T22:49:30,2021-05-04 12:38:30Z,0,"I am trying to use the 
flashlight
 package with the 
h2o
 package. An example of doing this on a regression model can be found 
here
. However, I am trying to make it work for a classification model... to achieve this I was following the example given in the link. 
flashlight
 will work with 
h2o
 if you provide your own custom predict function. However, the predict function that is in the example below does not work for classification.


Here is the code I'm using:


library(flashlight)
library(h2o)

h2o.init()
h2o.no_progress()

iris_hf <- as.h2o(iris)
iris_dl <- h2o.deeplearning(x = 1:4, y = ""Species"", training_frame = iris_hf, seed=123456)

pred_fun <- function(mod, X) as.vector(unlist(h2o.predict(mod, as.h2o(X))))
fl_NN <- flashlight(model = iris_dl, data = iris, y = ""Species"", label = ""NN"", 
                    predict_function = pred_fun)



But when I try and check the importance or interactions, I get an error.... for example:


light_interaction(fl_NN, type = ""H"",
              pairwise = TRUE)



Throws back the error:




Error: Assigned data 
predict(x, data = X[, cols, drop = FALSE])
 must
be compatible with existing data. Existing data has 22500 rows.
Assigned data has 90000 rows. ℹ Only vectors of size 1 are recycled.




I need to change the predict function somehow to make it work... but I have had no success yet... any suggestion as to how I could change the predict function to work?


EDIT UPDATE: So, I found a custom predict function that works with the 
light_interaction
 function. That is:


pred_fun <- function(mod, X) as.vector(unlist(h2o.predict(mod, as.h2o(X))[,2]))



Where the above is indexed for the specific category. However, The above doesn't work for calculating the importance. For example:


light_importance(fl_NN)



Gives the error:


Warning messages: 
1: In Ops.factor(actual, predicted) : ‘-’ not meaningful for factors 
2: In Ops.factor(actual, predicted) : ‘-’ not meaningful for factors
3: In Ops.factor(actual, predicted) : ‘-’ not meaningful for factors
4: In Ops.factor(actual, predicted) : ‘-’ not meaningful for factors
5: In Ops.factor(actual, predicted) : ‘-’ not meaningful for factors



So, Im still trying to figure this out!?","['r', 'h2o']",Unknown,,N/A
67371448,67371448,2021-05-03T15:16:34,2021-05-06 03:34:51Z,680,"I am using spark version 2.4.4 and h2o-pysparkling-2.4 on the databricks and running following code


h2oConf = H2OConf().set('spark.sql.autoBroadcastJoinThreshold', '-1')
hc = H2OContext.getOrCreate(conf=h2oConf)



Sometimes it is working well but sometimes it is giving me following error


Py4JJavaError: An error occurred while calling o2542.getOrCreate.
: org.apache.spark.SparkException: Exception thrown in awaitResult:



Please suggest me the way to resolve this issue","['pyspark', 'h2o', 'sparkling-water']",user9482910,https://stackoverflow.com/users/9482910/user9482910,25
67328267,67328267,2021-04-30T04:40:55,2021-05-02 19:07:21Z,117,"My goal is to export data from a h2o frame to sql.


I receive the following error when using iterrows, and would like to replace it with an .apply function. An example (preferred) or resources regarding exporting data to SQL from a h2o frame would be very much be appreciated.


The code snippet is:


import numpy as np
import pandas as pd
from pandas import DataFrame
import pyodbc
import h2o
h2o.init()

data = {'COL_1': ['C1 First value', 'C1 Second value'],
        'COL_2': ['C2 First value', 'C2 Second value'],
        'COL_3': ['C3 First value', 'C3 Second value'],
        'COL_4': ['C4 First value', 'C4 Second value'],
        'COL_5': ['C5 First value', 'C5 Second value'],
        'COL_6': ['C6 First value', 'C6 Second value'],
        'COL_7': ['C7 First value', 'C7 Second value'],
        'COL_8': ['C8 First value', 'C8 Second value'],
        'COL_9': ['C9 First value', 'C9 Second value']}

df = pd.DataFrame (data, columns = ['COL_1','COL_2','COL_3','COL_4','COL_5','COL_6','COL_7','COL_8','COL_9'])
h2oframe = h2o.H2OFrame(df)

# removed odbcName and serverName
odbcName = 'xxxxxx'
serverName = 'xxxxxxx'

odbcConnection = pyodbc.connect ('Driver={Sql Server};Server='+serverName+';Database='+odbcName+';Trusted_Connection=yes')
cursor = odbcConnection.cursor()

# TempTable has already been created in the database
for index, row in h2oframe.iterrows():
    cursor.execute(""INSERT INTO TempTable (COL_1,COL_2,COL_3,COL_4,COL_5,COL_6,COL_7,COL_8,COL_9) VALUES(?,?,?,?,?,?,?,?,?)"",row.COL_1, row.COL_2, row.COL_3, row.COL_4, row.COL_5, row.COL_6, row.COL_7, row.COL_8, row.COL_9)
odbcConnection.commit()
cursor.close()





AttributeError                            Traceback (most recent call last)
<ipython-input-36-19764dc050b1> in <module>
----> 1 for index, row in h2oframe.iterrows():
      2     cursor.execute(""INSERT INTO TempTable (COL_1,COL_2,COL_3,COL_4,COL_5,COL_6,COL_7,COL_8,COL_9) VALUES(?,?,?,?,?,?,?,?,?)"",row.COL_1, row.COL_2, row.COL_3, row.COL_4, row.COL_5, row.COL_6, row.COL_7, row.COL_8, row.COL_9)
      3 odbcConnection.commit()
      4 cursor.close()

AttributeError: 'H2OFrame' object has no attribute 'iterrows'","['python', 'sql', 'h2o']",Unknown,,N/A
67314618,67314618,2021-04-29T09:26:40,2021-05-02 19:37:46Z,0,"I want to use fastshap with h2o model. But there are lots of difficult things that I have to solve.
I load completed model, and use predict_contribusions function.


Here's part of my code.


# data: dataXY
library(h2o)
h2o.init(bind_to_localhost = F)

# data split
data.hex <- as.h2o(dataXY)
splits <- h2o.splitFrame(data.hex,ratios=c(0.6,0.2),seed=1234)
train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]

# load model from server
Basemodel <- h2o.loadModel(path)
pred <- predict_contrbutions.H2OModel(Basemodel, newdata = train)
explainer <- explain(Basemodel, X = train, pred_wrapper = pred, newdata = test[1], nsim = 1)



Then, some error is generated.


Error in `[<-.H2OFrame`(`*tmp*`, O, value = <environment>) : 
  `row` must be missing or a numeric vector



I really want to know how to figure out this.
Could you help me?


Or other package recommendation for making SHAP plot with h2o model is also Okay.","['r', 'h2o', 'shap']",이광현,https://stackoverflow.com/users/15791721/%ec%9d%b4%ea%b4%91%ed%98%84,1
67277764,67277764,2021-04-27T06:07:21,2021-05-01 14:44:33Z,0,"I created the following 
environment.yml
 file from my local Anaconda that contains an openjdk package.


name: venv
channels:
  - defaults
dependencies:
  - openjdk=11.0.6





However, Azure Machine Learning couldn't install the openjdk package from the 
environment.yml
 file as module is not found.




Backstory:


I'm building a machine learning model using H2O.ai Python library. Unfortunately, H2O.ai is written in Java so it requires Java to run. I've installed openjdk to my local Anaconda venv for running H2O.ai locally - it runs perfectly. However, I couldn't deploy this model to Azure Machine Learning because it couldn't install openjdk from requirements.txt or environment.yml as module not found.","['java', 'python', 'anaconda', 'h2o', 'azure-machine-learning-service']",Mark Rotteveel,https://stackoverflow.com/users/466862/mark-rotteveel,108k
67201421,67201421,2021-04-21T18:16:20,2021-04-22 20:27:21Z,0,"Problem


I want to use H2O's Sparkling Water on multi-node clusters in Azure Databricks, interactively and in jobs through RStudio and R notebooks, respectively. I can start an H2O cluster and a Sparkling Water context on a 
rocker/verse:4.0.3
 and a 
databricksruntime/rbase:latest
 (as well as 
databricksruntime/standard
) Docker container on my local machine but currently not on a Databricks cluster. There seems to be a classic classpath problem.


Error : java.lang.ClassNotFoundException: ai.h2o.sparkling.H2OConf
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
    at com.databricks.backend.daemon.driver.ClassLoaders$LibraryClassLoader.loadClass(ClassLoaders.scala:151)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at sparklyr.StreamHandler.handleMethodCall(stream.scala:106)
    at sparklyr.StreamHandler.read(stream.scala:61)
    at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:58)
    at scala.util.control.Breaks.breakable(Breaks.scala:42)
    at sparklyr.BackendHandler.channelRead0(handler.scala:39)
    at sparklyr.BackendHandler.channelRead0(handler.scala:14)
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321)
    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:295)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.lang.Thread.run(Thread.java:748)



What I've Tried


Setup: Single node Azure Databricks cluster, 7.6 ML (includes Apache Spark 3.0.1, Scala 2.12) with ""Standard_F4s"" driver (My use case is multi node, but I was trying to keep things simple)




Setting 
options()
, e.g., 
options(rsparkling.sparklingwater.version = ""2.3.11"")
 or 
options(rsparkling.sparklingwater.version = ""3.0.1"")




Setting 
config
, e.g.,


  conf$`sparklyr.shell.jars` <- c(""/databricks/spark/R/lib/h2o/java/h2o.jar"") 







or 
sc <- sparklyr::spark_connect(method = ""databricks"", version = ""3.0.1"", config = conf, jars = c(""/databricks/spark/R/lib/h2o/java/h2o.jar""))
 (or 
""~/R/x86_64-pc-linux-gnu-library/3.6/h2o/java/h2o.jar""
 or 
""~/R/x86_64-pc-linux-gnu-library/3.6/rsparkling/java/sparkling_water_assembly.jar""
 as the .jar location on Databricks RStudio)




Following directions here: 
http://docs.h2o.ai/sparkling-water/3.0/latest-stable/doc/deployment/rsparkling_azure_dbc.html






For Sparkling Water 3.32.1.1-1-3.0 select Spark 3.0.2




Spark 3.0.2 is not available as a cluster, chose 3.0.1 as in rest of my approach


Error in h2o_context(sc) : could not find function ""h2o_context""



Dockerfile that works on local machine


# get the base image (https://hub.docker.com/r/databricksruntime/standard; https://github.com/databricks/containers/blob/master/ubuntu/standard/Dockerfile)
FROM databricksruntime/standard

# not needed if using `FROM databricksruntime/r-base:latest` at top
ENV DEBIAN_FRONTEND noninteractive

# go into the repo directory
RUN . /etc/environment \
  # Install linux depedendencies here
  && apt-get update \
  && apt-get install libcurl4-openssl-dev libxml2-dev libssl-dev -y \
  # not needed if using `FROM databricksruntime/r-base:latest` at top
  && apt-get install r-base -y

# install specific R packages
RUN R -e 'install.packages(c(""httr"", ""xml2""))'
# sparklyr and Spark
RUN R -e 'install.packages(c(""sparklyr""))'
# h2o
# RSparkling 3.32.0.5-1-3.0 requires H2O of version 3.32.0.5.
RUN R -e 'install.packages(c(""statmod"", ""RCurl""))'
RUN R -e 'install.packages(""h2o"", type = ""source"", repos = ""http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/5/R"")'
# rsparkling
# RSparkling 3.32.0.5-1-3.0 is built for 3.0.
RUN R -e 'install.packages(""rsparkling"", type = ""source"", repos = ""http://h2o-release.s3.amazonaws.com/sparkling-water/spark-3.0/3.32.0.5-1-3.0/R"")'

# connect to H2O cluster with Sparkling Water context
RUN R -e 'library(sparklyr); sparklyr::spark_install(""3.0.1"", hadoop_version = ""3.2""); Sys.setenv(SPARK_HOME = ""~/spark/spark-3.0.1-bin-hadoop3.2""); library(rsparkling); sc <- sparklyr::spark_connect(method = ""databricks"", version = ""3.0.1""); sparklyr::spark_version(sc); h2oConf <- H2OConf(); hc <- H2OContext.getOrCreate(h2oConf)'","['r', 'apache-spark', 'h2o', 'azure-databricks', 'sparklyr']",Unknown,,N/A
67186743,67186743,2021-04-20T21:47:17,2021-04-21 20:45:56Z,0,"I'm trying to run H2o's automl and I want to see the results of XGboost in automl. When I'm trying to run this code:


aml1 <- h2o.automl(y = y, x = x, training_frame = train, keep_cross_validation_models = F, seed = 123)



I'm getting this message:




  |                            |   0%
16:25:37.796: AutoML: XGBoost is not available; skipping it.
Job $03017f00000132d4ffffffff$_8c51b0759fd2e77fd8940b41f83340c2 was cancelled.




I'm using H2o 3.32.1.1 version on a Centos 7 linux server. I'm using R 3.6.0 version. I installed H2o using:


install.packages(""h2o"", type=""source"", repos=(c(""http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R""))) 

as given in:

https://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html


Please help. I don't know why xgboost is not running in automl. And, how it could be fixed. I couldn't find any solution online yet to solve this issue.


EDIT


Issue got resolved by installing 
xgboost
 package in R. Thanks guys for your comments and directing me towards installing 
xgboost
 in R.","['r', 'linux', 'h2o', 'automl', 'h2o.ai']",Unknown,,N/A
67132186,67132186,2021-04-16T21:05:14,2021-04-20 20:34:50Z,0,"I was trying to run an automl model on a linux server with 32 GB RAM using h2o in R on a data set having 5 million records and 70 features.


Here's a code that I was trying to run:


aml <- h2o.automl(y = y, x = x, training_frame = train, seed = 123)



I got this error:




|====                                         |   8%Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  :   Unexpected CURL error: Recv failure: Connection reset by peerError in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  :   Unexpected CURL error: Failed connect to localhost:54321; Connection refused




And, then after restarting the R session and doing all the clean up when I tried to initialize h2o using:


h2o.init()


It gave me an error:




H2O is not running yet, starting it now...Error in system(paste(which, shQuote(names[i])), intern = TRUE, ignore.stderr = TRUE) :   cannot popen '/usr/bin/which 'java' 2>/dev/null', probable reason 'Cannot allocate memory'




Does anyone know how to fix it. I couldn't find any solutions online yet.


EDIT 1


By restarting the server; h2o got was getting initialized with 
h2o.init
. And, by using 
keep_cross_validation_models = F
 in 
h2o.automl
; model generation got completed to 100%.


But, I started getting this new error after 
h2o.automl
 progress bar reached 100%.


aml <- h2o.automl(y = y, x = x, training_frame = train, keep_cross_validation_models = F, seed = 123)





|======================================================| 100%ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://localhost:54321/3/Jobs/$03017f00000132d4ffffffff$_8d3c6f4c4fd73944d1ce4624042d44a9) Error: parse error: premature EOF                                                            (right here) ------^




EDIT 2:


The problem got solved by rebooting the server and initializing h2o by adding 
max_mem_size = ""12g""
 option.","['r', 'linux', 'h2o', 'h2o.ai']",Unknown,,N/A
67095498,67095498,2021-04-14T16:21:10,2021-04-15 15:57:53Z,0,"I have an automl model created with the H2O package. Currently, H2O only calculates Shapley values on tree-based models. I've used the IML package to calculate the values on the AML model. However, because I have a large number of features, the plot is too jumbled to read. I'm looking for a way to select/show only the top X number of features. I can't find anything in the IML CRAN PDF nor in other documentation I've found by Googling.


#initiate h2o
h2o.init()
h2o.no_progress()

#create automl model (data cleaning and train/test split not shown)
set.seed(1911)
num_models <- 10
aml <- h2o.automl(y = label, x = features,
                   training_frame = train.hex,
                   nfolds = 5,
                   balance_classes = TRUE,
                   leaderboard_frame = test.hex,
                   sort_metric = 'AUCPR',
                   max_models = num_models,
                   verbosity = 'info',
                   exclude_algos = ""DeepLearning"", #exclude for reproducibility
                   seed = 27)

# 1. create a data frame with just the features
features_eval <- as.data.frame(test) %>% dplyr::select(-target)

# 2. Create a vector with the actual responses
response <- as.numeric(as.vector(test$target))

# 3. Create custom predict function that returns the predicted values as a
#    vector (probability of purchasing in our example)
pred <- function(model, newdata)  {
  results <- as.data.frame(h2o.predict(model, as.h2o(newdata)))
  return(results[[3L]])
}

# example of prediction output
pred(aml, features_eval) %>% head()

#create predictor needed
predictor.aml <- Predictor$new(
  model = aml, 
  data = features_eval, 
  y = response, 
  predict.fun = pred,
  class = ""classification""
  )

high <- predict(aml, test.hex) %>% .[,3] %>% as.vector() %>% which.max()

high_prob_ob <- features_eval[high, ]

shapley <- Shapley$new(predictor.aml, x.interest = high_prob_ob, sample.size = 200) 

plot(shapley, sort = TRUE)



Any suggestions/help appreciated.


Thank you,
Brian","['r', 'h2o', 'iml']",Brian Head,https://stackoverflow.com/users/10290113/brian-head,57
66973512,66973512,2021-04-06T17:24:23,2021-04-07 04:45:53Z,307,"I'm using a generalized low-rank estimator to infer missing values in a data set regarding sensor readings. I'm using H2O to create and train the model:


glrm = H2OGeneralizedLowRankEstimator(k=10,
                                      loss=""quadratic"",
                                      gamma_x=0.5,
                                      gamma_y=0.5,
                                      max_iterations=2000,
                                      recover_svd=True,
                                      init=""SVD"",
                                      transform=""standardize"")
glrm.train(training_frame=train)



After the model is trained, the information provided regarding the performance metrics (MSE and RMSE) both return NaN. Does anybody know why? Firstly I thought it could be related to NaN entries in the data set, but I have already tried with one that is complete, and the same problem occurs.
I need this information to perform a grid search over some of the model parameters to select the best one.


Thank you very much,


Luísa Nogueira","['python', 'h2o', 'h2o.ai']",Luisa Nogueira,https://stackoverflow.com/users/15263363/luisa-nogueira,13
66911104,66911104,2021-04-01T20:23:22,2021-04-01 22:00:55Z,0,"The H2O package provides a function to plot shap values per observations. See code below taken from the r documentation.


The function returns a ggplot object but I am not able to change the color of the bars. I am able to add titles, annotations, etc but can't access the geom. I'd like to color the positive values pink and the negative values in blue


library(h2o)
h2o.init()

# Import the wine dataset into H2O:
f <- ""https://h2o-public-test-data.s3.amazonaws.com/smalldata/wine/winequality-redwhite-no-BOM.csv""
df <-  h2o.importFile(f)

# Set the response
response <- ""quality""

# Split the dataset into a train and test set:
splits <- h2o.splitFrame(df, ratios = 0.8, seed = 1)
train <- splits[[1]]
test <- splits[[2]]

# Build and train the model:
gbm <- h2o.gbm(y = response,
               training_frame = train)

# Create the SHAP row explanation plot
shap_explain_row_plot <- h2o.shap_explain_row_plot(gbm, test, row_index = 1)
print(shap_explain_row_plot)","['r', 'ggplot2', 'h2o']",stefan,https://stackoverflow.com/users/12993861/stefan,121k
66902471,66902471,2021-04-01T10:02:54,2021-04-06 10:19:30Z,361,"I need to know which loss functions are used in the h2o gbm and xgboost functions for the gaussian, binomial and multinomial distributions. Unfortunately, my knowledge of Java is very limited and I can't really decipher the source code, and there doesn't seem to be any document clarifying which distribution is associated with which function. I think I gather from 
here
 that it's logloss for binomial and MSE for gaussian, but I can't find anything for multinomial. Does anybody here maybe know the answer?","['h2o', 'loss-function']",AberLan,https://stackoverflow.com/users/15529486/aberlan,3
66865411,66865411,2021-03-30T05:47:22,2021-04-09 12:31:08Z,0,"I had just removed Java 16 from my Mac (downgrade to Java 11) in order to run H2O. H2O cluster is finally able to connect after running 
h2o.init()
.


However, I run into an error right after trying to import a CSV file using 
h2o.importFile()
. I have checked that the file is in my R default working directory and running a 
read.csv()
 of the same file works. Does anyone know the fix for this? Thank you very much in advance for your help!


Java version:


java version ""11.0.10"" 2021-01-19 LTS.

Java(TM) SE Runtime Environment 18.9 (build 11.0.10+8-LTS-162).

Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.10+8-LTS-162, mixed mode)


The code entered is:


sample <- read.csv(""sample_csv.csv"") #able to read file

library(h2o)
h2o.init()
data.h2o <- h2o.importFile(""sample_csv.csv"") #error



The error message:


Connection successful!

R is connected to the H2O cluster:   
    H2O cluster uptime:         13 hours 41 minutes   
    H2O cluster timezone:       Asia/Singapore   
    H2O data parsing timezone:  UTC   
    H2O cluster version:        3.32.0.1   
    H2O cluster version age:    5 months and 21 days !!!   
    H2O cluster name:           H2O_started_from_R_User_pmr248   
    H2O cluster total nodes:    1   
    H2O cluster total memory:   4.00 GB   
    H2O cluster total cores:    16   
    H2O cluster allowed cores:  16   
    H2O cluster healthy:        TRUE   
    H2O Connection ip:          localhost   
    H2O Connection port:        54321   
    H2O Connection proxy:       NA   
    H2O Internal Security:      FALSE   
    H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4   
    R Version:                  R version 4.0.4 (2021-02-15)   

Your H2O cluster version is too old (5 months and 21 days)!  
Please download and install the latest version from http://h2o.ai/download/. 

ERROR: Unexpected HTTP Status code: 404 Not Found (url = http://localhost:54321/3/ImportFiles?path=sample_csv.csv&pattern=)

water.exceptions.H2ONotFoundArgumentException
 [1] ""water.exceptions.H2ONotFoundArgumentException: File sample_csv.csv does not exist""                           
 [2] ""    water.persist.PersistNFS.importFiles(PersistNFS.java:127)""                                               
 [3] ""    water.persist.PersistManager.importFiles(PersistManager.java:386)""                                       
 [4] ""    water.api.ImportFilesHandler.importFiles(ImportFilesHandler.java:25)""                                    
 [5] ""    jdk.internal.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)""                                    
 [6] ""    java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""
 [7] ""    java.base/java.lang.reflect.Method.invoke(Method.java:566)""                                              
 [8] ""    water.api.Handler.handle(Handler.java:60)""                                                               
 [9] ""    water.api.RequestServer.serve(RequestServer.java:470)""                                                   
[10] ""    water.api.RequestServer.doGeneric(RequestServer.java:301)""                                               
[11] ""    water.api.RequestServer.doGet(RequestServer.java:225)""                                                   
[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:687)""                                            
[13] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:790)""                                            
[14] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)""                                  
[15] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)""                              
[16] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)""                       
[17] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)""                      
[18] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)""                        
[19] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)""                               
[20] ""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)""                        
[21] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)""                       
[22] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)""                           
[23] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                   
[24] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                         
[25] ""    water.webserver.jetty9.Jetty9ServerAdapter$LoginHandler.handle(Jetty9ServerAdapter.java:130)""            
[26] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)""                   
[27] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)""                         
[28] ""    org.eclipse.jetty.server.Server.handle(Server.java:531)""                                                 
[29] ""    org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)""                                       
[30] ""    org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)""                             
[31] ""    org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)""             
[32] ""    org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)""                                       
[33] ""    org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)""                                    
[34] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)""                  
[35] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)""                
[36] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)""               
[37] ""    org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)""                      
[38] ""    org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)""
[39] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)""                        
[40] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)""                         
[41] ""    java.base/java.lang.Thread.run(Thread.java:834)""                                                         

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page, : ERROR MESSAGE: File sample_csv.csv does not exist","['r', 'h2o', 'h2o4gpu']",Mykola Semenov,https://stackoverflow.com/users/11914501/mykola-semenov,802
66850203,66850203,2021-03-29T07:28:28,2021-03-30 01:38:31Z,240,"Is it possible (and how?) to provide time series for binary classification in H2O.ai's Driverless AI? I have dataframe that looks like this:




ID


Status/Target [0/1]


TimeStamp for events that happened on given ID, in last 90 days


Details of those events (category, description, values, etc...)




Ideally what i want is to build a model that predict status for given ID, based on provided history of events.","['h2o', 'driverless-ai']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
66832971,66832971,2021-03-27T15:24:52,2021-03-29 18:36:09Z,616,"I created a ML model with h2o's AutoML in Python and exported it as MOJO.
I know I can run it again in Python when using the h2o package and an h2o server / the java application.


But I was wondering if and how I could convert/transform/use this model without using an h2o server.
I only want to use h2o for the model generation, but not for running it.


Is this even possible, and if yes, how?


Thanks in advance,
T0Bi","['python', 'machine-learning', 'h2o']",TOBi,https://stackoverflow.com/users/4965276/tobi,35
66806500,66806500,2021-03-25T19:42:43,2021-03-25 19:57:34Z,0,"I installed ""h2o"" using the R command 
install.packages(""h2o"")
.  This prompted me to install the latest version of Java, which I did: version 16.  All ran perfectly – except that on launch, h2o would nag me to install the latest version of h2o from the h2o.ai website, as the version packaged with the CRAN download is 5 months out of date.


When I downloaded h2o version 3.32.0.5, I was unable to start h2o with 
h2o.init()
: I receive the message ""only Java 8...14 are supported, system version is 16"".


Java 14 is listed as having security flaws, so I'd rather not downgrade from Java 16; instead, I'd like to revert to the previous version of h2o (as installed from CRAN).  I uninstalled Java and the h2o R package completely, then reinstalled Java 16 and ran 
install.packages(""h2o"")
: but I still see the ""only Java 8-14 supported"" message.  How can I get h2o running again?","['java', 'r', 'h2o']",Martin Smith,https://stackoverflow.com/users/3438001/martin-smith,"4,047"
66805422,66805422,2021-03-25T18:18:16,2021-03-26 17:10:08Z,379,"I ran a h2o gradient boosting classifier model to predict probabilities for three classes 0,1 and 2. There is a heavy class imbalance (93:5:2) in the training data.


Although the individual classes 1 & 2 are not correctly predicted in confusion matrix (as expected), the AUC is decent for these classes individually.


I plan to manually predict the final classes


My understanding is that the resulting probabilities (P0,P1 & P2) are calibrated and sum up to 1.


Since multinomial model in h2o is essentially one vs many approach, but the scores are summing up to 1, is it correct to add or compare probabilities?


So if P0 = 0.40 , P1 =0.35  and P2=0.25, the predicted class will be 0 (based on max probability)




Does this mean P(1,2) = 0.6
Or p(not 0) = 0.6? (
Since the model for class 0 is actually 0 against all other classes
)




Can I  then compare the probabilities of 1&2 and say P1 (0.35) > P2 (0.25), so the predicted class should be 1?
(
Since the resulting classes are mutually exclusive and probabilities add up to 1, will these be comparable?)","['python', 'h2o', 'multiclass-classification', 'multinomial', 'gbm']",Unknown,,N/A
66677651,66677651,2021-03-17T16:51:23,2021-03-21 19:48:25Z,208,"I've trained a continuous variable using H2O AutoML, and selected the best model to predict on a new dataset. Besides the point estimation, I would like to obtain a prediction interval. In H2O documentation, there is no option to include prediction interval in AutoML. I was wondering if there is an additional package or a manual approach I can follow to construct the prediction interval of the best model in the leaderboard.","['prediction', 'h2o', 'confidence-interval', 'standard-deviation', 'automl']",Lisseth,https://stackoverflow.com/users/14035190/lisseth,33
66653093,66653093,2021-03-16T10:12:28,2021-03-17 13:03:39Z,154,"h2o_model.accuracy
 prints model validation data when executed in a Jupyter Notebook cell (which is desirable, despite the function name). How to save this whole validation output (entire notebook cell contents) to a file? Please test before suggesting redirections.","['python', 'jupyter-notebook', 'h2o']",Unknown,,N/A
66615090,66615090,2021-03-13T15:16:04,2021-03-16 17:25:59Z,344,"I was trying use 
H2OAutoML
 in Python to create a regression model, but I can't find how to pass 'weights_column'.


I try this two ways:


# Create the AutoML model.
aml = H2OAutoML(
        seed=0,
        max_runtime_secs = None,
        include_algos=['GBM', 'DRF'],
        stopping_metric='RMSE',
        exploitation_ratio=0.1,
        weights_column='weight'
    )



This code raise an 
TypeError
:


TypeError: H2OAutoML got an unexpected keyword argument 'weights_column'


# Create the AutoML model.
aml = H2OAutoML(
        seed=0,
        max_runtime_secs = None,
        include_algos=['GBM', 'DRF'],
        stopping_metric='RMSE',
        exploitation_ratio=0.1,
        algo_parameters={'weights_column': 'weight'}
    )



And this code raise 
H2oResponseError
 on train step:


H2OResponseError: Server error water.exceptions.H2OIllegalValueException:
Error: Illegal value for field: algo_parameters: weights_column



Can Someone help me to use this parameter? thanks","['python', 'machine-learning', 'data-science', 'h2o']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
66587667,66587667,2021-03-11T17:37:13,2021-03-12 20:13:26Z,0,"I use machine learning algorithms for species distribution modeling. This involves fitting a model in R and applying the model to a raster stack of environmental predictors using the predict() function. This is relatively straightforward for functions such as randomForest, glm, and maxent. However, I would like to use the best model selected using the h2o.automl function in the package h2o to predict occurrence probabilities across a stack of environmental predictors. Now, it is possible to convert raster stacks into data frames and then use these for h2o prediction. However, this tends to eat up a lot of RAM for large raster stacks. Below is an example using some publicly available low-resolution data.


library(sp)
library(raster)
library(maptools)
library(rgdal)
library(dismo)
library(h2o)
library(sp)
library(h2o)

bioclim.data <- getData(name = ""worldclim"",
                        var = ""bio"",
                        res = 2.5)
obs.data <- read.csv(""https://raw.githubusercontent.com/jcoliver/learn-r/gh-pages/data/Carnegiea-gigantea-GBIF.csv"")
obs.data <- obs.data[!is.na(obs.data$latitude), ]
obs.data <- obs.data[, c(""longitude"", ""latitude"")]
obs.sp<-SpatialPoints(obs.data, proj4string = CRS(""+proj=longlat +datum=WGS84 +no_defs""))

# Determine geographic extent of our data
max.lat <- ceiling(max(obs.data$latitude))
min.lat <- floor(min(obs.data$latitude))
max.lon <- ceiling(max(obs.data$longitude))
min.lon <- floor(min(obs.data$longitude))
geographic.extent <- extent(x = c(min.lon, max.lon, min.lat, max.lat))

bioclim.data <- crop(x = bioclim.data, y = geographic.extent)
background<-randomPoints(mask = bioclim.data,
                         n = nrow(obs.data),
                         ext = geographic.extent,
                         extf = 1.25)
colnames(background)<-c(""longitude"", ""latitude"")

background.sp<-SpatialPoints(background, proj4string = CRS(""+proj=longlat +datum=WGS84 +no_defs""))

presence<-data.frame(Presence = rep(1, times = nrow(obs.data)), extract(bioclim.data, obs.sp))
absence<-data.frame(Presence = rep(0, times = nrow(obs.data)), extract(bioclim.data, background.sp))                     

df<-rbind(presence, absence)                     
df$Presence<-as.factor(df$Presence)
h2o.init(max_mem_size = ""4g"")
set.seed(1234)
df_h2o <- as.h2o(df)
splits <- h2o.splitFrame(df_h2o, c(0.7, 0.15), seed = 1234)
train <- h2o.assign(splits[[1]], ""train"")
valid <- h2o.assign(splits[[2]], ""valid"")
test <- h2o.assign(splits[[3]], ""test"")
y <- ""Presence""
x <- setdiff(names(df_h2o), y)

aml <- h2o.automl(x = x,
                  y= y,
                  training_frame = train,
                  leaderboard_frame = valid,
                  max_runtime_secs = 60) #default 1 hour 3600 secs, more time, more accurate

pred<-as.data.frame(predict(aml, test)) # this works. It reports predicted probabilities of presence vs. absence.
pred_rstr<-predict(aml, bioclim.data) # this doesn't work. Must be an H2oFrame

biodf<-as.data.frame(bioclim.data) # this forces the raster stack into a dataframe. This works, but only for low resolution rasters.
bio_h2o<-as.h2o(biodf)
pred2<-as.data.frame(predict(aml, bio_h2o))

pred_rstr<-bioclim.data[[1]] # pull one of the rasters from the stack to serve as a template
values(pred_rstr)<-as.vector(pred2$p1) # set raster values to predicted probabilities
plot(pred_rstr)","['r', 'h2o']",Unknown,,N/A
66573496,66573496,2021-03-10T21:59:44,2021-03-11 05:11:30Z,47,Is it possible to extract base models prediction given the final stacked ensemble mojo model using java in production for scoring,"['java', 'h2o', 'mojo', 'ensembles']",blehblehbleh,https://stackoverflow.com/users/2513990/blehblehbleh,154
66569899,66569899,2021-03-10T17:28:08,2021-04-27 20:25:48Z,444,"Currently trying to run an h2o-wave app on a remote server.  I'm restricted to using 
0.0.0.0
 as the host on the server (the specific port is not as restrictive).


I've looked at the h2o 
configuration documentation
 and tried several variations of what they suggest:


H2O_WAVE_INTERNAL_ADDRESS=ws://0.0.0.0:8000
H2O_WAVE_EXTERNAL_ADDRESS=ws://0.0.0.0:8000
H2O_WAVE_APP_ADDRESS=ws://0.0.0.0:8000



But, the app is still running on the default localhost: 
http://127.0.0.1:8000","['python', 'h2o', 'h2o-wave']",Keith,https://stackoverflow.com/users/9086383/keith,154
66552460,66552460,2021-03-09T18:09:09,2021-03-09 18:09:09Z,143,"After training h2o, if I want to predict classes for a dataframe like 
hf_pred
, I get NaN columns like following:


predictions = model.predict(hf_pred)

print(f'predictions: {predictions} ')





Any idea why the columns tag are 
0.0
 and 
nan
 ?","['python', 'h2o', 'automl']",Phoenix,https://stackoverflow.com/users/10178162/phoenix,399
66469322,66469322,2021-03-04T05:49:59,2021-03-05 21:13:43Z,123,"I have to import mysql table in h2o.I am able to do this in local h2o. I need to do this at docker container.


How to use mysql JDBC(mysql connector jar) in docker compose or docker file? h2o has given only environment variables for driverlessAI. What is method or environment variables for h2o opensource?
I am using below docker compose file but its not able to pick 
DRIVERLESS_AI_CONFIG_FILE
 environment variable.


version: '3.1'

services:

  h2o:
    image: h2oai/h2o-open-source-k8s:3.32.0.3
    container_name: secure-h2o
    ports:
      - 6041:54321
    environment:
      - DRIVERLESS_AI_CONFIG_FILE: ""/home/renosecure/docker-container-mount/vol-h2o/tmp/config.toml""
      - JARPATH: ""/home/renosecure/docker-container-mount/vol-h2o/tmp/mysql-connector-java-8.0.23.jar""   
    volumes:
      - /home/renosecure/docker-container-mount/vol-h2o/data:/data
      - /home/renosecure/docker-container-mount/vol-h2o/log:/log
      - /home/renosecure/docker-container-mount/vol-h2o/license:/license
      - /home/renosecure/docker-container-mount/vol-h2o/tmp:/tmp","['mysql', 'docker-compose', 'h2o', 'h2o4gpu']",James Z,https://stackoverflow.com/users/4420967/james-z,12.3k
66339379,66339379,2021-02-23T18:57:12,2021-02-25 23:47:11Z,195,"pros_gbm = H2OGradientBoostingEstimator(nfolds=0,seed=1234, keep_cross_validation_predictions = False, ntrees=1000, max_depth=3, learn_rate=0.01, distribution='multinomial')
pros_gbm.train(x=predictors, y=target, training_frame=hf_train, validation_frame = hf_test)


pros_gbm.predict(hf_test)


Currently, I am predicting my test data like above, but how can I predict my test data for the nth tree(out of 1000 trees) of this model? is there any option in ""predict"" for that, or is there any other way?","['python', 'h2o', 'interaction', 'multilabel-classification', 'gbm']",Adrish Ray,https://stackoverflow.com/users/5248451/adrish-ray,43
66324765,66324765,2021-02-22T23:11:03,2021-09-02 00:03:40Z,622,"I am running a binary classification model using H2O autoML. I have explicitly told autoML to treat this as a classification model with the following line of code.


# This line of code turns our int variable into a factor.
# This is necessary to tell H2O that we want a classification model
feature_data['Radius'] = feature_data['Radius'].asfactor()



After running H20 autoML for a minute and then using the following line of code;


lb = aml.leaderboard
lb.head()
lb.head(rows=lb.nrows) # Entire leaderboard



I got the output in the screenshot below



As you can see, the metrics used for classification are AUC and logloss but what I want to see is accuracy. What should I add to get such an output?","['python', 'machine-learning', 'h2o', 'automl', 'h2o.ai']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
66309697,66309697,2021-02-22T03:28:38,2021-02-22 07:27:56Z,229,"While using H2O DAI to build models, I noticed that in the end model there would be some correlated variables.
For instance, variables ""max number of saving accounts in past 9 months"" and ""max number of saving accounts in past 3 months"" both show up in the final model, but they are having a high correlation.
Understand there are ways we can check this prior to feeding the data for H2O DAI, but I am wondering if there is some settings or good way to let H2O DAI check variable multicollinearity automatically while selecting features to build models?


Thanks for the help in advance.","['h2o', 'driverless-ai']",Kai,https://stackoverflow.com/users/11942120/kai,5
66299204,66299204,2021-02-21T04:40:24,2021-02-21 09:35:27Z,0,"I am using the H2O R package.


My understanding is, that this package requires you to have an internet connection as well as connect to the the h2o servers? If you use the h2o package run machine learning models on your data, does h2o ""see"" your data? I turned off my wifi and tried running some machine learning models using h2o :


data(iris) 
library(h2o)
h2o.init() 
iris_hf <- as.h2o(iris) 
iris_dl <- h2o.deeplearning(x = 1:4, y = 5, training_frame = iris_hf, seed=123456) 
predictions <- h2o.predict(iris_dl, iris_hf) 



This seems to work, but could someone please confirm? If you do not want anyone to see your data, is it still a good idea to use the ""h2o"" library? Since the code above runs without an internet connection, I am not sure about this.","['r', 'h2o', 'privacy']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
66288297,66288297,2021-02-20T05:20:03,2021-02-20 06:18:04Z,690,"I am able to run the following example code and get an F1 score:


import h2o
from h2o.estimators.gbm import H2OGradientBoostingEstimator
h2o.init()

# import the airlines dataset:
# This dataset is used to classify whether a flight will be delayed 'YES' or not ""NO""
# original data can be found at http://www.transtats.bts.gov/
airlines= h2o.import_file(""https://s3.amazonaws.com/h2o-public-test-data/smalldata/airlines/allyears2k_headers.zip"")

# convert columns to factors
airlines[""Year""]= airlines[""Year""].asfactor()
airlines[""Month""]= airlines[""Month""].asfactor()
airlines[""DayOfWeek""] = airlines[""DayOfWeek""].asfactor()
airlines[""Cancelled""] = airlines[""Cancelled""].asfactor()
airlines['FlightNum'] = airlines['FlightNum'].asfactor()

# set the predictor names and the response column name
predictors = [""Origin"", ""Dest"", ""Year"", ""UniqueCarrier"",
              ""DayOfWeek"", ""Month"", ""Distance"", ""FlightNum""]
response = ""IsDepDelayed""

# split into train and validation sets
train, valid = airlines.split_frame(ratios = [.8], seed = 1234)

# train your model
airlines_gbm = H2OGradientBoostingEstimator(sample_rate = .7, seed = 1234)
airlines_gbm.train(x = predictors,
                   y = response,
                   training_frame = train,
                   validation_frame = valid)

# retrieve the model performance
perf = airlines_gbm.model_performance(valid)
perf



With output like so:


ModelMetricsBinomial: gbm
** Reported on test data. **

MSE: 0.20546330299964743
RMSE: 0.4532806007316521
LogLoss: 0.5967028742962095
Mean Per-Class Error: 0.31720065289432364
AUC: 0.7414970113257631
AUCPR: 0.7616331690362552
Gini: 0.48299402265152613

Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.35417599264806404: 
NO  YES Error   Rate
0   NO  1641.0  2480.0  0.6018  (2480.0/4121.0)
1   YES 595.0   4011.0  0.1292  (595.0/4606.0)
2   Total   2236.0  6491.0  0.3524  (3075.0/8727.0)

...



However, my dataset doesn't work in a similar manner, despite appearing to be of the same form. My dataset target variable also has a binary label. Some information about my dataset:


y_test.nunique()
failure    2
dtype: int64



Yet my performance (
perf
) metrics are a much smaller subset of the example code:


perf = gbm.model_performance(hf_test)
perf
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.02363221438767555
RMSE: 0.1537277281028883
MAE: 0.07460874699751764
RMSLE: 0.12362377397478382
Mean Residual Deviance: 0.02363221438767555



It is difficult to share my data due to its sensitive nature. Any ideas on what to check?","['python', 'h2o', 'gbm']",user2205916,https://stackoverflow.com/users/2205916/user2205916,"3,426"
66242735,66242735,2021-02-17T13:19:29,2021-04-21 21:41:45Z,0,"I have been using h2o's autoencoder (deeplearning with autoencoder=TRUE) and h2o.anomaly for anomaly detection. So far these have been performed on the entire dataset. However, now I need to build a separate model for each segment (subpopulation) of the dataset. I found h2o.train_segments so I tried it, but it gave me this error message:


Error in is.numeric(y) : argument ""y"" is missing, with no default


Does this mean that h2o.train_segments doesn't work for h2o's autoencoder?


Here is my code:


mod <- h2o.train_segments(algorithm = ""deeplearning"",
                          segment_columns = ""some_col"",
                          parallelism = 1,
                          training_frame = data.hex,
                          activation = ""tanh"",
                          standardize = TRUE,
                          hidden = c(10, 1, 10),
                          epochs = 5,
                          autoencoder = TRUE,
                          loss = ""CrossEntropy"",
                          seed = 42)



Thank you for your insights!","['r', 'h2o', 'training-data', 'autoencoder', 'anomaly-detection']",Fanwei Zeng,https://stackoverflow.com/users/13756465/fanwei-zeng,55
66192230,66192230,2021-02-14T03:38:40,2021-07-19 17:40:28Z,394,"The 
H2OSupportVectorMachineEstimator
 in H2O seems to only support ""gaussian"" as the value of the kernel_type parameter. Is there a way to train a linear SVM with H2O?","['svm', 'h2o']",aburkov,https://stackoverflow.com/users/1372785/aburkov,13
66188791,66188791,2021-02-13T18:59:17,2021-02-18 04:36:30Z,0,"I'd like to know, how to add time data to h2o using as.h2o()?
I'm using a dataframe, which features minute records. R studio's function to form such a data type is as.POSIXct (or as.POSIXlt), which is not supported by h2o:


Provided column type POSIXct is unknown.  Cannot proceed with parse due to invalid argument.



The only other time function I've ran into is as.Date, which is not good, since it drops pretty much everything apart from the date. The official 
FAQ
 to h2o mentions its time FORMAT requirements, but not the method to feed the data. I tried to import the data in that format, but it lists as CHR string type. I've also tried the chron package, but that data is imported as Chron objects to the h2o cluster. Not sure if h2o recognizes any of these as time data.","['r', 'h2o']",JP Satrio,https://stackoverflow.com/users/10287139/jp-satrio,1
66167474,66167474,2021-02-12T06:50:31,2021-02-12 22:34:26Z,327,"I need to use the interaction variable feature of multiclass classification in 
H2OGradientBoostingEstimator
 in H2O in Python. I am not sure which parameter to use & how to use that. Can anyone please help me out with this?


Currently, I am using the below code -


pros_gbm = H2OGradientBoostingEstimator(nfolds=0,seed=1234, keep_cross_validation_predictions = False, ntrees=10, max_depth=3, learn_rate=0.01, distribution='multinomial')
hist_gbm = pros_gbm.train(x=predictors, y=target, training_frame=hf_train, validation_frame = hf_test,verbose=True)","['h2o', 'interaction', 'multilabel-classification', 'gbm']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
66080630,66080630,2021-02-06T18:47:25,2021-02-12 20:01:48Z,0,"I recently updated the h2o version from 
3.32.0.3
 to 
3.32.0.4
, and many codes that used to work do not work anymore.
More specifically, every time that I try to fit a 
gam
 model I always get the same error.


Error: DistributedException from localhost/127.0.0.1:54321: 'Index -1 out of bounds for length 7', caused by java.lang.ArrayIndexOutOfBoundsException: Index -1 out of bounds for length 7



Here the step to reproduce the error:


library(tidyverse)
library(h2o)

h2o.init()

mtcars_h2o <- as.h2o(mtcars)

att_model <- h2o.gam(y = ""mpg"",
                     gam_columns = c(""disp"", ""hp"", ""drat"", ""wt""),
                     family = ""gamma"",
                     link = ""log"",
                     training_frame = mtcars_h2o,
                     nfold = 3,
                     standardize = TRUE,
                     alpha = .5,
                     lambda_search = TRUE,
                     model_id = ""GAM_Model"")



And the h2o version


R is connected to the H2O cluster: 
     H2O cluster uptime:         1 seconds 630 milliseconds 
     H2O cluster timezone:       America/Chicago 
     H2O data parsing timezone:  UTC 
     H2O cluster version:        3.32.0.4 
     H2O cluster version age:    4 days  
     H2O cluster name:           H2O_started_from_R_marco_xbw859 
     H2O cluster total nodes:    1 
     H2O cluster total memory:   3.85 GB 
     H2O cluster total cores:    8 
     H2O cluster allowed cores:  8 
     H2O cluster healthy:        TRUE 
     H2O Connection ip:          localhost 
     H2O Connection port:        54321 
     H2O Connection proxy:       NA 
     H2O Internal Security:      FALSE 
     H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4 
     R Version:                  R version 4.0.3 (2020-10-10)



Is there a bug in the actual package, considering this version is fairly or am I doing something wrong here?


Thank you,

Marco","['r', 'h2o', 'gam']",Marco De Virgilis,https://stackoverflow.com/users/4332914/marco-de-virgilis,"1,067"
66053665,66053665,2021-02-04T20:54:43,2021-02-08 07:02:07Z,0,"I installed version 
3.2.0.4
 of 
h2o
 on 
R 4.0.3
, when I start 
h2o
 I get the error message:


simpleError in system2 (command, ""-version"", stdout = TRUE, stderr = TRUE): '""C: \ Program Files \ Java \ jre1.8.0_261 \ bin \ java.exe ""'not found>
Error in value [[3L]] (cond):
   You have a 32-bit version of Java. H2O works best with 64-bit Java.
Please download the latest Java SE JDK from the following URL:
https://www.oracle.com/technetwork/java/javase/downloads/index.html
In addition: Warning message:
In normalizePath (path.expand (path), winslash, mustWork):
   path [1] = ""C: \ Program Files \ Java \ jre1.8.0_261 / bin / java.exe"": The system cannot find the path specified



It turns out that the version of java installed is 64 bits. There is no other version 32 installed.


System = Windows 10","['java', 'r', 'h2o']",Frightera,https://stackoverflow.com/users/13726668/frightera,"5,079"
66011434,66011434,2021-02-02T14:12:51,2021-02-03 07:39:46Z,0,"I have installed h2o package(in R from RStudio console). Post h2o.init() I am trying to use the built in function upload_model()/upload_mojo() but I am getting following error.




h2o.upload_mojo()
Error in h2o.upload_mojo() : could not find function ""h2o.upload_mojo""
h2o.upload_model()
Error in h2o.upload_model() : could not find function ""h2o.upload_model""","['r', 'h2o']",Phil,https://stackoverflow.com/users/5221626/phil,"8,077"
66007770,66007770,2021-02-02T10:23:30,2021-02-02 16:56:22Z,0,"H2O returns wrong results when applying the unary ""-"" operator.


If we take a vector with different real values and calculate ""-U"", ""0-U"" and ""1-U""


require(h2o)
h2o.init()

set.seed(5988765)
u = runif(10,-1,1)
u[1] = 0
u[2] = 1
u[3] = -1
U = as.h2o(u)
uU = data.frame(
  u = u,
  U = as.data.frame(U)[,1],
  `0-U` = as.data.frame(0-U)[,1],
  `-U` = as.data.frame(-U)[,1],
  `1-U` = as.data.frame(1-U)[,1]
)
colnames(uU)=c(""u"",""U"",""0-U"",""-U"",""1-U"")
print(uU)



then we obtain this table, where -U is equal to 1-U instead of 0-U


            u          U        0-U        -U       1-U
1   0.0000000  0.0000000  0.0000000 1.0000000 1.0000000
2   1.0000000  1.0000000 -1.0000000 0.0000000 0.0000000
3  -1.0000000 -1.0000000  1.0000000 2.0000000 2.0000000
4  -0.7438718 -0.7438718  0.7438718 1.7438718 1.7438718
5   0.1917035  0.1917035 -0.1917035 0.8082965 0.8082965
6   0.3603398  0.3603398 -0.3603398 0.6396602 0.6396602
7  -0.7400701 -0.7400701  0.7400701 1.7400701 1.7400701
8  -0.4906903 -0.4906903  0.4906903 1.4906903 1.4906903
9   0.3773852  0.3773852 -0.3773852 0.6226148 0.6226148
10  0.1957617  0.1957617 -0.1957617 0.8042383 0.8042383



My H2O version is


> h2o.getVersion()
[1] ""3.32.0.1""","['r', 'h2o']",Konrad Rudolph,https://stackoverflow.com/users/1968/konrad-rudolph,544k
66004255,66004255,2021-02-02T05:46:08,2021-02-02 07:38:41Z,135,"I am exploring the functionalities of H2O DAI at the moment. Understand that H2O has the capability of choosing what variables to use and what transformers to apply on them during the feature selection/engineering phase. But is there a way to config in H2O DAI to limit the maximum number of features it could use out of the provided list? E.g., there are 100 features given, I only want H2O DAI to select 20 features out of it and apply feature engineering on it. Tried to browse through the user manual but did not find any hints on this so far.


Many thanks in advance.","['h2o', 'driverless-ai']",Kai,https://stackoverflow.com/users/11942120/kai,5
65944291,65944291,2021-01-28T19:53:56,2021-01-29 03:17:39Z,0,"I am trying to find the best parameters for my Neural Network I want to create in R. I am using the h2o package and following the tutorial from 
https://www.kaggle.com/wti200/deep-neural-network-parameter-search-r/comments


The code I have seems to run in 1 minute and from what I understood grid search should run multiple models until the best parameters are determined and that would take while to run. Please let me know where I am going wrong and how I can do the gird search to optimise my parameters.


h2o.init(nthreads=-1,max_mem_size='6G')
testHex = as.h2o(test)
trainHex = as.h2o(training)

predictors <-colnames(training)[!(colnames(training) %in% c(""responseVar""))]
response = ""responseVar""

hyper_params <- list(
  activation=c(""Rectifier"",""Tanh"",""Maxout"",""RectifierWithDropout"",""TanhWithDropout"",""MaxoutWithDropout""),
  hidden=list(c(20,20),c(50,50),c(75,75),c(100,100),c(30,30,30),c(25,25,25,25)),
  input_dropout_ratio=c(0,0.03,0.05),
  #rate=c(0.01,0.02,0.05),
  l1=seq(0,1e-4,1e-6),
  l2=seq(0,1e-4,1e-6)
)
h2o.rm(""dl_grid_random"")

search_criteria = list(strategy = ""RandomDiscrete"", max_runtime_secs = 360, max_models = 100, seed=1234567, stopping_rounds=5, stopping_tolerance=1e-2)
dl_random_grid <- h2o.grid(
  algorithm=""deeplearning"",
  grid_id = ""dl_grid_random"",
  training_frame=trainHex,
  x=predictors, 
  y=response,
  epochs=1,
  stopping_metric=""RMSE"",
  stopping_tolerance=1e-2,        ## stop when logloss does not improve by >=1% for 2 scoring events
  stopping_rounds=2,
  score_validation_samples=10000, ## downsample validation set for faster scoring
  score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time
  max_w2=10,                      ## can help improve stability for Rectifier
  hyper_params = hyper_params,
  search_criteria = search_criteria
)                            

grid <- h2o.getGrid(""dl_grid_random"",sort_by=""mae"",decreasing=FALSE)
grid

grid@summary_table[1,]
best_model <- h2o.getModel(grid@model_ids[[1]]) ## model with lowest logloss
best_model","['r', 'neural-network', 'h2o', 'grid-search', 'hyperparameters']",user123231322,https://stackoverflow.com/users/13382158/user123231322,65
65897690,65897690,2021-01-26T08:03:06,2021-01-29 06:53:26Z,395,"When I run an experiment with H2O AutoML, I got the error: ""
terminate called after throwing an instance of 'thrust::system::system_error' what():  parallel_for failed: invalid resource handle
"". This error message 
comes from
 XGBoost and it is because of the GPU limit exceed.


While I'm using the regular XGBoost, I set the cuda visible devices parameter to blank to disable GPUs. However, this arguments seems to be ignored in H2O AutoML - XGBoost implementation.


import os
os.environ[""CUDA_VISIBLE_DEVICES""] = """"



Currently, the only xgboost 
can be run
 on gpu in H2O AutoML.


The question it that anybody knows how to disable GPUs in H2O AutoML?


As a workaround, I excluded XGBoost algorithm to run my experiment for now. The trouble is passed when I exclude XGBoost but I do not want to give up the power of XGBoost.


from h2o.automl import H2OAutoML
model = H2OAutoML(max_runtime_secs = 60*60*2, exclude_algos = [""XGBoost""])","['gpu', 'xgboost', 'h2o', 'automl', 'h2o.ai']",sefiks,https://stackoverflow.com/users/7846405/sefiks,"1,585"
65871708,65871708,2021-01-24T14:23:29,2021-01-24 16:17:31Z,0,"I am trying to model same data in h2o. The problem I have is that beside the fact that the model fit process goes smoothly I get an error when I try to use the model to make prediction.
I do not have a good knowledge of Java so I do not know what the error is.
The strange thing is that sometimes occurs and sometimes does not.
Here the error message:


java.lang.NullPointerException

java.lang.NullPointerException
    at water.MRTask.dfork(MRTask.java:459)
    at water.MRTask.doAll(MRTask.java:396)
    at water.MRTask.doAll(MRTask.java:403)
    at hex.gam.GAMModel.predictScoreImpl(GAMModel.java:533)
    at hex.Model.score(Model.java:1618)
    at water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:403)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1575)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

Error: java.lang.NullPointerException



What can I do?
Thank you","['java', 'r', 'h2o', 'predict']",Marco De Virgilis,https://stackoverflow.com/users/4332914/marco-de-virgilis,"1,067"
65850262,65850262,2021-01-22T17:50:23,2021-01-22 18:17:49Z,0,"I am trying to split the data into training, validation, and test sets, with the h2o package, following the code in this link: 
https://www.r-bloggers.com/2017/09/hr-analytics-using-machine-learning-to-predict-employee-turnover/
.


Running this code:


split_h2o <- h2o.splitFrame(DatiRev_h2o, c(0.7, 0.15), seed = 12345 )



It returns the following error : ""Error in h2o.splitFrame(DatiRev_h2o, c(0.7, 0.15), seed = 12345) :
unused argument (seed = 12345)"".


Can someone please explain why?


Thanks for the help","['r', 'split', 'h2o', 'seed']",Shahin Manafi,https://stackoverflow.com/users/15052924/shahin-manafi,1
65836484,65836484,2021-01-21T22:09:16,2022-07-01 01:45:30Z,452,"I am relatively new to deploying python web applications but I was trying to deploy my H2O wave app to Heroku but kept running into issues and I couldn't find much help in the documentation.


Everything works fine locally if I start the server using the command (in the SDK for wave):


$ ./waved
2021/01/22 10:26:38 # 
2021/01/22 10:26:38 # ┌─────────────────────────┐
2021/01/22 10:26:38 # │  ┌    ┌ ┌──┐ ┌  ┌ ┌──┐  │ H2O Wave
2021/01/22 10:26:38 # │  │ ┌──┘ │──│ │  │ └┐    │ 0.11.0 20210118061246
2021/01/22 10:26:38 # │  └─┘    ┘  ┘ └──┘  └─┘  │ © 2020 H2O.ai, Inc.
2021/01/22 10:26:38 # └─────────────────────────┘
2021/01/22 10:26:38 # 
2021/01/22 10:26:38 # {""address"":"":10101"",""t"":""listen"",""webroot"":""/Users/kenjohnson/Documents/TTT/H2O Wave/wave/www""}
2021/01/22 10:26:39 # {""addr"":""127.0.0.1:64065"",""route"":""/tennis-pred"",""t"":""ui_add""}
2021/01/22 10:46:04 # {""host"":""http://127.0.0.1:8000"",""route"":""/counter"",""t"":""app_add""}



then in the root directory of the project running:


uvicorn tennis_pred_app:main



For deployment, all I have other than my wave python file is a 
requirements.txt
 and a 
Procfile
:


web: uvicorn tennis_pred_app:main --host 0.0.0.0 --port 10101



this is what my foo (tennis_pred_app.py) looks like (simplified):


from h2o_wave import Q, main, app, ui

@app(""/tennis-pred"")
async def serve(q: Q):
    show_form(q)
    await q.page.save()



The error I am currently running into is:


2021-01-22T00:28:41.000000+00:00 app[api]: Build started by user x
2021-01-22T00:31:07.040695+00:00 heroku[web.1]: State changed from crashed to starting
2021-01-22T00:31:06.879674+00:00 app[api]: Deploy 1dc65130 by user x
2021-01-22T00:31:06.879674+00:00 app[api]: Release v23 created by user x
2021-01-22T00:31:26.580199+00:00 heroku[web.1]: Starting process with command `uvicorn tennis_pred_app:main --host 0.0.0.0 --port 20819`
2021-01-22T00:31:30.299421+00:00 app[web.1]: INFO:     Uvicorn running on http://0.0.0.0:20819 (Press CTRL+C to quit)
2021-01-22T00:31:30.299892+00:00 app[web.1]: INFO:     Started parent process [4]
2021-01-22T00:31:46.000000+00:00 app[api]: Build succeeded
2021-01-22T00:32:27.041954+00:00 heroku[web.1]: Error R10 (Boot timeout) -> Web process failed to bind to $PORT within 60 seconds of launch
2021-01-22T00:32:27.093099+00:00 heroku[web.1]: Stopping process with SIGKILL
2021-01-22T00:32:27.199933+00:00 heroku[web.1]: Process exited with status 137
2021-01-22T00:32:27.242769+00:00 heroku[web.1]: State changed from starting to crashed","['heroku', 'h2o', 'h2o.ai', 'h2o-wave']",Unknown,,N/A
65766697,65766697,2021-01-17T22:34:11,2021-01-25 18:14:39Z,0,"Is there a way to extract the dispersion parameter (also called 
phi
) from an 
h2o.glm()
 object?


In standard R this can va achieved by doing


summary(glm_object)$dispersion



Is it possible to achieve the same with an 
h2o.glm()
 object?


Thank you","['r', 'h2o', 'glm']",AlSub,https://stackoverflow.com/users/10292638/alsub,"1,153"
65736436,65736436,2021-01-15T12:50:00,2021-01-15 19:55:37Z,0,"I've got a number of h2o model objects of class H2ORegressionModel and I want to get the number of observations (train, test or total) used to produce every model. Is it possible to get this info programmatically from the model object itself?


I've seen this information listed by ""nobs"" parameter inside the 
H2O Flow web interface report
 in ""Output training metrics"" and by nobs() function in H2O Python API, but couldn't find it anywhere in R.


Any help/hint would be appreciated. Thanks","['r', 'h2o']",dkmina,https://stackoverflow.com/users/10851493/dkmina,11
65705305,65705305,2021-01-13T15:49:48,2023-10-24 17:55:18Z,489,"The dataset has 177927 rows and 820 columns of one-hot encoded features. There is no NaN in the dataset. I want to build two H2O XGBoost models for regression on two kinds of labels ('count_5' and 'count_overlap') respectively, using the same feature matrix. I use python 3.8 on Ubuntu.


'count_5' has 4 unique numeric labels (from 0 to 4).










label


frequency










0


159466






1


18102






2


346






3


13










'count_overlap' has 2416 unique numeric labels.










label


count_overlap










0


53077






1


9989






2


5430






3


3224






4


2570






...


...






6558


1






2257


1






2385


1






2204


1






2047


1










Here is the main part of code for both models:


# Generate H2O frame
train = h2o.H2OFrame(mydf)
y = label_name
X = list(train.columns)
X.remove(y)
train[y] = train[y].asnumeric() 

# Model
estimator = H2OXGBoostEstimator(
            seed=1,
            distribution=""poisson"",
            model_id='XGB_default',
            keep_cross_validation_predictions=True,
            keep_cross_validation_fold_assignment=True,
            nfolds=2,
        )
estimator.train(X, y, train)

# save predictions
y_pred = estimator.cross_validation_holdout_predictions()
y_true = train[y]
y_true_pd = h2o.as_list(y_true)
y_pred_pd = h2o.as_list(y_pred)

# performance
estimator.cross_validation_metrics_summary().as_data_frame()



The H2O XGBoost model on 'count_5' gave reasonable results:


Training: Label: count_5 Model: XGB
xgboost Model Build progress: |███████████████████████████████████████████| 100%












mean


sd


cv_1_valid


cv_2_valid










mae


0.20095341


2.6120833E-4


0.20076871


0.20113811






mean_residual_deviance


0.74664176


0.0035013587


0.74911755


0.7441659






mse


0.11081107


0.0011397477


0.11161699


0.11000515






r2


-0.027853519


9.893299E-4


-0.027153956


-0.02855308






residual_deviance


0.74664176


0.0035013587


0.74911755


0.7441659






rmse


0.33288077


0.0017119459


0.3340913


0.33167022






rmsle


0.22899812


5.8065885E-4


0.22940871


0.22858754










Scoring History:










timestamp


duration


number_of_trees


training_rmse


training_mae


training_deviance










2021-01-13 13:35:09


15.256 sec


0.0


0.506659


0.503162


1.158219






2021-01-13 13:35:12


18.632 sec


1.0


0.433015


0.422635


1.004022






2021-01-13 13:35:12


18.830 sec


2.0


0.387392


0.363154


0.899638






2021-01-13 13:35:13


19.034 sec


3.0


0.360412


0.319287


0.830496






... ...


... ...


... ...


... ...


... ...


... ...






2021-01-13 13:35:15


21.244 sec


14.0


0.325060


0.203695


0.706665






2021-01-13 13:35:15


21.452 sec


15.0


0.324720


0.202657


0.704868






2021-01-13 13:35:16


22.861 sec


50.0


0.311705


0.191559


0.649280










Here are the y_true ('count_5') and y_pred










count_5


y_pred










0


0.098148






1


0.129788






1


0.181357






0


0.037972






0


0.165198






...


... ...






0


0.156512






0


0.138887






1


0.257443






0


0.077034






0


0.037227










However, the H2O XGBoost model on 'count_overlap' gave NaN predictions without warning or error raised:


Training: Label: count_overlap Model: XGB
xgboost Model Build progress: |███████████████████████████████████████████| 100%












mean


sd


cv_1_valid


cv_2_valid










mae


NaN


0.0


NaN


NaN






mean_residual_deviance


NaN


0.0


NaN


NaN






mse


NaN


0.0


NaN


NaN






r2


NaN


0.0


NaN


NaN






residual_deviance


NaN


0.0


NaN


NaN






rmse


NaN


0.0


NaN


NaN






rmsle


NaN


0.0


NaN


NaN
















timestamp


duration


number_of_trees


training_rmse


training_mae


training_deviance










2021-01-13 17:04:44


12.047 sec


0.0


415.741082


110.880732


154.986121






2021-01-13 17:04:47


15.042 sec


1.0


inf


inf


NaN










Here are the y_true ('count_overlap') and y_pred:










count_overlap


y_pred










0


NaN






1247


NaN






960


NaN






0


NaN






39


NaN






...


... ...






24


NaN






0


NaN






540


NaN






0


NaN






57


NaN










H2O XGBoost did quite well for the 'count_5' label. I also tried other H2O models. Random Forest, SVM, Deep Learning, and GLM all gave good results for both labels (no NaN at all). Why H2O XGBoost predicted NaN 'count_overlap' label? Is there any suggestion or solution?","['python', 'nan', 'xgboost', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
65674849,65674849,2021-01-11T21:34:57,2021-01-11 23:05:34Z,0,"I would like to be able to examine the node structure on my neural networks. Specifically, I use L1 and L2 regularisation and would like to know what proportion of my weights have atrophied to zero. Does my trained neuralnet use every single node, or can I get away with using much fewer hidden nodes? That sort of thing.


Here's a toy problem:


library(h2o)
h2o.init()

x <-seq(-10,10,by=0.0002)
y <- dnorm(x,sd=2)*sin(2*x)     # The function the neuralnet will attempt to fit
plot(x,y,type=""l"")

df <- data.frame(x=x,y=y)
df2 <- as.h2o(df)
model <- h2o.deeplearning(df2,
                x=1,
                y=2,
                hidden=c(200,100,50,10,5),  # way more hidden nodes than it needs
                l1=0.0000001,           # regularisation to reduce the number of unnecessary nodes
                l2=0.0000001,
                activation=""Tanh"",
                export_weights_and_biases=TRUE)

P <- as.data.frame(h2o.predict(model,df2))

lines(x,P$predict,col=2)
legend(""topleft"",legend=c(""Training data"",""nn output""),col=c(1,2),lwd=1)





Is there a function within h2o that will give me the information on what all the weights are?


(I've tried h2o.weights(), it only appears to give me the first layer)


Failing that, given that the model is a S4 object, what are the ways of inspecting an S4 object?


Bonus question: Is there any ability within h2o for visualising the node structure?","['r', 'deep-learning', 'h2o']",Ingolifs,https://stackoverflow.com/users/8968617/ingolifs,311
65672022,65672022,2021-01-11T17:48:38,2021-01-26 07:19:18Z,0,"I would like to check h2o version assigned to a model saved in file.
Is it possible in current h2o editions (I have eg. 
3.29.0.4930
)?


I can't find it in the messages written during 
h2o.loadModel()
 in R console unless there is a mismatch in h2o versions: ""Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page, : ERROR MESSAGE: Found version 3.14.0.3, but running version 3.29.0.4930""


Can I access it before loading model using h2o?","['r', 'h2o']",Unknown,,N/A
65630915,65630915,2021-01-08T14:32:28,2021-01-10 19:22:58Z,208,"Have your H2O ever exceeded the default training time budget (1 hour) and you had to stop it? what is your interpretation of this problem?


I am very familiar with this solution and this is happening for the very first time with a multiclass dataset of 20k instances, 40 features and 103 classes. However, surprisingly, when I lower the time budget to 15 mn (for example), h2O returns a model and makes predictions.


Thank you,


Yassine","['h2o', 'automl', 'h2o.ai']",Unknown,,N/A
65621234,65621234,2021-01-07T23:10:11,2021-01-07 23:10:11Z,0,"I'm looking at using h2o's AutoML functionality to benchmark different model algorithms, but I wish to do it with a custom cross-validation strategy. Based on the current documentation, I understand that AutoML default CV method is the conventional K-Fold CV.


However, I'm looking at performing a Forward next day CV method to replicate daily retraining of the data. For example, assuming I have 100 days of data:




For the first iteration, I will train it from Day 1 to Day 80, and score the prediction for Day 81


For the 2nd iteration, I will train it from Day 1 to 81, and score the prediction for Day 82


This process is repeated for all remaining days, and the validation score is based on the predictions from Day 81 to 100.




Is it possible to do that in either the Python or R version of h2o's package?


Thanks in advance!","['python', 'r', 'h2o', 'automl']",Kenneth Foo,https://stackoverflow.com/users/12914714/kenneth-foo,33
65530817,65530817,2021-01-01T15:04:56,2021-01-01 16:03:35Z,106,"I have successfully built an h2o.ai automl object.  The problem I am having, however, is that whenever I am accessing the object, it repeats all of the object performance stats in the console output (including when I create a scored object which is particularly annoying).  How can I supress this output?


As an example, everytime I perform a task in the console, I now get model details listed in the std output.


results_data[:, i] = (cwc_as_df.values).flatten()
Model Details
=============
H2OStackedEnsembleEstimator :  Stacked Ensemble
Model Key:  StackedEnsemble_AllModels_AutoML_20210101_103720
No model summary for this model
ModelMetricsBinomialGLM: stackedensemble
** Reported on train data. **
MSE: 0.24104117322022564
RMSE: 0.49095944152264315
LogLoss: 0.6747021871111782
Null degrees of freedom: 9906
Residual degrees of freedom: 9897
Null deviance: 13734.012930120512
Residual deviance: 13368.549135420886
AIC: 13388.549135420886      p1
--------
0.611681
0.498122
0.468903
0.480892
0.78083
0.420298
0.541082
0.479242
...","['python', 'pycharm', 'h2o']",Unknown,,N/A
65512435,65512435,2020-12-30T19:18:32,2021-01-05 21:39:28Z,0,"I am struggling to find the correct API for releasing memory for an object created by the H2O grid. This code was pre-written by someone else and I am currently maintaining it.


#train grid search
gbm_grid1 <- h2o.grid(algorithm = ""gbm""                                  #specifies gbm algorithm is used
                      ,grid_id = paste(""gbm_grid1"",current_date,sep=""_"")   #defines a grid identification
                      ,x = predictors                                    #defines column variables to use as predictors
                      ,y = y                                             #specifies the response variable
                      ,training_frame = train1                           #specifies the training frame
                      
                      #gbm parameters to remain fixed
                      ,nfolds = 5                     #specify number of folds for cross-validation is 5 (this acceptable here in order to reduce training time)
                      ,distribution = ""bernoulli""     #specify that we are predicting a binary dependent variable
                      ,ntrees = 1000                  #specify the number of trees to build (1000 as essentially the maximum number of trees that can be built. Early stopping parameters defined later will make it unlikely our model will reach 1000 trees)
                      ,learn_rate = 0.1               #specify the learn rate used of for gradient descent optimization (goal is to use as small a learn rate as possible)
                      ,learn_rate_annealing = 0.995   #specifies that the learn rate will perpetually decrease by a factor of 0.995 (this can help speed up traing for our grid search)
                      ,max_depth = tuned_max_depth
                      ,min_rows = tuned_min_rows
                      ,sample_rate = 0.8              #specify the amount of row observations used when making a split decision
                      ,col_sample_rate = 0.8          #specify the amount of column observations used when making a split decision
                      ,stopping_metric = ""logloss""    #specify loss function
                      ,stopping_tolerance = 0.001     #specify minimum change required in stopping metric for individual model to continue training
                      ,stopping_rounds = 5            #specify maximum amount of training rounds stopping metric has to change in excess of stopping tolerance
                      
                      #specifies hyperparameters to fluctuate during model building in the grid search
                      ,hyper_params = gbm_hp2
                      
                      #specifies the search criteria that includes stop training etrics to speed up model building
                      ,search_criteria = search_criteria2
                      
                      #sets a reproducible seed
                      ,seed = 123456                         
)

h2o.rm(gbm_grid1)



The problem is I believe this code was written awhile ago and has been deprecated since. 
h2o.rm(gbm_grid1)
 fails and R Studio tells me that I require a hex identifier. So I assigned my object an identifier and tried 
h2o.rm(gbm_grid1, ""identifier.hex"")
 and it tells me I cannot release this type of object.


The issue is I run out of memory if I move onto the next steps of the script. What should I do?


This is what I get with H2O.ls()","['r', 'h2o']",Unknown,,N/A
65419605,65419605,2020-12-23T05:49:01,2021-01-04 14:20:21Z,767,"I have installed official h2o package in Pycharm IDE as below image and after installing when I am initializing h2o using h2o.init(), h2o session starting and closing immediately.Please suggest why it is closing immediately.




import h2o
h2o.init(ip=""localhost"", port=54323)


:\Users\sarvendra.singh\PycharmProjects\H2o\venv\Scripts\python.exe C:/Users/sarvendra.singh/PycharmProjects/H2o/main.py
Checking whether there is an H2O instance running at http://localhost:54323 ..... not found.
Attempting to start a local H2O server...
; Java HotSpot(TM) 64-Bit Server VM (build 25.271-b09, mixed mode)
  Starting server from C:\Users\sarvendra.singh\PycharmProjects\H2o\venv\lib\site-packages\h2o\backend\bin\h2o.jar
  Ice root: c:\users\sarven~1.sin\appdata\local\temp\tmpmhrvqf
  JVM stdout: c:\users\sarven~1.sin\appdata\local\temp\tmpmhrvqf\h2o_sarvendra_singh_started_from_python.out
  JVM stderr: c:\users\sarven~1.sin\appdata\local\temp\tmpmhrvqf\h2o_sarvendra_singh_started_from_python.err
  Server is running at http://127.0.0.1:54323
Connecting to H2O server at http://127.0.0.1:54323 ... successful.
Warning: Your H2O cluster version is too old (8 months and 19 days)! Please download and install the latest version from http://h2o.ai/download/
--------------------------  ---------------------------------------------------------
H2O_cluster_uptime:         03 secs
H2O_cluster_timezone:       Asia/Kolkata
H2O_data_parsing_timezone:  UTC
H2O_cluster_version:        3.30.0.1
H2O_cluster_version_age:    8 months and 19 days !!!
H2O_cluster_name:           H2O_from_python_sarvendra_singh_y4j13p
H2O_cluster_total_nodes:    1
H2O_cluster_free_memory:    3.535 Gb
H2O_cluster_total_cores:    4
H2O_cluster_allowed_cores:  4
H2O_cluster_status:         accepting new members, healthy
H2O_connection_url:         http://127.0.0.1:54323
H2O_connection_proxy:       {""http"": null, ""https"": null}
H2O_internal_security:      False
H2O_API_Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4
Python_version:             2.7.18 final
--------------------------  ---------------------------------------------------------
Closing connection _sid_973f at exit
H2O session _sid_973f closed.

Process finished with exit code 0","['python', 'pycharm', 'h2o', 'h2o4gpu']",Sarvendra Singh,https://stackoverflow.com/users/10871102/sarvendra-singh,139
65408098,65408098,2020-12-22T11:42:58,2020-12-22 12:53:44Z,0,"I am running the following snippet of code


h2o_xgb <- h2o.xgboost(x = features, 
                       y = response, 
                       training_frame = train,
                       nfolds = 3)
h2o_xgb



I am getting the following error


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  

ERROR MESSAGE:

Algorithm 'xgboost' is not registered. Available algos: [targetencoder,deeplearning,glm,glrm,kmeans,naivebayes,pca,svd,drf,gbm,isolationforest,aggregator,word2vec,stackedensemble,coxph,generic,gam,psvm,rulefit]","['r', 'xgboost', 'h2o']",Ahmad H. Ibrahim,https://stackoverflow.com/users/14049184/ahmad-h-ibrahim,"1,079"
65384846,65384846,2020-12-20T21:13:57,2020-12-21 10:37:39Z,353,"I have successfully installed both h2o on my AWS Databricks cluster, and then successfully started the h2o server with:


h2o.init()



When I attempt to import the iris CSV file that is stored in my Databricks DBFS:


train, valid = h2o.import_file(path=""/FileStore/tables/iris.csv"").split_frame(ratios=[0.7])



I get an H2OResponseError:  Server error water.exceptions.H2ONotFoundArgumentException


The CSV file is absolutely there; in the same Databricks notebook, I am able to read it directly into a DataFrame and view the contents using the exact same fully qualified path:


df_iris = ks.read_csv(""/FileStore/tables/iris.csv"")
df_iris.head()



I've also tried calling:


h2o.upload_file(""/FileStore/tables/iris.csv"")



but to no avail; I get H2OValueError: File /FileStore/tables/iris.csv does not exist.  I've also tried uploading the file directly from my local computer (C drive), but that doesn't succeed either.


I've tried not using the fully qualified path, and just specifying the file name, but I get the same errors.  I've read through the H2O documentation and searched the web, but cannot find anyone who has ever encountered this problem before.


Can someone please help me?


Thanks.","['python-3.x', 'databricks', 'importerror', 'h2o', 'aws-databricks']",Unknown,,N/A
65354927,65354927,2020-12-18T09:57:34,2020-12-31 17:25:51Z,0,"Following the example for h2o rulefit model from the documentation
(
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/rulefit.html
),
that works fine, here is the example:


library(h2o)
h2o.init()

# Import the titanic dataset:
f <- ""https://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv""
coltypes <- list(by.col.name = c(""pclass"", ""survived""), types=c(""Enum"", ""Enum""))
df <- h2o.importFile(f, col.types = coltypes)

# Split the dataset into train and test
splits <- h2o.splitFrame(data = df, ratios = 0.8, seed = 1)
train <- splits[[1]]
test <- splits[[2]]

# Set the predictors and response; set the factors:
response <- ""survived""
predictors <- c(""age"", ""sibsp"", ""parch"", ""fare"", ""sex"", ""pclass"")

# Build and train the model:
rfit <- h2o.rulefit(y = response,
                    x = predictors,
                    training_frame = train,
                    max_rule_length = 10,
                    max_num_rules = 100,
                    seed = 1)

# Retrieve the rule importance:
print(rfit@model$rule_importance)

# Predict on the test data:
h2o.predict(rfit, newdata = test)



BUT when i save the model, close the h2o connection and start new one, load the model and try to predict (i use the latest h2o on linux):


h2o.saveModel(rfit, path = '/my/path/')

h2o.shutdown()

h2o.init()


H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    /tmp/RtmpWvc6F9/file238276aca9de/h2o_Martin_Hulin_started_from_r.out
    /tmp/RtmpWvc6F9/file2382281d69c3/h2o_Martin_Hulin_started_from_r.err

java version ""1.8.0_121""
Java(TM) SE Runtime Environment (build 1.8.0_121-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)

Starting H2O JVM and connecting: ..... Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         4 seconds 333 milliseconds 
    H2O cluster timezone:       America/New_York 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.32.0.1 
    H2O cluster version age:    2 months and 9 days  
    H2O cluster name:           H2O_started_from_R_Martin_Hulin_lag006 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   26.64 GB 
    H2O cluster total cores:    36 
    H2O cluster allowed cores:  36 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4 
    R Version:                  R version 3.5.1 (2018-07-02)



loaded_model <- h2o.loadModel(path = '/ma/path/rfit')

h2o.predict(loaded_model, newdata = test)



it throws the NullPointerException error.


java.lang.NullPointerException
    at hex.rulefit.RuleFitModel.updateModelMetrics(RuleFitModel.java:144)
    at hex.rulefit.RuleFitModel.score(RuleFitModel.java:124)
    at water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:396)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1577)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

Error: java.lang.NullPointerException



I tried with different data and other ways of saving, with the same issue.


Any idea how the rulefit model should be saved/loaded, or is it a bug?


Thanks!","['r', 'nullpointerexception', 'h2o']",Unknown,,N/A
65239478,65239478,2020-12-10T17:34:38,2020-12-11 00:17:44Z,0,"After trained a random forest classifier using 
h2o H2ORandomForestEstimator 
, how could I extract the model performance report for different metrics?


ModelMetricsBinomial: drf
** Reported on train data. **

MSE: 0.3585693234630144
RMSE: 0.5988065826817658
LogLoss: 1.042047808195111
Mean Per-Class Error: 0.3219540722169749
AUC: 0.7464490458314232
AUCPR: 0.751360355831092
Gini: 0.4928980916628465

Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.06774124943480285: 
False   True    Error   Rate

...



For example I trained many different models using this process and want to compare the final results so it's better to save model_performance for later use.
I expect to save as:


 {model1 : model_performance_1,
  model2 : model_performance_2, ........}



Moreover How could I extract the model performance values? For example how to
extract the values of confusion_matrix, like to extract the values of confusion matrix itself?

perf.confusion_matrix()
 return h2o object, but how to extract it?","['python', 'h2o']",Unknown,,N/A
65145485,65145485,2020-12-04T14:35:22,2020-12-04 14:35:22Z,83,"Is there a way to use a custom aggregate function when using group_by on h2o frame in Python? Something like:


frame.group_by('column').aggfun()","['python', 'h2o']",danas.zuokas,https://stackoverflow.com/users/1249481/danas-zuokas,"4,633"
65122818,65122818,2020-12-03T09:06:53,2020-12-03 09:12:06Z,330,"I am trying to load the saved model using this path from dbfs file store from my databricks notebook


h2o.import_mojo(""dbfs:/dbfs/FileStore/df/Churn_Models/h2o_leader_model_train_Cape Town_2020-11-18.model/mojo_model/"")



it keeps throwing exception whenever I have a space in the path name like in 
Cape Town
 but it loads when the city name does not have any space in between like 
Bahrain


OSError: Job with key $03010a8b400432d4ffffffff$_a0ec664b5212ee40152607d08d30445d failed with an exception: java.lang.IllegalArgumentException: Illegal character in path at index 64: dbfs:/dbfs/FileStore/df/Churn_Models/h2o_leader_model_train_Cape Town_2020-11-18.model/mojo_model/



I have tried encoding the path but didn't work. I have different models for each city which I need to load and run. some have space in it as well. So I want one of the following solutions: 




Rename all saved folders by removing spaces


Deal with the spaces while loading the model.





Note:

I am using H2o models in pyspark notebook on databricks","['python', 'pyspark', 'databricks', 'h2o', 'automl']",Unknown,,N/A
65010226,65010226,2020-11-25T17:54:02,2020-11-28 14:46:51Z,0,"I would like to know the in-bag training metrics of my random forest fit in h2o R version:


rf_cv = h2o.randomForest(x = x, y = y,
                         training_frame = cali,
                         ntrees = 800,
                         nfolds = 5,
                         mtries = 3,
                         seed = 98)



When I print the scoring history I get the following training metrics:


> rf_cv@model$scoring_history
number_of_trees training_rmse training_mae training_deviance
1               0.70767       0.45476      0.50080
...
800             0.47283       0.30862      0.22357



But those metrics are from the out-of bag samples, as shown in the performance summary:


> h2o.performance(rf_cv)
H2ORegressionMetrics: drf
** Reported on training data. **
** Metrics reported on Out-Of-Bag training samples **

MSE:  0.2235729
RMSE:  0.4728349
MAE:  0.3086151
RMSLE:  0.1403068
Mean Residual Deviance :  0.2235729



I know I could just get the overall in-bag training performance with 
h2o.performance(rf_cv, data = train)
 but I need the scoring history. I've gone through the documentation and looked for similar questions but I've found nothing so far. Any help would be appreciated.","['r', 'random-forest', 'h2o']",Unknown,,N/A
64985991,64985991,2020-11-24T11:56:23,2021-01-27 21:15:16Z,0,"I am currently working on multiple datasets using differend machine learning methods with R and the 
h2o
 library. Since I have several 10-fold cross validations for each dataset, I ran a random GridSearch for each and saved the grids using 
h2o.saveGrid
. When I loaded those grids again to build ensembles using  
h2o.stackedEnsemble
 it returns the error message


Error: water.exceptions.H2OIllegalArgumentException: Failed to find the xval predictions frame id. Looks like keep_cross_validation_predictions wasn't set when building the models.



However, 
keep_cross_validation_predictions
 is set to true and it runs perfectly fine if I use the grid without saving and loading it. So I guess that something along the line of loading and saving gets lost.


Does anyone have an idea if there is a way to use loaded grids for stacked ensembles in 
h2o
 or is it simply not supported yet? I appreciate any insight since this would save me a lot of time. I cannot keep them all in my h2o cluster all the time


I am using R 3.6.3 and h2o 3.32.0.1


A minimal working example does reproduces the error for me:


library(h2o)
h2o.init()

train_data <- data.frame(y = rnorm(100,1,2),
                         x1 = rnorm(100,5,5),
                         x2 = rnorm(100,4,4),
                         x3 = rnorm(100,3,3),
                         x4 = rnorm(100,2,2))

params <- list(max_depth = seq(1, 6, 1),
               sample_rate = seq(0.2, 1.0, 0.1))
search_criteria <- list(strategy = ""RandomDiscrete"", max_models = 10, seed = 2102)

train_h2o <- as.h2o(train_data,destination_frame = ""Train"")

gbm_grid <- h2o.grid(""gbm"",y = ""y"", x = c(""x1"",""x2"",""x3"",""x4""), training_frame = train_h2o,
                     grid_id = ""gbm_1"",  nfolds = 10, ntrees = 50, seed= 1111,
                     keep_cross_validation_predictions = TRUE,
                     hyper_params = params, 
                     search_criteria = search_criteria)
h2o.performance(test_ens)

test_ens <- h2o.stackedEnsemble(y = ""y"", x = c(""x1"",""x2"",""x3"",""x4""), training_frame = train_h2o,
                                metalearner_algorithm = ""glm"", model_id = ""Ens1"",
                                base_models = gbm_grid@model_ids[1:10])

h2o.saveGrid(grid_directory = paste0(getwd(),""/Data""),grid_id = ""gbm_1"")



When loading the grid, training the ensemble produces the error


h2o.removeAll()

train_h2o <- as.h2o(train_data,destination_frame = ""Train"")
gbm_grid <- h2o.loadGrid(paste0(getwd(),""/Data/gbm_1""))

test_ens <- h2o.stackedEnsemble(y = ""y"", x = c(""x1"",""x2"",""x3"",""x4""), training_frame = train_h2o,
                                metalearner_algorithm = ""glm"", model_id = ""Ens2"",
                                base_models = gbm_grid@model_ids[1:10])



I have also tried setting 
export_checkpoints_dir
 in 
h2o.grid
 and manually loading all the models (including their auto-generated cv folds which are, contrary to 
h2o.saveGrid
, also saved this way) but it does not change anything.


Cheers","['r', 'h2o', 'grid-search']",R.Pickman,https://stackoverflow.com/users/14698636/r-pickman,13
64902398,64902398,2020-11-18T22:27:04,2020-11-19 16:35:15Z,92,"For H2O version 3.30.1.1. I create stacked ensembles of two models, one Deep Learning and one XGBoost and export the MOJO. I have two APIs working with other MOJO files, but for these stacked ensembles they fail. The MOJO returns an empty prediction. The models work independently, and it appears that the H2O binary works as well. I'm simply creating the model as:


   ensemble = H2OStackedEnsembleEstimator(base_models=[DeepLearningModel, XGBoostModel])
   ensemble.metalearner_fold_column = 'fold_numbers'
   ensemble.train(x=parameters, y=response, training_frame=model_trainer.h2odata)



These fail independent of the dataset I'm training on. Also, StackedEnsemble_BestOfFamily model MOJOs fail in the same manner if DeepLearning is included as an algorithm.


Why are do these MOJOs fail to return predictions, and what can I do to stop it? Could Deep Learning be the problem somehow?",['h2o'],Unknown,,N/A
64879426,64879426,2020-11-17T16:37:05,2020-12-01 17:43:20Z,59,"H20 says in the 
documentation
 that splitting on a feature for regression gbms is based on the reduction in squared error.


Is this squared error based on the node residuals, i.e., (resid - mean resid)^2 or is it the true response, i.e., (response - mean response)? I'm using gamma/ Poisson distributions.


In the case of gamma/Poisson, the loss is the deviance so why is the squared error used instead?","['h2o', 'gbm', 'h2o.ai']",Shayan Shafiq,https://stackoverflow.com/users/12340179/shayan-shafiq,"1,469"
64857221,64857221,2020-11-16T11:33:25,2020-11-16 11:33:25Z,182,"I am trying to perform a test, train and validation split. But I am getting the following error.


Code:


hf = h2o.H2OFrame(df_train_transformed)
hf.types



Output:


{'Country': 'enum',
 'CustomerID': 'enum',
 'DayOfWeek': 'enum',
 'Description': 'enum',
 'Quantity': 'real',
 'StockCode': 'enum',
 'UnitPrice': 'real',
 'WeekOfYear': 'enum'}



Test, Train Validation Split:


train, valid, test = hf.split_frame(ratios=[0.7, 0.15], seed=42)



Error:




H2OResponseError: Server error java.lang.IllegalArgumentException:
Error: Requires same count of rows in the number-list (284780) as in the source (122049)
Request: POST /99/Rapids
data: {'ast': ""(tmp= py_51_sid_9de8 (rows (tmp= py_49_sid_9de8 (:= (tmp= py_48_sid_9de8 (:= (tmp= py_47_sid_9de8 (:= (tmp= py_46_sid_9de8 (:= (tmp= py_45_sid_9de8 (:= (tmp= py_44_sid_9de8 (:= Key_Frame__upload_b047d82e4093133936d7ae7cf1534bc5.hex (as.factor (cols_py Key_Frame__upload_891dc3c514d9a9b9bcbcb14cef0c43b6.hex 'CustomerID')) 4 [])) (as.factor (cols_py py_44_sid_9de8 'Country')) 5 [])) (as.factor (cols_py py_45_sid_9de8 'Description')) 1 [])) (as.factor (cols_py py_46_sid_9de8 'DayOfWeek')) 7 [])) (as.factor (cols_py py_47_sid_9de8 'WeekOfYear')) 6 [])) (as.factor (cols_py py_48_sid_9de8 'StockCode')) 0 [])) (<= (tmp= py_50_sid_9de8 (h2o.runif py_49_sid_9de8 42)) 0.7)))"", 'session_id': '_sid_9de8'}","['python', 'machine-learning', 'h2o']",Karthik K V,https://stackoverflow.com/users/13838510/karthik-k-v,33
64801593,64801593,2020-11-12T09:50:55,2020-11-12 18:35:30Z,0,"I am aware that a similar question has been posted in the context of Python: 
How to install specific versions of H2O


However, I need to find the relevant link for the R releases. I am currently trying to use an h2o model which I doesn't work because of conflicting releases. How can I get a previous version of h2o, namely 3.30.0.1?","['r', 'h2o']",xxx33xxx,https://stackoverflow.com/users/11820022/xxx33xxx,75
64790872,64790872,2020-11-11T17:02:30,2020-11-11 18:07:17Z,84,"I run my model with H2o library.
I run with 5 folds cross-validation.


model = H2OGradientBoostingEstimator(
                        balance_classes=True, 
                        nfolds=5,
                        keep_cross_validation_fold_assignment=True,
                        seed=1234)
model.train(x=predictors,y=response,training_frame=data)
print('rmse: ',model.rmse(xval=True))
print('R2: ',model.r2(xval=True))
data_nfolds = model.cross_validation_fold_assignment()



I got the cross-validation fold assignment.
I try to reuse it for a new model with other parameters such as ntrees or stopping_rounds, but I did not find it in the documents.




https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/keep_cross_validation_fold_assignment.html","['python', 'h2o', 'gbm']",cnp,https://stackoverflow.com/users/13576600/cnp,339
64745423,64745423,2020-11-09T03:14:20,2021-02-06 04:08:26Z,0,"I try to use the 
h2o.grid()
 function from the h2o package to do some tuning using R, when I set the parameter 
parallelism
 larger then 1, it always shows the warning




Some models were not built due to a failure, for more details run `summary(grid_object, show_stack_traces = TRUE)




And the model_ids in the final grid object includes many models end with 
_cv_1
, 
_cv_2
 etc, and the number of the models is not equal to the setting of my 
max_models
 in 
search_criteria
 list, I think they are just the models in the 
cv
 process, not the final model.


When I set 
parallelism
 larger than 1:



When I leave the 
parallelism
 default or set it to 1, the result is normal, all models end with 
_model_1
, 
_model_2
 etc.


When I leave the ""parallelism"" default or set it to 1:



Here is my code:


# set the grid
rf_h2o_grid <- list(mtries = seq(3, ncol(train_h2o), 4),
                    max_depth = c(5, 10, 15, 20))

# set the search_criteria
sc <- list(strategy = ""RandomDiscrete"", 
           seed = 100,
           max_models = 5
           )

# random grid tuning
rf_h2o_grid_tune_random <- h2o.grid(
  algorithm = ""randomForest"", 
  x = x, 
  y = y,
  training_frame = train_h2o,
  nfolds = 5,                     # use cv to validate the parameters
  fold_assignment = ""Stratified"",   
  ntrees = 100,
  seed = 100,
  hyper_params = rf_h2o_grid,
  search_criteria = sc
  # parallelism = 6           # when I set it larger than 1, the result always includes some ""cv_"" models
  )



So how can I use the 
parallelism
 correctly in 
h2o.grid()
? Thanks for helping!","['r', 'parallel-processing', 'h2o', 'grid-search', 'h2o.ai']",Werner Hertzog,https://stackoverflow.com/users/1409165/werner-hertzog,"2,022"
64694979,64694979,2020-11-05T10:06:29,2020-11-05 19:56:14Z,150,"I am using GBM model, and I wanna compare to other machine learning methods. I run with 5 folds. As I knew, they will separate the data into 5 folds, and chose one of them for the testing and the others for training. How to get 5 folds data from gbm of H2o lib?


I run it with Python language.


folds = 5
cars_gbm = H2OGradientBoostingEstimator(nfolds = folds, seed = 1234)","['python', 'h2o', 'gbm']",Subbu VidyaSekar,https://stackoverflow.com/users/5602871/subbu-vidyasekar,"2,615"
64653397,64653397,2020-11-02T21:12:16,2020-11-03 05:48:36Z,355,"I am trying to visualize my H2O XGBoost model in JSON format using below command:

java -cp h2o-genmodel.jar hex.genmodel.tools.PrintMojo -i XGBoost_model_R_1597776279050_3.zip --tree 1 --format json


The above command output the tree structure in JSON format like below:


""rightChild"": {
      ""nodeNumber"": 2,
      ""weight"": 0.0,
      ""colId"": 382,
      ""colName"": ""var_2"",
      ""leftward"": false,
      ""isCategorical"": false,
      ""inclusiveNa"": false,
      ""splitValue"": 0.195,
      ""rightChild"": {
        ""nodeNumber"": 6,
        ""weight"": 0.0,
        ""colId"": 340,
        ""colName"": ""var_6"",
        ""leftward"": false,
        ""isCategorical"": false,
        ""inclusiveNa"": true,
        ""splitValue"": 1.0,
        ""rightChild"": {
          ""nodeNumber"": 10,
          ""weight"": 0.0,
          ""predValue"": 0.011794609
        },
        ""leftChild"": {
          ""nodeNumber"": 9,
          ""weight"": 0.0,
          ""predValue"": 0.011531689
        }



I am trying to understand how the missing child can be calculated using above JSON for each node. The same structure can be view in png format and the missing node for the node var_6 is coming as left child. Is there a way to figure out the missing node by looking at the JSON?","['xgboost', 'h2o']",user2335004,https://stackoverflow.com/users/2335004/user2335004,121
64587004,64587004,2020-10-29T07:55:34,2020-10-29 18:54:10Z,0,"I'm using H2ORandomForestEstimator for multiclass classification.


After building and training as follows:


train, valid = hdf.split_frame(ratios=[.8], seed=1234)
# Build and train the model:
drf = H2ORandomForestEstimator(model_id=""drf"", seed=1234)
drf.train(x=predictors,
               y=response,
               training_frame=train,
               validation_frame=valid)

drf.model_performance(valid)



I can see RMSE, MSE and Mean Error per class in the output


ModelMetricsMultinomial: drf
** Reported on test data. **

MSE: 0.12204577776460168
RMSE: 0.34935050846478194
LogLoss: 0.4781165975023516
Mean Per-Class Error: 0.23864386780117242



How do I obtain other metrics such as accuracy, precision, recall and F-Score?","['python', 'random-forest', 'h2o']",Prithvi,https://stackoverflow.com/users/1740036/prithvi,39
64490264,64490264,2020-10-22T20:43:27,2020-10-24 01:23:55Z,144,"I am using H2O autoencoder in R for anomaly detection. I don’t have a training dataset, so I am using the data.hex to train the model, and then the same data.hex to calculate the reconstruction errors. The rows in data.hex with the largest reconstruction errors are considered anomalous. Mean squared error (MSE) of the model, which is calculated by the model itself, would be the sum of the squared reconstruction errors and then divided by the number of rows (i.e. examples). Below is some sudo code of the model.


# Deeplearning Model

model.dl <- h2o.deeplearning(x = x, training_frame = data.hex, autoencoder = TRUE, activation = ""Tanh"", hidden = c(25,25,25), variable_importances = TRUE) 

# Anomaly Detection Algorithm 

errors <- h2o.anomaly(model.dl, data.hex, per_feature = FALSE) 



Currently there are about 10 features (factors) in my data.hex, and they are all categorical features. I have two questions below:


(1) Do I need to perform feature selection to select a subset of the 10 features before the data go into the deep learning model (with autoencoder=TRUE), in case some features are significantly associated with each other? Or I don’t need to since the data will go into an autoencoder which compresses the data and selects only the most importance information already, so feature selection would be redundant?


(2) The purpose of using the H2O autoencoder here is to identify the senders in data.hex whose action is anomalous. Here are two examples of data.hex. Example B is a transformed version of Example A, by concatenating all the actions for each sender-receiver pair in Example A.




After running the model on data.hex in Example A and in Example B separately, what I got is


(a)   MSE from Example A (~0.005) is 20+ times larger than MSE from Example B;


(b)  When I put the reconstruction errors in ascending order and plot them (so errors increase from left to right in the plot), the reconstruction error curve from Example A is steeper (e.g. skyrocketing) on the right end, while the reconstruction error curve from Example B increases more gradually.


My question is, which example of data.hex works better for my purpose to identify anomalies?


Thanks for your insights!","['h2o', 'feature-selection', 'autoencoder', 'anomaly-detection', 'mse']",Fanwei Zeng,https://stackoverflow.com/users/13756465/fanwei-zeng,55
64352366,64352366,2020-10-14T11:34:53,2020-10-22 11:12:04Z,0,"I'm trying to move from running some services in docker-compose to kubernetes, and struggling with the move from nginx reverse proxy to ingress nginx. The service uses 
h2o ai
 which has a web interface. I'm failing to bring up the web interface in the k8s version.


I think that I need to amend the spec in the ingress file to route appropriately, grateful for any pointers on how to do this.


In the docker solution with nginx as reverse proxy, when I access 
http://k8s-master:3002
 it redirects to 
http://k8s-master:3002/flow/index.html
 and displays correctly


For k8s,(I have the 
ingress-nginx-controller
 running as 
NodePort
 on 
32000
) , I try to access 
https://k8s-master:32000/h2otest
 and get an error message as follows:


{""__meta"":{""schema_version"":3,""schema_name"":""H2OErrorV3"",""schema_type"":""H2OError""},""timestamp"":1602674406013,""error_url"":""Resource /h2otest"",""msg"":""\n\nERROR MESSAGE:\n\nResource /h2otest not found\n\n"",""dev_msg"":""\n\nERROR MESSAGE:\n\nResource /h2otest not found\n\n"",""http_status"":404,""values"":{},""exception_type"":""water.exceptions.H2ONotFoundArgumentException"",""exception_msg"":""\n\nERROR MESSAGE:\n\nResource /h2otest not found\n\n"",""stacktrace"":[""water.exceptions.H2ONotFoundArgumentException: Resource /h2otest not found"",""    water.api.RequestServer.response404(RequestServer.java:743)"",""    water.api.RequestServer.getResource(RequestServer.java:846)"",""    water.api.RequestServer.serve(RequestServer.java:465)"",""    water.api.RequestServer.doGeneric(RequestServer.java:301)"",""    water.api.RequestServer.doGet(RequestServer.java:225)"",""    javax.servlet.http.HttpServlet.service(HttpServlet.java:687)"",""    javax.servlet.http.HttpServlet.service(HttpServlet.java:790)"",""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)"",""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)"",""    org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)"",""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)"",""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)"",""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)"",""    org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)"",""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)"",""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)"",""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)"",""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)"",""    water.webserver.jetty9.Jetty9ServerAdapter$LoginHandler.handle(Jetty9ServerAdapter.java:130)"",""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)"",""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)"",""    org.eclipse.jetty.server.Server.handle(Server.java:531)"",""    org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)"",""    org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)"",""    org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)"",""    org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)"",""    org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)"",""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)"",""    org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)"",""    java.lang.Thread.run(Thread.java:748)""]}





UPDATE: based on Ken Chen's comment


I've changed the ingress below to use 
nginx.ingress.kubernetes.io/rewrite-target: /$2


Now when I type 
https://k8s-master:32000/h2otest
 it redirects to: 
https://k8s-master:32000/flow/index.html


Typing 
https://k8s-master:32000/h2otest/flow/index.html
 directly works. 
How do I amend the rule so that
 
https://k8s-master:32000/h2otest
 
goes directly to
 
https://k8s-master:32000/h2otest/flow/index.html
 ?




The following contains the files for each version:

docker version

docker-compose.yml


services :
  test-h2o-svc:
    build:
      context: h2o
      dockerfile: Dockerfile
    image: test-h2o:3.30.1.1
    restart: always
    networks:
      - testnet
#    ports:
#      - 54323:54321

  test-reverse-proxy:
    image: nginx:alpine
    hostname: reverse-proxy
    restart: always
    ports:
      - 3002:3002
    volumes:
      - ""./nginx_files/nginx.conf:/etc/nginx/nginx.conf:rw""
      - ""./nginx_files/sites-available/test.conf:/etc/nginx/conf.d/sites-available/test.conf:rw""
      - ""./nginx_files/sites-available/test.conf:/etc/nginx/conf.d/sites-enabled/test.conf:rw""
    networks:
      - testnet

networks:
 testnet:
version: ""3.5""




nginx.conf


user  nginx;
worker_processes  4;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events {
    worker_connections  1024;
}



http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] ""$request"" '
                      '$status $body_bytes_sent ""$http_referer"" '
                      '""$http_user_agent"" ""$http_x_forwarded_for""';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;
    keepalive_timeout  180;
    #gzip  on;

    include /etc/nginx/conf.d/sites-enabled/*.conf;
    client_body_buffer_size     50M;
    client_max_body_size 2000M;
}





test.conf


upstream link_h2o-svc {
  server        test-h2o-svc:54321;
}

server {
  listen        3002;
  location / {
    proxy_pass  http://link_h2o-svc;
   }
}




Dockerfile
 for container: 
test-h2o:3.30.1.1


FROM ubuntu:latest

ARG H2O_VERSION=3.30.1.1

RUN apt-get update \
    && apt-get install default-jdk unzip wget -y

RUN wget https://h2o-release.s3.amazonaws.com/h2o/rel-zeno/1/h2o-${H2O_VERSION}.zip \
    && unzip h2o-${H2O_VERSION}.zip

ENV H2O_VERSION ${H2O_VERSION}
CMD java -jar h2o-${H2O_VERSION}/h2o.jar




k8s version

kind: Namespace
apiVersion: v1
metadata:
  name: ns-test
  labels:
    name: ns-test
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kompose.cmd: /snap/kompose/19/kompose-linux-amd64 convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    app: test-h2o
  name: test-h2o
  namespace: ns-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-h2o
  
  strategy: {}
  template:
    metadata:
      annotations:
        kompose.cmd: /snap/kompose/19/kompose-linux-amd64 convert
        kompose.version: 1.21.0 (992df58d8)
      creationTimestamp: null
      labels:
        app: test-h2o
    spec:
      containers:
      - image: test-h2o:3.30.1.1
        imagePullPolicy: """"
        name: test-h2o
        resources: {}
      restartPolicy: Always
      serviceAccountName: """"
      volumes: null
status: {}
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kompose.cmd: /snap/kompose/19/kompose-linux-amd64 convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null

  name: test-h2o-svc
  namespace: ns-test
spec:
  ports:
  - name: ""54321""
    port: 54321
    targetPort: 54321
  selector:
    app: test-h2o
status:
  loadBalancer: {}
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: ns-test
  annotations:
    #ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  rules:
  - http:
      paths:
        - path: /h2otest(/|$)(.*)
          pathType: Prefix
          backend:
            service:
              name: test-h2o-svc
              port: 
                number: 54321","['kubernetes', 'h2o', 'nginx-ingress']",Unknown,,N/A
64286080,64286080,2020-10-09T19:17:30,2020-10-12 07:36:42Z,400,"What preprocessing methods are used in the autoML pipeline? It would be nice to have a brief summary of all these steps in the doc. By preprocessing, I mean: one-hot-encoding, normalization, imputation, etc.


Thank you,


Yassine","['h2o', 'automl']",Yass Light,https://stackoverflow.com/users/14022410/yass-light,23
64280444,64280444,2020-10-09T12:49:31,2020-10-09 14:17:59Z,0,"I've been using 
h2o
 for about 3 years and first time these kind of error happens to me. I can't share a reproducible example because I'm using sensitive data but the dataset contains about 20K observations, R version 4.0.2, macOS Catalina 10.15.6, latest stable version h20 3.30.1.3. Could you please help me understand the error? This is the result after a couple of minutes training models (53% done according to the progress bar):


java.lang.NullPointerException

java.lang.NullPointerException
    at hex.ModelMetrics.getMetricFromModelMetric(ModelMetrics.java:151)
    at ai.h2o.automl.leaderboard.Leaderboard.getMetrics(Leaderboard.java:558)
    at ai.h2o.automl.leaderboard.Leaderboard.updateModels(Leaderboard.java:422)
    at ai.h2o.automl.leaderboard.Leaderboard.lambda$addModels$0(Leaderboard.java:381)
    at ai.h2o.automl.leaderboard.Leaderboard.atomicUpdate(Leaderboard.java:442)
    at ai.h2o.automl.leaderboard.Leaderboard.addModels(Leaderboard.java:378)
    at ai.h2o.automl.leaderboard.Leaderboard.addModel(Leaderboard.java:459)
    at ai.h2o.automl.ModelingStepsExecutor.addModel(ModelingStepsExecutor.java:186)
    at ai.h2o.automl.ModelingStepsExecutor.monitor(ModelingStepsExecutor.java:163)
    at ai.h2o.automl.ModelingStepsExecutor.submit(ModelingStepsExecutor.java:82)
    at ai.h2o.automl.AutoML.learn(AutoML.java:604)
    at ai.h2o.automl.AutoML.run(AutoML.java:407)
    at ai.h2o.automl.H2OJob$1.compute2(H2OJob.java:33)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1563)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



I have a hunch that if I use the same data input for 
training_frame
 and 
leaderboard_frame
 this error happens. If there's anything else I can share to help understand this issue, let me know! Thanks.","['java', 'r', 'h2o', 'automl']",Unknown,,N/A
64247245,64247245,2020-10-07T15:23:48,2020-10-12 06:44:39Z,0,"I am building xgboost models using h2o.xgboost() in R.


Could feature interaction constraints (as described in 
https://xgboost.readthedocs.io/en/latest/tutorials/feature_interaction_constraint.html
) be used? I did not find anything related in the official documentation of h2o.xgboost.


Thank you!","['r', 'xgboost', 'h2o', 'interaction']",soniCYouth,https://stackoverflow.com/users/9469401/sonicyouth,33
64237745,64237745,2020-10-07T05:29:41,2020-12-27 17:37:59Z,37,"In H2O Driverless AI, there is a feature in which after we upload a dataset, we can see visualization of the dataset in various plots. So is there a way I can share this graph to a different person in H2O Driverless AI, so that he/she can see the graph dynamically.


Please note that : I dont want to download the graphs, because if I do that, the graph will become static.


What I want is to share the graph to a different person so that he can view the live graph without giving my account credentials.


Is there any way to do this in H2O Driverless AI ?","['graph', 'visualization', 'share', 'h2o', 'driverless-ai']",Firenze,https://stackoverflow.com/users/13845475/firenze,367
64220636,64220636,2020-10-06T06:46:31,2020-10-12 06:33:41Z,117,"Suppose we have three datasets containing data from a company.




employee.csv
 : This dataset contains the details of the employees working in the company, like employee ID, employee name, dept id of the dept he works in, country code of the country where he is from and his annual salary.


dept.csv
 : This dataset has information about the department of the company, like the dept id, dept name, dept specialization.


country.csv
 : This dataset contains some country names with its country code and the capital city of the country.




Is there a feature in H2O Driverless AI where we can upload these datasets (without merging using python) and merge it in H2O Driverless AI platform and use it for training using overlapping columns ?","['csv', 'h2o', 'training-data', 'merging-data', 'driverless-ai']",Firenze,https://stackoverflow.com/users/13845475/firenze,367
64165952,64165952,2020-10-02T03:48:17,2020-10-02 03:57:41Z,145,"Is there a way to slice an H20 Frame based on boolean restrictions? I made a function that collects an array of Trues and False'. My goal is to slice an H2O frame based on this boolean array.


For replication purposes:


#Creation of Dataset    
X,y = make_classification(n_samples=5000, n_features=15,n_informative=15, n_redundant=0, n_repeated=0, n_classes=4
                              ,n_clusters_per_class=2,class_sep=3,flip_y=0.1,weights=[0.4,0.20,0.10,0.05], shuffle=True,random_state=1234)
    
    dataset_x = pd.DataFrame({'var1': X[:, 0], 'var2': X[:, 1],'var3': X[:, 2]})
    
    dataset_x['var2'] = dataset_x['var3'].round(0)
    
    dataset_x['var3'] = dataset_x['var3']*(-1)
    
    dataset_x['var4'] =np.where(dataset_x['var1']<=0, 0, 1)
    
    conditions = [(dataset_x['var2'] <= 0) & (dataset_x['var4'] == 0)
                  ,(dataset_x['var2'] <=0) & (dataset_x['var4'] == 1)
                  ,(dataset_x['var2'] >=0) & (dataset_x['var4'] == 0)
                  ,(dataset_x['var2'] >=0) & (dataset_x['var4'] == 1)]
    
    choices = [0, 1, 2, 3]
    
    dataset_x['var5'] = np.select(conditions, choices, default=0)
    
    dataset_x['var6'] = dataset_x['var3'].abs().round(0)
    
    mean_var1 = dataset_x['var3'].mean()
    len_var1 = len(dataset_x['var3'])
    
    dataset_x['var7'] =(mean_var1*(2.718)**((mean_var1)*(dataset_x['var1'].round(0))*-1))
    
    dataset_x['var8'] =dataset_x['var1'].round(0)
    
    dataset_x['var8'] =abs(dataset_x['var1'].round(0))*2
    
    dataset_y = pd.DataFrame({'target': y})
    
    simulated_irregular_dataset = pd.concat([dataset_x,dataset_y], axis=1)



Defining the boolean slicer:


def boolean_slicer(size,num_feat):
    array_slicer = []
    for i in range(size):
        slicer = np.ones(num_feat,dtype=np.bool)
        slicer[:int(0.5*num_feat)]=False
        np.random.shuffle(slicer)
        array_slicer.append(slicer)
    return array_slicer



Applying the boolean restriction to an H2O Frame:


h2o.init(min_mem_size_GB=8)    
#Transform data into a H2O Frame
H20_df = h2o.H2OFrame(X_train)
print(H20_df)

for i in list_of_Boolean:
    print (i)
    print(H20_df[:,i.tolist()])



Error Received:


H2OResponseError: Server error water.rapids.Rapids.IllegalASTException:
  Error: java.lang.NumberFormatException: For input string: ""False""
  Request: POST /99/Rapids
    data: {'ast': '(tmp= py_16_sid_9474 (cols_py Key_Frame__upload_8abf91f6bc0ddcd442f8fa9b6f8b4822.hex [False True True True True True False True]))', 'session_id': '_sid_9474'}","['python-3.x', 'numpy', 'dataframe', 'numpy-ndarray', 'h2o']",Gerard,https://stackoverflow.com/users/5760497/gerard,518
64154312,64154312,2020-10-01T11:19:10,2020-10-05 05:22:36Z,870,"Just started with H2O AutoML so apologies in advance if I have missed something basic.


I have a binary classification problem where data are observations from K years. I want to train on the K-1 years and tune the models and select the best one explicitly based on the remaining K year.


If I switch off cross-validation (with nfolds=0) to avoid randomly blending of years into the N folds and define data of year K as the validation_frame then I don't have the ensemble created (as expected according to the documentation) which in fact I need.


If I train with cross-validation (default nfolds) and defining a validation frame to be the K-year data


aml = H2OAutoML(max_runtime_secs=3600, seed=1)
aml.train(x=x,y=y, training_frame=k-1_years, validation_frame=k_year)



then according to

http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html

the validation_frame is ignored
""...By default and when nfolds > 1, cross-validation metrics will be used for early stopping and thus validation_frame will be ignored.""


Is there a way to get the tuning of the models and the selection of the best one(ensemble or not) based on the K-year data only, and while the ensemble of models is also available in the output?


Thanks a lot!","['h2o', 'automl']",Unknown,,N/A
64139685,64139685,2020-09-30T14:24:43,2020-09-30 14:24:43Z,204,"The file 
h2otest.py
 contains a shortened version of code on 
https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html
:


import h2o
h2o.init()
from h2o.estimators.glm import H2OGeneralizedLinearEstimator
prostate = h2o.import_file(""https://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv"")
prostate['CAPSULE'] = prostate['CAPSULE'].asfactor()
prostate['RACE'] = prostate['RACE'].asfactor()
prostate['DCAPS'] = prostate['DCAPS'].asfactor()
prostate['DPROS'] = prostate['DPROS'].asfactor()
predictors = [""AGE"", ""RACE"", ""VOL"", ""GLEASON""]
response_col = ""CAPSULE""
glm_model = H2OGeneralizedLinearEstimator(family=""binomial"", lambda_= 0, compute_p_values=True)
glm_model.train(predictors, response_col, training_frame= prostate)
print('Finished')



I run the code on the command line in a Conda environment 
pycharm
, where h2o 3.30.1.3 was installed from PyPi. I am using Python 3.8. I get the same error both on my Mac and a Linux server. There is no existing H2O server at start up :


(pycharm) $> python h2otest.py



The code does it's job, but gives error messages at the end:


...
Parse progress: |█████████████████████████████████████████████████████████████████████████████| 100%
glm Model Build progress: |███████████████████████████████████████████████████████████████████| 100%
Finished
Closing connection _sid_a7a8 at exit
H2O session _sid_a7a8 closed.
Exception ignored in: <function ExprNode.__del__ at 0x7f9ee65f83a0>
Traceback (most recent call last):
  File ""/Users/geiringesandnes/miniconda3/envs/pycharm/lib/python3.8/site-packages/h2o/expr.py"", line 204, in __del__
  File ""/Users/geiringesandnes/miniconda3/envs/pycharm/lib/python3.8/site-packages/h2o/expr.py"", line 258, in rapids
TypeError: 'NoneType' object is not callable
Exception ignored in: <function ExprNode.__del__ at 0x7f9ee65f83a0>
Traceback (most recent call last):
  File ""/Users/geiringesandnes/miniconda3/envs/pycharm/lib/python3.8/site-packages/h2o/expr.py"", line 204, in __del__
  File ""/Users/geiringesandnes/miniconda3/envs/pycharm/lib/python3.8/site-packages/h2o/expr.py"", line 258, in rapids
TypeError: 'NoneType' object is not callable
Exception ignored in: <function ExprNode.__del__ at 0x7f9ee65f83a0>
Traceback (most recent call last):
  File ""/Users/geiringesandnes/miniconda3/envs/pycharm/lib/python3.8/site-packages/h2o/expr.py"", line 204, in __del__
  File ""/Users/geiringesandnes/miniconda3/envs/pycharm/lib/python3.8/site-packages/h2o/expr.py"", line 258, in rapids
TypeError: 'NoneType' object is not callable
Exception ignored in: <function ExprNode.__del__ at 0x7f9ee65f83a0>
Traceback (most recent call last):
  File ""/Users/geiringesandnes/miniconda3/envs/pycharm/lib/python3.8/site-packages/h2o/expr.py"", line 204, in __del__
  File ""/Users/geiringesandnes/miniconda3/envs/pycharm/lib/python3.8/site-packages/h2o/expr.py"", line 258, in rapids
TypeError: 'NoneType' object is not callable



How can I get rid of those error messages?","['python-3.x', 'command-line', 'h2o']",Geir Inge,https://stackoverflow.com/users/8771368/geir-inge,189
64047807,64047807,2020-09-24T13:43:17,2020-10-05 00:09:10Z,495,"Suppose I have an H2OFrame called 
df
. What is the quickest way to get the values of column 
x
 from said frame as a 
numpy
 array?


One could do


x_array = df['x'].as_data_frame()['x'].values


But that seems unnecessarily verbose. Especially passing via a 
pandas DataFrame
 with 
as_data_frame
 seems superfluous. I was hoping for something more elegant like, e.g. 
df['x'].to_array()
. But I can't find it.","['python', 'h2o']",Willem,https://stackoverflow.com/users/8236076/willem,"1,080"
64032018,64032018,2020-09-23T16:07:10,2023-01-31 20:19:10Z,0,"I am training a binary classification model with h2o AutoML using the default cross-validation (
nfolds=5
). I need to obtain the AUC score for each holdout fold in order to compute the variability.


This is the code I am using:


h2o.init()

prostate = h2o.import_file(""https://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv"")
# convert columns to factors
prostate['CAPSULE'] = prostate['CAPSULE'].asfactor()
prostate['RACE'] = prostate['RACE'].asfactor()
prostate['DCAPS'] = prostate['DCAPS'].asfactor()
prostate['DPROS'] = prostate['DPROS'].asfactor()

# set the predictor and response columns
predictors = [""AGE"", ""RACE"", ""VOL"", ""GLEASON""]
response_col = ""CAPSULE""

# split into train and testing sets
train, test = prostate.split_frame(ratios = [0.8], seed = 1234)


aml = H2OAutoML(seed=1, max_runtime_secs=100, exclude_algos=[""DeepLearning"", ""GLM""],
                    nfolds=5, keep_cross_validation_predictions=True)

aml.train(predictors, response_col, training_frame=prostate)

leader = aml.leader



I check that 
leader
 is not a StackedEnsamble model (for which the validation metrics are not available). Anyway, I am not able to retrieve the five AUC scores.


Any idea on how to do so?","['python', 'machine-learning', 'cross-validation', 'h2o', 'automl']",Unknown,,N/A
64004836,64004836,2020-09-22T07:20:09,2020-09-27 06:15:37Z,0,"i have parquet file that is consisted with both numeric and categorical columns.


i want to create a GBM model that later on i can implement it's mojo in java program for inference.
the problem is that until now i imputed the nulls in the data as 0 for numeric and ""EMPTY"" for categorical columns.


when i am doing it in h2o i can enforce my scheme in h2o import_file with col_types parameter.


when i don't use the imputation , and let h2o impute by itself - it transform my numeric columns to enum type.


if i try to enforce the types on import without my imputation i get the following error for all numeric features:


 error = 'Unsupported type override (String -> Numeric). Column XXX will be parsed as String'



so continuous features will not get the right treatment...why it is converted to enum?
how can i avoid this conversion?","['python', 'h2o']",user1450410,https://stackoverflow.com/users/1450410/user1450410,301
64001253,64001253,2020-09-21T23:43:38,2020-09-22 01:29:10Z,259,"I have two computers that I want to connect to a single H2O cluster. One has 4 cores, the other has 6. When I run


h2o.init(ip = '10.0.0.89', port = 54321)



on the first computer, I get the following output:


H2O cluster uptime: 1 minutes 56 seconds 846 milliseconds
H2O cluster version:    3.8.2.3
H2O cluster name:   H2O_started_from_python_samerens_dii030
H2O cluster total nodes:    1
H2O cluster total free memory:  3.4 GB
H2O cluster total cores:    4
H2O cluster allowed cores:  4
H2O cluster healthy:    True
H2O Connection ip:  10.0.0.89
H2O Connection port:    54321
H2O Connection proxy:   None
Python Version: 3.7.4



When I run the same command on the second computer, I get the same output (except for the uptime of course). Shouldn't the total nodes have increased to 2 and the total cores have increased to 10? Am doing something wrong?","['python', 'distributed-computing', 'h2o']",Unknown,,N/A
63928863,63928863,2020-09-16T22:42:41,2020-09-17 18:52:44Z,0,"I have trained a word2vec model in the python h2o package.
Is there a simple way for me to save that word2vec model and load it back later for use?


I have tried the h2o.save_model() and h2o.load_model() functions with no luck.
I get an error using that approach like


ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/99/Models.bin/)

water.exceptions.H2OIllegalArgumentException
[1] ""water.exceptions.H2OIllegalArgumentException: Illegal argument: dir of function: importModel:



I am using the same version of h2o to train and load the model back in so the issue outlined in this question is not applicable 
Can't import binay h2o model with h2o.loadModel() function: 412 Precondition Failed


Any one with any insights on how to save and load an h2o word2vec model?


My sample code with a few of the important snippets


import h2o
from h2o.estimators import H2OWord2vecEstimator

df['text'] = df['text'].ascharacter()
  
# Break text into sequence of words
words = tokenize(df[""text""])
    
# Initializing h2o
print('Initializing h2o.')
h2o.init(ip=h2o_ip, port=h2o_port, min_mem_size=h2o_min_memory) 
   
# Build word2vec model:
w2v_model = H2OWord2vecEstimator(sent_sample_rate = 0.0, epochs = 10)
w2v_model.train(training_frame=words)
    
    
# Calculate a vector for each row
word_vecs = w2v_model.transform(words, aggregate_method = ""AVERAGE"")

#Save model to path
wv_path = '/models/wordvec/'
model_path = h2o.save_model(model = w2v_model, path= wv_path ,force=True)

# Load model in later script
w2v_model = h2o.load_model(model_path)","['python', 'word2vec', 'h2o']",Unknown,,N/A
63850930,63850930,2020-09-11T16:27:47,2020-09-11 16:27:47Z,288,"I'm building a target encoder model using the given python 
example
 on h2o documentation and trying to predict the target encodings through java using mojo of this model. But the mojo prediction fails on the categories which are present in test data only and not in training data with following error


Exception in thread ""main"" java.lang.NullPointerException
    at hex.genmodel.algos.targetencoder.TargetEncoderMojoModel.computeEncodings(TargetEncoderMojoModel.java:87)
    at hex.genmodel.algos.targetencoder.TargetEncoderMojoModel.score0(TargetEncoderMojoModel.java:72)
    at hex.genmodel.easy.EasyPredictModelWrapper.predict(EasyPredictModelWrapper.java:889)
    at hex.genmodel.easy.EasyPredictModelWrapper.transformWithTargetEncoding(EasyPredictModelWrapper.java:618)
    at main.main(main.java:26)



After digging into the target encoder mojo, found that categories which are present in test data  only, are present in 
domains.txt
, so the target encoder doesn't treat these categories as missing categories. But the target encodings are missing for these categories from 
encoding_map.ini
, so model throws 
NullPointerException
 when it tries to access encodings for such categories using 
encoding_map.ini


Code to train model:


h2o.init()
from h2o.estimators import H2OTargetEncoderEstimator

titanic = h2o.import_file(""https://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv"")
titanic['survived'] = titanic['survived'].asfactor()
response='survived'

train, test = titanic.split_frame(ratios = [.5], seed = 1234)

encoded_columns = [""home.dest"", ""cabin"", ""embarked""]

blended_avg= True
inflection_point = 3
smoothing = 10
noise = 0.15
data_leakage_handling = ""k_fold""
fold_column = ""kfold_column""
train[fold_column] = train.kfold_column(n_folds=5, seed=3456)

titanic_te = H2OTargetEncoderEstimator(fold_column=fold_column,
                                       data_leakage_handling=data_leakage_handling,
                                       blending=blended_avg,
                                       k=inflection_point,
                                       f=smoothing)

titanic_te.train(x=encoded_columns,
                 y=response,
                 training_frame=train)

titanic_te.download_mojo(get_genmodel_jar=True)



Code to get encodings:


import java.io.*;
import java.util.Arrays;
import hex.genmodel.easy.RowData;
import hex.genmodel.easy.EasyPredictModelWrapper;
import hex.genmodel.easy.prediction.*;
import hex.genmodel.MojoModel;
import hex.genmodel.algos.targetencoder.TargetEncoderMojoModel;

public class main {

    public static void main(String[] args) throws Exception {

        EasyPredictModelWrapper model = new EasyPredictModelWrapper(MojoModel.load(""TargetEncoder_model_python_1599838802418_2.zip""));

        String[] temp_home = { ""?Havana  Cuba"", ""Aberdeen / Portland  OR"", ""Albany  NY"", ""Altdorf  Switzerland"",
                               ""Amenia  ND"", ""Antwerp  Belgium / Stanton  OH"", ""Asarum  Sweden Brooklyn  NY"",
                               ""Ascot  Berkshire / Rochester  NY"", ""Auburn  NY"", ""Aughnacliff  Co Longford  Ireland New York  NY"",
                               ""Australia Fingal  ND"", ""Austria Niagara Falls  NY"", ""Austria-Hungary"",
                               ""Austria-Hungary / Germantown  Philadelphia  PA""};

        for(int j=0; j<temp_home.length; j++){
            RowData row = new RowData();
            row.put(""cabin"", ""D43"");
            row.put(""embarked"", ""C"");
            row.put(""home.dest"", temp_home[j]);

            TargetEncoderPrediction p = model.transformWithTargetEncoding(row);
            System.out.println(Arrays.toString(p.transformations));
        }
    }
}




Compile Command:
 javac -cp h2o-genmodel.jar -J-Xmx2g main.java


Run Command:
 java -cp .:h2o-genmodel.jar main",['h2o'],Ajay,https://stackoverflow.com/users/14260830/ajay,21
63836412,63836412,2020-09-10T19:24:25,2023-02-08 18:23:44Z,86,"I am working on an Isolation Forests model in H2O that is saved as MOJO in order to run in an Android app. So far so good.


However I have some data preparation code that has to be run before this model - such as transform GPS coordinates into a known location, and provide the Isolation Forests model this location instead of the GPS coordinates. And it needs to be encapsulated in a MOJO as well (could be the same MOJO or a different one).


I thought about creating a custom pipeline function in H2O that could be saved as a MOJO as well, but I don't know if it's possible. How could I do that?","['python', 'apache-spark', 'pyspark', 'h2o']",nicolezk,https://stackoverflow.com/users/4534835/nicolezk,106
63746299,63746299,2020-09-04T18:24:36,2020-09-08 19:06:26Z,0,"When being used in R, h2o generates some temp files named Rtmp******. How can I stop h2o doing it? If such files are crucial, how to change the location where they are saved?


Thank you.","['r', 'h2o', 'temp']",soniCYouth,https://stackoverflow.com/users/9469401/sonicyouth,33
63628501,63628501,2020-08-28T06:21:48,2020-08-30 21:55:02Z,0,"I have just started learn to use H2O Auto ML and I am trying out a binary Classification model.


I am trying to understand why do the rankings of the model change with every run.


The top 5 models remain in top 5, but the models slightly shift to a higher or lower rank.


While DRF was ranked 2nd once, the other time it raked 3rd.


There are couple of reasons I can speculate that causes changes.




Seed to the algorithm changes each time


There is no leader board frame assigned


RF involve random sampling as part of the process resulting in different trees built each time


The leader board will not change, some other change to data / code is responsible for the change.




Could you please help me understand this better.","['machine-learning', 'h2o', 'automl']",learner,https://stackoverflow.com/users/9628339/learner,"1,001"
63615325,63615325,2020-08-27T11:48:46,2022-01-06 21:26:26Z,525,"I successfully initialise a cluster and train a DRF model. Then on the same cluster I try do a grid search for an XGBoost model.


H2OGridSearch(
    H2OXGBoostEstimator(my_model_params),
    hyper_params=my_grid_params,
    search_criteria=my_search_criteria
)



Sometimes (not always) the grid search never finishes. Upon inspection in the H2O flow I found the job stuck at 0% progress with a 'RUNNING' status.

What I saw in the logs is the following


WARN: XGBoost task of type 'Boosting Iteration (tid=0)' is taking unexpectedly long, it didn't finish in 360 seconds.  
WARN: XGBoost task of type 'Boosting Iteration (tid=0)' is taking unexpectedly long, it didn't finish in 420 seconds.
...
WARN: XGBoost task of type 'Boosting Iteration (tid=0)' is taking unexpectedly long, it didn't finish in 60240 seconds.



and after that I get


ERRR: water.api.HDFSIOException: HDFS IO Failure: 



but the job's status is still 'RUNNING'.


I'm using h2o 3.30.0.6 via Python 3.7.
The problem is that the error is not reproducible and sometimes it just works fine.


Any hints on how to track down the root cause?

Is there a parameter I can set for killing the whole job when a boosting iteration takes too long?","['xgboost', 'h2o']",topchef,https://stackoverflow.com/users/59470/topchef,19.8k
63615228,63615228,2020-08-27T11:41:40,2020-09-08 18:16:10Z,0,"I'm trying a simple use of hyperopt with H2O XGBoost for which I'm taking elements out of a numpy array for the parameters, but I'm getting this H2OTypeError and I don't understand why the condition of 
?integer
 isn't met by 
int64
.


To simplify the example, H2O XGBoost does work when called as:


xgb = H2OXGBoostEstimator(nfolds=5, max_depth=list(range(10,11))[0])



But the following returns this H2OTypeError:


xgb = H2OXGBoostEstimator(nfolds=5, max_depth=np.arange(10,11,1)[0])



...


H2OTypeError: Argument `max_depth` should be an ?integer, got int64



I can work around the error for now, but I don't understand it.","['numpy', 'typeerror', 'xgboost', 'h2o', 'hyperopt']",Andrew,https://stackoverflow.com/users/10156171/andrew,13
63610554,63610554,2020-08-27T06:47:02,2020-08-27 09:59:37Z,0,"While running h2o.init() function I am facing an error as mentioned below, can you please help me understand why am I getting the error and what should in do inorder to avoid this error in future.


H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    C:\Users\ransingh.ray\AppData\Local\Temp\abc/h2o_RANSINGH_RAY_started_from_r.out
    C:\Users\ransingh.ray\AppData\Local\Temp\abc/h2o_RANSINGH_RAY_started_from_r.err

java version ""14.0.1"" 2020-04-14
Java(TM) SE Runtime Environment (build 14.0.1+7)
Java HotSpot(TM) 64-Bit Server VM (build 14.0.1+7, mixed mode, sharing)

Starting H2O JVM and connecting: ............................................................Diagnostic HTTP Request:
   HTTP Status Code: -1
HTTP Error Message: Failed to connect to localhost port XXXXX: Connection refused
 
Error Output:
   Only Java 8, 9, 10, 11, 12 and 13 are supported, system version is 14.0.1 
Error in h2o.init() : H2O failed to start, stopping execution.","['r', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
63456521,63456521,2020-08-17T18:19:27,2020-08-17 18:19:27Z,0,"I am trying to run H2o to Titanic dataset. I know there shouldn't be ordered factor and there is not:


> str(dataset)
'data.frame':   889 obs. of  5 variables:
 $ Survived: Factor w/ 2 levels ""0"",""1"": 1 2 2 2 1 1 1 1 2 2 ...
 $ Pclass  : Factor w/ 3 levels ""1"",""2"",""3"": 3 1 3 1 3 3 1 3 3 2 ...
 $ Sex     : Factor w/ 2 levels ""0"",""1"": 2 1 1 1 2 2 2 2 1 1 ...
 $ Age     : num [1:889, 1] -0.592 0.638 -0.285 0.408 0.408 ...
  ..- attr(*, ""dimnames"")=List of 2
  .. ..$ : NULL
  .. ..$ : chr ""Age""
 $ Embarked: Factor w/ 3 levels ""1"",""2"",""3"": 1 2 1 1 1 3 1 1 1 2 ...
 - attr(*, ""na.action"")= 'omit' Named int [1:2] 62 830
  ..- attr(*, ""names"")= chr [1:2] ""62"" ""830""



But there is still this mistake:


> dataset <- as.h2o(dataset)

ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/3/Parse)

water.exceptions.H2OIllegalArgumentException
 [1] ""water.exceptions.H2OIllegalArgumentException: Provided column type matrix is unknown.  Cannot proceed with parse due to invalid argument.""
...
org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)""                             
[39] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)""                                                     
[40] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)""                                                      
[41] ""    java.base/java.lang.Thread.run(Thread.java:834)""                                                                                      

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  

ERROR MESSAGE:

Provided column type matrix is unknown.  Cannot proceed with parse due to invalid argument.



Could you help me how to fix this?","['r', 'h2o']",Arduan,https://stackoverflow.com/users/13804653/arduan,253
63434929,63434929,2020-08-16T08:51:52,2020-08-16 09:28:14Z,50,"I currently use the DRF implementation of H2O-3 to create a binary classification model. However, I was wondering, that H2O only supports squared error as impurity measure which is usually used to contruct regression trees but no other measure like e.g. Gini which I think are better suited for classification tasks.


Within the documentation I was not able to identify the reasonning for applying this metric also for classification tasks. Can anyone explain to, why this appraoch makes sense?","['classification', 'random-forest', 'decision-tree', 'h2o']",Unknown,,N/A
63430711,63430711,2020-08-15T20:52:12,2020-08-17 06:42:38Z,0,"I am using 
hf[[x1, x2]] = hf[[x1, x2]].asfactor()
 to transform 
X1
 and 
X2
 to categorical variables and then train a classification model with automl(). Now for the new and unseen data, how should I convert the data? If I simply use the above method, is there any guarantee that it will be transformed similar to transformation in training phase?


In scklearn you should save the fitted object and use it for transforming train and new dataset but here I have no idea what to do?!","['data-manipulation', 'h2o', 'categorical-data', 'automl']",Unknown,,N/A
63342853,63342853,2020-08-10T15:06:19,2020-08-15 20:50:26Z,0,"I get an error message when I want to use the H2o method in caret on this example:


library(caret)
library(h2o)

data(HELPrct)
ds = HELPrct
fitControl= trainControl(method=""repeatedcv"", number = 5)
ds$sub = as.factor(ds$substance)
h2oFit1 <- train(homeless ~ female + i1 + sub + sexrisk + mcs + pcs, 
               trControl=fitControl, 
               method = ""gbm_h2o"", 
               data=ds[complete.cases(ds),])



Then R tells me:


Something is wrong; all the Accuracy metric values are missing:
   Accuracy       Kappa    
 Min.   : NA   Min.   : NA
 1st Qu.: NA   1st Qu.: NA
 Median : NA   Median : NA
 Mean   :NaN   Mean   :NaN 
 3rd Qu.: NA   3rd Qu.: NA
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9 
Error: Stopping
In addition: Warning message:
In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.



Does anyone know how I can make caret work with h2o? Other methods don't make any problems.","['r', 'r-caret', 'h2o']",qhalinaq,https://stackoverflow.com/users/14081444/qhalinaq,11
63310789,63310789,2020-08-08T00:52:55,2020-08-17 11:23:52Z,0,"I am planning to run glm, lasso and randomForest across different sets of predictors to see which model combination is the best. I am going to be doing v-fold cross validation. To compare the ML algorithms consistently, the same fold has to be fed into each of the ML algorithms. Correct me if I am wrong here.


How can we achieve that in h2o package in R? Should I set




fold_assignment = 
Modulo
 within each algo function such as h2o.glm(), h2o.randomForest() etc.


Hence, would the training set be split the same way across the ML algos?




If I use fold_assignment = 
Modulo
 and what if I have to stratify my outcome? The stratification option is with fold_assignment parameter as well? I am not sure I can specify 
Modulo
 and and 
Stratified
 both at the same time.


Alternatively, if I set the same 
seed
 in each of the model, would they have the same folds as input?


I have the above questions after reading Chapter 4 from [Practical Machine Learning with H2O by Darren Cook] (
https://www.oreilly.com/library/view/practical-machine-learning/9781491964590/ch04.html
)


Further, for generalizability in site level data in a scenario as in the quotation below:




For example, if you have observations (e.g., user transactions) from K cities and you want to build models on users from only K-1 cities and validate them on the remaining city (if you want to study the generalization to new cities, for example), you will need to specify the parameter “fold_column” to be the city column. Otherwise, you will have rows (users) from all K cities randomly blended into the K folds, and all K cross-validation models will see all K cities, making the validation less useful (or totally wrong, depending on the distribution of the data). 
(source)




In that case, since we are cross folding by a column, it would be consistent across all the different models, right?","['r', 'machine-learning', 'cross-validation', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
63177610,63177610,2020-07-30T16:37:23,2020-07-31 16:02:49Z,674,"I am getting below error while trying to load MOJO using pyspark.


code:
settings = H2OMOJOSettings(convertUnknownCategoricalLevelsToNa = True, convertInvalidNumbersToNa = True)
model = H2OMOJOModel.createFromMojo(in_dir + '/sparkle_comm.zip', settings)


error:
py4j.protocol.Py4JJavaError: An error occurred while calling z:py_sparkling.ml.models.H2OMOJOModel.createFromMojo.
: java.io.IOException: MOJO version incompatibility - the model MOJO version (1.10) is higher than the current h2o version (1.00) supports. Please, use the older version of h2o to load MOJO model.


spark version : version 2.4.4
Python version : Python 3.5.2


using h2o_pysparkling_2.4-3.26.2-2.4.zip


Thanks","['pyspark', 'h2o', 'mojo']",Ashutosh kashyap,https://stackoverflow.com/users/14023178/ashutosh-kashyap,1
63160845,63160845,2020-07-29T19:11:40,2020-07-30 00:07:22Z,0,"In R, there is a very convenient optimization package called 
""optim""
.  You can feed it a function, an initial position, and sometimes other control input, and it will optimize your function.


I am trying to use this with h2o.ai model in the following way:


make_model <- h2o.stuff(x,y,training, parameters)
f <- function(x,make_model){ h2o.predict() %>% ...}
f2 <- function(x){f(x,make_model)}

optim(start, function=f2, ...)



When I run test cases they work:


f2(start_point) 
f2(known_values)



These return exactly what I'm looking for.  They return the values they should.


When I try to run the optim on the function, it doesn't want to work, and gives this error:


java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set

java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set
    at hex.Model.adaptTestForTrain(Model.java:1383)
    at hex.Model.adaptTestForTrain(Model.java:1222)
    at hex.Model.score(Model.java:1509)
    at water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:396)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1557)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

Error: java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set
Called from: doTryCatch(return(expr), name, parentenv, handler)



Now it looks to me like the C-based optim and the Java based h2o.ai don't want to play well together.  The function is a bit nonlinear, but it evaluates the known points directly and properly.


Is there a decent way to get around this without changing architectures?","['r', 'optimization', 'h2o', 'nonlinear-optimization']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
63144417,63144417,2020-07-28T23:56:31,2020-07-29 16:16:38Z,0,"I am currently trying to build a decision tree model using H2O.


I saw a few questions, saying that we can use a random forest by twisting the command inside.


How can I change the R code to H2O Python code?


tree = rpart(test ~ ., control = rpart.control(minbucket = nrow(data_test)/100, maxdepth = 2))



The one I have seems weird and does not work!


rf_model = H2ORandomForestEstimator(balance_classes=True, ntrees=1, max_depth=2,
                                 mtries=10, seed=123, score_each_iteration=True)

rf_model.train(x=features, y='test', training_frame=h2o_df)","['python', 'r', 'decision-tree', 'h2o']",Peter Mortensen,https://stackoverflow.com/users/63550/peter-mortensen,31.6k
63135959,63135959,2020-07-28T14:03:45,2021-07-21 19:30:48Z,505,"I'm trying to understand how does cross-valiation work in H2o when the folds are specified passing the  'fold_column' parameter. The library says:




The fold_column option specifies the column in the dataset that
contains the cross-validation fold index assignment per observation.




I suppose that for each cross-validation iteration, the rows with fold_column = i are used as the test set, and the remaining rows are used as train set. Though, if I instead train and test the model separately with the these splits, I get different performance results. In the example below I create a column with values between 1 and 5 to be used as split index, and use it to run H2o cross-validation (with fold_column parameter). After that, I use the same column to train and test a model with the same index and compare results.


Here's a reproducible example:


h2o.init()
from h2o.estimators import H2ORandomForestEstimator
import numpy as np
import pandas as pd

# Import the prostate dataset
prostate = h2o.import_file(""http://s3.amazonaws.com/h2o-public-test-data/smalldata/prostate/prostate.csv.zip"")

# Set the predictor names and the response column name
response = ""CAPSULE""
predictors = prostate.names[3:8]

# Convert the response column to a factor
prostate['CAPSULE'] = prostate['CAPSULE'].asfactor()

# Add column with random value between 1 and 5 to use for cross-validation
np.random.seed(21)
random_folds = np.random.randint(1, 6, len(prostate))
df_folds = pd.DataFrame(random_folds, columns=['folds'])
df_h20 = prostate.cbind(h2o.H2OFrame(df_folds))


##### Train the model using H2o cross-validation #####

# Train model using fold_column argument
drf = H2ORandomForestEstimator(fold_column = 'folds', max_depth=5, ntrees=1, seed=21)
drf.train(x=predictors, y=response, training_frame=df_h20)

# Get folds prediction single models
models = drf.cross_validation_models()

# Prin test and train AUC performance for each CV-fold
print('Fold 1, AUC (test) {} AUC (train) {}'.format(models[0].auc(valid=True), models[0].auc(train=True)))
print('Fold 2, AUC (test) {} AUC (train) {}'.format(models[1].auc(valid=True), models[1].auc(train=True)))
print('Fold 3, AUC (test) {} AUC (train) {}'.format(models[2].auc(valid=True), models[2].auc(train=True)))
print('Fold 4, AUC (test) {} AUC (train) {}'.format(models[3].auc(valid=True), models[3].auc(train=True)))
print('Fold 5, AUC (test) {} AUC (train) {}'.format(models[4].auc(valid=True), models[4].auc(train=True)))


##### Train the model on a single K-fold without using H2o cross-validation #####

# select one the of the 5 folds and create test/train set
test = df_h20[df_h20['folds'] == 1]
train = df_h20[df_h20['folds'] != 1]

# Train the model
drf = H2ORandomForestEstimator(max_depth=5, ntrees=1, seed=21)
drf.train(x=predictors,
                       y=response,
                       training_frame=train,
                       validation_frame=test
                 )


perf_valid = drf.model_performance(test)
perf_train = drf.model_performance(train)
print('AUC (test) {} AUC (train) {}'.format(perf_valid.auc(), perf_train.auc()))



The output is:


Fold 1, AUC (test) 0.8352221702976504 AUC (train) 0.835269468426379

Fold 2, AUC (test) 0.8215820406943912 AUC (train) 0.8203464750008381

Fold 3, AUC (test) 0.833563260744653 AUC (train) 0.8376839384943596

Fold 4, AUC (test) 0.8295902318635076 AUC (train) 0.8287798683714774

Fold 5, AUC (test) 0.825246953403821 AUC (train) 0.8264781593374212

AUC (test) 0.838142980551675 AUC (train) 0.8382107902781438



The results of the model trained and tested on a single fold without using H2o cross-validation does not correspond to any of the 5 results of the 5-fold cross validation, which is not what I expected it. I was actually expecting to see the latest results corresponding to one the 5 CV folds. As far as I understand, the H2o cross-validation should internally train the model in the same way I've done in the last part of my code.


Does anyone know why this happens?


Edit:
 I added the argument ntrees=1. I've done this to reduce the complexity of the model, making sure we're dealing with a single decision tree. I also added the argument seed=21 to both models.","['python', 'machine-learning', 'random-forest', 'h2o']",Unknown,,N/A
63114129,63114129,2020-07-27T11:14:26,2020-08-02 01:35:08Z,291,"I am running H2O with Python.


So far, what I am able to do is build a GBM model and print its data. Below is my sample code.


gbm_model = H2OGradientBoostingEstimator(ntrees=100, max_depth=4, learn_rate=0.1)
gbm_model.train(predictors, response, training_frame=trainingFrame)
print(gbm_model)



Also, here is a sample snapshot when printing 
gbm_model
.


GBM Model


What I want to achieve is retrieve each data (with its header name)
 so that I can map and display those data on my own way. So, I tried to access the Variable Importances data by looping through it.


print(""Loop through Variable Importance Items"")
varImp = gbm_model.varimp()

for varImpItem in varImp:
    for item in varImpItem:
        print(item)
    print("" "")



For additional info, 
gbm_model.varimp()
 returns a 
ModelBase
 object.


gbm_model.varimp()


Then, this was the data that I retrieved.


GBM Model (Variable Importances Loop)


As what can be seen, it was only the data itself. The header names (
variable
, 
relative_importance
, 
scaled_importance
, 
percentage
) were not included for the display.


I want to ask, is there a way to retrieve the header names for this? If so, how can I do it?","['python', 'h2o']",gabfields02,https://stackoverflow.com/users/13920599/gabfields02,1
63068076,63068076,2020-07-24T06:32:00,2020-07-24 16:42:25Z,551,"In h2o automl can we find which models are completed and see their metrics while training is going on ?


eg: maxmodels=5 , so can we extract the information of model 1 while others(models 2 ,3) are getting trained.","['h2o', 'automl']",Unknown,,N/A
63061965,63061965,2020-07-23T19:41:02,2020-07-27 22:30:20Z,0,"I am trying to train an auto-encoder model in R with h2o to detect anomalies in my dataset:



Here is my code:


df <- read.csv(file=inputFile) # extract dataframe

feature_names <- names(df)

train_df <- df # Use whole dataset for training for this example

# -- Now train auto-encoder model --
library(h2o)
localH2O = h2o.init()
h2o.removeAll() # Close clusters that were already running

train_h2o <- as.h2o(train_df) # Put data in h2o dataframe

# Create deep learning model
result_model = h2o.deeplearning(x = feature_names, training_frame = train_h2o,
                               autoencoder = TRUE,
                               hidden = c(6,5,6),
                               epochs = 50)




Then after the model trains successfully, I enter 
result_model
 and get:


  layer units      type dropout       l1       l2 mean_rate rate_rms momentum
1     1   798     Input  0.00 %       NA       NA        NA       NA       NA
2     2     6 Rectifier  0.00 % 0.000000 0.000000  0.018308 0.110107 0.000000
3     3     5 Rectifier  0.00 % 0.000000 0.000000  0.002325 0.001377 0.000000
4     4     6 Rectifier  0.00 % 0.000000 0.000000  0.001975 0.001191 0.000000
5     5   798 Rectifier      NA 0.000000 0.000000  0.010888 0.064831 0.000000



The layer units are:
798, 6, 5, 6, 798, even though it was supposed to have 7 input nodes.


Can anyone help with this? It would be much appreciated.","['r', 'h2o', 'autoencoder']",nernac,https://stackoverflow.com/users/12279823/nernac,185
63059262,63059262,2020-07-23T16:49:10,2020-07-24 16:39:32Z,74,"I am wondering what meta learners are used by h2o.automl() to build the ensembles. So far all the ensembles I've seen were GLMs. Is it because h2o.automl() uses only glm as the meta learner or due to the limited number of base models (25 -50 with my setting), glm is always the best choice?


Thank you.","['h2o', 'automl']",soniCYouth,https://stackoverflow.com/users/9469401/sonicyouth,33
63034065,63034065,2020-07-22T12:19:55,2020-07-25 18:42:57Z,220,"I am trying to install h2o driverless ai and so far I am unable to find any way. I am using jupyter notebook and I am unable to import h2oai_client.


My python version :


pip 20.0.2 from C:\Users\user\miniconda3\lib\site-packages\pip (python 3.7)



When i try to import h2oai_client, it says not found


---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-1-afa0aad75588> in <module>
----> 1 from h2oai_client import Client

ModuleNotFoundError: No module named 'h2oai_client'","['python-3.x', 'jupyter-notebook', 'time-series', 'h2o', 'driverless-ai']",Firenze,https://stackoverflow.com/users/13845475/firenze,367
62973260,62973260,2020-07-18T20:05:25,2020-07-18 20:20:20Z,154,"I noticed that unlike H2O GBM that returns two class probabilities (p0 and p1), H2O Stacked ensemble only returns a single class probability. Can anyone explain why is this the case and does this probability correspond to p1 by default?


H2O stacked ensemble prediction:


H2O GBM model prediction:","['machine-learning', 'h2o']",Frank001,https://stackoverflow.com/users/11289178/frank001,63
62968836,62968836,2020-07-18T12:56:19,2020-07-18 20:14:09Z,0,"I run H2O anomaly with per_feature = TRUE which results in a Java Heap Space error. In some other posts about this error message, I see people suggest using h2o.remove(df) to release the used memory. However, in my case I don’t have any loop, and it seems that there is nothing I can remove to release some used memory.


Here is my code:


library(h2o)
h2o.init(min_mem_size = ""10G"", max_mem_size = ""15G"")

data.hex <- as.h2o(data)

x <- names(data.hex)

random_seed <- 42

# Deeplearning Model
print(""Deep learning model begins ..."")
model.dl = h2o.deeplearning(x = x, 
                              training_frame = data.hex, 
                              autoencoder = TRUE, 
                              activation = ""Tanh"",
                              hidden = c(5, 5, 5, 5, 5), 
                              mini_batch_size = 64,  
                              epochs = 100, 
                              stopping_rounds = 15,  
                              variable_importances = TRUE,
                              seed = random_seed) 

# Calculating anomaly per feature
print('Calculating anomaly per feature ...')
errors_per_feature <- h2o.anomaly(model.dl, data.hex, per_feature = TRUE) # Anomaly Detection Algorithm

print('Converting from H2O frame to dataframe ...')
errors1_per_feature <- as.data.frame(errors_per_feature) # Convert back to data frame



Here is the detailed error message:


[1] ""Deep learning model begins ...""
  |======================================================================| 100%
[1] ""Calculating anomaly per feature ...""

ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://localhost:54321/3/Predictions/models/DeepLearning_model_R_1594826474037_2/frames/Accesses_sid_a71f_1)

water.util.DistributedException
 [1] ""DistributedException from localhost/127.0.0.1:54321: 'Java heap space', caused by java.lang.OutOfMemoryError: Java heap space""
 [2] ""    water.MRTask.getResult(MRTask.java:494)""                                                                                  
 [3] ""    water.MRTask.getResult(MRTask.java:502)""                                                                                  
 [4] ""    water.MRTask.doAll(MRTask.java:397)""                                                                                      
 [5] ""    water.MRTask.doAll(MRTask.java:403)""                                                                                      
 [6] ""    hex.deeplearning.DeepLearningModel.scoreAutoEncoder(DeepLearningModel.java:761)""                                          
 [7] ""    water.api.ModelMetricsHandler.predict(ModelMetricsHandler.java:469)""                                                      
 [8] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                           
 [9] ""    java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                         
[10] ""    java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                 
[11] ""    java.base/java.lang.reflect.Method.invoke(Method.java:567)""                                                               
[12] ""    water.api.Handler.handle(Handler.java:60)""                                                                                
[13] ""    water.api.RequestServer.serve(RequestServer.java:470)""                                                                    
[14] ""    water.api.RequestServer.doGeneric(RequestServer.java:301)""                                                                
[15] ""    water.api.RequestServer.doPost(RequestServer.java:227)""                                                                   
[16] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                                             
[17] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                             
[18] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                                   
[19] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)""                                               
[20] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                                       
[21] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)""                                                
[22] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                                        
[23] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                            
[24] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                    
[25] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                          
[26] ""    water.webserver.jetty8.Jetty8ServerAdapter$LoginHandler.handle(Jetty8ServerAdapter.java:119)""                             
[27] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                    
[28] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                          
[29] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                                  
[30] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                           
[31] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                            
[32] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)""                                 
[33] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)""                 
[34] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)""                                                         
[35] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)""                                                    
[36] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                                   
[37] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                             
[38] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                                         
[39] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                          
[40] ""    java.base/java.lang.Thread.run(Thread.java:830)""                                                                          
[41] ""Caused by:java.lang.OutOfMemoryError: Java heap space""                                                                        

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  

ERROR MESSAGE:

DistributedException from localhost/127.0.0.1:54321: 'Java heap space'

Calls: h2o.anomaly -> .h2o.__remoteSend -> .h2o.doSafeREST
Execution halted



R and H2O version:


    H2O cluster version:        3.30.0.6  
    H2O cluster total nodes:    1 
    H2O cluster total memory:   13.43 GB 
    H2O cluster total cores:    16 
    H2O cluster allowed cores:  16 
    H2O cluster healthy:        TRUE 
    R Version:                  R version 3.6.3 (2020-02-29)



I have 16 GB of memory on my macOS.


There are 6 variables (columns) in data: 5 categorical variables and 1 numeric variable. The number of unique values in the 5 categorical variables is 17, 49, 52, 85 and 5032, respectively. The number of rows is ~500k. The data file size is 44 MB (before encoding within H2O).


What can I do in my case to resolve the issue? Please let me know if there is any other information I can provide. Thanks for your help!","['java', 'r', 'eclipse', 'out-of-memory', 'h2o']",Fanwei Zeng,https://stackoverflow.com/users/13756465/fanwei-zeng,55
62934509,62934509,2020-07-16T12:18:09,2020-07-17 00:10:06Z,0,"I need to have replicated runs that give different results with the same hyperparameters in h2o.gbm function.


Even though I've created a loop that provides double runs for each configuration and the results of this h2o gbm model runs are being extracted by using h2o.performance function; I've just realized that each twin run has exactly same results.


What do you suggest to me for having different results by running two h2o.gbm models with the same hyperparameters?


Things that I've tried:




h2o.shutdown and h2o.init with different nthreads have been tried


seed argument inside of h2o.gbm has been changed and deleted


Deleting score_tree_interval and stopping_round arguments




All these tries failed, and two runs with the same hyperparameters gave exact same results. Besides, I am sharing a sample hyperparameter configuration which I would like to get different results by running it twice.


h2o.gbm(x = x_col_names, y = y, 
        training_frame = train_h2o, 
        fold_column = ""index_4seasons"",
        ntrees = 1000, 
        max_depth = 5, 
        learn_rate = 0.1, 
        stopping_rounds = 5, 
        score_tree_interval = 10, 
        seed = 1)



Any help and comment would be appreciated.","['r', 'machine-learning', 'h2o']",Unknown,,N/A
62924057,62924057,2020-07-15T21:29:14,2020-07-15 21:29:14Z,0,"I am using the h2o package, in which I have a data.frame with special characters, however when using the 
as.h2o
 function there is a character conversion issue. Anyone know a solution.


library(h2o)
h2o.init()

df = data.frame(Nombre = c(""Juan Díaz"", ""Pedro Sánchez"", ""Sandra López""), Edad = c(23, 34, 56))
df
         Nombre Edad
1     Juan Díaz   23
2 Pedro Sánchez   34
3  Sandra López   56
> as.h2o(df)
  |============================================================================================| 100%
              Nombre Edad
1     Juan DÃ¯Â¿Â½az   23
2 Pedro SÃ¯Â¿Â½nchez   34
3  Sandra LÃ¯Â¿Â½pez   56

[3 rows x 2 columns] 



I am using the next version of h2o


packageVersion(""h2o"")
[1] ‘3.30.0.6’","['r', 'h2o']",Rafael Díaz,https://stackoverflow.com/users/8133525/rafael-d%c3%adaz,"2,257"
62922255,62922255,2020-07-15T19:14:31,2020-07-17 17:37:22Z,42,"I observed something interesting when using h2o.stackedensemble() to build ensembles.


The training frame was a data frame in R and I imported it into h2o first:


df.h2o = as.h2o(df, destination_frame='df.h2o)


then I used df.h2o to build a glm.


Later I ran the importing command (df.h2o = as.h2o(df, destination_frame='df.h2o)) again accidentally and used df.h2o to build a boosting model.


Although these two models were built with the same seed, cross-validation folds and actually the same training frame, it turned out that I could not blend them using h2o.stackedensemble(); it returned an error message ""Error: java.lang.NullPointerException"". I did some trouble shooting and the conclusion was that because I imported the same data frame twice, h2o believed that the two base models were built using different training frames.


Could anyone tell me how h2o.stackedensemble() compares the training frames used by base models? Does it actually check the content of the training frames or just use some ids generated internally when the frames were created? When there are lots of base models, it is quite common that they have to be built, saved and reloaded in different sessions, making the scenario described above inevitable.


Thank you.","['java', 'h2o', 'ensemble-learning']",Unknown,,N/A
62893580,62893580,2020-07-14T10:55:32,2020-07-14 11:59:24Z,147,"I am using H2O to perform a classification problem in a very imbalanced scenario. I decide to specify the instance weights, mapping all the 1's (positive and poorly represented class) with a high weight, and the 0's with a lower weight.


I would like to understand 
how the weights interact with the loss function
 and other components of the models (I am using AutoML) during training. I am not able to identify the core step in the source code.","['python', 'classification', 'h2o', 'loss-function']",Unknown,,N/A
62852544,62852544,2020-07-11T17:40:07,2020-07-11 21:33:00Z,234,"Im using H2O library and i want to apply undersampling data balancing, setting parameter 0.8. How can i do this? I wrote this command:


from h2o.estimators.gbm import H2OGradientBoostingEstimator
cov_gbm = H2OGradientBoostingEstimator(balance_classes = True)



but 
balance_classes = True
 use a random parameter. I want to make this parameter 0.8.


I will appreciate any help. Thanks.","['python', 'h2o']",Unknown,,N/A
62835901,62835901,2020-07-10T13:57:52,2020-07-30 07:54:42Z,229,"I can confirm the 3-replica cluster of h2o inside K3s is correctly deployed, as executing in the Python3 interpreter 
h2o.init(ip=""x.x.x.x"")
 works as expected. I followed the instructions noted here: 
https://www.h2o.ai/blog/running-h2o-cluster-on-a-kubernetes-cluster/


Nevertheless, I had to modify the 
service.yaml
 and comment out the line which says 
clusterIP: None
, as K3s was complaining about something related to its inability to set the clusterIP to None. But even though, I can certify it is working correctly, and I am able to use an external IP to connect to the cluster.


If I try to load the dataset using the h2o cluster inside the K3s cluster using the exact same steps as described here 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html
, this is the output that I get:


>>> train = h2o.import_file(""https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv"")
...
h2o.exceptions.H2OResponseError: Server error java.lang.IllegalArgumentException:
  Error: Key not loaded: Key<Frame> https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv
  Request: POST /3/ParseSetup
    data: {'check_header': '0', 'source_frames': '[""https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv""]'}



The same error occurs if I use the 
h2o.upoad_file(""x.csv"")
 method.


There is a clue about what may be happening here: 
Key not loaded: Key<Frame> while POSTing source frame through ParseSetup in H2O API call
 but I am not using curl, and I can not find any parameter that could help me overcome this issue: 
http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/h2o.html?highlight=import_file#h2o.import_file


I need to use the Python client inside the same K3s cluster due to different technical reasons, so I am not able to kick off nor Flow nor Firebug to know what may be happening.


I can confirm it is working correctly when I simply issue a 
h2o.init()
, using the local Java instance.


UPDATE 1:


I have tried in different K3s clusters without success. I changed the 
service.yaml
 to a NodePort, and now this is the error traceback:


>>> train = h2o.import_file(""https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv"")
...
h2o.exceptions.H2OResponseError: Server error java.lang.IllegalArgumentException:
  Error: Job is missing
  Request: GET /3/Jobs/$03010a2a016132d4ffffffff$_a2366be93ec99a78d7bc161de8c54d67




UPDATE 2:


I have tried using different services (NodePort, LoadBalancer, ClusterIP) and none of them work. I also have tried using 
Minikube
 with the official image, and with a custom image made by me, without success. I suspect this is something related to either h2o itself, or the clustering between pods. I will keep digging and let's think there will be some gold in it.


UPDATE 3:


I also found out that the post about running H2O in Docker is really outdated 
https://www.h2o.ai/blog/h2o-docker/
 nor is working the Dockerfile present at GitHub (I changed it to uncomment the ENTRYPOINT section without success): 
https://github.com/h2oai/h2o-3/blob/master/Dockerfile


Even though, I tried with the custom image I built for h2o-k8s and it is working seamlessly in pure Docker. I am wondering why it is still not working in K8s...


UPDATE 4:


I have tried modifying the environment variable called 
H2O_KUBERNETES_SERVICE_DNS
 without success.


In the meantime, the cluster started to be unavailable, that is, the 
readinessProbe
's would not successfully complete. No matter what I change now, it does not work.


I spinned up a K3d cluster in local to see what happened, and surprisingly, the 
readinessProbe
's were not failing, using 
v3.30.0.6
. But now I started testing it with R instead of Python. I am glad I tried, because I may have pinpointed what was wrong. There is a version mismatch between the client and the server. So I updated accordingly the image to 
v3.30.0.1
.


But now again, the 
readinessProbe
 is not working in my k3d cluster, so I am unable to test it.","['python-3.x', 'h2o', 'k3s']",Unknown,,N/A
62817956,62817956,2020-07-09T15:06:33,2020-07-13 05:43:46Z,138,"I know that there are different methods in H2O such as H2OGridSearch, H2ORandomSearch to perform hyperparameter optimization. However, is there a way to include hyperparameter optimization method when we use H2OAutoML to train many models at once? Does it already include it as a default?
Any inputs would be beneficial.","['h2o', 'automl']",Aditya Baser,https://stackoverflow.com/users/13770823/aditya-baser,11
62774361,62774361,2020-07-07T11:34:45,2020-07-07 13:12:57Z,831,"A question on h2o mojo models.


Is my understanding correct that GLM MOJO models don't hold variable importance for the model?


Or is there something that am missing?


I get the below message on screenshot sometimes when I query varimp/varimp_plot from the GLM model.


""Warning: This model doesn't have variable importance.""


Is this usual? while we get varimp from the same model within the kernel which generated them. Just curious to understand this.


Any leads would be much appreciated.","['python-3.x', 'h2o', 'h2o4gpu']",Prabhu Subramanian,https://stackoverflow.com/users/10602742/prabhu-subramanian,37
62769892,62769892,2020-07-07T07:14:00,2021-10-20 23:29:20Z,203,"I have 3 h2o models:


$ ls dataset/mojo
1. DeepLearning_model_python_1582176092021_2.zip
2. StackedEnsemble_BestOfFamily_AutoML_20200220_073620.zip
3. Word2Vec_model_python_1582176092021_1.zip



The binary models for these 3 were generated on v3.28.0.3, but I am trying to upgrade the h2o version and 
productionize
 it onto v3.30.0.5
So i converted those 3 binaries successfully to MOJO models (as listed above)


When trying to upload these mojo models using the 
h2o.upload_mojo
, for Word2Vec alone, am getting the error:



In [15]: w2v_path = 'dataset/mojo/Word2Vec_model_python_1582176092021_1.zip'

In [16]: w2v_model = h2o.upload_mojo(w2v_path)
generic Model Build progress: | (failed)                                                      |   0%
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-16-734005ed70a8> in <module>
----> 1 w2v_model = h2o.upload_mojo(w2v_path)

~/.envs/h2o-test/lib/python3.8/site-packages/h2o/h2o.py in upload_mojo(mojo_path)
   2149     frame_key = response[""destination_frame""]
   2150     mojo_estimator = H2OGenericEstimator(model_key = get_frame(frame_key))
-> 2151     mojo_estimator.train()
   2152     print(mojo_estimator)
   2153     return mojo_estimator

~/.envs/h2o-test/lib/python3.8/site-packages/h2o/estimators/estimator_base.py in train(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, max_runtime_secs, ignored_columns, model_id, verbose)
    113                                  validation_frame=validation_frame, max_runtime_secs=max_runtime_secs,
    114                                  ignored_columns=ignored_columns, model_id=model_id, verbose=verbose)
--> 115         self._train(parms, verbose=verbose)
    116
    117     def train_segments(self, x=None, y=None, training_frame=None, offset_column=None, fold_column=None,

~/.envs/h2o-test/lib/python3.8/site-packages/h2o/estimators/estimator_base.py in _train(self, parms, verbose)
    205             return
    206
--> 207         job.poll(poll_updates=self._print_model_scoring_history if verbose else None)
    208         model_json = h2o.api(""GET /%d/Models/%s"" % (rest_ver, job.dest_key))[""models""][0]
    209         self._resolve_model(job.dest_key, model_json)

~/.envs/h2o-test/lib/python3.8/site-packages/h2o/job.py in poll(self, poll_updates)
     75         if self.status == ""FAILED"":
     76             if (isinstance(self.job, dict)) and (""stacktrace"" in list(self.job)):
---> 77                 raise EnvironmentError(""Job with key {} failed with an exception: {}\nstacktrace: ""
     78                                        ""\n{}"".format(self.job_key, self.exception, self.job[""stacktrace""]))
     79             else:

OSError: Job with key $03010a64051932d4ffffffff$_8d0c64127137bd1eef16202889cf4fca failed with an exception: java.lang.IllegalArgumentException: Unsupported MOJO model 'word2vec'.
stacktrace:
java.lang.IllegalArgumentException: Unsupported MOJO model 'word2vec'.
  at hex.generic.Generic$MojoDelegatingModelDriver.computeImpl(Generic.java:99)
  at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:248)
  at hex.generic.Generic$MojoDelegatingModelDriver.compute2(Generic.java:78)
  at water.H2O$H2OCountedCompleter.compute(H2O.java:1557)
  at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
  at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
  at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
  at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
  at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)




The other two models succeed without any issues, and returns a valid 
model_id
. Any idea what the issue here is, coz from docs its understood that all three model types are 
supported by MOJO


I tried this with a cluster of 2 pods on K8s with 2Gi/1cpu memory each, but results in same outcome as above.","['java', 'python', 'h2o', 'mojo']",Sreenadh T C,https://stackoverflow.com/users/6220432/sreenadh-t-c,611
62769714,62769714,2020-07-07T07:01:00,2020-07-07 14:20:47Z,62,"I am trying to import elasticsearch index table in a third party application(h2o) as sql table using opendistro jdbc driver by giving below connection url but only index columns(schema) are imported.Data(records) is not imported.


    conn_url = “jdbc:elasticsearch://https://127.0.0.1:9200?trustSelfSigned=true”
    table = “testindex”
    username = “admin”
    password = “admin”
    
    Can you Please advice this above connection url is correct or not.In opendistro sql jdbc documentation there are many property key values are mentioned in connection url.Am i missing anything here.
    [![enter image description here][1]][1]
    jdbc:elasticsearch://[scheme://][host][:port][/context-path]?[property-key=value]&[property-key2
    =value2]…&[property-keyN=valueN]
    
    I am attaching screenshots of table import .Please suggest.","['elasticsearch', 'h2o', 'elasticsearch-opendistro']",Unknown,,N/A
62761759,62761759,2020-07-06T17:54:46,2020-07-06 17:54:46Z,0,"I am working with 
caret
 function 
train()
 in order to develop a support vector machine model. My dataset 
Matrix
 has a considerable number of rows 
255099
 and few columns/variables (
8
 including response/target variable). Target variable has 
10
 groups and is a factor. My issue is about the speed to train the model. My dataset 
Matrix
 is included next, and also the code I used for the model. I have also used 
parallel
 in order to make faster but is not working.


#Libraries
library(rsample)
library(caret)
library(dplyr)
#Original dataframe
set.seed(1854)
Matrix <- data.frame(Var1=rnorm(255099,mean = 20,sd=1),
                     Var2=rnorm(255099,mean = 30,sd=10),
                     Var3=rnorm(255099,mean = 15,sd=11),
                     Var4=rnorm(255099,mean = 50,sd=12),
                     Var5=rnorm(255099,mean = 100,sd=20),
                     Var6=rnorm(255099,mean = 180,sd=30),
                     Var7=rnorm(255099,mean = 200,sd=50),
                     Target=sample(1:10,255099,prob = c(0.15,0.1,0.1,
                                                           0.15,0.1,0.14,
                                                           0.10,0.05,0.06,
                                                           0.05),replace = T))
#Format target variable
Matrix %>% mutate(Target=as.factor(Target)) -> Matrix
# Create training and test sets
set.seed(1854)
strat <- initial_split(Matrix, prop = 0.7,
                             strata = 'Target')
traindf <- training(strat)
testdf <- testing(strat)
#SVM model
#Enable parallel computing
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
#SVM radial basis kernel
set.seed(1854) # for reproducibility
svmmod <- caret::train(
  Target ~ .,
  data = traindf,
  method = ""svmRadial"",
  preProcess = c(""center"", ""scale""),
  trControl = trainControl(method = ""cv"", number = 10),
  tuneLength = 10
)
#Stop parallel
stopCluster(cl)



Even using 
parallel
, the 
train()
 process defined in previous code did not finish. My computer with Windows system, intel core i3 and 6GB RAM was not able to finish this training in 
3 days
. For 3 days the computer was turned on but the model was not trained and I stopped it.


Maybe I am doing something wrong that is making 
train()
 pretty slow. I would like to know if there is any way to boost the training method I defined. Also, I do not know why is taking too much time if there is only 
8
 variables.


Please, could you help me to solve this issue? I have looked for solutions to this problem without success. Any suggestion on how to improve my training method is welcome. Moreover, some solutions mention that 
h2o
 can be used but I do not know how to set up my 
SVM
 scheme into that architecture.


Many thanks for your help.","['r', 'machine-learning', 'r-caret', 'h2o']",Duck,https://stackoverflow.com/users/2080848/duck,39.5k
62706595,62706595,2020-07-02T23:51:37,2020-07-13 17:23:13Z,223,"I'm trying to fine best parameters for h2o model using sklearn 
RandomizedSearchCV
. Code (taken from this 
documentation
):


from sklearn.model_selection import RandomizedSearchCV
from sklearn.pipeline import Pipeline
from h2o.estimators.gbm import H2OGradientBoostingEstimator
iris_df = h2o.import_file(path=""iris.data"")

params = {
          ""gbm__ntrees"":            [10,20],
          ""gbm__max_depth"":         [1,2,3],
          ""gbm__learn_rate"":        [0.1,0.2]
         }

pipeline = Pipeline([(""gbm"", H2OGradientBoostingEstimator(distribution=""gaussian""))])

random_search = RandomizedSearchCV(pipeline, params,
                                   n_iter=5,
                                   scoring=""roc_auc"",
                                   random_state=42,
                                   n_jobs=1)

random_search.fit(iris_df[1:], iris_df[0])



But it gives me the following error:


ValueError: Unexpected __getitem__ selector: <class 'numpy.ndarray'> <class 'numpy.ndarray'>
.


I tried different datasets and also tried to pass 
pandas.DataFrame
 instead of 
h2o.frame
, and it gives the following:


AttributeError: 'DataFrame' object has no attribute 'cbind'


What is happening? 
h2o
 now is not compatible with 
sklearn
?","['python', 'machine-learning', 'scikit-learn', 'h2o']",demo,https://stackoverflow.com/users/7875813/demo,441
62703474,62703474,2020-07-02T19:11:49,2020-07-02 19:11:49Z,51,"I am a newbee in AI/ML and found h2o recently.


I am experimenting with grid search for deep learning using Cartesian search since I want to run all different combinations. I had two runs using the same train and validation files and same set of hyper search parameters along with the grid.train parameters. Both runs generate same number of models and each model is generated with same input parameters ""activation  adaptive_rate  epsilon  hidden  hidden_dropout_ratios  input_dropout_ratio  rho"".


My observation is that for each run using same input parameters, the model generated has a different logloss, mean per class error, MSE, RMSE, etc.
To reduce further user errors, I just limited the grid search to only 1 set of parameters. My finding are below with detailed logs, etc.


My question is how do I guarantee that the model generated are exactly the same, given the same set of parameters and train/validation frame.


Train & Validation file format and data



BPS1,BPS2,ZSRTN,PCNT_RTN,PCNT_RTN100,Open,High,Low,Close,Time
58,18  ,  3.00  , -0.12  , -12  , 297.2700  , 297.3100  , 297.0800  , 297.1700  , 201907050935
18,20  ,  3.00  , -0.11  , -11  , 297.1800  , 297.1900  , 296.9300  , 296.9400  , 201907050940
20,20  ,  5.00  ,  0.01  , 1  , 296.9400  , 297.2600  , 296.8200  , 297.2150  , 201907050945
20,30  ,  5.00  ,  0.03  , 3  , 297.2200  , 297.2600  , 297.0400  , 297.0400  , 201907050950




Values


activation = RectifierWithDropout
adaptive_rate = true
epsilon = 1.0E-6
hidden = [200]
hidden_dropout_ratios = [0.1]
input_dropout_ratio 0.05
rho = 0.9




Python Code



    hyper_parameters = {
            ""hidden"": [[200]],
            ""epsilon"" : 1.0E-6,
            ""adaptive_rate"": True,
            ""activation"": [""RectifierWithDropout""],
            ""input_dropout_ratio"" : [0.05],
            ""hidden_dropout_ratios"" : [0.1],
            ""rho"":[0.9]
            }



.....

    search_criteria = {""strategy"": ""Cartesian""}

.....

    model_grid = H2OGridSearch(model = H2ODeepLearningEstimator,
        grid_id = project_name,
        hyper_params=hyper_parameters,
        search_criteria=search_criteria)



    model_grid.train(x=x,
        y = response_column,
        distribution=default_distribution, epochs=10000,
        training_frame=train, validation_frame=test,
        score_interval=0, stopping_rounds=5,
        stopping_tolerance=1e-3,
        stopping_metric=""mean_per_class_error"")





Prepare for First run




07-02 09:27:45.989 192.168.123.5:54321   #7248  #75857-26 INFO: Starting gridsearch: estimated size of search space = 1
07-02 09:27:45.990 192.168.123.5:54321   #7248  FJ-1-51   INFO: Due to the grid time limit, changing model max runtime to: 1.7976931348623157E308 secs.
07-02 09:27:45.992 192.168.123.5:54321   #7248  FJ-1-51   INFO: Building H2O DeepLearning model with these parameters:
07-02 09:27:45.992 192.168.123.5:54321   #7248  FJ-1-51   INFO: {""_train"":{""name"":""py_1_sid_b81b"",""type"":""Key""},""_valid"":{""name"":""py_2_sid_b81b"",""type"":""Key""},""_nfolds"":0,""_keep_cross_validation_models"":true,""_keep_cross_validation_predictions"":false,""_keep_cross_validation_fold_assignment"":false,""_parallelize_cross_validation"":true,""_auto_rebalance"":true,""_seed"":-1,""_fold_assignment"":""AUTO"",""_categorical_encoding"":""AUTO"",""_max_categorical_levels"":10,""_distribution"":""AUTO"",""_tweedie_power"":1.5,""_quantile_alpha"":0.5,""_huber_alpha"":0.9,""_ignored_columns"":[""Close"",""PCNT_RTN100"",""High"",""Low"",""PCNT_RTN"",""Time"",""Open""],""_ignore_const_cols"":true,""_weights_column"":null,""_offset_column"":null,""_fold_column"":null,""_check_constant_response"":true,""_is_cv_model"":false,""_score_each_iteration"":false,""_max_runtime_secs"":1.7976931348623157E308,""_stopping_rounds"":5,""_stopping_metric"":""mean_per_class_error"",""_stopping_tolerance"":0.001,""_response_column"":""ZSRTN"",""_balance_classes"":false,""_max_after_balance_size"":5.0,""_class_sampling_factors"":null,""_max_confusion_matrix_size"":20,""_checkpoint"":null,""_pretrained_autoencoder"":null,""_custom_metric_func"":null,""_custom_distribution_func"":null,""_export_checkpoints_dir"":null,""_overwrite_with_best_model"":true,""_autoencoder"":false,""_use_all_factor_levels"":true,""_standardize"":true,""_activation"":""RectifierWithDropout"",""_hidden"":[200],""_epochs"":10000.0,""_train_samples_per_iteration"":-2,""_target_ratio_comm_to_comp"":0.05,""_adaptive_rate"":true,""_rho"":0.9,""_epsilon"":1.0E-6,""_rate"":0.005,""_rate_annealing"":1.0E-6,""_rate_decay"":1.0,""_momentum_start"":0.0,""_momentum_ramp"":1000000.0,""_momentum_stable"":0.0,""_nesterov_accelerated_gradient"":true,""_input_dropout_ratio"":0.05,""_hidden_dropout_ratios"":[0.1],""_l1"":0.0,""_l2"":0.0,""_max_w2"":3.4028235E38,""_initial_weight_distribution"":""UniformAdaptive"",""_initial_weight_scale"":1.0,""_initial_weights"":null,""_initial_biases"":null,""_loss"":""Automatic"",""_score_interval"":0.0,""_score_training_samples"":10000,""_score_validation_samples"":0,""_score_duty_cycle"":0.1,""_classification_stop"":0.0,""_regression_stop"":1.0E-6,""_quiet_mode"":false,""_score_validation_sampling"":""Uniform"",""_diagnostics"":true,""_variable_importances"":true,""_fast_mode"":true,""_force_load_balance"":true,""_replicate_training_data"":true,""_single_node_mode"":false,""_shuffle_training_data"":false,""_missing_values_handling"":""MeanImputation"",""_sparse"":false,""_col_major"":false,""_average_activation"":0.0,""_sparsity_beta"":0.0,""_max_categorical_features"":2147483647,""_reproducible"":false,""_export_weights_and_biases"":false,""_elastic_averaging"":false,""_elastic_averaging_moving_rate"":0.9,""_elastic_averaging_regularization"":0.001,""_mini_batch_size"":1}
07-02 09:27:45.992 192.168.123.5:54321   #7248  FJ-1-51   INFO: Dropping ignored columns: [Close, PCNT_RTN100, High, Low, PCNT_RTN, Time, Open]
07-02 09:27:45.992 192.168.123.5:54321   #7248  FJ-1-51   INFO: Dataset already contains 128 chunks. No need to rebalance.
07-02 09:27:45.993 192.168.123.5:54321   #7248  FJ-1-51   INFO: Starting model DeepLearning__gen_202007020927_m_5_r_2_b_2_pp_0.05_l_1_t_10_model_1



Result of 1st run


07-02 09:27:51.513 192.168.123.5:54321   #7248  #75857-30 INFO: Hyper-Parameter Search Summary (ordered by increasing logloss):
07-02 09:27:51.513 192.168.123.5:54321   #7248  #75857-30 INFO:             activation  adaptive_rate  epsilon  hidden  hidden_dropout_ratios  input_dropout_ratio  rho                                                            model_ids             logloss
07-02 09:27:51.513 192.168.123.5:54321   #7248  #75857-30 INFO:   RectifierWithDropout           true   1.0E-6   [200]                  [0.1]                 0.05  0.9  DeepLearning__gen_202007020927_m_5_r_2_b_2_pp_0.05_l_1_t_10_model_1  1.7762588168309075



Prepare for Second run



07-02 09:32:49.293 192.168.123.5:54321   #7248  #75857-29 INFO: Starting gridsearch: estimated size of search space = 1
07-02 09:32:49.293 192.168.123.5:54321   #7248  FJ-1-25   INFO: Due to the grid time limit, changing model max runtime to: 1.7976931348623157E308 secs.
07-02 09:32:49.294 192.168.123.5:54321   #7248  FJ-1-25   INFO: Building H2O DeepLearning model with these parameters:
07-02 09:32:49.294 192.168.123.5:54321   #7248  FJ-1-25   INFO: {""_train"":{""name"":""py_1_sid_aeed"",""type"":""Key""},""_valid"":{""name"":""py_2_sid_aeed"",""type"":""Key""},""_nfolds"":0,""_keep_cross_validation_models"":true,""_keep_cross_validation_predictions"":false,""_keep_cross_validation_fold_assignment"":false,""_parallelize_cross_validation"":true,""_auto_rebalance"":true,""_seed"":-1,""_fold_assignment"":""AUTO"",""_categorical_encoding"":""AUTO"",""_max_categorical_levels"":10,""_distribution"":""AUTO"",""_tweedie_power"":1.5,""_quantile_alpha"":0.5,""_huber_alpha"":0.9,""_ignored_columns"":[""Time"",""Open"",""PCNT_RTN"",""PCNT_RTN100"",""Low"",""Close"",""High""],""_ignore_const_cols"":true,""_weights_column"":null,""_offset_column"":null,""_fold_column"":null,""_check_constant_response"":true,""_is_cv_model"":false,""_score_each_iteration"":false,""_max_runtime_secs"":1.7976931348623157E308,""_stopping_rounds"":5,""_stopping_metric"":""mean_per_class_error"",""_stopping_tolerance"":0.001,""_response_column"":""ZSRTN"",""_balance_classes"":false,""_max_after_balance_size"":5.0,""_class_sampling_factors"":null,""_max_confusion_matrix_size"":20,""_checkpoint"":null,""_pretrained_autoencoder"":null,""_custom_metric_func"":null,""_custom_distribution_func"":null,""_export_checkpoints_dir"":null,""_overwrite_with_best_model"":true,""_autoencoder"":false,""_use_all_factor_levels"":true,""_standardize"":true,""_activation"":""RectifierWithDropout"",""_hidden"":[200],""_epochs"":10000.0,""_train_samples_per_iteration"":-2,""_target_ratio_comm_to_comp"":0.05,""_adaptive_rate"":true,""_rho"":0.9,""_epsilon"":1.0E-6,""_rate"":0.005,""_rate_annealing"":1.0E-6,""_rate_decay"":1.0,""_momentum_start"":0.0,""_momentum_ramp"":1000000.0,""_momentum_stable"":0.0,""_nesterov_accelerated_gradient"":true,""_input_dropout_ratio"":0.05,""_hidden_dropout_ratios"":[0.1],""_l1"":0.0,""_l2"":0.0,""_max_w2"":3.4028235E38,""_initial_weight_distribution"":""UniformAdaptive"",""_initial_weight_scale"":1.0,""_initial_weights"":null,""_initial_biases"":null,""_loss"":""Automatic"",""_score_interval"":0.0,""_score_training_samples"":10000,""_score_validation_samples"":0,""_score_duty_cycle"":0.1,""_classification_stop"":0.0,""_regression_stop"":1.0E-6,""_quiet_mode"":false,""_score_validation_sampling"":""Uniform"",""_diagnostics"":true,""_variable_importances"":true,""_fast_mode"":true,""_force_load_balance"":true,""_replicate_training_data"":true,""_single_node_mode"":false,""_shuffle_training_data"":false,""_missing_values_handling"":""MeanImputation"",""_sparse"":false,""_col_major"":false,""_average_activation"":0.0,""_sparsity_beta"":0.0,""_max_categorical_features"":2147483647,""_reproducible"":false,""_export_weights_and_biases"":false,""_elastic_averaging"":false,""_elastic_averaging_moving_rate"":0.9,""_elastic_averaging_regularization"":0.001,""_mini_batch_size"":1}
07-02 09:32:49.295 192.168.123.5:54321   #7248  FJ-1-25   INFO: Dropping ignored columns: [Time, Open, PCNT_RTN, PCNT_RTN100, Low, Close, High]
07-02 09:32:49.295 192.168.123.5:54321   #7248  FJ-1-25   INFO: Dataset already contains 128 chunks. No need to rebalance.
07-02 09:32:49.295 192.168.123.5:54321   #7248  FJ-1-25   INFO: Starting model DeepLearning__gen_202007020932_m_5_r_2_b_2_pp_0.05_l_1_t_10_model_1



Results of second run


07-02 09:32:53.914 192.168.123.5:54321   #7248  #75857-32 INFO: Hyper-Parameter Search Summary (ordered by increasing logloss):
07-02 09:32:53.914 192.168.123.5:54321   #7248  #75857-32 INFO:             activation  adaptive_rate  epsilon  hidden  hidden_dropout_ratios  input_dropout_ratio  rho                                                            model_ids             logloss
07-02 09:32:53.914 192.168.123.5:54321   #7248  #75857-32 INFO:   RectifierWithDropout           true   1.0E-6   [200]                  [0.1]                 0.05  0.9  DeepLearning__gen_202007020932_m_5_r_2_b_2_pp_0.05_l_1_t_10_model_1  1.7002255980952898",['h2o'],mwahal,https://stackoverflow.com/users/4515831/mwahal,36
62684389,62684389,2020-07-01T19:51:51,2020-07-01 20:45:21Z,248,"I have build a model in H2o and while testing the same getting the below error. So while testing I am converting the input pandas dataframe to h2o frame so that it can feed into the model for prediction. Below is the sample code for the same. Please help me in getting this issue resolved.


Code Snippet:


na_list = ['NA', 'none', 'nan', 'etc']  
df = pd.DataFrame(np_data['tempdict'], index=[0])  
**hf = h2o.H2OFrame(df,na_strings=na_list)**    #this is the line where it is failing.  
prediction = model.predict(hf)[:, 1]



Error while converting to h2oframe","['python', 'pandas', 'dataframe', 'machine-learning', 'h2o']",abestrad,https://stackoverflow.com/users/1535270/abestrad,908
62644117,62644117,2020-06-29T18:02:02,2020-06-30 08:35:07Z,63,"My training frame is rather large, so I'd like to import them in a way similar to S3's multipart upload. Is the correct way to do this to manually import_file for all the parts, then call rbind on all of these parts? Or is there a more correct way or built-in of doing this?",['h2o'],jrdzha,https://stackoverflow.com/users/5437470/jrdzha,181
62636399,62636399,2020-06-29T10:43:50,2020-06-29 10:43:50Z,0,"I have a requirement to deploy h2o models on Azure . I have successfully handled sklearn models but for sklearn the dependencies in my view are easier . For h2o the java runtime dependency is my bottle-neck.


Will the container that i create will have java runtime ?> Else what are the suggested strategies ?


Should I go for a VM instead ?


Thanks,","['containers', 'h2o', 'azure-machine-learning-service']",Rajesh Rajamani,https://stackoverflow.com/users/3510820/rajesh-rajamani,501
62571907,62571907,2020-06-25T09:14:03,2020-06-25 19:49:25Z,225,"I cannot seem to get the h2o code to load in R. I tried to start up h2o using the following codes:


h2o.no_progress()

h2o.init(max_mem_size = ""5g"")



This did not work so I tried the code below and got the following error message.


h2o.init()





'''H2O is not running yet, starting it now...


<simpleError in system2(command, ""-version"", stdout = TRUE, stderr = TRUE): '""""' not found>


Error in value[3L] :


You have a 32-bit version of Java. H2O works best with 64-bit Java.


Please download the latest Java SE JDK from the following URL:


https://www.oracle.com/technetwork/java/javase/downloads/index.html'''




I tried downloading the Java update, but the link does not work. I am not sure how to fix the error. I am trying to do PCA on my dataset.","['java', 'pca', 'h2o']",Aimery,https://stackoverflow.com/users/9428564/aimery,"1,743"
62571240,62571240,2020-06-25T08:39:02,2021-06-20 16:24:47Z,139,"I am running glm model in H2o with highly imbalanced binary response variable, my problem is that when setting the argument balance_classes True, it doesn't work and I get the same result as false. Thanks",['h2o'],metales,https://stackoverflow.com/users/12252246/metales,11
62532777,62532777,2020-06-23T10:36:14,2020-06-23 10:36:14Z,50,"I have to solve a simple binary classification problem using 
H2O AutoML
. I'd like to know if the parameters  
sort_metric
 and 
stopping_metric
 can somehow influence the order of the trained model.


I try to change these two parameters using both 
AUC
 or 
AUCPR
, but the performances are almost identical.


My principal objective is to obtain the best algorithms in terms of 
AUCPR
, so I would like to somehow influence the order of the trained models.


Does someone know how can I do so?","['python', 'classification', 'h2o', 'automl']",A1010,https://stackoverflow.com/users/11271317/a1010,360
62516100,62516100,2020-06-22T13:55:30,2020-06-22 13:55:30Z,0,"I am running H2O isolation forest on a dataset without labels in R to detect outliers. It’s impossible for me to get the labels for my data. There are categorical features in my dataset.


Basically, what I am doing is I use the same dataset to train the model and predict the anomaly scores, and then arbitrarily pick the top 1% with the largest anomaly scores and the shortest lengths as the outliers. This is my code.


seed <- 12345

ntrees <- 100

max_depth <- 8        # default is 8

sample_size <- 256  # default is 256

isoforest <- h2o.isolationForest(training_frame=dataset.hex, ntrees=ntrees, seed=seed)

 

score <- h2o.predict(isoforest, dataset.hex)

 

quantile_thres <- 0.99

quantile_frame <- quantile(score, probs=quantile_thres)

quantile_frame <- as.data.frame(quantile_frame)

 

threshold <- quantile_frame[1,]

score$predicted_class <- score$predict>threshold



I have two questions:


(1)   Is what I did a proper way to identifier outliers in my dataset?


(2)   I would like to tune the hyperparameters (e.g., ntress, max_depth and sample_size) to improve the model performance on my dataset. Is there a metric in the model output for me to know which model is better? I found that MSE and RMSE are NaN when I checked model performance.


My computer: OS X 10.14.6, 16 GB memory


H2O cluster version:        3.30.0.1

H2O cluster total nodes:    1

H2O cluster total memory:   15.00 GB

H2O cluster total cores:    16

H2O cluster allowed cores:  16

H2O cluster healthy:        TRUE

R Version:                  R version 3.6.3 (2020-02-29)



Please let me know if there is any other information I can provide. Thanks for your help!","['r', 'machine-learning', 'h2o']",Fanwei Zeng,https://stackoverflow.com/users/13756465/fanwei-zeng,55
62506409,62506409,2020-06-22T01:56:46,2020-06-26 11:07:36Z,894,"I am trying to create an ML application in which a front end takes user information and data, cleans it, and passes it to h2o AutoML for modeling, then recovers and visualizes the results. Since the back end will be a stand-alone / always-on service that gets called many times, I want to ensure that all objects created in each session are removed, so that h2o doesn't get cluttered and run out of resources. The problem is that many objects are being created, and I am unsure how to identify/track them, so that I can remove them before disconnecting each session.


Note that I would like the ability to run more than one analysis concurrently, which means I cannot just call remove_all(), since this may remove objects still needed by another session. Instead, it seems I need a list of session objects, which I can pass to the remove() method. Does anyone know how to generate this list?


Here's a simple example:


import h2o
import pandas as pd

df = pd.read_csv(""C:\iris.csv"")
my_frame = h2o.H2OFrame(df, ""my_frame"")

aml = H2OAutoML(max_runtime_secs=100)
aml.train(y='class', training_frame=my_frame)



Looking in the Flow UI shows that this simple example generated 5 new frames, and 74 models. Is there a session ID tag or something similar that I can use to identify these separately from any objects created in another session, so I can remove them?","['python', 'garbage-collection', 'h2o', 'automl']",Helenus the Seer,https://stackoverflow.com/users/9681061/helenus-the-seer,737
62450138,62450138,2020-06-18T12:36:51,2022-01-07 15:32:37Z,0,"I have been trying to compute SHAP values for a Gradient Boosting Classifier in H2O module in Python. Below there is the adapted example in the documentation for the 
predict_contibutions
 method (adapted from 
https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/predict_contributionsShap.ipynb
).


import h2o
import shap
from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o import H2OFrame

# initialize H2O
h2o.init()

# load JS visualization code to notebook
shap.initjs()

# Import the prostate dataset
h2o_df = h2o.import_file(""https://raw.github.com/h2oai/h2o/master/smalldata/logreg/prostate.csv"")

# Split the data into Train/Test/Validation with Train having 70% and test and validation 15% each
train,test,valid = h2o_df.split_frame(ratios=[.7, .15])

# Convert the response column to a factor
h2o_df[""CAPSULE""] = h2o_df[""CAPSULE""].asfactor()

# Generate a GBM model using the training dataset
model = H2OGradientBoostingEstimator(distribution=""bernoulli"",
                                     ntrees=100,
                                     max_depth=4,
                                     learn_rate=0.1)

model.train(y=""CAPSULE"", x=[""AGE"",""RACE"",""PSA"",""GLEASON""],training_frame=h2o_df)

# calculate SHAP values using function predict_contributions
contributions = model.predict_contributions(h2o_df)

# convert the H2O Frame to use with shap's visualization functions
contributions_matrix = contributions.as_data_frame().to_numpy() # the original method is as_matrix()

# shap values are calculated for all features
shap_values = contributions_matrix[:,0:4]

# expected values is the last returned column
expected_value = contributions_matrix[:,4].min()

# force plot for one observation
X=[""AGE"",""RACE"",""PSA"",""GLEASON""]
shap.force_plot(expected_value, shap_values[0,:], X)



The image I get from the code above is:

force plot for one observation


What does the output means? Considering the problem above is a classification problem, the predicted value should be a probability (or even the category predicted - 0 or 1), right? Both the base value and the predicted value are negative.


Can anyone help me with this?","['python', 'h2o', 'gbm', 'shap']",Tomasz Bartkowiak,https://stackoverflow.com/users/8741356/tomasz-bartkowiak,14.6k
62437404,62437404,2020-06-17T19:59:59,2020-06-20 04:05:33Z,0,"I am trying to apply grid search to H2O unsupervised isolation forest in R. Here is my code:


Accesses.hex <- as.h2o(Accesses)

x <- names(Accesses.hex)

seed <- 12345

 

# Model hyperparameters

hyper_params <- list(ntrees = c(50, 100, 150, 200),

                       max_depth = c(8, 15, 20, 30), # default is 8

                       sample_size = c(128, 256, 512))


# Early stopping criteria

search_criteria <- list(strategy = ""RandomDiscrete"",

                          max_models = 100,

                          max_runtime_secs = 4000,

                          stopping_rounds = 15,

                          seed = seed)

 

model.grid <- h2o.grid(algorithm = ""isolationForest"",

                         x = x,

                         grid_id = ""model_grid"",

                         training_frame = Accesses.hex,

                         hyper_params = hyper_params,

                         search_criteria = search_criteria,

                         seed = seed)




However, I got an error saying:




Error in h2o.grid(algorithm = ""isolationForest"", x = x, grid_id = ""model_grid"",  :


Must specify response, y




I am using isolation forest for unsupervised learning here, so I don’t have the response variable y. Is it possible to do a grid search within H2O in this case?


My computer: OS X 10.14.6, 16 GB memory


H2O cluster version:        3.30.0.1

H2O cluster total nodes:    1

H2O cluster total memory:   15.00 GB

H2O cluster total cores:    16

H2O cluster allowed cores:  16

H2O cluster healthy:        TRUE

R Version:                  R version 3.6.3 (2020-02-29)



Please let me know if there is any other information I can provide. Thanks for your help!","['r', 'machine-learning', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
62436821,62436821,2020-06-17T19:21:34,2020-06-24 03:01:12Z,188,"Isolation Forest in H2O (3.30.0.1, R 3.6.1) computed scores greater than 1 when model applied to the test set. Here is the code to reproduce scores greater than 1. Looks like h2o is not using the normalization used in the original paper [https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest] which is score=2^(-mean length/c(n)), c(n) is alway positive for n>0, so the scores should be always less than 1.


Other implementations of isolation forest produce scores less than 1 for the same data set.


Download 
train
 and 
test
 data files.


library(data.table)
library(h2o)
h2o.init()
#import data
train<-h2o.importFile('train.csv')
test<-h2o.importFile('test.csv')
#Train model
model <- h2o.isolationForest(training_frame = train)
# Calculate score
scores <- h2o.predict(model,test)
max(scores[,1])","['h2o', 'outliers', 'unsupervised-learning']",Unknown,,N/A
62410289,62410289,2020-06-16T13:58:52,2020-06-16 18:00:22Z,0,"When I run H2O autoencoder on two different data sets of about the same size (see below), I can finish one data set (A) within 5 minutes, but the other data set (B) is really slow. It takes >30 minutes to complete only 1% for data set B. I tried restarting R session and H2O a couple of times, but that didn’t help. There are about the same number of parameters (or coefficients) in the model for both data sets. 


Data set A: 4 * 1,000,000 in size (<5 minutes)


Data set B: 8 * 477,613 in size (very slow)


The model below is used for both data sets:


model.dl = h2o.deeplearning(x = x, training_frame = data.hex, autoencoder = TRUE, activation = ""Tanh"", hidden = c(25,25,25), variable_importances = TRUE)



The memory of the H2O cluster is 15GB for both data sets. The same computer is used (OS X 10.14.6, 16 GB memory). Below is some information about the versions of H2O and R.


H2O cluster version:        3.30.0.1
H2O cluster total nodes:    1
H2O cluster total memory:   15.00 GB
H2O cluster total cores:    16
H2O cluster allowed cores:  16
H2O cluster healthy:        TRUE
R Version:                  R version 3.6.3 (2020-02-29)



Please let me know if there is any other information I can provide to get this issue resolved.","['r', 'machine-learning', 'deep-learning', 'h2o', 'autoencoder']",Unknown,,N/A
62377746,62377746,2020-06-14T20:19:20,2020-09-12 16:09:18Z,0,"I am nt able to start the h2o in R due to version of java and receiving the below error


H2O is not running yet, starting it now...


Note:  In case of errors look at the following log files:
    C:\Users\Vaibhav\AppData\Local\Temp\RtmpMBUt0r\file1db069d01678/h2o_Vaibhav_started_from_r.out
    C:\Users\Vaibhav\AppData\Local\Temp\RtmpMBUt0r\file1db02fcb36fc/h2o_Vaibhav_started_from_r.err


java version ""14.0.1"" 2020-04-14
Java(TM) SE Runtime Environment (build 14.0.1+7)
Java HotSpot(TM) 64-Bit Server VM (build 14.0.1+7, mixed mode, sharing)


Starting H2O JVM and connecting: ............................................................Diagnostic HTTP Request:
   HTTP Status Code: -1
HTTP Error Message: Failed to connect to localhost port 54321: Connection refused


Error Output:
   Only Java 8, 9, 10, 11, 12 and 13 are supported, system version is 14.0.1 
Error in h2o.init(nthreads = -1, max_mem_size = ""4g"") : 
  H2O failed to start, stopping execution","['r', 'implementation', 'h2o']",Vaibhav,https://stackoverflow.com/users/10405216/vaibhav,31
62345352,62345352,2020-06-12T13:33:34,2021-11-18 00:18:21Z,466,"I'm trying to train a ML algorithm to predict some data (real numbers).


I successfully used h2o automl to find a model that predicts my variable almost perfectly (max error in 16k+ test observations is < 0.15%). The leader model is a GLM, and I can inspect its internals using the h2o python API.


Now I would like to reproduce that model using scikit-learn and pandas, since those are the libraries that I used heavily in other parts of the project.


Could anyone here assist me on how to do this port, so I don't need to use h2o in the future?


This is what the h2o model looks like:


Model Details
=============
H2OGeneralizedLinearEstimator :  Generalized Linear Modeling
Model Key:  GLM_1_AutoML_20200611_172640


GLM Model: summary

        family  link    regularization  lambda_search   number_of_predictors_total  number_of_active_predictors     number_of_iterations    training_frame
0       gaussian    identity    Ridge ( lambda = 2.52E-5 )  nlambda = 30, lambda.max = 15.647, lambda.min = 7.073E-4, lambda.1...   32  32  30  automl_training_py_2_sid_b7bf



ModelMetricsRegressionGLM: glm
** Reported on train data. **

MSE: 3.339037335446298e-07
RMSE: 0.0005778440391183678
MAE: 0.00034964760501650187
RMSLE: 0.000428181614223859
R^2: 0.9999869058722919
Mean Residual Deviance: 3.339037335446298e-07
Null degrees of freedom: 5470919
Residual degrees of freedom: 5470887
Null deviance: 139509.91273673013
Residual deviance: 1.826760613923986
AIC: -66058752.72303456

ModelMetricsRegressionGLM: glm
** Reported on cross-validation data. **

MSE: 3.8803522694657527e-07
RMSE: 0.0006229247361813266
MAE: 0.0003746132971400872
RMSLE: 0.00046256895714739585
R^2: 0.9999847830907341
Mean Residual Deviance: 3.8803522694657527e-07
Null degrees of freedom: 5470919
Residual degrees of freedom: 5470887
Null deviance: 139510.0902560055
Residual deviance: 2.1229096838065575
AIC: -65236783.11156034

Cross-Validation Metrics Summary: 

        mean    sd  cv_1_valid  cv_2_valid  cv_3_valid  cv_4_valid  cv_5_valid
0   mae     3.6770012E-4    4.1263074E-6    3.6505258E-4    3.706906E-4     3.6763187E-4    3.7266721E-4    3.6245832E-4
1   mean_residual_deviance  3.7148237E-7    6.667412E-9     3.680999E-7     3.7594532E-7    3.6979083E-7    3.802558E-7     3.6332003E-7
2   mse     3.7148237E-7    6.667412E-9     3.680999E-7     3.7594532E-7    3.6979083E-7    3.802558E-7     3.6332003E-7
3   null_deviance   27902.018   24.152164   27880.328   27876.885   27900.943   27932.406   27919.527
4   r2  0.99998546  2.5938294E-7    0.9999856   0.9999852   0.9999855   0.9999851   0.99998575
5   residual_deviance   0.4064701   0.007295375     0.40276903  0.41135335  0.40461922  0.41606984  0.397539
6   rmse    6.094739E-4     5.466305E-6     6.0671236E-4    6.131438E-4     6.081043E-4     6.166489E-4     6.0276035E-4
7   rmsle   4.5219096E-4    3.5735677E-6    4.508027E-4     4.5445774E-4    4.5059595E-4    4.5708878E-4    4.4800964E-4


Scoring History: 

        timestamp   duration    iteration   lambda  predictors  deviance_train  deviance_test   deviance_xval   deviance_se
0       2020-06-11 17:29:06     0.000 sec   1   .16E2   33  0.014121    NaN     0.015569    7.637899e-06
1       2020-06-11 17:29:07     0.596 sec   2   .97E1   33  0.010911    NaN     0.012416    7.202663e-06
2       2020-06-11 17:29:08     1.141 sec   3   .6E1    33  0.007864    NaN     0.009245    6.969671e-06
3       2020-06-11 17:29:08     1.748 sec   4   .37E1   33  0.005319    NaN     0.006438    6.863783e-06
4       2020-06-11 17:29:09     2.294 sec   5   .23E1   33  0.003427    NaN     0.004231    6.587463e-06
5       2020-06-11 17:29:09     2.851 sec   6   .14E1   33  0.002145    NaN     0.002679    5.981752e-06
6       2020-06-11 17:29:10     3.437 sec   7   .9E0    33  0.001332    NaN     0.001665    5.137335e-06
7       2020-06-11 17:29:10     3.983 sec   8   .56E0   33  0.000835    NaN     0.001036    4.157958e-06
8       2020-06-11 17:29:11     4.563 sec   9   .35E0   33  0.000531    NaN     0.000654    3.186066e-06
9       2020-06-11 17:29:12     5.165 sec   10  .21E0   33  0.000343    NaN     0.000417    2.279007e-06
10      2020-06-11 17:29:12     5.690 sec   11  .13E0   33  0.000220    NaN     0.000269    1.508395e-06
11      2020-06-11 17:29:13     6.277 sec   12  .83E-1  33  0.000141    NaN     0.000174    9.034596e-07
12      2020-06-11 17:29:13     6.853 sec   13  .51E-1  33  0.000090    NaN     0.000111    4.893595e-07
13      2020-06-11 17:29:14     7.396 sec   14  .32E-1  33  0.000057    NaN     0.000071    2.314916e-07
14      2020-06-11 17:29:14     7.990 sec   15  .2E-1   33  0.000035    NaN     0.000044    1.386863e-07
15      2020-06-11 17:29:15     8.560 sec   16  .12E-1  33  0.000021    NaN     0.000027    3.953892e-08
16      2020-06-11 17:29:16     9.146 sec   17  .77E-2  33  0.000012    NaN     0.000016    3.271502e-08
17      2020-06-11 17:29:16     9.753 sec   18  .48E-2  33  0.000007    NaN     0.000009    3.171957e-08
18      2020-06-11 17:29:17     10.309 sec  19  .3E-2   33  0.000004    NaN     0.000005    1.575243e-08
19      2020-06-11 17:29:17     10.875 sec  20  .18E-2  33  0.000002    NaN     0.000003    1.565016e-08


See the whole table with table.as_data_frame()



It seems I should be able to get the same results using Ridge or Lasso (or maybe Tweedy?), but I tried several parameters and got awful results.


Can anyone help me? I already read both 
h2o docs
 and 
scikit docs
 but can't figure how to proceed.","['machine-learning', 'scikit-learn', 'linear-regression', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
62332216,62332216,2020-06-11T19:37:45,2020-06-12 15:15:19Z,124,"I'm using the 
H2ODRF
 and 
H2OGridSearch
 models to create a random forest pipeline with random discrete grid search hyper-parameter optimization. However, when I set nfolds to any number greater than 1 and call 
fit()
, I get an error. My code looks like this:


val drf =  new H2ODRF()
    .setFeaturesCols(featuresCols)
    .setLabelCol(labelCol)
    .setColumnsToCategorical(categoricalCols)
    .setSplitRatio(splitRatio)
    .setNfolds(4)

val nps = Map(
        ""ntrees"" -> Array(10, 50).map(_.asInstanceOf[AnyRef]))

val search = new H2OGridSearch()
    .setHyperParameters(hyperParams)
    .setAlgo(drf)

val model = search.fit(data) // data is a Spark DataFrame



com.google.gson.JsonSyntaxException: java.lang.IllegalStateException: Expected BEGIN_OBJECT but was STRING at line 1 column 608096 path $.cross_validation_metrics_summary[0].data[0][0]
  at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:224)
  at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.read(TypeAdapterRuntimeTypeWrapper.java:41)
  at com.google.gson.internal.bind.ArrayTypeAdapter.read(ArrayTypeAdapter.java:72)
  at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.read(TypeAdapterRuntimeTypeWrapper.java:41)
  at com.google.gson.internal.bind.ArrayTypeAdapter.read(ArrayTypeAdapter.java:72)
  at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:129)
  at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:220)
  at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.read(TypeAdapterRuntimeTypeWrapper.java:41)
  at com.google.gson.internal.bind.ArrayTypeAdapter.read(ArrayTypeAdapter.java:72)
  at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:129)
  at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:220)
  at com.google.gson.Gson.fromJson(Gson.java:887)
  at com.google.gson.Gson.fromJson(Gson.java:852)
  at com.google.gson.Gson.fromJson(Gson.java:801)
  at ai.h2o.sparkling.backend.utils.RestCommunication$class.ai$h2o$sparkling$backend$utils$RestCommunication$$deserialize(RestCommunication.scala:164)
  at ai.h2o.sparkling.backend.utils.RestCommunication$$anonfun$request$1.apply(RestCommunication.scala:147)
  at ai.h2o.sparkling.backend.utils.RestCommunication$$anonfun$request$1.apply(RestCommunication.scala:145)
  at ai.h2o.sparkling.utils.ScalaUtils$.withResource(ScalaUtils.scala:28)
  at ai.h2o.sparkling.backend.utils.RestCommunication$class.request(RestCommunication.scala:145)
  at ai.h2o.sparkling.ml.algos.H2OGridSearch.request(H2OGridSearch.scala:46)
  at ai.h2o.sparkling.backend.utils.RestCommunication$class.query(RestCommunication.scala:54)
  at ai.h2o.sparkling.ml.algos.H2OGridSearch.query(H2OGridSearch.scala:46)
  at ai.h2o.sparkling.ml.algos.H2OGridSearch.getGridModels(H2OGridSearch.scala:129)
  at ai.h2o.sparkling.ml.algos.H2OGridSearch.fit(H2OGridSearch.scala:163)
  at ai.h2o.sparkling.ml.algos.H2OGridSearch.fit(H2OGridSearch.scala:46)
  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)
  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)
  at scala.collection.Iterator$class.foreach(Iterator.scala:891)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
  at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)
  at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)
  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)
  ... 59 elided
Caused by: java.lang.IllegalStateException: Expected BEGIN_OBJECT but was STRING at line 1 column 608096 path $.cross_validation_metrics_summary[0].data[0][0]
  at com.google.gson.stream.JsonReader.beginObject(JsonReader.java:385)
  at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:213)
  ... 90 more



The error seems to be caused by a 
cross_validation_metrics_summary
 field which is only returned when Nfolds is greater than 1. Is there a work around to this problem?


Edit
:
I'm using the 
Prostate Data
 and using Spark version 
2.4.4
, Scala Version 
2.11.12
, and using the following sparkling water version 
ai.h2o:sparkling-water-package_2.11:3.30.0.4-1-2.4
.


Edit
:
After scouring through the Sparkling Water source code, it's starting to look like the issue is with a misconfigured schema in 
GridSchemaV99
. Is there a setting/configuration I am supposed to update that looks for a different schema?","['apache-spark', 'h2o', 'sparkling-water']",Unknown,,N/A
62308496,62308496,2020-06-10T16:28:54,2020-06-16 20:24:27Z,321,"I have sample model built using python and scitkit learn, panda frames etc. I want to be able to import and view/run that model in H2O. After looking at the import and save options in the H2O library, it requires proprietary use of the H2O API to do any sort of importing or saving of models not created with the H2O algorithms.


Question: 
Are there ways around forcing the use the H2O algorithms and allowing the saving of models from other proprietors using the H2O API?


Below: this is the current approach to saving a model in H2O, but does not allow interaction with proprietary objects like pandas.


If I build my model using scikit learn algorithm and try to save it in H2O, the API disallows it.


Attempting to Save the below clf model using the H2O h2o.save_model method fails.


# Train Test Model
clf = RandomForestClassifier(n_estimators=100, n_jobs=2, random_state=seed)
clf.fit(x, labels)

# build the model
model = H2ODeepLearningEstimator(params)
model.train(params)

# save the model
# This works
model_path = h2o.save_model(model=model, path=""/tmp/mymodel"", force=True)
# This doesn't work
model_path = h2o.save_model(model=clf, path=""/tmp/mymodel"", force=True)
print model_path
/tmp/mymodel/DeepLearning_model_python_1441838096933

# load the model
saved_model = h2o.load_model(model_path)

# download the model built above to your local machine
my_local_model = h2o.download_model(model, path=""/Users/UserName/Desktop"")

# upload the model that you just downloded above
# to the H2O cluster
uploaded_model = h2o.upload_model(my_local_model)","['python', 'scikit-learn', 'h2o']",Unknown,,N/A
62262881,62262881,2020-06-08T13:12:16,2020-06-08 18:28:53Z,532,"I've got a python docker container that starts in port 
8000
. This process, after a user interaction, starts, in the same container, the 
h20
 java process in background in the port 
54321
. 


Both ports are exposed in the Dockerfile:


FROM python:3
ENV PYTHONUNBUFFERED 1
RUN mkdir /code
WORKDIR /code
COPY requirements.txt /code/
RUN pip install -r requirements.txt
RUN apt update && apt install -y default-jre
EXPOSE 8000
EXPOSE 54321
COPY . /code/



Both ports are mapped in 
docker-compose.yml
:


  backend:
    image: xxxxx/xxx-backend
    build: backend/xxx/.
    ports:
      - ""8000:8000""
      - ""54321:54321""
    environment:
      - ""KEY: value""
    command: python manage.py runserver 0.0.0.0:8000
    depends_on:
      - db
    networks:
      - xxx



The problem is:


When the container runs, port 8000 is mapped, but when I start the 
h2o
 server (it's python 
h2o.init()
), the server starts and it's up and running (and it is accesible from lynx inside the container to localhost:54321), but the mapping doesn't work. Seems that 
54321
 port is not mapped at all because it's not in the CMD or in the startup process.


Is there a solution for map ports even if the process is not started on container startup? 


Update


This is my 
docker-compose ps
:


     Name                   Command                State                          Ports                      
-------------------------------------------------------------------------------------------------------------
some_container_1   python manage.py runserver ...   Up         0.0.0.0:54321->54321/tcp, 0.0.0.0:8000->8000/tcp","['docker', 'docker-compose', 'h2o']",Unknown,,N/A
62259924,62259924,2020-06-08T10:23:46,2020-06-08 16:30:39Z,74,"Is there anyway to import data directly from Elasticsearch index  to h2o through any api.


Thanks
Sarvendra","['elasticsearch', 'h2o', 'h2o4gpu']",Sarvendra Singh,https://stackoverflow.com/users/10871102/sarvendra-singh,139
62157155,62157155,2020-06-02T16:51:16,2020-06-02 16:51:16Z,155,"I am using the example provided in the H2O 
documentation
 to define the grid parameters in the H2OStackedEnsembleEstimator object with two base models and meta-learner. However, it returns an error whenever i pass the grid params. Below is the code sample for reference.


gbm_params = {""ntrees"": 100, ""max_depth"": 3}

stack_gbm = H2OStackedEnsembleEstimator(base_models=[RF_model, XGB_model],
                                        metalearner_algorithm=""gbm"",
                                        metalearner_params=gbm_params)

stack_gbm.train(x=x, y=y, training_frame=train)

---------------------------------------------------------------------------
H2OValueError                             Traceback (most recent call last)
<ipython-input-245-ebd1246afa1b> in <module>
      3 stack_gbm = H2OStackedEnsembleEstimator(base_models=[RF_model, XGB_model],
      4                                         metalearner_algorithm=""gbm"",
----> 5                                         metalearner_params=gbm_params)
      6 
      7 stack_gbm.train(x=x, y=y, training_frame=train)

/usr/local/lib/python3.6/site-packages/h2o/estimators/stackedensemble.py in __init__(self, **kwargs)
     61                 setattr(self, pname, pvalue)
     62             else:
---> 63                 raise H2OValueError(""Unknown parameter %s = %r"" % (pname, pvalue))
     64         self._parms[""_rest_version""] = 99
     65 

H2OValueError: Unknown parameter metalearner_params = {'ntrees': 100, 'max_depth': 3}","['python', 'machine-learning', 'h2o']",Frank001,https://stackoverflow.com/users/11289178/frank001,63
62143598,62143598,2020-06-02T01:54:37,2020-06-09 07:50:46Z,0,"I have a classification model using H2o in Python for which the AUC = 71% 


But the accuracy based on confusion Matrix is only 61%. I Understand that confusion matrix is based on .5 threshold


How do I determine for which threshold the accuracy will be 71%?","['classification', 'h2o', 'threshold', 'auc']",Usama Abdulrehman,https://stackoverflow.com/users/12825713/usama-abdulrehman,"1,033"
62115849,62115849,2020-05-31T11:56:53,2020-05-31 14:09:29Z,0,"I'm working on ANN model (a binary classification problem) in R using the 
h2o
 package because 
neuralnet
 takes too long with the data set I'm working on 😔 .


model <- h2o.deeplearning(model_id = ""h2o_ANN"" ,
                          training_frame = aa.h2o.tra ,
                          seed = 1 ,y = target ,
                          x = predictors ,epochs = 500 ,
                          hidden = 16 ,adaptive_rate = F ,
                          rate = 0.01 ,activation = ""Tanh"")



However, when I plot loss vs epochs, the line graph looks weird (for me!). I only have a small number of values within the linechart. Perhaps this is because 
h2o
 stores only a summary of the results


loss vs epoch




How can I plot the results for each epoch?


This is a sample of the data I am using.


       LIMIT_BAL        SEX  EDUCATION   MARRIAGE        AGE
6756   0.7065360 -0.8112315 -1.2947658 -0.9235809 -0.2603721
8362  -1.0616096 -0.8112315  0.5637609  1.0827051  1.2644609
25951 -0.9847337  1.2326514 -1.2947658 -0.9235809 -1.0227887
18182 -0.5234783 -0.8112315  0.5637609  1.0827051 -0.6960387
28379  1.7827986  1.2326514  0.5637609 -0.9235809 -1.0227887
23527  0.2452807  1.2326514  0.5637609 -0.9235809 -0.8049554
           PAY_1      PAY_2      PAY_3      PAY_4      PAY_5
6756  0.01322092  0.1083065  0.1357685  0.1858918  0.2313689
8362  0.01322092  0.1083065  0.1357685  0.1858918  0.2313689
25951 0.90161187 -1.5567129 -1.5299515 -1.5196811 -0.6477726
18182 0.01322092  0.1083065  0.1357685  0.1858918  0.2313689
28379 0.90161187 -0.7242032 -0.6970915 -0.6668946 -0.6477726
23527 0.01322092  0.1083065  0.1357685 -0.6668946 -0.6477726
           PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4
6756   0.2495171 -0.4432728 -0.4042181 -0.3504430 -0.3479263
8362   0.2495171 -0.2973090 -0.2785499 -0.2412887  0.2332462
25951  0.2495171 -0.6944606 -0.6900911 -0.6772422 -0.6719405
18182  0.2495171  0.9112907  0.9514314  0.9672028  1.0530965
28379 -0.6177853 -0.6816971 -0.6851955 -0.6730279 -0.4750689
23527 -0.6177853  0.6555853  0.7186927 -0.1841930 -0.6608832
       BILL_AMT5  BILL_AMT6     PAY_AMT1    PAY_AMT2
6756  -0.2428634 -0.1717688 -0.166281672 -0.12988389
8362  -0.1785104 -0.1490618 -0.260590656 -0.19971668
25951 -0.6206216 -0.5939683 -0.355530470 -0.26565731
18182  1.0982856  1.1123754 -0.001382886 -0.01221359
28379 -0.6363075 -0.6281210 -0.333451444 -0.25244203
23527 -0.6517141 -0.6390807 -0.021190927 -0.12988389
         PAY_AMT3    PAY_AMT4    PAY_AMT5    PAY_AMT6 DEFAULT
6756  -0.18856887  0.05263758 -0.06912059 -0.23740829       0
8362  -0.24714368 -0.23512972 -0.21064007 -0.23740829       0
25951 -0.30762337 -0.14425583 -0.25017065 -0.18093455       0
18182  0.01977649 -0.02107557  0.01336656  0.04496042       0
28379  0.44700354 -0.20633349 -0.21617435  0.38476293       0
23527  0.21097801 -0.26730609 -0.25926268 -0.25412452       0","['r', 'deep-learning', 'neural-network', 'h2o']",Unknown,,N/A
61980077,61980077,2020-05-24T00:05:14,2020-05-28 22:15:59Z,0,"I'm running Windows 10 64 bit. 
I went to the command prompt and typed java -version to check the version.


The result told me java version ""1.8.0_251"" and 64-Bit Server VM (mixed mode)


However, when I'm trying to run H2o in R, error occurs:


You have a 32-bit version of Java. H2O works best with 64-bit Java. 
Please download the latest Java SE JDK from the following URL: https://www.oracle.com/technetwork/java/javase/downloads/index.html



I have tried downloading the recent Java SE 14 64 bit version and restarted R session, but the same error occurs. I have looked at my control panel and checked that my Java is 64 bit too. 


So now I am completely lost in dealing with this Java problem. Anybody got some clue?","['java', 'r', 'h2o']",phil,https://stackoverflow.com/users/12685231/phil,3
61950298,61950298,2020-05-22T07:56:35,2020-05-23 21:08:55Z,215,"I tried to set up a local cluster with just 2 nodes, via h2o. I tried to set up the computers in the terminal like it is specified under :
https://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/deployment/multinode.html
 . Although my second node says for a short period that he joined, he exists right after due to ""Attempting to join an H2O cloud that is no longer accepting new H2O nodes from""


I really would appreciate your help since I am quite new in this area.


Running MacOS 10.15.4, r version 4.00, spark version
Please have a look on the terminal output:


Cannot load library from path lib/osx_64/libxgboost4j_gpu.dylib
Cannot load library from path lib/libxgboost4j_gpu.dylib
Failed to load library from both native path and jar!
Cannot load library from path lib/osx_64/libxgboost4j_omp.dylib
Cannot load library from path lib/libxgboost4j_omp.dylib
Failed to load library from both native path and jar!
05-21 22:40:23.640 192.168.1.168:54321   2272         main  INFO water.default: ----- H2O started  -----
05-21 22:40:23.641 192.168.1.168:54321   2272         main  INFO water.default: Build git branch: master
05-21 22:40:23.641 192.168.1.168:54321   2272         main  INFO water.default: Build git hash: d3d24b7c6059f15c6b6333a84ccb70e70bc5d3dc
05-21 22:40:23.641 192.168.1.168:54321   2272         main  INFO water.default: Build git describe: jenkins-master-5076-39-gd3d24b7
05-21 22:40:23.642 192.168.1.168:54321   2272         main  INFO water.default: Build project version: 3.31.0.5077
05-21 22:40:23.642 192.168.1.168:54321   2272         main  INFO water.default: Build age: 16 hours and 8 minutes
05-21 22:40:23.642 192.168.1.168:54321   2272         main  INFO water.default: Built by: 'jenkins'
05-21 22:40:23.642 192.168.1.168:54321   2272         main  INFO water.default: Built on: '2020-05-21 06:31:36'
05-21 22:40:23.642 192.168.1.168:54321   2272         main  INFO water.default: Found H2O Core extensions: [XGBoost, KrbStandalone]
05-21 22:40:23.642 192.168.1.168:54321   2272         main  INFO water.default: Processed H2O arguments: [-flatfile, flatfile.txt, -port, 54321]
05-21 22:40:23.643 192.168.1.168:54321   2272         main  INFO water.default: Java availableProcessors: 8
05-21 22:40:23.643 192.168.1.168:54321   2272         main  INFO water.default: Java heap totalMemory: 123,0 MB
05-21 22:40:23.643 192.168.1.168:54321   2272         main  INFO water.default: Java heap maxMemory: 17,78 GB
05-21 22:40:23.643 192.168.1.168:54321   2272         main  INFO water.default: Java version: Java 1.8.0_251 (from Oracle Corporation)
05-21 22:40:23.643 192.168.1.168:54321   2272         main  INFO water.default: JVM launch parameters: [-Xmx20g]
05-21 22:40:23.643 192.168.1.168:54321   2272         main  INFO water.default: JVM process id: 
[email protected]

05-21 22:40:23.643 192.168.1.168:54321   2272         main  INFO water.default: OS version: Mac OS X 10.15.4 (x86_64)
05-21 22:40:23.644 192.168.1.168:54321   2272         main  INFO water.default: Machine physical memory: 8,00 GB
05-21 22:40:23.644 192.168.1.168:54321   2272         main  INFO water.default: Machine locale: de_DE
05-21 22:40:23.644 192.168.1.168:54321   2272         main  INFO water.default: X-h2o-cluster-id: 1590093617567
05-21 22:40:23.644 192.168.1.168:54321   2272         main  INFO water.default: User name: 'Max'
05-21 22:40:23.644 192.168.1.168:54321   2272         main  INFO water.default: IPv6 stack selected: false
05-21 22:40:23.645 192.168.1.168:54321   2272         main  INFO water.default: Network address/interface is not reachable in 150ms: /fe80:0:0:0:8045:3098:f44:71b2%utun1/name:utun1 (utun1)
05-21 22:40:23.645 192.168.1.168:54321   2272         main  INFO water.default: Network address/interface is not reachable in 150ms: /fe80:0:0:0:2118:84f2:fc7a:7ea1%utun0/name:utun0 (utun0)
05-21 22:40:23.645 192.168.1.168:54321   2272         main  INFO water.default: Possible IP Address: llw0 (llw0), fe80:0:0:0:6084:87ff:fe44:8adf%llw0
05-21 22:40:23.645 192.168.1.168:54321   2272         main  INFO water.default: Network address/interface is not reachable in 150ms: /fe80:0:0:0:6084:87ff:fe44:8adf%awdl0/name:awdl0 (awdl0)
05-21 22:40:23.645 192.168.1.168:54321   2272         main  INFO water.default: Possible IP Address: en1 (en1), fe80:0:0:0:95:3412:ed28:553f%en1
05-21 22:40:23.645 192.168.1.168:54321   2272         main  INFO water.default: Possible IP Address: en1 (en1), 192.168.1.87
05-21 22:40:23.646 192.168.1.168:54321   2272         main  INFO water.default: Possible IP Address: en0 (en0), fe80:0:0:0:8ca:d39f:9e33:5b4d%en0
05-21 22:40:23.646 192.168.1.168:54321   2272         main  INFO water.default: Possible IP Address: en0 (en0), 192.168.1.168
05-21 22:40:23.646 192.168.1.168:54321   2272         main  INFO water.default: Possible IP Address: lo0 (lo0), fe80:0:0:0:0:0:0:1%lo0
05-21 22:40:23.646 192.168.1.168:54321   2272         main  INFO water.default: Possible IP Address: lo0 (lo0), 0:0:0:0:0:0:0:1%lo0
05-21 22:40:23.646 192.168.1.168:54321   2272         main  INFO water.default: Possible IP Address: lo0 (lo0), 127.0.0.1
05-21 22:40:23.646 192.168.1.168:54321   2272         main  WARN water.default: Multiple local IPs detected:
05-21 22:40:23.647 192.168.1.168:54321   2272         main  WARN water.default:   /192.168.1.87  /192.168.1.168
05-21 22:40:23.647 192.168.1.168:54321   2272         main  WARN water.default: Attempting to determine correct address...
05-21 22:40:23.647 192.168.1.168:54321   2272         main  WARN water.default: Using /192.168.1.168
05-21 22:40:23.647 192.168.1.168:54321   2272         main  INFO water.default: H2O node running in unencrypted mode.
05-21 22:40:23.648 192.168.1.168:54321   2272         main  INFO water.default: Internal communication uses port: 54322
05-21 22:40:23.649 192.168.1.168:54321   2272         main  INFO water.default: Listening for HTTP and REST traffic on http://192.168.1.168:54321/
05-21 22:40:23.653 192.168.1.168:54321   2272         main  WARN water.default: -flatfile specified but not found: flatfile.txt
05-21 22:40:23.653 192.168.1.168:54321   2272         main  INFO water.default: H2O cloud name: 'Max' on /192.168.1.168:54321, static configuration based on -flatfile flatfile.txt
05-21 22:40:23.654 192.168.1.168:54321   2272         main  INFO water.default: If you have trouble connecting, try SSH tunneling from your local machine (e.g., via port 55555):
05-21 22:40:23.654 192.168.1.168:54321   2272         main  INFO water.default:   1. Open a terminal and run 'ssh -L 55555:localhost:54321 
[email protected]
'
05-21 22:40:23.654 192.168.1.168:54321   2272         main  INFO water.default:   2. Point your browser to http://localhost:55555
05-21 22:40:24.102 192.168.1.168:54321   2272         main  INFO water.default: Log dir: '/tmp/h2o-Max/h2ologs'
05-21 22:40:24.102 192.168.1.168:54321   2272         main  INFO water.default: Cur dir: '/Users/Max/Downloads/h2o-3.31.0.5077'
05-21 22:40:24.108 192.168.1.168:54321   2272         main  INFO water.default: Subsystem for distributed import from HTTP/HTTPS successfully initialized
05-21 22:40:24.108 192.168.1.168:54321   2272         main  INFO water.default: HDFS subsystem successfully initialized
05-21 22:40:24.111 192.168.1.168:54321   2272         main  INFO water.default: S3 subsystem successfully initialized
05-21 22:40:24.122 192.168.1.168:54321   2272         main  INFO water.default: GCS subsystem successfully initialized
05-21 22:40:24.123 192.168.1.168:54321   2272         main  INFO water.default: Flow dir: '/Users/Max/h2oflows'
05-21 22:40:24.134 192.168.1.168:54321   2272         main  INFO water.default: Cloud of size 1 formed [/192.168.1.168:54321]
05-21 22:40:24.141 192.168.1.168:54321   2272         main  INFO water.default: Registered parsers: [GUESS, ARFF, XLS, SVMLight, AVRO, PARQUET, CSV]
05-21 22:40:24.141 192.168.1.168:54321   2272         main  INFO water.default: XGBoost extension initialized
05-21 22:40:24.142 192.168.1.168:54321   2272         main  INFO water.default: KrbStandalone extension initialized
05-21 22:40:24.142 192.168.1.168:54321   2272         main  INFO water.default: Registered 2 core extensions in: 300ms
05-21 22:40:24.143 192.168.1.168:54321   2272         main  INFO water.default: Registered H2O core extensions: [XGBoost, KrbStandalone]
05-21 22:40:24.272 192.168.1.168:54321   2272         main  INFO hex.tree.xgboost.XGBoostExtension: Found XGBoost backend with library: xgboost4j_minimal
05-21 22:40:24.272 192.168.1.168:54321   2272         main  WARN hex.tree.xgboost.XGBoostExtension: Your system supports only minimal version of XGBoost (no GPUs, no multithreading)!
05-21 22:40:24.350 192.168.1.168:54321   2272         main  INFO water.default: Registered: 214 REST APIs in: 207ms
05-21 22:40:24.351 192.168.1.168:54321   2272         main  INFO water.default: Registered REST API extensions: [Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4]
05-21 22:40:24.474 192.168.1.168:54321   2272         main  INFO water.default: Registered: 284 schemas in 123ms
05-21 22:40:24.474 192.168.1.168:54321   2272         main  INFO water.default: H2O started in 6901ms
05-21 22:40:24.474 192.168.1.168:54321   2272         main  INFO water.default: 
05-21 22:40:24.474 192.168.1.168:54321   2272         main  INFO water.default: Open H2O Flow in your web browser: http://192.168.1.168:54321
05-21 22:40:24.474 192.168.1.168:54321   2272         main  INFO water.default: 
05-21 22:40:40.217 192.168.1.168:54321   2272   1.91:54321 ERROR water.default: Attempting to join an H2O cloud that is no longer accepting new H2O nodes from /192.168.1.91:54321
05-21 22:40:40.220 192.168.1.168:54321   2272   1.91:54321 FATAL water.default: Exiting.```","['java', 'cluster-computing', 'h2o', 'local-network']",Max,https://stackoverflow.com/users/13593968/max,13
61945874,61945874,2020-05-22T00:20:36,2020-05-22 00:20:36Z,16,"Lets say theres a Frame 
fr
 consisting of 3 vectors with column names ""a"" ""b"" ""c"". I have an 
MRTask
 giving me a new vector. 
Vec new_vec_a = FooReturningNewVec(fr.vec(""a""))
. Now I want to swap 
fr.vec(""a"")
 with 
new_vec_a
. 


Can I use an already existing method, if not how would you suggest on creating this method.","['java', 'frame', 'swap', 'h2o', 'vec']",ard,https://stackoverflow.com/users/10609192/ard,73
61912819,61912819,2020-05-20T12:11:45,2020-05-20 14:12:37Z,295,"We are trying to make a cluster of h20 using docker image what we are using is a ENTRYPOINT command with ip of host machine when we are trying to execute the image it give me the following error(Ip Address not found).Please suggest as we have a demo.


ENTRYPOINT [""java"",""-Xmx10g"",""-jar"",""h2o-3.22.1.6/h2o.jar"",""-ip"",""192.168.29.164"",""-port"",""54321"",""-client""]



 


IPv6 stack selected: false
Possible IP Address: eth0 (eth0), 172.17.0.2
Possible IP Address: lo (lo), 127.0.0.1
IP address not found on this machine","['docker', 'dockerfile', 'h2o', 'h2o4gpu']",SiHa,https://stackoverflow.com/users/3714940/siha,"8,371"
61817203,61817203,2020-05-15T10:36:16,2020-05-21 23:44:58Z,234,"i am trying to build model on a large data(2 millions transaction data) and getting below error.Thee is no progress in model building in progress bar and after some time job stops with below error.We are running this in single node and h2o is not distributed.
Please suggest is this is related to memory issue.Like If we have 20 GB training data then how much memory,heap size should be given to h2o?
Does all the complete training frame stores in heap memory?


Error fetching job '$03010a010d6832d4ffffffff$_9bf0e32df1dba1c2d24eb8a513f47a4'
Error calling GET /3/Jobs/%2403010a010d6832d4ffffffff%24_9bf0e32df1dba1c2d24eb8a513f47a4
HTTP connection failure: status=error, code=503, error=Service Temporarily Unavailable



Thanks
Deepti","['machine-learning', 'h2o', 'predictive', 'h2o4gpu']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
61779581,61779581,2020-05-13T16:13:38,2020-05-15 01:47:01Z,211,"I'm trying to access the results of some H2O models using python.


I specifically want the cross-validation results. I'm able to get r2 and mae using the code below. I'd ideally like the standard deviation scores too.


I can see the data using 
.cross_validation_metrics_summary
 , but can't work out how to return the specific values (e.g. cross validation sd)


import h2o 

h2o.init()

def get_model_det(current_model):
    r2_score = current_model.r2(xval = ""TRUE"")
    mae_score = current_model.mae(xval = ""True"")
    varimp = current_model.varimp()
    print(current_model.cross_validation_metrics_summary)
    print(r2_score, mae_score)

current_model = h2o.get_model(""XGBoost_2_AutoML_20200513_153924"")
get_model_det(current_model)","['python', 'h2o']",ceharep,https://stackoverflow.com/users/12246875/ceharep,439
61769186,61769186,2020-05-13T07:56:06,2020-05-22 17:53:48Z,0,"""After running the following Code…""


gbm = h2o.get_model(sorted_final_grid.sorted_metric_table()['model_ids'][0])

params = gbm.params
new_params = {""nfolds"":5, ""model_id"":None}
for key in new_params.keys():
    params[key]['actual'] = new_params[key] 
gbm_best = H2OGradientBoostingEstimator()
for key in params.keys():
    if key in dir(gbm_best) and getattr(gbm_best,key) != params[key]['actual']:
        setattr(gbm_best,key,params[key]['actual'])



""I get the following error…H2OTypeError: 'training_frame' must be a valid H2OFrame!


It is a valid H2OFrame as I have not only imported using the import_file but also ran successfully all the
GBM hyperparameter tuning code until I ran into this error.


I am using Python 3.6. I have been following this particular notebook 
https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.ipynb
 ""","['python-3.x', 'h2o']",Jeff King,https://stackoverflow.com/users/13531721/jeff-king,41
61752303,61752303,2020-05-12T12:56:09,2020-05-15 01:23:33Z,0,"I'd like to build a 
one-class classification (OCC)
 model using the H2O package either for Python or R but I couldn't find any reference on the internet. So I wonder, does H2O support one-class classification? If it does, could someone here help me to give an example code to do it using the Setosa class from the 
Iris Dataset
 and then test it to predict the other classes?","['python', 'r', 'machine-learning', 'h2o', 'one-class-classification']",Anastasiya-Romanova 秀,https://stackoverflow.com/users/3397819/anastasiya-romanova-%e7%a7%80,"3,358"
61722372,61722372,2020-05-11T04:37:27,2020-05-13 04:57:56Z,166,"I am getting H2OResponseError: Server error water.exceptions.H2OIllegalArgumentException: Error: unimplemented 


H2OResponseError: Server error water.exceptions.H2OIllegalArgumentException:
  Error: unimplemented
  Request: POST /3/PartialDependence/
    data: {'cols': '[offer_id]', 'model_id': 'GBM_grid__1_AutoML_20200510_220603_model_14', 'frame_id': 'digiq_wine_multiclass_training_1__scrubbed3.hex', 'nbins': '20', 'add_missing_na': 'False', 'row_index': '-1'}



When I try to find PDP for one of the datasets I am working with. This doesn't break for all the dataset but, in few peculiar cases like this one, it does. I am not able to find the reason, and not sure if this could be a bug of H2O. Having checked the log file, it shows the same error as above with no other relevant information.
H2O_cluster_version:    3.30.0.1
Language: Python 3.7.6


I am providing the reproducible example pointing to my GitHub to readily see the visible error also can be downloaded and just run to experience it. I would appreciate it if anyone from the H2O team or relevant person to help in resolving or find the root cause of the issue. Thank you.


GitHub replicating example - 
https://github.com/prabhuSub/H2O-Water-Exception


Notebook in the repo - 
https://github.com/prabhuSub/H2O-Water-Exception/blob/master/Sample_issue.ipynb


logs - 
https://github.com/prabhuSub/H2O-Water-Exception/tree/master/h2ologs_node0_127.0.0.1_54321


The log file is added too for reference if required, for a quick investigation.",['h2o'],Rob,https://stackoverflow.com/users/162698/rob,15.1k
61713239,61713239,2020-05-10T13:59:26,2020-05-10 15:44:52Z,199,"Trying to use H20 with docker-compose. Their website has 
instructions on running with Docker
 which I'm using as a basis.


I can't work out how to persist the appropriate folders to keep the models accessible in H2O Flow. Which folders do I need to persist locally for this?


I've used the 
Dockerfile here
 and the 
docker-compose.yaml
 below. I'm able to store models locally by mounting the 
/tmp
 folder, but which other folders do I need to mount?


version: '3.1'
services :
  h2o-svc:
    build:
      context: .
      dockerfile: Dockerfile
    image: h2o:latest
    restart: always
    volumes:
    - ./app/h2o_models:/tmp
    ports:
      - 54321:54321","['docker', 'h2o']",ceharep,https://stackoverflow.com/users/12246875/ceharep,439
61692023,61692023,2020-05-09T05:00:04,2021-11-11 14:39:19Z,0,"I'm trying to extract variable importance with the 
iml
 package in 
R
, at first I thought the error was due to my implementation but found out that it's not the case when I reproduced the same example which has worked fine 
here
.
Here is the code which is fairly easy, straightforward, and reproducible:


library(rsample)   # data splitting
library(ggplot2)   # allows extension of visualizations
library(dplyr)     # basic data transformation
library(h2o)       # machine learning modeling
library(iml)       # ML interprtation

# initialize h2o session
h2o.no_progress()
h2o.init()

# classification data
df <- rsample::attrition %>% 
  mutate_if(is.ordered, factor, ordered = FALSE) %>%
  mutate(Attrition = recode(Attrition, ""Yes"" = ""1"", ""No"" = ""0"") %>% factor(levels = c(""1"", ""0"")))

# convert to h2o object
df.h2o <- as.h2o(df)

# create train, validation, and test splits
set.seed(123)
splits <- h2o.splitFrame(df.h2o, ratios = c(.7, .15), destination_frames = 
    c(""train"",""valid"",""test""))
names(splits) <- c(""train"",""valid"",""test"")

# variable names for resonse & features
y <- ""Attrition""
x <- setdiff(names(df), y) 

# elastic net model 
glm <- h2o.glm(
  x = x, 
  y = y, 
  training_frame = splits$train,
  validation_frame = splits$valid,
  family = ""binomial"",
  seed = 123
  )

# 1. create a data frame with just the features
features <- as.data.frame(splits$valid) %>% select(-Attrition)

# 2. Create a vector with the actual responses
response <- as.numeric(as.vector(splits$valid$Attrition))

# 3. Create custom predict function that returns the predicted values as a
#    vector (probability of purchasing in our example)
pred <- function(model, newdata)  {
  results <- as.data.frame(h2o.predict(model, as.h2o(newdata)))
  return(results[[3L]])
}

# create predictor object to pass to explainer functions
predictor.glm <- Predictor$new(
  model = glm, 
  data = features, 
  y = response, 
  predict.fun = pred,
  class = ""classification""
  )

imp.glm <- FeatureImp$new(predictor.glm, loss = ""mse"")



This is the error I get:


Error in `[.data.frame`(prediction, , self$class, drop = FALSE): undefined columns 
selected
Traceback:

1. FeatureImp$new(predictor.glm, loss = ""mse"")

2. .subset2(public_bind_env, ""initialize"")(...)

3. private$run.prediction(private$sampler$X)

4. self$predictor$predict(data.frame(dataDesign))

5. prediction[, self$class, drop = FALSE]

6. `[.data.frame`(prediction, , self$class, drop = FALSE)

7. stop(""undefined columns selected"")



How do I solve it?","['r', 'machine-learning', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
61593799,61593799,2020-05-04T13:48:12,2020-05-04 18:09:51Z,0,"Does anyone know has to see variable importance plot for an ensemble model built in H2O using R?


This code h2o.varimp_plot(ensemble_model) returns an error 


In max(tvi$coefficients) : no non-missing arguments to max; returning -Inf","['r', 'h2o', 'ensemble-learning']",kskirpic,https://stackoverflow.com/users/11700883/kskirpic,155
61590772,61590772,2020-05-04T11:06:18,2020-05-04 18:14:17Z,212,"The H2O manual describes how the data is split for k-fold cross validation. The example given is for a 5 fold cross validation. 


see here: 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/nfolds.html
 which states: 


""The first 5 models (cross-validation models) are built on 80% of the training data, and a different 20% is held out for each of the 5 models.""


If a different fold value was chosen, will these percentages be different for example, suppose 10 was chosen as the number of folds, would the following be true? 


'The first 10 models (cross-validation models) are built on 90% of the training data, and a different 10% is held out for each of the 10 models.'","['python', 'cross-validation', 'h2o', 'k-fold']",Nosey,https://stackoverflow.com/users/8633769/nosey,724
61585373,61585373,2020-05-04T04:49:06,2020-05-06 21:09:09Z,42,"The all algorithms that are available in h2o will applicable in Automl. For example, will H2O automl run on these algorithms like time series, Cox Proportional Hazards (CoxPH), naive bayes.",['h2o'],ammu,https://stackoverflow.com/users/9676349/ammu,1
61541709,61541709,2020-05-01T11:34:31,2020-05-13 00:09:56Z,121,"Tried on 28.0.2 and latest 30.0.1 versions.


Create first DRF:


rf1 <- h2o.randomForest(
  model_id=""first_drf1_x1"",
  x = f2,
  y = r1,
  training_frame = train1,
  validation_frame = valid1,
  ntrees = 49,
  nfolds = 5,
 seed = 1
)



Train it and they try to continue training from this model like this:


rf2 <- h2o.randomForest(
  model_id=""second_drf1_x2"",
  x = f2,
  y = r1,
  training_frame = train2,
  validation_frame = valid2,
  ntrees = (49+50),
  nfolds = 5,
  checkpoint = ""first_drf1_x1"",
  seed = 1

)



Immediately in logs this can be seen:


POST /3/ModelBuilders/drf, parms: {model_id=second_drf1_x2, validation_frame=RTMP_sid_aea1_16, response_column=pcs7_e_dt_4010u, training_frame=RTMP_sid_aea1_14, seed=1, nfolds=5, ntrees=99, ignored_columns=[""ts"",""leve_batch_nbr""], checkpoint=first_drf1_x1}
04-30 10:20:34.601 127.0.0.1:54321       55804  FJ-1-5    INFO: Creating 5 cross-validation splits with random number seed: 1
04-30 10:20:34.612 127.0.0.1:54321       55804  FJ-1-5    ERRR: _weights_column: Weights column '__internal_cv_weights__' not found in the training frame



When the first model created, there are 5 CV models created and they have that internal field set like this:


“_weights_column"":""internal_cv_weights"",



but when main first model is trained then :


Building main model.
...
“_weights_column"":null,



I've opened a bug in h2o jira, but maybe somebody already has seen this issue and have a workaround.
If nfolds set to 0 (disabling cross validation) - then everything works just fine",['h2o'],bob,https://stackoverflow.com/users/5889446/bob,25
61536248,61536248,2020-05-01T03:11:23,2020-05-07 01:43:58Z,236,"I am training a set of 
glm
 models using h2o where the 
very
 sparse training matrix (
4million x 50k
) is the same but the response variable (y) is different for each model.  The steps I am using are




training matrix is read as a 3col pandas table (row_id, col_id, value)  [time: <5s]


scipy.sparse.csc_matrix
 is created using the table  [time: <5s]


train_h2o_orig = h2o.H2OFrame(csc_matrix)


train in this loop




for y in cols:
    train_h2o = train_h2o_orig.cbind(h2o.H2OFrame(y))
    train_h2o[-1] = train_h2o[-1].asfactor()
    glm_h2o = H2PGeneralizedLinearEstimator(family=""binomial"", nfolds=4, nlambdas=20,
                              lambda_search=True, max_active_predictors=100, seed=12345)
    glm_h2o.train(y=train_h2o.names[-1], training_frame=train_h2o)



Questions:




is there a version of the GLM model training function where the training matrix and response vector can be provided separately (as 
H2OFrame
s) so that I do not have to cbind and copy frames around.


the slowest step here is the `h2o.H2OFrame(.) (>30mins).  Is there a sparse matrix format which is more efficient (csc? coo? csr?)


in the past I have preferred writing a SVMLight file and reading it back.  But with that I have to create 20 of those on disk and read it back.  Is create a way of creating that file without the response variable?






Setup: 32cores, 512GB mem, RHEL7 (single user) / Python 3.6.9 / h2o 3.30.0.2 / jre 1.8.0_251","['python', 'h2o', 'glm']",ironv,https://stackoverflow.com/users/2789334/ironv,"1,058"
61504433,61504433,2020-04-29T14:37:01,2021-10-11 13:57:28Z,402,"MOJO model generation done in H2O(3.28.0.2). File named ep_gbm_grid03_model_49.zip exported.


When a MOJO import job (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html#mojo-models
) issued in H2O (3.28.0.2) Python terminal like below,


import h2o
h2o.init()
path = ""/Users/maya/Downloads/ep_gbm_grid03_model_49.zip""
imported_model = h2o.import_mojo(path)



the following exception dumped...


EnvironmentError: Job with key $03017f00000132d4ffffffff$_8faceff652ec107419c1688af40247ee failed with an exception: java.lang.IllegalArgumentException: colTypes values must be one of ""double"", ""float"", ""int"", ""long"", or ""string""
stacktrace: 
java.lang.IllegalArgumentException: colTypes values must be one of ""double"", ""float"", ""int"", ""long"", or ""string""
    at water.util.TwoDimTable.<init>(TwoDimTable.java:85)
    at hex.generic.GenericModelOutput.convertTable(GenericModelOutput.java:250)
    at hex.generic.GenericModelOutput.<init>(GenericModelOutput.java:35)
    at hex.generic.Generic$MojoDelegatingModelDriver.computeImpl(Generic.java:95)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:239)
    at hex.generic.Generic$MojoDelegatingModelDriver.compute2(Generic.java:71)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1455)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)","['python', 'h2o', 'illegalargumentexception', 'mojo']",XentneX,https://stackoverflow.com/users/1928229/xentnex,117
61474033,61474033,2020-04-28T06:32:46,2020-04-29 00:57:08Z,256,"I have a dataset with more than 100k rows and about 1k columns including the target column for a 
binary classification
 prediction problem. I am using 
H2O
 GBM (latest 3.30xx) in 
python
 with 5 folds cross validation and 80-20 train-test split. I have noticed that H2O is automatically stratifying it which is good. The problem I have is, I have this whole dataset from one product with some sub-products within it as a separate column or group. Each of these sub-product has decent size of 5k to 10k rows and therefore good to check separate model on each of them I thought. I am looking for if I can specify this sub-product groups for cross validation in H2O model training. Currently I am looping over these sub-products while doing a train-test split as it is not clear to me how to do it otherwise based on the document I have read so far. Is there any option I can use within H2O to have this sub-product column directly for cross validation? That way I have to control less all the model outputs in my scripts.

I hope the question is clear. If not, let me know. Thank you.","['python', 'machine-learning', 'h2o']",Manjit P.,https://stackoverflow.com/users/5906999/manjit-p,85
61460911,61460911,2020-04-27T14:15:36,2020-04-27 15:42:42Z,0,"I was wondering if there is a convenient way to train multiple h2o models from a nested data frame in R.
Assume, we have a dataset with the following structure and I want to train one model for each Species:


dataset(iris)
iris_nested<-iris%>%
  dplyr::mutate(dataset=dplyr::if_else(sample(1:nrow(iris))<100,""train"",""val""))%>%
  dplyr::group_by(Species,dataset)%>%
  tidyr::nest()%>%
  tidyr::pivot_wider(names_from = dataset,values_from = data)





Is there a way of loading and using the dataset into h2o without building a loop to break up the nested list? I would like to avoid the step of creating h2o objects for each row.


Edit:
For example to predict Sepal.Length with other numeric inputs, I would train a single model for row i with: 


library(h2o)
h2o.init()   
h2o_train<-as.h2o(iris_nested[[""train""]][[i]])
h2o_val<-as.h2o(iris_nested[[""val""]][[i]])

h2o_trainedmodel <- h2o.automl(
  x = c(""Sepal.Width"",""Petal.Length"",""Petal.Width""), 
  y = ""Sepal.Length"",
  training_frame = h2o_train,
  leaderboard_frame = h2o_val,
  project_name = ""run1"")



Afterward, extract and save the trained model and generate a mapping table, so that I know which model belongs to which species.","['r', 'h2o']",Unknown,,N/A
61435532,61435532,2020-04-26T03:45:43,2020-04-29 06:04:22Z,0,"I have a database of about 500G. It comprises of 16 tables, each containing 2 or 3 column (first column can be discarded) and 1,375,328,760 rows. I need all the tables to be joined as one dataframe in h2o as they are needed for running a prediction in an XGB model. I have tried to convert the individual sql tables into the h2o environment using as.h2o, and h2o.cbind them 2 or 3 tables at a time, until they are one dataset. However, I get this ""GC overhead limit exceeded: java.lang.OutOfMemoryError"", after converting 4 tables.
Is there a way around this?
My machine specs are 124G RAM, OS (Rhel 7.8), Root(1tb), Home(600G) and 2TB external HDD.
The model is run on this local machine and the max_mem_size is set at 100G. The details of the code are below.


library(data.table)
library(h2o)          
h2o.init(
  nthreads=14,          
  max_mem_size = ""100G"")    
h2o.removeAll() 

setwd(""/home/stan/Documents/LUR/era_aq"")

l1.hex <- as.h2o(d2)
l2.hex <- as.h2o(lai)
test_l1.hex <-h2o.cbind(l1.hex,l2.hex[,-1])
h2o.rm (l1.hex,l2.hex)
l3.hex <- as.h2o(lu100)
l4.hex <- as.h2o(lu1000)
test_l2.hex <-h2o.cbind(l3.hex,l4.hex[,-1])
h2o.rm(l3.hex,l4.hex)
l5.hex <- as.h2o(lu1250)
l6.hex <- as.h2o(lu250)
test_l3.hex <-h2o.cbind(l5.hex,l6.hex[,-1])
h2o.rm(l5.hex,l6.hex)
l7.hex <- as.h2o(pbl)
l8.hex <- as.h2o(msl)
test_l4.hex <-h2o.cbind(l7.hex,l8.hex[,-1])
h2o.rm(ll7.hex,l8.hex)

test.hex <-h2o.cbind(test_l1.hex,test_l2.hex[,-1],test_l3.hex[,-1],test_l4.hex[,-1])
test <- test.hex[,-1]
test[1:3,]```","['r', 'sqlite', 'h2o']",Unknown,,N/A
61420508,61420508,2020-04-25T03:10:30,2020-05-06 23:54:54Z,133,"I have time series data with two column: Date and Volume such as

enter image description here


I want to predict volume of next day, so how can I setup parameters?


P/S: i set forecast horizon equal 1 but output predict show only 5 last day in time series. :(","['prediction', 'h2o', 'driverless-ai']",Shenlong,https://stackoverflow.com/users/13288450/shenlong,11
61384553,61384553,2020-04-23T10:08:46,2020-04-23 10:08:46Z,563,"as per release (
https://www.h2o.ai/blog/h2o-release-3-26-yau/
) it is said that SHAP values can be retrieved from MOJO as well. However in there is no function such as 
h2o.mojo_predict_contributions
 or equivalent ?


Once model is imported : 
gbm_m=h2o.import_mojo('GBM_model_R.zip')

and 
h2o.predict_contributions(gbm_m,data)
 is run ( note data is already a h2o data frame and h20 cluster is active ) . Below is the output : 




There is another link (
http://docs.h2o.ai/sparkling-water/2.2/latest-stable/doc/tutorials/shap_values.html
) which doesn't give clear guidance on how to retrieve SHAP values in other than the sparking water h2o version. How can we extract SHAP values with a MOJO object directly without the need to spin off a cluster i.e. functions such as 
h2o.mojo_predict_df","['h2o', 'shap']",Learner_seeker,https://stackoverflow.com/users/7609862/learner-seeker,544
61322491,61322491,2020-04-20T12:25:49,2020-04-20 12:25:49Z,129,"I have I guess a moderately sized dataframe of ~500k rows and 200 columns with 8GB of memory.


My problem is that when I got to slice my data, even very small sized datasets when this gets trimmed down to 6k rows and 200 columns, that it just hangs and hangs for 10/15 min+.   Then if I hit the STOP button for python interactive and re-try the process happens in 2-3 seconds.  


I don't know why I can do my row-slicing in this 2-3 seconds normally. It is making it impossible to run programs as things just hang and hang and have to be manually stopped before it works.


I am following the approach laid out on the h2o webpage:


import h2o
h2o.init()

# Import the iris with headers dataset
path = ""http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris_wheader.csv""
df = h2o.import_file(path=path)

# Slice 1 row by index
c1 = df[15,:]
c1.describe

# Slice a range of rows
c1_1 = df[range(25,50,1),:]
c1_1.describe

# Slice using a boolean mask. The output dataset will include rows with a sepal length
# less than 4.6.
mask = df[""sepal_len""] < 4.6
cols = df[mask,:]
cols.describe

# Filter out rows that contain missing values in a column. Note the use of '~' to
# perform a logical not.
mask = df[""sepal_len""].isna()
cols = df[~mask,:]
cols.describe



The error message from the console is as follows.  I have this same error message repeated several times.:


/opt/anaconda3/lib/python3.7/site-packages/h2o/expr.py in (.0)
    149             return self._cache._id  # Data already computed under ID, but not cached
    150         assert isinstance(self._children,tuple)
--> 151         exec_str = ""({} {})"".format(self._op, "" "".join([ExprNode._arg_to_expr(ast) for ast in self._children]))
    152         gc_ref_cnt = len(gc.get_referrers(self))
    153         if top or gc_ref_cnt >= ExprNode.MAGIC_REF_COUNT:

~/opt/anaconda3/lib/python3.7/site-packages/h2o/expr.py in _arg_to_expr(arg)
    161             return ""[]""  # empty list
    162         if isinstance(arg, ExprNode):
--> 163             return arg._get_ast_str(False)
    164         if isinstance(arg, ASTId):","['python', 'h2o']",runningbirds,https://stackoverflow.com/users/3788557/runningbirds,"6,565"
61305294,61305294,2020-04-19T13:40:52,2020-04-19 17:59:10Z,608,"I run the following python code which results in a Java Heap Space error. Why is the garbage collector not releasing the used memory after returning from the method 
do_something()
? Do I have to manually release memory at the end of the method?  


import h2o 
import numpy as np
import pandas as pd


h2o.init(max_mem_size='2G')

df_input = pd.DataFrame(data=np.random.randn(10, 10))

def do_something(df):
    frame = h2o.H2OFrame(df)
    #predict some output based on frame 
    #more h2o objects are being created here eventually ..... 
    return

for i in range(10000000):
    do_something(df_input)
    print(h2o.ls())","['python', 'memory-management', 'h2o']",Unknown,,N/A
61277101,61277101,2020-04-17T17:16:57,2020-04-20 16:43:08Z,0,"So I am using h2o.ai to create a binomial classification model however when I use
as.h2o to convert my data sets. It takes my target variable's column header which is ""BUY""
and adds that to the levels so instead of just 2 levels 1 and 2 it becomes three levels which are
BUY, 1, and 2. This makes it multinomial and not wanted how do i fix this? 


when I run perfH2o this is the output:

H2OMultinomialMetrics: gbm

Test Set Metrics: 
=====================

MSE: (Extract with `h2o.mse`) 0.3260208
RMSE: (Extract with `h2o.rmse`) 0.5709823
Logloss: (Extract with `h2o.logloss`) 1.016186
 Mean Per-Class Error: 0.2755556
 R^2: (Extract with `h2o.r2`) -0.1913934
Confusion Matrix: Extract with `h2o.confusionMatrix(<model>, <data>)`)
=========================================================================
Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
   BUY NO YES  Error      Rate
BUY      1  0   0 0.0000 =   0 / 1    #see here it is taking the header and thinking it is a level
NO       0 16   9 0.3600 =  9 / 25
YES      0  7   8 0.4667 =  7 / 15
Totals   1 23  17 0.3902 = 16 / 41

Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>, <data>)`
=======================================================================
Top-3 Hit Ratios: 
 k hit_ratio
1 1  0.609756
2 2  0.975610
3 3  1.000000



Here is my code


#Getting packages
#install.packages(""dplyr"")
library(dplyr)
library(tidyverse)
library(tidyr)
#install.packages(""tidyquant"") #Used to quickly load the ""tidyverse"" (dplyr, tidyr, ggplot, etc) 
along with custom, 
#business-report-friendly ggplot themes. Also great for time series analysis (not featured)
library(tidyquant)
#install.packages(""unbalanced"")
library(unbalanced)#contains various methods for working with unbalanced data. I will be using 
ubSMOTE() function

#installing H20 latest stable release H20 is a professional machine learning package

# The following two commands remove any previously installed H2O packages for R.
#if (""package:h2o"" %in% search()) { detach(""package:h2o"", unload=TRUE) }
#if (""h2o"" %in% rownames(installed.packages())) { remove.packages(""h2o"") }

# Next, we download packages that H2O depends on.
#pkgs <- c(""RCurl"",""jsonlite"")
#for (pkg in pkgs) {
# if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
#}

# Now we download, install and initialize the H2O package for R.
#install.packages(""h2o"", type=""source"", repos=""http://h2o-release.s3.amazonaws.com/h2o/rel-yule/2/R"")

# Finally, let's load H2O and start up an H2O cluster
library(h2o)
h2o.init()

#Now getting the data
ngData <- read.csv(file.choose())

#Now I am going to create my Train, validation, and test set
splitPercentage1 <- .70
splitPercentage2 <- .5
numRows1 <- nrow(ngData)
sampleSize1 <- floor(splitPercentage1*numRows1)

set.seed(1)

idxTrain1 <- sample(1:numRows1, size = sampleSize1)
validationRaw <- ngData[-idxTrain1,]
trainRaw <- ngData[idxTrain1,]
#validation set created now time to make test set out of validation set

numRows2 <- nrow(validationRaw)
sampleSize2 <- floor(splitPercentage2*numRows2)
idxTrain2 <- sample(1:numRows2, size = sampleSize2)
testRaw <- validationRaw[-idxTrain2,]
validationRaw <- validationRaw[idxTrain2,]

#Now I have a randomly set train set, validation set, and test set

View(trainRaw)
View(testRaw)
View(validationRaw)

#all look good however we need our target variable ""BUY"" to be a factor not numeric
#also Buy = 1 Sell = 0 in the BUY column

trainRaw[,11] <- as.factor(trainRaw[,11])
testRaw[,11] <- as.factor(testRaw[,11])
validationRaw[,11] <- as.factor(validationRaw[,11])

View(trainRaw)
View(testRaw)
View(validationRaw)

#now to balance the data which i don't know if that is very necessary so I 
#will check how balanced it is

Buytable <- table(trainRaw$BUY)
Buydistr <- prop.table(Buytable)
Buydistr

 #very balanced with 52% sell and 47% buy so no need to balance

 h2o.no_progress()

#converting into h2o data frames
trainH20 <- as.h2o(trainRaw)
validH20 <- as.h2o(validationRaw)
testH20 <- as.h2o(testRaw)

#now to find a classification model

y <- ""BUY""
x <- setdiff(names(trainH20), y)

automl_models_h2o <- h2o.automl(
  x = x, 
  y = y,
  training_frame    = trainH20,
  validation_frame  = validH20,
  leaderboard_frame = testH20,
  max_runtime_secs  = 60
 )

 #time to extract the leading model

 NGLeader <- automl_models_h2o@leader

 #making predicitons using h2o.predict()

predH2o <- h2o.predict(NGLeader, newdata = testH20)
as_tibble(predH2o)

#now to check the performance
perfH2o <- h2o.performance(NGLeader, newdata = testH20)
perfH2o

 h2o.r2(perfH2o)
 #very bad r^2

  #turns out my model believes that BUY is one of the possible outcomes of Y so it is multinomial I 
 must fix that

 #######################################################################



Here is a glimpse() of my data:


Rows: 185


Columns: 11


$ ï..Month  April, July, August, August, July, February, September, January, March, February, June,...


$ East.Region           -12, 24, 26, 21, 19, -43, 25, -43, -15, -9, 27, -28, 26, -27, 22, 23, 32, -54, 21, 12, ...


$ Midwest.Region        -20, 20, 36, 29, 16, -47, 35, -38, -7, -4, 35, -31, 45, -27, 22, 29, 27, -56, 30, 14, -...


$ Mountain.Region       -4, 6, 4, 3, 2, -6, 3, -10, 2, 0, 9, -2, 5, -9, 5, 3, 6, -6, 4, 2, -4, 5, 5, 3, -1, -7,...


$ Pacific.Region        5, 5, 2, 0, -1, -10, 5, -13, 9, -1, 11, -3, 0, -14, 7, 0, 9, -11, 0, -3, -8, 5, 5, 6, 0...


$ South.Central.Region  12, 3, 2, -2, -2, -41, 37, -15, 35, 21, 18, 1, 20, -10, 5, -6, 32, -38, 12, -14, -6, 17...


$ Salt                  8, -5, -2, -5, -6, -19, 14, 13, 19, 5, -1, -1, 3, 15, -5, -3, 12, -8, 1, -13, -3, 3, -2...


$ NonSalt               3, 7, 4, 4, 3, -22, 22, -28, 18, 16, 18, 3, 17, -25, 10, -4, 19, -29, 11, -2, -3, 15, 1...


$ Total.Lower.48        -19, 58, 69, 51, 34, -149, 105, -119, 23, 7, 98, -63, 96, -87, 61, 49, 106, -163, 67, 1...


$ Flow.Change           -0.34, -0.06, 0.41, 3.64, -0.47, -0.10, 0.42, -0.51, -1.64, -1.08, -0.15, -0.27, 0.43, ...


$ BUY                   0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, ...","['r', 'h2o']",Unknown,,N/A
61230237,61230237,2020-04-15T13:41:12,2020-09-26 16:29:17Z,0,"I have installed h2o package on anaconda python, however, I get the following error on jupyter notebook:


import h2o



---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-1-accdebc0c7de> in <module>
----> 1 import h2o

ModuleNotFoundError: No module named 'h2o'



From the conda list I can see that the installed h2o version is the following:


conda list h2o



#
# Name                    Version                   Build  Channel
h2o                       3.18.0.2                      0    anaconda





(Is it normal that the build version is 0 here?)




The python version is 3.7.4.


I also tried the following


conda install -c h2oai h2o



and I get


# All requested packages already installed.



Any ideas why the h2o package isn't working?
Thanks for your time!




AMC, regarding the anaconda environments, after running:


conda info --envs



I get:


# conda environments:
#
base                  *  C:\Users\ncham***\AppData\Local\Continuum\anaconda3



As far as I can see, there is only one environment...","['python', 'anaconda', 'conda', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
61172226,61172226,2020-04-12T13:29:55,2020-04-12 15:36:06Z,779,"My decision engine is built on the python-flask framework with uWSGI and Nginx.
As part of evaluating a user through an HTTP request, I run scorecards with h2o==3.20.0.7 to generate a score to make a decision on the user. Given below some clarity on how I'm using h2o in my app


h2o.init()  # initialize 

predictions = h2o.mojo_predict_pandas(features_df, MODEL_MOJO_ZIP_FILE_PATH, MODEL_GENMODEL_JAR_PATH)  # generate score
# features_df -> pandas DF



H2o details from app start 


--------------------------  ----------------------------------------
H2O cluster uptime:         01 secs
H2O cluster timezone:       Etc/UTC
H2O data parsing timezone:  UTC
H2O cluster version:        3.20.0.7
H2O cluster version age:    1 year, 7 months and 10 days !!!
H2O cluster name:           H2O_from_python_unknownUser_t8cqu9
H2O cluster total nodes:    1
H2O cluster free memory:    1.656 Gb
H2O cluster total cores:    4
H2O cluster allowed cores:  4
H2O cluster status:         accepting new members, healthy
H2O connection url:         http://localhost:54321
H2O connection proxy:
H2O internal security:      False
H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4
--------------------------  ----------------------------------------



Both H2o (running as a separate service) and the flask app are running in the same server (3 - 8 servers under a load balancer). 


Sometimes the memory usage is steadily increasing and throwing 
Cannot allocate memory


while computing the scorecards. Then it is settling automatically by itself after sometimes. The scorecard runs along with the other rules (sequential run) under an HTTP request but the error is reporting only while computing scorecards. Assuming, it requires more memory as it involves with h2o. The traffic looks to be the same across this cycle. So I hope this is not due to the high traffic.


As per my investigation, some memory is hanging somewhere and it is not releasing.


I did the following workarounds
 to release the hanged memory and reduce the impact   


1
 GC in h2o from python 


https://aichamp.wordpress.com/2016/11/10/calling-h2o-garbage-collect-from-python


Python H2O Memory Management




Not experienced a positive impact. 




2
 Scheduled service restart - Gracefully replacing the old servers with new servers.




Experienced a positive impact. 60-70% of the errors are gone.




I would like to understand what is happening internally and introduce a proper fix rather than a workaround. Help would be highly appreciated. 


For your information,


I haven't tried


1
 updating H2o cluster to a new version as the current version is too old (1 year, 7 months and 11 days) - Agree that it is better to use the latest version but no guarantee that the same will not happen again and the effort required is also more in terms of validating the score, result, etc   


2
 I haven't limited the memory usage of H2o by using 
min_mem_size
 as I don't want the scorecard evaluation to fail. 


and 


I'm planning to
 


1
 add a memory profiler to easily understand the memory utilization of each piece/process related to my app


edit


2
 separate out h2o from the flask app and host it in different servers so that scaling is easy.
- still, the same issue is possible. 




I go through some memory profiler but still couldn't finalize one which is best for my current situation. I would like to get a suggestion on this as well. 




Thanks","['python', 'pandas', 'h2o']",Unknown,,N/A
61125642,61125642,2020-04-09T16:19:45,2020-04-09 23:50:54Z,477,"I am trying to replicate the GAM model example in the 
h2o documentation- GAM
, however, I am getting the following error:


*Error: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GAM model: GAM_model_R_1586448366888_1.  Details: ERRR on field: knots formation: knots not sorted in ascending order. Knots at index 0: 1,000000.  Knots at index 1: 0,000000*



I don't know why the error occurs, I copied and pasted exactly the same code as in the example.


The script I'm running is the same one that is in the h2o documentation example.


This is thecode:


# create frame knots
knots1 <- c('-1.99905699', '-0.98143075', '0.02599159', '1.00770987', '1.99942290')
frameKnots1 <- as.h2o(knots1)
knots2 <- c('-1.999821861', '-1.005257990', '-0.006716042', '1.002197392', '1.999073589')
frameKnots2 <- as.h2o(knots2)
knots3 <- c('-1.999675688', '-0.979893796', '0.007573327', '1.011437347', '1.999611676')
frameKnots3 <- as.h2o(knots3)

# import the dataset
h2o_data <- h2o.importFile(""https://s3.amazonaws.com/h2o-public-test-data/smalldata/glm_test/multinomial_10_classes_10_cols_10000_Rows_train.csv"")

# Convert the C1, C2, and C11 columns to factors
h2o_data[""C1""] <- as.factor(h2o_data[""C1""])
h2o_data[""C2""] <- as.factor(h2o_data[""C2""])
h2o_data[""C11""] <- as.factor(h2o_data[""C11""])

# split into train and test sets
h2o_data.splits <- h2o.splitFrame(data=h2o_data, ratios=.8)
train <- h2o_data.splits[[1]]
test <- h2o_data.splits[[2]]

# Set the predictor and response columns
predictors <- colnames(train[1:2])
response <- 'C11'

# specify the knots array
numKnots <- c(5,5,5)

# build the GAM model
gam_model <- h2o.gam(x=predictors,
                     y=response,
                     training_frame = train,
                     family='multinomial',
                     gam_columns=c(""C6"",""C7"",""C8""),
                     scale=c(1,1,1),
                     num_knots=numKnots,
                     knot_ids=c(h2o.keyof(frameKnots1), h2o.keyof(frameKnots2), h2o.keyof(frameKnots3)))



Thank you.","['machine-learning', 'h2o', 'gam']",Amarth Gûl,https://stackoverflow.com/users/7693707/amarth-g%c3%bbl,"1,068"
61098457,61098457,2020-04-08T10:25:27,2020-04-08 10:25:27Z,110,"I build application with h2o. Each call to my application do prediction on H2O.
H2O instant is starting with application, then each call is connecting to this instant.


My problem is that with a time my free memory counter is decreasing.


This is my logs during connecting to existed H2O instance.


2020-04-08 12:11:23.821 --------------------------  ---------------------------------------------------
2020-04-08 12:11:23.821 Python version:             3.6.4 final
2020-04-08 12:11:23.821 H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4
2020-04-08 12:11:23.821 H2O internal security:      False
2020-04-08 12:11:23.821 H2O connection proxy:
2020-04-08 12:11:23.821 H2O connection url:         http://localhost:54321
2020-04-08 12:11:23.821 H2O cluster status:         locked, healthy
2020-04-08 12:11:23.821 H2O cluster allowed cores:  2
2020-04-08 12:11:23.821 H2O cluster total cores:    2
2020-04-08 12:11:23.821 H2O cluster free memory:    5.903 Gb
2020-04-08 12:11:23.821 H2O cluster total nodes:    1
2020-04-08 12:11:23.821 H2O cluster name:           H2O_from_python_cdsw_dwwqwt
2020-04-08 12:11:23.821 H2O cluster version age:    1 year and 7 days !!!
2020-04-08 12:11:23.821 H2O cluster version:        3.24.0.1
2020-04-08 12:11:23.821 H2O data parsing timezone:  UTC
2020-04-08 12:11:23.821 H2O cluster timezone:       Europe/Zurich
2020-04-08 12:11:23.821 H2O cluster uptime:         39 mins 45 secs
2020-04-08 12:11:23.821 --------------------------  ---------------------------------------------------



2020-04-08 12:11:46.578 --------------------------  ---------------------------------------------------
2020-04-08 12:11:46.578 Python version:             3.6.4 final
2020-04-08 12:11:46.578 H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4
2020-04-08 12:11:46.578 H2O internal security:      False
2020-04-08 12:11:46.578 H2O connection proxy:
2020-04-08 12:11:46.578 H2O connection url:         http://localhost:54321
2020-04-08 12:11:46.578 H2O cluster status:         locked, healthy
2020-04-08 12:11:46.578 H2O cluster allowed cores:  2
2020-04-08 12:11:46.578 H2O cluster total cores:    2
2020-04-08 12:11:46.577 H2O cluster free memory:    5.890 Gb
2020-04-08 12:11:46.577 H2O cluster total nodes:    1
2020-04-08 12:11:46.577 H2O cluster name:           H2O_from_python_cdsw_v1eq0y
2020-04-08 12:11:46.577 H2O cluster version age:    1 year and 7 days !!!
2020-04-08 12:11:46.577 H2O cluster version:        3.24.0.1
2020-04-08 12:11:46.577 H2O data parsing timezone:  UTC
2020-04-08 12:11:46.577 H2O cluster timezone:       Europe/Zurich
2020-04-08 12:11:46.577 H2O cluster uptime:         40 mins 8 secs
2020-04-08 12:11:46.577 --------------------------  ---------------------------------------------------



It works until it has memory and then it stopped working (because of OOM). How can I handle with this case? Can I removing something in schedule (old logs) to keep the level of free memory constant?","['python-3.x', 'memory', 'h2o']",CezarySzulc,https://stackoverflow.com/users/6422477/cezaryszulc,"1,999"
61071343,61071343,2020-04-07T01:17:54,2020-04-07 03:56:19Z,0,"I have built a GBM model in R with the below code.


gbm_model_sample <- h2o.gbm(x = c(1:78,80:688), y =79, training_frame = train.h2o, seed = 0xDECAF,ntrees = 1000, max_depth = 4,learn_rate = 0.1,stopping_rounds=50,min_rows = 50,distribution =""bernoulli"",ignore_const_col=F,
                   histogram_type='QuantilesGlobal',sample_rate=0.7,col_sample_rate=0.7,keep_cross_validation_models = T)



The model gets built and i save the Mojo object as :


h2o.download_mojo(gbm_model_sample,get_genmodel_jar = T)



which is saved as ""GBM_model_R_1586221409024_1.zip"" in my working directory.


Now i use function 
h2o.mojo_predict_csv
 and/or 
h2o.mojo_predict_df
 to predict on test data frame which is where i get the error as below 


for 
h2o.mojo_predict_csv


h2o.mojo_predict_csv('Test_sample_.csv','GBM_model_R_1586221409024_1.zip',genmodel_jar_path = 'h2o-genmodel.jar',verbose = F)





for 
h2o.mojo_predict_df


h2o.mojo_predict_df(test, 'GBM_model_R_1586221409024_1.zip',verbose = T)





when i use the same test and use the within R 
h2o.predict
 it works completely fine , however the above two codes which had been working fine for me before have started giving the errors as above. My packages loaded are as below. What is causing this error? i haven't manage to find much info on this online.


library(rJava)
require(h2o)
require(readr)
require(dplyr)
require(forcats)
require(ggplot2)
require(scales)
require(caret)
require(stringr)
library(data.table)
require(getPass)","['java', 'r', 'h2o', 'gbm']",Learner_seeker,https://stackoverflow.com/users/7609862/learner-seeker,544
60918535,60918535,2020-03-29T17:58:51,2022-06-15 00:43:48Z,0,"I am currently modelling using H20, nonetheless i am working with Scikit learn pipelines.




First i created the baseline model with the following code:




clf_pipe = Pipeline(
        steps = [(""impute missing"", replace_missing_vars),
                 (""scale"", scale),
                 (""dim_reduction"", pca),
                 (""classifier"", gbm)])

clf = clf_pipe.fit(train[x], train[y])
r2 = clf[len(clf)-1].r2(test[x], test[y])





Now i want to hypertune the model by running a gridsearch as follows:




parameters = {'ntree': [24, 50, 100], 'max_depth': [5,10], 'learn_rate':[0.25, 0.5, 0.65]}

gs_clf = GridSearchCV(clf_pipe, param_grid=params)
clf = gs_clf.fit(train[x], train[y])





But i get the following explicit error message (It is not incomplete, it ends like that):




/usr/local/lib/python3.7/site-packages/sklearn/utils/multiclass.py in type_of_target(y)
    239     if not valid:
    240         raise ValueError('Expected array-like (array or non-string sequence), '
--> 241                          'got %r' % y)
    242 
    243     sparse_pandas = (y.__class__.__name__ in ['SparseSeries', 'SparseArray'])

ValueError: Expected array-like (array or non-string sequence), got



If you wonder how does the train data looks like, this is the dataframe structure:


trin[x] = {'nartd_share': 'real',
 'nanrtd_share': 'real',
 'hot_beverages_share': 'real',
 'alcoholic_beverages_share': 'real',
 'all_beverages_share': 'int',
 'pfand_share': 'int',
 'if_top7_cities': 'enum',
 'opening_days': 'real',
 'opening_hours': 'real',
 'closing_hours': 'real',
 'open_at_b_of': 'real',
 'close_at_e_of': 'real',
 'busiest_at': 'real',
 'most_revenue_at': 'real',
 'opening_at_cosine': 'real',
 'closing_at_cosine': 'real',
 'busiest_at_cosine': 'real',
 'most_revenue_at_cosine': 'real',
 'weekend_opening_hours': 'real',
 'weekday_opening_hours': 'real',
 'avg_overnight_hours': 'real',
 'if_overnight': 'enum',
 'if_sunday': 'enum',
 'monthly_revenue': 'real',
 'monthly_quantity': 'real',
 'monthly_alcohol_revenue': 'real',
 'monthly_7vat_share': 'real',
 'weekly_revenue': 'real',
 'weekly_quantity': 'real',
 'weekly_alcohol_revenue': 'real',
 'weekly_7vat_share': 'real',
 'daily_revenue': 'real',
 'daily_quantity': 'real',
 'daily_alcohol_revenue': 'real',
 'daily_7vat_share': 'real',
 'avg_alcohol_price': 'real',
 'avg_nartd_price': 'real',
 'max_alcohol_price': 'real',
 'max_nartd_price': 'real',
 'top1_product': 'enum',
 'top2_product': 'enum',
 'top3_product': 'enum'}

train[y] = {'segment': 'enum'}","['python', 'scikit-learn', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
60797040,60797040,2020-03-22T07:52:11,2020-03-22 09:11:07Z,0,"Rfmodel<-h2o.grid(""randomForest"",
              search_criteria = list(
                strategy=""RamdomDiscrete"",
                stopping_metric =""mse"",
                stopping_tolerance= 0.001,
                stopping_rounds=5,
                max_runtime_secs=240
              ),
              hyper_params = list(
                ntrees=c(50,100,200,250),
                mtries=c(1,2,4,6),
                sample_rate=c(0.3,0.5,0.75,0.90),
                col_sample_rate_per_tree=c(0.25,0.50,0.75,0.1)
              ),
              x=8,y=1:7,training_frame = data.train,nfolds=4,max_depth=50,
              stopping_metric=""misclassification"",
              stopping_tolerance=0,
              stopping_rounds=5,
              seed=2
              )



Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:


Can't parse the search_criteria dictionary; got error: search_criteria.strategy for raw value: {""strategy""=""RamdomDiscrete"",""stopping_metric""=""misclassification"",""stopping_tolerance""=0.001,""stopping_rounds""=5,""max_runtime_secs""=240}


Please help me to solve this.","['r', 'h2o']",Subhadip Datta,https://stackoverflow.com/users/11991386/subhadip-datta,29
60709464,60709464,2020-03-16T16:12:51,2020-03-16 16:18:06Z,64,I cannot import h2o when even when it's successfully installed.,"['python', 'h2o']",Chenying Gao,https://stackoverflow.com/users/8419421/chenying-gao,310
60696897,60696897,2020-03-15T19:41:55,2020-03-16 00:48:52Z,437,"What is the external  CSV file maximum size that can be imported in h2o Flow?is there any file size limit?


Thanks
Sarvendra","['machine-learning', 'data-science', 'h2o', 'h2o4gpu']",Sarvendra Singh,https://stackoverflow.com/users/10882376/sarvendra-singh,13
60667920,60667920,2020-03-13T09:29:45,2020-03-13 14:42:50Z,179,"I feel really dumb right now, but, what I am dealing with for the last 15 minutes is that I want to find detailed 
h2o
 documentation.


My problem is that I have 
h2o.save_model()
 method and I want to know, which parameters to use and how. So, I wrote ""python h2o.save_model()"" to 
Google
 and I was expecting something similar to 
this
 in case of writing ""python pandas.DataFrame.groupby"". However, the only link I found is 
this one
, which does not provide a detailed description of a method.


What am I doing wrong? Does detailed h2o documentation exist? If so, could you provide me a link, please?","['python', 'documentation', 'h2o']",Jaroslav Bezděk,https://stackoverflow.com/users/7122272/jaroslav-bezd%c4%9bk,"7,545"
60617031,60617031,2020-03-10T11:49:24,2020-03-10 11:50:13Z,183,"I'm currently working with h2o.xgboost in h2o version 3.26.0.2 and i get the 
java.lang.NullPointerException
 (full errror below).


Dataset is 25 GB in csv
 with 6.000.000 Rows (trian + test) and the cluster info that I use is: 


R is connected to the H2O cluster: 
    H2O cluster uptime:         30 minutes 18 seconds 
    H2O cluster timezone:       Europe/Madrid 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.26.0.2 
    H2O cluster version age:    7 months and 12 days !!! 
    H2O cluster name:           H2O_started_from_R_xxx
    H2O cluster total nodes:    1 
    H2O cluster total memory:   343.27 GB 
    H2O cluster total cores:    1 
    H2O cluster allowed cores:  48 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        60576 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4 
    R Version:                  R version 3.5.1 (2018-07-02) 



Data h2o Types are:


 types   N
1:   int 319
2:  real 316



And My code: 


start <- proc.time()
model_trans.h2o <- h2o::h2o.xgboost( 
  distribution              = ""gaussian""
  ,model_id                 = ""xgb_test""
  ,training_frame           = data_train.h2o
  ,validation_frame         = data_test.h2o
  ,x                        = vars
  ,y                        = target_str
  ,seed                     = 1234
  ,ntrees                   = 1500
  ,learn_rate               = 0.08
  ,col_sample_rate_per_tree = 0.8
  ,max_dept                 = 5
  ,verbose                  = F
)
end <- proc.time(); end - start;



The error is given usually after 2 mins and I have made the following tests:




Use less columns. With 
500 columns is working fine
 (I did not use a random sample just 1:500 from colnames).


Use less data. If I reduce the data, let's say 
2.000.000 Rows it's working fine
 also.




After this tests I guessed that what is happening it's that somehow h2o.xgboost is not prepared to handle this much data and and when tries to expand it's ""matrix"" is crushing.


Could it be an error of the h2o version?


Can it be solved without updating the version? Since h2o models are not compatible cross versions this can be a problem for me.


Thanks in advance!


Note 1:
 


I've done my previous research on the subject and only found:


What is a NullPointerException, and how do I fix it?
 SO discussion abount null pointer exeption


https://0xdata.atlassian.net/browse/PUBDEV-6921
 Some h2o forum without clear answer   (this second one is pretty much useless)


Note 2:
 The full error is:


java.lang.NullPointerException

java.lang.NullPointerException
    at hex.tree.xgboost.matrix.SparseMatrixFactory$NestedArrayPointer.set(SparseMatrixFactory.java:87)
    at hex.tree.xgboost.matrix.SparseMatrixFactory$InitializeCSRMatrixFromChunkIdsMrFun.map(SparseMatrixFactory.java:178)
    at water.LocalMR.compute2(LocalMR.java:84)
    at water.LocalMR.compute2(LocalMR.java:76)
    at water.LocalMR.compute2(LocalMR.java:76)
    at water.LocalMR.compute2(LocalMR.java:76)
    at water.LocalMR.compute2(LocalMR.java:76)
    at water.LocalMR.compute2(LocalMR.java:76)
    at water.LocalMR.compute2(LocalMR.java:76)
    at water.LocalMR.compute2(LocalMR.java:76)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1417)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)","['java', 'nullpointerexception', 'xgboost', 'h2o']",Eric Vilarrasa Canadell,https://stackoverflow.com/users/6008810/eric-vilarrasa-canadell,11
60593198,60593198,2020-03-08T23:55:15,2020-05-23 00:43:37Z,256,"I have uninstalled and re-installed the latest version of 
datatable
 from the repo


16:42:49/seirdc2.March8.in $sudo pip3 install 'datatable==0.10.1' 
Successfully installed datatable-0.10.1



Let's see the version:


import datatable as dt
print(f'datatable version={dt.__version__}')



Um 
oops
 !


Traceback (most recent call last):
  File ""/git/corona/python/pointr/experiments/python/datatable.py"", line 18, in <module>
    import datatable as dt
  File ""/git/corona/python/pointr/experiments/python/datatable.py"", line 19, in <module>
    print(f'datatable version={dt.__version__}')
AttributeError: module 'datatable' has no attribute '__version__'



But why?




Note: I have seen other strangeness with this package: e.g. not finding 
Frame
 - though not consistently.","['python', 'h2o', 'py-datatable']",Pasha,https://stackoverflow.com/users/958624/pasha,"6,540"
60563604,60563604,2020-03-06T11:56:57,2021-02-11 13:08:59Z,0,"Using the python library, I'm training a GLM as part of a H2O ensemble that I'm creating:


(relevant snippet from script):


from h2o.estimators.glm import H2OGeneralizedLinearEstimator
estimator = H2OGeneralizedLinearEstimator(
            nfolds=5, keep_cross_validation_predictions=True,
            fold_assignment='Modulo',
            solver='COORDINATE_DESCENT',
            alpha=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
            lambda_=[
                319.3503133509223, 198.32195498930167,
                123.16129399741205, 76.48525015768037,
                47.49863615273374, 29.49745776759067,
                18.318421016404645, 11.376049799889723,
                7.064719657533626, 4.387310597042732,
                2.7245942101039184, 1.6920191642541902,
                1.050772567007053, 0.652547566186266,
                0.4052430939917762, 0.2516628269534475,
                0.1562868791824152, 0.09705679976763945,
                0.060273916981481074, 0.03743112359966524,
                0.023245361909429163, 0.01443576356615629,
                0.008964853743724, 0.005567326056432596,
                0.003457403802078978, 0.0021471063360513445,
                0.0013333894107306014, 0.0008280574142025093,
                0.0005142376830786764, 0.0003193503133509216],
            lambda_search=True, nlambdas=30, max_iterations=300,
            objective_epsilon=0.0001, gradient_epsilon=1.00E-06,
            link='identity', lambda_min_ratio=1.00E-06,
            max_active_predictors=5000, obj_reg=1.03E-05,
            max_runtime_secs=342.6666667)

estimator.train(x=predictors, y=response, training_frame=df_h20)




I run this training in parallel with other dataframes containing different combinations of features


with futures.ThreadPoolExecutor(
                    max_workers=len(persona_list)) as executor:
                    future_list = {
                        executor.submit(
                            AVM_H2O.regressor,
                            area,
                            [x[1]],
                            dataset,
                            h20_mms_GB,
                            timestamp,
                            datestring,
                            S3_upload_bucket,
                            logfile,
                            54320 + x[0]): x for x in enumerate(persona_list, 1)}
                    for future in futures.as_completed(future_list):
                        future.result()



I do this many times over many different datasets and I only seemingly randomly run in to this error. When I try to recreate the error I can't seem to do so.


The full error message is:


H2OResponseError: ModelBuilderErrorV3  (water.exceptions.H2OModelBuilderIllegalArgumentException):
    timestamp = 1583433807040
    error_url = '/3/ModelBuilders/glm'
    msg = 'Illegal argument(s) for GLM model: GLM_model_python_1583433786455_5.  Details: ERRR on field: _train: Missing training frame: py_7_sid_b8c3'
    dev_msg = 'Illegal argument(s) for GLM model: GLM_model_python_1583433786455_5.  Details: ERRR on field: _train: Missing training frame: py_7_sid_b8c3'
    http_status = 412
    values = {'messages': [{'_log_level': 1, '_field_name': '_train', '_message': 'Missing training frame: py_7_sid_b8c3'}, {'_log_level': 5, '_field_name': '_balance_classes', '_message': 'Not applicable since class balancing is not required for GLM.'}, {'_log_level': 5, '_field_name': '_max_after_balance_size', '_message': 'Not applicable since class balancing is not required for GLM.'}, {'_log_level': 5, '_field_name': '_class_sampling_factors', '_message': 'Not applicable since class balancing is not required for GLM.'}, {'_log_level': 5, '_field_name': '_tweedie_variance_power', '_message': 'Only applicable with Tweedie family'}, {'_log_level': 5, '_field_name': '_tweedie_link_power', '_message': 'Only applicable with Tweedie family'}, {'_log_level': 5, '_field_name': '_theta', '_message': 'Only applicable with Negative Binomial family'}], 'algo': 'GLM', 'parameters': {'_train': {'name': 'py_7_sid_b8c3', 'type': 'Key'}, '_valid': None, '_nfolds': 5, '_keep_cross_validation_models': True, '_keep_cross_validation_predictions': True, '_keep_cross_validation_fold_assignment': False, '_parallelize_cross_validation': True, '_auto_rebalance': True, '_seed': -1, '_fold_assignment': 'Modulo', '_categorical_encoding': 'AUTO', '_max_categorical_levels': 10, '_distribution': 'AUTO', '_tweedie_power': 1.5, '_quantile_alpha': 0.5, '_huber_alpha': 0.9, '_ignored_columns': None, '_ignore_const_cols': True, '_weights_column': None, '_offset_column': None, '_fold_column': None, '_check_constant_response': True, '_is_cv_model': False, '_score_each_iteration': False, '_max_runtime_secs': 342.6666667, '_stopping_rounds': 3, '_stopping_metric': 'deviance', '_stopping_tolerance': 0.0001, '_response_column': 'price_lr', '_balance_classes': False, '_max_after_balance_size': 5.0, '_class_sampling_factors': None, '_max_confusion_matrix_size': 20, '_checkpoint': None, '_pretrained_autoencoder': None, '_custom_metric_func': None, '_custom_distribution_func': None, '_export_checkpoints_dir': None, '_standardize': True, '_useDispersion1': False, '_family': 'gaussian', '_rand_family': None, '_link': 'identity', '_rand_link': None, '_solver': 'COORDINATE_DESCENT', '_tweedie_variance_power': 0.0, '_tweedie_link_power': 1.0, '_theta': 1e-10, '_invTheta': 10000000000.0, '_alpha': [0.0, 0.2, 0.4, 0.6, 0.8, 1.0], '_lambda': [319.3503133509223, 198.32195498930167, 123.16129399741205, 76.48525015768037, 47.49863615273374, 29.49745776759067, 18.318421016404645, 11.376049799889723, 7.064719657533626, 4.387310597042732, 2.7245942101039184, 1.6920191642541902, 1.050772567007053, 0.652547566186266, 0.4052430939917762, 0.2516628269534475, 0.1562868791824152, 0.09705679976763945, 0.060273916981481074, 0.03743112359966524, 0.023245361909429163, 0.01443576356615629, 0.008964853743724, 0.005567326056432596, 0.003457403802078978, 0.0021471063360513445, 0.0013333894107306014, 0.0008280574142025093, 0.0005142376830786764, 0.0003193503133509216], '_startval': None, '_calc_like': False, '_random_columns': None, '_missing_values_handling': None, '_prior': -1.0, '_lambda_search': True, '_HGLM': False, '_nlambdas': 30, '_non_negative': False, '_exactLambdas': False, '_lambda_min_ratio': 1e-06, '_use_all_factor_levels': False, '_max_iterations': 300, '_intercept': True, '_beta_epsilon': 0.0001, '_objective_epsilon': 0.0001, '_gradient_epsilon': 1e-06, '_obj_reg': 1.03e-05, '_compute_p_values': False, '_remove_collinear_columns': False, '_interactions': None, '_interaction_pairs': None, '_early_stopping': True, '_beta_constraints': None, '_plug_values': None, '_max_active_predictors': 5000, '_stdOverride': False}, 'error_count': 2}
    exception_msg = 'Illegal argument(s) for GLM model: GLM_model_python_1583433786455_5.  Details: ERRR on field: _train: Missing training frame: py_7_sid_b8c3'
    stacktrace =
        water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GLM model: GLM_model_python_1583433786455_5.  Details: ERRR on field: _train: Missing training frame: py_7_sid_b8c3
        water.exceptions.H2OModelBuilderIllegalArgumentException.makeFromBuilder(H2OModelBuilderIllegalArgumentException.java:19)
        hex.ModelBuilder.trainModelOnH2ONode(ModelBuilder.java:304)
        water.api.ModelBuilderHandler.handle(ModelBuilderHandler.java:64)
        water.api.ModelBuilderHandler.handle(ModelBuilderHandler.java:17)
        water.api.RequestServer.serve(RequestServer.java:471)
        water.api.RequestServer.doGeneric(RequestServer.java:301)
        water.api.RequestServer.doPost(RequestServer.java:227)
        javax.servlet.http.HttpServlet.service(HttpServlet.java:755)
        javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
        org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
        org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
        org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
        org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
        org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
        org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)
        org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        water.webserver.jetty8.Jetty8ServerAdapter$LoginHandler.handle(Jetty8ServerAdapter.java:119)
        org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)
        org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        org.eclipse.jetty.server.Server.handle(Server.java:370)
        org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
        org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)
        org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)
        org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)
        org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)
        org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)
        org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)
        org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)
        org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        java.base/java.lang.Thread.run(Thread.java:834)
    parameters = {'__meta': {'schema_version': 3, 'schema_name': 'GLMParametersV3', 'schema_type': 'GLMParameters'}, 'model_id': None, 'training_frame': None, 'validation_frame': None, 'nfolds': 5, 'keep_cross_validation_models': True, 'keep_cross_validation_predictions': True, 'keep_cross_validation_fold_assignment': False, 'parallelize_cross_validation': True, 'distribution': 'AUTO', 'tweedie_power': 1.5, 'quantile_alpha': 0.5, 'huber_alpha': 0.9, 'response_column': {'__meta': {'schema_version': 3, 'schema_name': 'ColSpecifierV3', 'schema_type': 'VecSpecifier'}, 'column_name': 'price_lr', 'is_member_of_frames': None}, 'weights_column': None, 'offset_column': None, 'fold_column': None, 'fold_assignment': 'Modulo', 'categorical_encoding': 'AUTO', 'max_categorical_levels': 10, 'ignored_columns': None, 'ignore_const_cols': True, 'score_each_iteration': False, 'checkpoint': None, 'stopping_rounds': 3, 'max_runtime_secs': 342.6666667, 'stopping_metric': 'deviance', 'stopping_tolerance': 0.0001, 'custom_metric_func': None, 'custom_distribution_func': None, 'export_checkpoints_dir': None, 'seed': -1, 'family': 'gaussian', 'rand_family': None, 'tweedie_variance_power': 0.0, 'tweedie_link_power': 1.0, 'theta': 1e-10, 'solver': 'COORDINATE_DESCENT', 'alpha': [0.0, 0.2, 0.4, 0.6, 0.8, 1.0], 'lambda': [319.3503133509223, 198.32195498930167, 123.16129399741205, 76.48525015768037, 47.49863615273374, 29.49745776759067, 18.318421016404645, 11.376049799889723, 7.064719657533626, 4.387310597042732, 2.7245942101039184, 1.6920191642541902, 1.050772567007053, 0.652547566186266, 0.4052430939917762, 0.2516628269534475, 0.1562868791824152, 0.09705679976763945, 0.060273916981481074, 0.03743112359966524, 0.023245361909429163, 0.01443576356615629, 0.008964853743724, 0.005567326056432596, 0.003457403802078978, 0.0021471063360513445, 0.0013333894107306014, 0.0008280574142025093, 0.0005142376830786764, 0.0003193503133509216], 'lambda_search': True, 'early_stopping': True, 'nlambdas': 30, 'standardize': True, 'missing_values_handling': 'MeanImputation', 'plug_values': None, 'non_negative': False, 'max_iterations': 300, 'beta_epsilon': 0.0001, 'objective_epsilon': 0.0001, 'gradient_epsilon': 1e-06, 'obj_reg': 1.03e-05, 'link': 'identity', 'rand_link': None, 'startval': None, 'random_columns': None, 'calc_like': False, 'intercept': True, 'HGLM': False, 'prior': -1.0, 'lambda_min_ratio': 1e-06, 'beta_constraints': None, 'max_active_predictors': 5000, 'interactions': None, 'interaction_pairs': None, 'balance_classes': False, 'class_sampling_factors': None, 'max_after_balance_size': 5.0, 'max_confusion_matrix_size': 20, 'max_hit_ratio_k': 0, 'compute_p_values': False, 'remove_collinear_columns': False}
    messages = [{'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'train', 'message': 'Missing training frame: py_7_sid_b8c3'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'balance_classes', 'message': 'Not applicable since class balancing is not required for GLM.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_after_balance_size', 'message': 'Not applicable since class balancing is not required for GLM.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'class_sampling_factors', 'message': 'Not applicable since class balancing is not required for GLM.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'tweedie_variance_power', 'message': 'Only applicable with Tweedie family'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'tweedie_link_power', 'message': 'Only applicable with Tweedie family'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'theta', 'message': 'Only applicable with Negative Binomial family'}]
    error_count = 2



Because I can't reproduce the error, I'm struggling to get an idea of what's causing it. Any help would be greatly appreciated.","['python', 'h2o', 'glm', 'http-status-code-412']",David Jacques,https://stackoverflow.com/users/7535311/david-jacques,348
60509766,60509766,2020-03-03T14:45:58,2021-04-09 05:55:30Z,0,"I am trying to convert the confusion matrix to a python 2D list so I can access the components.


I am getting an error when trying to convert a confusion matrix to a data frame.


import h2o
from h2o.estimators.gbm import H2OGradientBoostingEstimator
import pandas as pd

h2o.init()

training_file = ""AirlinesTrain.csv""
train = h2o.import_file(training_file)
response_col = ""IsDepDelayed""
distribution = ""multinomial""
project_name = ""airlines""
problem_type = ""binary-classification""
predictors = train.columns
gbm = H2OGradientBoostingEstimator(nfolds=3,
                                    distribution=distribution)
gbm.train(x=predictors,
           y=response_col,
           training_frame=train)
print(""gbm.confusion_matrix(train).as_data_frame()"")
print(gbm.confusion_matrix(train).as_data_frame())#This errors AttributeError: 'H2OFrame' object has no attribute 'lower'



NOTE: if I use the cars dataset, there are no errors:


cars = h2o.import_file(""https://s3.amazonaws.com/h2o-public-test-data/smalldata/junit/cars_20mpg.csv"")
cars[""cylinders""] = cars[""cylinders""].asfactor()
#r = cars[0].runif()
#train = cars[r > .2]
#valid = cars[r <= .2]
train=cars
response_col = ""cylinders""
distribution = ""multinomial""
predictors = [""displacement"",""power"",""weight"",""acceleration"",""year""]",['h2o'],Unknown,,N/A
60496404,60496404,2020-03-02T20:48:06,2020-11-19 11:57:43Z,839,"I use H2O to build and compare models but I wanted to know if there is a feature that allows me to split the data into train and test in a stratified way?


I know that for cross validation the option fold_assignment exist and I use it.


My question is more on the manipulation of the data and when it needs to be split before any model is built.


I had a look a this 
page
 but it does not specify if stratified splitting is done for classification.


Maybe for classification problems it is implicit and the stratified splitting is always achieved?","['machine-learning', 'h2o']",Donald Duck,https://stackoverflow.com/users/4284627/donald-duck,"8,822"
60475543,60475543,2020-03-01T13:22:41,2020-03-02 17:04:18Z,0,"I'm trying to use H20 randomForest for multiclass classification in R, but when I run the code, the randomForest always comes out as a regression model - despite the target variable being a factor. I am trying to predict 'Gradient', a factor with 5 levels, by one other factor 'Period' with 4 levels, and 21 numerical predictors. 


Any help would be appreciated. Code below....


>str(df)

Class 'H2OFrame' <environment: 0x000001f6b361abe0>

 - attr(*, ""op"")= chr "":=""
 - attr(*, ""eval"")= logi TRUE
 - attr(*, ""id"")= chr ""RTMP_sid_aecc_35""
 - attr(*, ""nrow"")= int 63878
 - attr(*, ""ncol"")= int 22
 - attr(*, ""types"")=List of 22
 - attr(*, ""data"")='data.frame':    10 obs. of  22 variables:

  ..$ Gradient: Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1

  ..$ Period  : Factor w/ 4 levels ""Dawn"",""Day"",""Dusk"",..: 2 2 2 2 2 2 2 2 2 2

  ..$ AC1     : num  1792 1793 1790 1790 1797 ...
  ..$ AC2     : num  316 316 318 317 324 ...
  ..$ AC3     : num  972 972 974 975 979 ...



etc for remaining numerical predictors.


>splits <- h2o.splitFrame(df,  c(0.6,0.2),  seed=1234)
>train <- h2o.assign(splits[[1]], ""train.hex"")  
>valid <- h2o.assign(splits[[2]], ""valid.hex"")   
>test <- h2o.assign(splits[[3]], ""test.hex"")   

>str(train)
        Class 'H2OFrame' <environment: 0x000002266fac7d40> 
     - attr(*, ""op"")= chr ""assign""
     - attr(*, ""id"")= chr ""train.hex""
     - attr(*, ""nrow"")= int 38259
     - attr(*, ""ncol"")= int 22
     - attr(*, ""types"")=List of 22
    - attr(*, ""data"")='data.frame': 10 obs. of  22 variables:
      ..$ Gradient: Factor w/ 5 levels ""LB"",""LU"",""PB"",..: 1 1 1 1 1 1 1 1 1 1
      ..$ Period  : Factor w/ 4 levels ""Dawn"",""Day"",""Dusk"",..: 2 2 2 2 2 2 2 2 2 2
      ..$ AC1     : num  1793 1797 1796 1805 1803 ...
      ..$ AC2     : num  316 324 322 322 323 ...
      ..$ AC3     : num  972 979 979 988 986 ...
      ..$ AC4     : num  663 662 664 673 670 ...
      ..$ AC5     : num  828 825 824 824 825 ...
      ..$ AD1     : num  1.22 1.42 1.73 2.25 1.99 ...
      ..$ AD2     : num  1.1 1.27 1.35 1.38 1.38 ...
      ..$ AD3     : num  1.22 1.42 1.72 2.24 1.99 ...
      ..$ AD4     : num  1.87 1.53 2.07 2.03 1.78 ...
      ..$ AD5     : num  2.33 2.33 2.33 2.33 2.33 ...
      ..$ AE1     : num  0.877 0.849 0.794 0.636 0.72 ...
      ..$ AE2     : num  0.3687 0.2332 0.1369 0.0433 0.0546 ...
      ..$ AE3     : num  0.774 0.723 0.624 0.335 0.487 ...
      ..$ AE4     : num  0.574 0.697 0.44 0.477 0.605 ...
      ..$ AE5     : num  0.542 0.542 0.554 0.543 0.542 ...
      ..$ BI1     : num  53 71.9 64 75.4 74.6 ...
      ..$ BI2     : num  6.51 5.88 4.54 2.3 2.34 ...
      ..$ BI3     : num  22.2 26 21.5 27.9 28 ...
      ..$ BI4     : num  7.86 9.58 8.59 12.17 12.5 ...
      ..$ BI5     : num  11.3 17.9 16.4 18.1 17.5 ...



 > train[1:5,]   ## rows 1-5, all columns
  Gradient Period     AC1      AC2      AC3      AC4      AC5      AD1      AD2      AD3      AD4      AD5      AE1      AE2      AE3      AE4      AE5
1       LB    Day 1792.97 316.4038 972.4288 663.2612 827.6400 1.217491 1.104860 1.217491 1.866627 2.332115 0.876794 0.368712 0.774123 0.574168 0.541993
2       LB    Day 1796.78 324.3562 979.2218 662.2341 824.6436 1.421910 1.274373 1.421910 1.526506 2.331810 0.848660 0.233177 0.722544 0.696906 0.542409
3       LB    Day 1796.09 321.9081 978.7464 664.1776 824.4437 1.726798 1.345030 1.721740 2.066543 2.326278 0.794230 0.136892 0.624107 0.440458 0.553766
4       LB    Day 1805.14 322.0390 987.9472 673.2841 824.3146 2.248474 1.381644 2.239061 2.028538 2.331881 0.636007 0.043267 0.334964 0.477149 0.542572
5       LB    Day 1803.15 323.1540 985.6376 669.7603 824.6003 1.992025 1.380468 1.992004 1.782532 2.331971 0.720153 0.054578 0.486951 0.604876 0.542420
       BI1      BI2      BI3       BI4      BI5
1 53.03567 6.506536 22.23446  7.862767 11.32708
2 71.94775 5.879407 26.04130  9.579798 17.94337
3 63.98763 4.535041 21.50727  8.590985 16.38780
4 75.38319 2.301110 27.89600 12.165991 18.06316
5 74.60517 2.342853 28.02568 12.499122 17.52902


rf1 <- h2o.randomForest(         
  training_frame = train,        
  validation_frame = valid,      
  x=2:22,                        
  y=1,                          
  ntrees = 200,                 
  stopping_rounds = 2,          
  score_each_iteration = T,     
  seed = 1000000) `
>perf <- h2o.performance(rf1, valid)
>h2o.mcc(perf)
Error in h2o.metric(object, thresholds, ""absolute_mcc"") : 
      No absolute_mcc for H2OMultinomialMetrics
    h2o.accuracy(perf)
Error in h2o.metric(object, thresholds, ""accuracy"") : 
  No accuracy for H2OMultinomialMetrics



and a summary from the model summary:


    H2OMultinomialMetrics: drf
** Reported on training data. **
** Metrics reported on Out-Of-Bag training samples **

Training Set Metrics: 
=====================

Extract training frame with `h2o.getFrame(""train.hex"")`
MSE: (Extract with `h2o.mse`) 0.2499334
RMSE: (Extract with `h2o.rmse`) 0.4999334
Logloss: (Extract with `h2o.logloss`) 0.9987891
Mean Per-Class Error: 0.2941914
R^2: (Extract with `h2o.r2`) 0.8683096","['r', 'random-forest', 'h2o']",Unknown,,N/A
60470926,60470926,2020-03-01T00:27:36,2020-03-01 00:27:36Z,23,"I have two machines of differing specs and am starting h2o via command line on each to form a cluster. I then use the flow interface for modeling work.


I’ve noticed that resource use is not evenly spread when running jobs. It tends to under-utilize one machine while maxing out another.


I was wondering if someone could take a look at what I’m doing and offer any tips to adjust / improve. Is this the best way to do it, or are there settings I could change to better maximize the use of resources?


Specs on the two machines:


Machine 1: Intel i7-8700 @ 3.2GHz, 64 GB RAM


Machine 2:  Xeon E5-2680v4 @ 2.4GHZ, 64 GB RAM


Command line to start H2o (running on both of them):


""C:\Program Files\Java\jdk-11.0.2\bin\java.exe"" -Xmx48g -jar .\h2o.jar -name b_h2o



I’m using the latest edition of H2O-3 (3.28.0.4)",['h2o'],Bob,https://stackoverflow.com/users/10643932/bob,41
60438883,60438883,2020-02-27T17:35:57,2020-02-27 18:53:06Z,433,"I am kicking off an H2O cluster on a server via the command line. The documentation says H2O needs space in the /tmp directory to write to. Is there a way to make h2o write to a different directory other than /tmp? For reference here is the documentation and the python code I use to launch h2o via command line with an os.system() call:


https://h2o-release.s3.amazonaws.com/h2o/rel-wolpert/8/docs-website/h2o-docs/starting-h2o.html


# Define string to be passed to command line to spin up cluster
h2o_init_command = (
    ""nohup java "" 
    ""-Xmx{cluster_mem_size} ""
    ""-jar {jar_file} ""
    ""-nthreads {cluster_threads} ""
    ""-name {cluster_port} "" 
    ""-port {cluster_port} "" 
    ""-ice_root {ice_root_file} "" 
    ""-hash_login ""
    ""-login_conf {authentication_file} "" 
    ""> {nohup_file} &""
    ).format(cluster_mem_size=cluster_mem_size, 
           jar_file=p.JAR_FILE, 
           cluster_threads=cluster_threads,
           cluster_port=cluster_port,
           ice_root_file=p.ICE_ROOT_FILE, 
           authentication_file=p.AUTHENTICATION_FILE, 
           nohup_file=nohup_file
            )

# Start an H2O cluster
return_cde = os.system(h2o_init_command)","['storage', 'h2o', 'tmp']",riddler747,https://stackoverflow.com/users/12974747/riddler747,11
60391514,60391514,2020-02-25T09:39:26,2020-03-06 10:01:57Z,0,"Getting a strange message from H2O ( 
h2o_3.26.0.2
 ) when predicting using a 
MOJO
 file:




Detected 14 unused columns in the input data set: {X8,X9,X10,X12,X1,X11,X2,X14,X3,X13,X4,X5,X6,X7}




I know that it is not a missing variable issue, as then H2O outputs:




There were 1 missing columns found in the input data set: {X1}




To reproduce the warning message I have created a small example with X15 being my target variable:


suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(h2o))

# read in data ------------------------------------------------------------

data_set <- read_csv(""https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"", col_names = FALSE) %>% 
  mutate_if(is.character, factor)
set.seed(3456)
trainIndex <- caret::createDataPartition(data_set$X15, p = .8, 
                                  list = FALSE, 
                                  times = 1)

train_strat <- data_set[ trainIndex,]
test_strat  <- data_set[-trainIndex,]

# start h2o ---------------------------------------------------------------

h2o.init(startH2O = T, max_mem_size = '400G')
#>  Connection successful!
#> 
#> R is connected to the H2O cluster: 
#>     H2O cluster uptime:         25 minutes 3 seconds 
#>     H2O cluster timezone:       Etc/UTC 
#>     H2O data parsing timezone:  UTC 
#>     H2O cluster version:        3.26.0.2 
#>     H2O cluster version age:    6 months and 29 days !!! 
#>     H2O cluster name:           H2O_started_from_R_rstudio_ltd590 
#>     H2O cluster total nodes:    1 
#>     H2O cluster total memory:   353.12 GB 
#>     H2O cluster total cores:    64 
#>     H2O cluster allowed cores:  64 
#>     H2O cluster healthy:        TRUE 
#>     H2O Connection ip:          localhost 
#>     H2O Connection port:        54321 
#>     H2O Connection proxy:       NA 
#>     H2O Internal Security:      FALSE 
#>     H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4 
#>     R Version:                  R version 3.5.1 (2018-07-02)
#> Warning in h2o.clusterInfo(): 
#> Your H2O cluster version is too old (6 months and 29 days)!
#> Please download and install the latest version from http://h2o.ai/download/
# h2o.shutdown()
train_strat_h2o <- as.h2o(train_strat)
test_strat_h2o <- as.h2o(test_strat)
Y <- 'X15'
X <- setdiff(names(train_strat), Y)

# train and predict --------------------------------------------------------------

rf_h2o <- h2o.randomForest(         
  training_frame = train_strat_h2o,
  x = X,                        
  y = Y,    
  nfolds = 0,
  model_id = ""big_rf"",   
  ntrees = 25,                 
  max_depth = 55,               
  stopping_rounds = 5,          
  stopping_tolerance = 1e-3,
  score_each_iteration = T,
  seed = 123
)



After the model has been trained and is sitting in the environment, I do a prediction and get the following results :


predict(rf_h2o, test_strat_h2o)
#>   predict     <=50K      >50K
#> 1   <=50K 1.0000000 0.0000000
#> 2    >50K 0.4980000 0.5020000
#> 3    >50K 0.2465993 0.7534007
#> 4    >50K 0.1200000 0.8800000
#> 5   <=50K 0.8040000 0.1960000
#> 6   <=50K 0.8628571 0.1371429
#> 
#> [6512 rows x 3 columns]



Now I move on to put the model into 
production
 by downloading the model:


h2o.download_mojo(rf_h2o, path = ""output/"", get_genmodel_jar = T)



And finally we can now use the 
MOJO
 file to predict. 
HERE
 is where I get the odd message, 
Detected 14 unused columns in the input data set
, although the predictions seem the same? 


head(
  h2o.mojo_predict_df(
    test_strat[, -15],
    mojo_zip_path = ""output/big_rf.zip"",
    genmodel_jar_path = ""output/h2o-genmodel.jar""
  )
)
#>  Detected 14 unused columns in the input data set: {X8,X9,X10,X12,X1,X11,X2,X14,X3,X13,X4,X5,X6,X7}
#>  predict    X..50K     X.50K
#> 1   <=50K 1.0000000 0.0000000
#> 2    >50K 0.4980000 0.5020000
#> 3    >50K 0.2465993 0.7534007
#> 4    >50K 0.1200000 0.8800000
#> 5   <=50K 0.8040000 0.1960000
#> 6   <=50K 0.8628571 0.1371429



Created on 2020-02-25 by the 
reprex package
 (v0.2.1)


Is this something I should be worried about?","['r', 'h2o']",Hanjo Odendaal,https://stackoverflow.com/users/5620975/hanjo-odendaal,"1,431"
60387031,60387031,2020-02-25T03:18:24,2020-05-23 01:03:43Z,209,"Following the directions on the 
datatable
 page  
https://datatable.readthedocs.io/en/latest/install.html
 




So I ran this:


$pip3 install git+https://github.com/h2oai/datatable



The result is :


Collecting git+https://github.com/h2oai/datatable
  Cloning https://github.com/h2oai/datatable to /private/var/folders/d6/m67jyndd7h754m3810cl3bpm0000gp/T/pip-req-build-hv991zd2
  Running command git clone -q https://github.com/h2oai/datatable /private/var/folders/d6/m67jyndd7h754m3810cl3bpm0000gp/T/pip-req-build-hv991zd2
  Getting requirements to build wheel ... done
ERROR: Exception:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/pip/_internal/cli/base_command.py"", line 178, in main
    status = self.run(options, args)
  File ""/usr/local/lib/python3.7/site-packages/pip/_internal/commands/install.py"", line 352, in run
    resolver.resolve(requirement_set)
  File ""/usr/local/lib/python3.7/site-packages/pip/_internal/resolve.py"", line 131, in resolve
    self._resolve_one(requirement_set, req)
  File ""/usr/local/lib/python3.7/site-packages/pip/_internal/resolve.py"", line 294, in _resolve_one
    abstract_dist = self._get_abstract_dist_for(req_to_install)
  File ""/usr/local/lib/python3.7/site-packages/pip/_internal/resolve.py"", line 242, in _get_abstract_dist_for
    self.require_hashes
  File ""/usr/local/lib/python3.7/site-packages/pip/_internal/operations/prepare.py"", line 362, in prepare_linked_requirement
    abstract_dist.prep_for_dist(finder, self.build_isolation)
  File ""/usr/local/lib/python3.7/site-packages/pip/_internal/operations/prepare.py"", line 169, in prep_for_dist
    self.install_backend_dependencies(finder=finder)
  File ""/usr/local/lib/python3.7/site-packages/pip/_internal/operations/prepare.py"", line 123, in install_backend_dependencies
    reqs = req.pep517_backend.get_requires_for_build_wheel()
  File ""/usr/local/lib/python3.7/site-packages/pip/_vendor/pep517/wrappers.py"", line 71, in get_requires_for_build_wheel
    'config_settings': config_settings
  File ""/usr/local/lib/python3.7/site-packages/pip/_vendor/pep517/wrappers.py"", line 162, in _call_hook
    raise BackendUnavailable
pip._vendor.pep517.wrappers.BackendUnavailable



This is not actionable for me: I'm not certain what to fix/ how to proceed. Hints?


Note: in the meantime I have installed version 0.10.1 as follows:


sudo pip3 install 'datatable==0.10.1'



This has important 
by
 (expression based 
groupby
) and 
sortvalues()
 that I need","['python', 'h2o', 'py-datatable']",Pasha,https://stackoverflow.com/users/958624/pasha,"6,540"
60370607,60370607,2020-02-24T06:31:41,2020-02-24 21:05:05Z,0,"I am attempting to deploy my H2O AutoML model to AWS following the instructions 
here
.


I keep getting the following empty result when I 
curl
 my endpoint:


curl http://<myIP>:8080/model?type=1\&row=value1,value2,value3



{""result"":""""}



If I don't submit any value for 
row
, I get results:


curl http://<myIP>:8080/model?type=1\&row=



{""result"":""Prediction = 0.723482072353363, 0.17580199241638184, 0.04721980169415474, 0.053496140986680984""}



If I submit a single value for 
row
, I get the same results:


curl http://<myIP>:8080/model?type=1\&row=value1



{""result"":""Prediction = 0.723482072353363, 0.17580199241638184, 0.04721980169415474, 0.053496140986680984""}



I have connected to the instance via SSH and confirmed my model has been downloaded to the correct location on boot.


Can anyone help me figure out how I'm supposed to be submitting row data to my endpoint? Thanks!","['amazon-web-services', 'machine-learning', 'h2o', 'automl']",Unknown,,N/A
60310899,60310899,2020-02-19T23:38:51,2020-04-03 06:43:18Z,439,"It's going to take some work for me to build a smaller test case PLUS I have to get permission to release the data (after I anonymize it), but H2O crashes on me consistently with this data and parameters.  (It usually succeeds with different combinations of feature inputs and parameters but seems to fail always with features and parameters below).


The data has 12847393 rows (which may be the provoking the problem?)


This is the ugly stack crawl I got.  (It seems reproducible.)


Filed as bug:  
https://0xdata.atlassian.net/projects/PUBDEV/issues/PUBDEV-7321


---------------------------------------------------------------------------
ConnectionResetError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    599                                                   body=body, headers=headers,
--> 600                                                   chunked=chunked)
    601 

/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    383                     # otherwise it looks like a programming error was the cause.
--> 384                     six.raise_from(e, None)
    385         except (SocketTimeout, BaseSSLError, SocketError) as e:

/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py in raise_from(value, from_value)

/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    379                 try:
--> 380                     httplib_response = conn.getresponse()
    381                 except Exception as e:

/usr/lib/python3.6/http/client.py in getresponse(self)
   1345             try:
-> 1346                 response.begin()
   1347             except ConnectionError:

/usr/lib/python3.6/http/client.py in begin(self)
    306         while True:
--> 307             version, status, reason = self._read_status()
    308             if status != CONTINUE:

/usr/lib/python3.6/http/client.py in _read_status(self)
    267     def _read_status(self):
--> 268         line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
    269         if len(line) > _MAXLINE:

/usr/lib/python3.6/socket.py in readinto(self, b)
    585             try:
--> 586                 return self._sock.recv_into(b)
    587             except timeout:

ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

ProtocolError                             Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    448                     retries=self.max_retries,
--> 449                     timeout=timeout
    450                 )

/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    637             retries = retries.increment(method, url, error=e, _pool=self,
--> 638                                         _stacktrace=sys.exc_info()[2])
    639             retries.sleep()

/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    366             if read is False or not self._is_method_retryable(method):
--> 367                 raise six.reraise(type(error), error, _stacktrace)
    368             elif read is not None:

/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py in reraise(tp, value, tb)
    684         if value.__traceback__ is not tb:
--> 685             raise value.with_traceback(tb)
    686         raise value

/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    599                                                   body=body, headers=headers,
--> 600                                                   chunked=chunked)
    601 

/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    383                     # otherwise it looks like a programming error was the cause.
--> 384                     six.raise_from(e, None)
    385         except (SocketTimeout, BaseSSLError, SocketError) as e:

/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py in raise_from(value, from_value)

/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    379                 try:
--> 380                     httplib_response = conn.getresponse()
    381                 except Exception as e:

/usr/lib/python3.6/http/client.py in getresponse(self)
   1345             try:
-> 1346                 response.begin()
   1347             except ConnectionError:

/usr/lib/python3.6/http/client.py in begin(self)
    306         while True:
--> 307             version, status, reason = self._read_status()
    308             if status != CONTINUE:

/usr/lib/python3.6/http/client.py in _read_status(self)
    267     def _read_status(self):
--> 268         line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
    269         if len(line) > _MAXLINE:

/usr/lib/python3.6/socket.py in readinto(self, b)
    585             try:
--> 586                 return self._sock.recv_into(b)
    587             except timeout:

ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/h2o/backend/connection.py in request(self, endpoint, data, json, filename, save_to)
    473                                     headers=headers, timeout=self._timeout, stream=stream,
--> 474                                     auth=self._auth, verify=verify, proxies=self._proxies)
    475             if isinstance(save_to, types.FunctionType):

/usr/local/lib/python3.6/dist-packages/requests/api.py in request(method, url, **kwargs)
     59     with sessions.Session() as session:
---> 60         return session.request(method=method, url=url, **kwargs)
     61 

/usr/local/lib/python3.6/dist-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    532         send_kwargs.update(settings)
--> 533         resp = self.send(prep, **send_kwargs)
    534 

/usr/local/lib/python3.6/dist-packages/requests/sessions.py in send(self, request, **kwargs)
    645         # Send the request
--> 646         r = adapter.send(request, **kwargs)
    647 

/usr/local/lib/python3.6/dist-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    497         except (ProtocolError, socket.error) as err:
--> 498             raise ConnectionError(err, request=request)
    499 

ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

During handling of the above exception, another exception occurred:

H2OConnectionError                        Traceback (most recent call last)
<ipython-input-1-56b68eefa416> in <module>
     87 start_time = time.time()
     88 model  = H2OXGBoostEstimator(**param)
---> 89 model.train(x=x, y=""y"", training_frame=hdf6)
     90 elapsed_time = time.time() - start_time

/usr/local/lib/python3.6/dist-packages/h2o/estimators/estimator_base.py in train(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, max_runtime_secs, ignored_columns, model_id, verbose)
    110         self._train(x=x, y=y, training_frame=training_frame, offset_column=offset_column, fold_column=fold_column,
    111                     weights_column=weights_column, validation_frame=validation_frame, max_runtime_secs=max_runtime_secs,
--> 112                     ignored_columns=ignored_columns, model_id=model_id, verbose=verbose)
    113 
    114 

/usr/local/lib/python3.6/dist-packages/h2o/estimators/estimator_base.py in _train(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, max_runtime_secs, ignored_columns, model_id, verbose, extend_parms_fn)
    263             return
    264 
--> 265         model.poll(poll_updates=self._print_model_scoring_history if verbose else None)
    266         model_json = h2o.api(""GET /%d/Models/%s"" % (rest_ver, model.dest_key))[""models""][0]
    267         self._resolve_model(model.dest_key, model_json)

/usr/local/lib/python3.6/dist-packages/h2o/job.py in poll(self, poll_updates)
     58                 pb.execute(self._refresh_job_status, print_verbose_info=ft.partial(poll_updates, self))
     59             else:
---> 60                 pb.execute(self._refresh_job_status)
     61         except StopIteration as e:
     62             if str(e) == ""cancelled"":

/usr/local/lib/python3.6/dist-packages/h2o/utils/progressbar.py in execute(self, progress_fn, print_verbose_info)
    169                 # Query the progress level, but only if it's time already
    170                 if self._next_poll_time <= now:
--> 171                     res = progress_fn()  # may raise StopIteration
    172                     assert_is_type(res, (numeric, numeric), numeric)
    173                     if not isinstance(res, tuple):

/usr/local/lib/python3.6/dist-packages/h2o/job.py in _refresh_job_status(self)
     96     def _refresh_job_status(self):
     97         if self._poll_count <= 0: raise StopIteration("""")
---> 98         jobs = h2o.api(""GET /3/Jobs/%s"" % self.job_key)
     99         self.job = jobs[""jobs""][0] if ""jobs"" in jobs else jobs[""job""][0]
    100         self.status = self.job[""status""]

/usr/local/lib/python3.6/dist-packages/h2o/h2o.py in api(endpoint, data, json, filename, save_to)
    121     # type checks are performed in H2OConnection class
    122     _check_connection()
--> 123     return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
    124 
    125 

/usr/local/lib/python3.6/dist-packages/h2o/backend/connection.py in request(self, endpoint, data, json, filename, save_to)
    481             if self._local_server and not self._local_server.is_running():
    482                 self._log_end_exception(""Local server has died."")
--> 483                 raise H2OConnectionError(""Local server has died unexpectedly. RIP."")
    484             else:
    485                 self._log_end_exception(e)

H2OConnectionError: Local server has died unexpectedly. RIP.



Parameters passed:


param = {
      ""ntrees"" : 15
    , ""min_rows"" : 5
    , ""max_depth"" : 5
    , ""learn_rate"" : 0.02
    , ""sample_rate"" : 0.7
    , ""col_sample_rate_per_tree"" : 0.9
    , ""seed"": 42
    , ""score_tree_interval"": 100
}



There were 14 input columns, of which 5 were categorical features.


I initialized H2O like this:


h2o.init(
    strict_version_check=False,
#    nthreads=1,   # Crashes either with 1 or 4 threads.
    log_dir=""/tmp/clem-h2o/"",
    log_level='TRACE'
)



Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.
Attempting to start a local H2O server...
  Java Version: openjdk version ""1.8.0_242""; OpenJDK Runtime Environment (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); OpenJDK 64-Bit Server VM (build 25.242-b08, mixed mode)
  Starting server from /usr/local/lib/python3.6/dist-packages/h2o/backend/bin/h2o.jar
  Ice root: /tmp/tmpkeo9aau1
  JVM stdout: /tmp/tmpkeo9aau1/h2o_unknownUser_started_from_python.out
  JVM stderr: /tmp/tmpkeo9aau1/h2o_unknownUser_started_from_python.err
  Server is running at http://127.0.0.1:54321
Connecting to H2O server at http://127.0.0.1:54321 ... successful.
H2O cluster uptime: 01 secs
H2O cluster timezone:   Etc/UTC
H2O data parsing timezone:  UTC
H2O cluster version:    3.28.0.3
H2O cluster version age:    14 days, 3 hours and 57 minutes
H2O cluster name:   H2O_from_python_unknownUser_xuimzh
H2O cluster total nodes:    1
H2O cluster free memory:    4.445 Gb
H2O cluster total cores:    4
H2O cluster allowed cores:  4
H2O cluster status: accepting new members, healthy
H2O connection url: http://127.0.0.1:54321
H2O connection proxy:   {'http': None, 'https': None}
H2O internal security:  False
H2O API Extensions: Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4
Python version: 3.6.9 final



Unfortunately, the warning, error and fatal log files are empty.  I can't release the other log files until I anonymize the feature names...


I don't know if there are some other debugging switches that could help diagnose the problem.


This is the version info:


H2O Version:  3.28.0.3
Python 3.6.9
Ubuntu 18.04.3 LTS","['xgboost', 'h2o']",Unknown,,N/A
60275392,60275392,2020-02-18T06:58:09,2020-02-19 23:15:43Z,0,"I'm following a tutorial from 
https://github.com/h2oai/h2o-tutorials/blob/master/tutorials/gbm-randomforest/GBM_RandomForest_Example.py


I have been following the tutorial until I reached the line with hit_ratio_table. when I executed ""rf_v1.hit_ratio_table(valid=True)"", I encounter the error below.


AttributeError                            Traceback (most recent call last)
<ipython-input-21-ff67e4484e12> in <module>
----> 1 rf_v1.hit_ratio_table(valid=True)

~\Anaconda3\lib\site-packages\h2o\utils\metaclass.py in __getattr__(self, name)
    191             if name in self._bci:
    192                 return self._bci[name]
--> 193             return getattr(new_clz, name)
    194 
    195         new_clz = extend_and_replace(clz, __init__=__init__, __getattr__=__getattr__)

~\Anaconda3\lib\site-packages\h2o\utils\metaclass.py in __getattribute__(cls, name)
    233             if attr is not MetaFeature.NOT_FOUND:
    234                 return attr
--> 235         return type.__getattribute__(cls, name)
    236 
    237     def __setattr__(cls, name, value):

AttributeError: type object 'ModelBase' has no attribute 'hit_ratio_table'



I have tried to converts the target to a factor using df.asfactor() but still not working.","['python', 'machine-learning', 'h2o']",Unknown,,N/A
60274066,60274066,2020-02-18T05:05:21,2020-02-18 05:17:55Z,0,"I am getting this error while initializing h2o 


h2o.init()
H2O is not running yet, starting it now...


Error in .h2o.startJar(ip = ip, port = port, name = name, nthreads = nthreads,  : 
  Your java is not supported: java version ""1.7.0_80""
Please download the latest Java SE JDK from the following URL:
https://www.oracle.com/technetwork/java/javase/downloads/index.html



I have tried installing latest version of Java, but still the problem persists. Kindly help.","['java', 'r', 'rstudio', 'h2o']",seenukarthi,https://stackoverflow.com/users/916225/seenukarthi,"8,591"
60269995,60269995,2020-02-17T20:32:41,2020-02-19 09:05:22Z,598,"I am attempting to run H2OAutoML on a 2.7MB training CSV on a system with 4GB RAM using the python API and it is running out of memory.


The error messages I am encountering are either:


h2o_ubuntu_started_from_python.out:
02-17 17:57:25.063 127.0.0.1:54321       27097  FJ-3-15   INFO: Stopping XGBoost training because of timeout
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 247463936 bytes for committing reserved memory.
# An error report file with more information is saved as:
# /home/ubuntu/h20.ai/h2o-3.28.0.2/hs_err_pid27097.log



or


03:37:07.509: XRT_1_AutoML_20200217_030816 [DRF XRT (Extremely Randomized Trees)] failed: java.lang.OutOfMemoryError: Java heap space



in the output of the python depending on the exact crash instance I look at.


My init is:


h2o.init(max_mem_size='3G',min_mem_size='2G',jvm_custom_args=[""-Xmx3g""])



Though I have tried with:

h2o.init()


My H2OAutoML call is:


H2OAutoML(nfolds=5,max_models=20, max_runtime_secs_per_model=600, seed=1,project_name =project_name)
aml.train(x=x, y=y, training_frame=train,validation_frame=test)



These are the server stats:


H2O cluster uptime:         02 secs
H2O cluster timezone:       Etc/UTC
H2O data parsing timezone:  UTC
H2O cluster version:        3.28.0.2
H2O cluster version age:    27 days
H2O cluster name:           H2O_from_python_ubuntu_htq5aj
H2O cluster total nodes:    1
H2O cluster free memory:    3 Gb
H2O cluster total cores:    2
H2O cluster allowed cores:  2
H2O cluster status:         accepting new members, healthy
H2O connection url:         http://127.0.0.1:54321
H2O connection proxy:       {'http': None, 'https': None}
H2O internal security:      False
H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4
Python version:             3.6.9 final



Does this sound right? Am I not able to run 20 models?


I can run this just fine setting the max_models=10. This takes about 60 min.


Are there guidelines for the amount of RAM needed for a given max_models and filesize?",['h2o'],stubble jumper,https://stackoverflow.com/users/1032341/stubble-jumper,313
60248668,60248668,2020-02-16T12:57:20,2020-02-16 13:04:27Z,276,"I'm a bit puzzled since I don't get the last example from 
""Machine Learning with Python and H2O"" manual
 working (page 36).


Here's the code:


import h2o
h2o.init()

from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o.transforms.preprocessing import H2OScaler
from h2o.cross_validation import H2OKFold
from h2o.model.regression import h2o_r2_score

from sklearn.pipeline import Pipeline
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics.scorer import make_scorer


h2o.__PROGRESS_BAR__=False
h2o.no_progress()

iris_data_path = ""http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris.csv""# load demonstration data1819In [5]: 
iris_df = h2o.import_file(path=iris_data_path)

params = {""standardize__center"":    [True, False],
          ""standardize__scale"":     [True, False],
          ""gbm__ntrees"":            [10,20],
          ""gbm__max_depth"":         [1,2,3],
          ""gbm__learn_rate"":        [0.1,0.2]}

custom_cv = H2OKFold(iris_df, n_folds=5, seed=42)

pipeline = Pipeline([(""standardize"", H2OScaler()),
                     (""gbm"", H2OGradientBoostingEstimator(distribution=""gaussian""))])

random_search = RandomizedSearchCV(pipeline, params, n_iter=5, scoring=make_scorer(h2o_r2_score),
                                               cv=custom_cv, random_state=42, n_jobs=1)

random_search.fit(iris_df[1:], iris_df[0])



It returns the error 
ValueError: No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed
.


The full terminal message:


Traceback (most recent call last):

  File ""untitled-Copy1.py"", line 34, in <module>
    random_search.fit(iris_df[1:], iris_df[0])
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/model_selection/_search.py"", line 710, in fit
    self._run_search(evaluate_candidates)
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/model_selection/_search.py"", line 1484, in _run_search
    random_state=self.random_state))
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/model_selection/_search.py"", line 689, in evaluate_candidates
    cv.split(X, y, groups)))
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/joblib/parallel.py"", line 1004, in __call__
    if self.dispatch_one_batch(iterator):
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/joblib/parallel.py"", line 835, in dispatch_one_batch
    self._dispatch(tasks)
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/joblib/parallel.py"", line 754, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/joblib/_parallel_backends.py"", line 209, in apply_async
    result = ImmediateResult(func)
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/joblib/_parallel_backends.py"", line 590, in __init__
    self.results = batch()
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/joblib/parallel.py"", line 256, in __call__
    for func, args, kwargs in self.items]
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/joblib/parallel.py"", line 256, in <listcomp>
    for func, args, kwargs in self.items]
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py"", line 508, in _fit_and_score
    X_train, y_train = _safe_split(estimator, X, y, train)
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/utils/metaestimators.py"", line 201, in _safe_split
    X_subset = _safe_indexing(X, indices)
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/utils/__init__.py"", line 390, in _safe_indexing
    indices_dtype = _determine_key_type(indices)
  File ""/department/jupyter-dev/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/utils/__init__.py"", line 288, in _determine_key_type
    raise ValueError(err_msg)
ValueError: No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed
Closing connection _sid_b8c1 at exit
H2O session _sid_b8c1 closed.



I'm using python 3.6.10 with sklearn 0.22.1 and h2o 3.28.0.3.


What am I doing wrong? Any help appreciated!


Have a great day :)","['python', 'scikit-learn', 'h2o', 'valueerror']",Unknown,,N/A
60235198,60235198,2020-02-15T01:08:57,2020-02-15 08:51:47Z,60,"Closed
. This question needs 
details or clarity
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Add details and clarify the problem by 
editing this post
.






Closed 
4 years ago
.















                        Improve this question
                    








I have done a binary classification using DRF in h2o. I have got the feature importance, then I have asked to find the criterion of each feature. 


E.g: I have to classify the user which to accept and which to reject and the features are age, salary, work_experience, live city (already decoded of course). So, my boss wanted to know which range of ages is the tendency of users got rejected or accepted, which range of salaries, which cities that have tendency got rejected or accepted.


I can't be wrong but the output I want may look like:


Tendency salary for accepted user = 10k - 50k
Tendency salary for accepted user = 5k - 30k

Tendency age for accepted user = 18 - 55
Tendency age for rejected user = 31 - 35

Tendency cities for accepted user = 1, 5, 10, 23
Tendency cities for rejected user = 3, 4, 12, 36

so on..



How to achieve it?


p.s: I have a list of accepted users.","['python', 'machine-learning', 'data-science', 'random-forest', 'h2o']",Unknown,,N/A
60233674,60233674,2020-02-14T21:20:01,2020-02-14 21:20:01Z,256,"I am running h2o automl using anaconda env on a CentOS 7 Linux machine(CentOS Linux release 7.7.1908 (Core)). I get this error for XGBoost model. 


Cannot build an XGBoost model - no backend found.


According to the docs, XGBoost option should work on a CentOS machine. I am using the latest h2o 3.28.0.3(pip installed in my anaconda env). 


Any help is much appreciated.","['xgboost', 'h2o', 'automl']",Deepak,https://stackoverflow.com/users/12900756/deepak,41
60210768,60210768,2020-02-13T15:01:58,2020-02-13 17:51:06Z,266,"I'm trying to use the new ""parallelism"" option of H2O's grid search to tune the hyperparameters of a GBM model with a 3-fold cross-validation. However, the search is failing, or rather just stopping, as soon as the first batch of the models is built. 


A bit of context: I am submitting this grid search job to an H2O cluster on the remote server on hadoop. I'm creating the cluster with, say, 5 nodes. Here's an example: 
hadoop jar /usr/local/h2o/bin28/h2odriver.jar -nodes 5 -mapperXmx 30g -baseport 54364 -disown
. I have a an indicator column for the fold assignment. 


With 
parallelism = 0
, the grid search is starting with 5 models in parallel (with 2 cv models for each first, and then the 3rd cv model once those are done). As soon as these 5 models complete, the search is just finishing.


The grid search works fine if I run it sequentially with parallelism turned off, but I can't figure out why it won't work with parallelism. 


I would appreciate any help with this.
Thank you!


EDIT:
Correction - looks the ""parallelism = 1"" option isn't working either. The search just stops after one model. This was not an issue with the previous version of H2O - v3.26.03.","['parallel-processing', 'h2o', 'grid-search', 'gridsearchcv']",Unknown,,N/A
60176991,60176991,2020-02-11T20:38:15,2020-02-18 03:12:33Z,56,"I am using H2O.ai to understand both current week and lagged week features that affect the target value of the current week.

Using the Walmart example.


For a particular week of sales, I am interested in the features that most likely affect how well or poorly sales performed. 
To set this problem up, I want H2O.ai to 'predict' what the current week of sales are using the current week feature values as well as the lagged values (including the lag of the target) -- i.e., this is not a forecast problem, but a problem to understand the drivers.


In forecasting terms, this would be setting the prediction horizon to 1 and the gap of negative 1.  


However, H2O.ai seems to not allow you to set it in this unconventional way. 


How would I set up this experiment in H2O.ai?","['h2o', 'forecasting', 'driverless-ai']",tonjo,https://stackoverflow.com/users/2783654/tonjo,"1,430"
60164981,60164981,2020-02-11T08:52:41,2020-02-11 11:58:39Z,65,"I import a flow-File using following curl command


curl http://localhost:54321/3/NodePersistentStorage.bin/notebook/my-model \
     -H ""Content-Type: multipart/form-data"" \
     -H ""Cookie: _gcl_au=123456"" \
     -F ""file=@path/to/my-model.flow""



The flow file get's imported. 


Question:
 Do you know if there is a REST/HTTP-call to run all commands which are contained in the flow file?",['h2o'],Rokko_11,https://stackoverflow.com/users/2432030/rokko-11,897
60112299,60112299,2020-02-07T11:11:38,2020-02-16 23:49:26Z,439,"I recently created a PySpark pipeline using Sparkling Water's AutoML in the last stage (very similar to 
https://github.com/h2oai/sparkling-water/blob/master/py/examples/pipelines/ham_or_spam_multi_algo.py
), but when I load my model from a file I get this error: 


Ex: 


model = loaded_pipeline.fit(data)
model.write().overwrite().save(""examples/build/model"")
loaded_model = PipelineModel.load(""examples/build/model"")


py4j.protocol.Py4JError: ai.h2o.sparkling.ml.models.H2OMOJOModel.H2OSupervisedMOJOModel does not exist in the JVM



I have the current packages/versions: H2O (3.28.0.3), h2o-pysparkling-2-4 (3.28.0.3-1), PySpark (2.4.3), Py4j (0.10.7).
I only got this error when I updated H2O/Sparkling Water to the 3.28 version. Can it be related to the definition of some environment variable or package version?","['python', 'apache-spark', 'pyspark', 'h2o', 'sparkling-water']",luis_ferreira223,https://stackoverflow.com/users/12857373/luis-ferreira223,84
60111812,60111812,2020-02-07T10:42:11,2020-02-07 15:41:52Z,0,"I've trained a model in 
H2O
 and save it as MOJO following 
this tutorial
.


Then, I've created a new Android app including the saved model and 
h2o-genmodel.jar
. Gradle build runs successfully, but when I try to run it on the Android phone I get the following error:


Installation did not succeed.
The application could not be installed: INSTALL_FAILED_NO_MATCHING_ABIS
Installation failed due to: 'null'



My guess is that the 
jar
 file has been generated only for x86 and not arm chips, but I'd like to know if it's possible at all to run it on Android.","['java', 'android', 'h2o']",Unknown,,N/A
60094702,60094702,2020-02-06T12:10:09,2020-02-18 13:42:59Z,328,"I'm trying to run a grid search for Gradient Boosting Machine in pyspark with H2O Sparkling Water.


Produced a reproducible example with the famous iris dataset.


from pysparkling import H2OContext, H2OConf
import pyspark
from pyspark.sql.types import StructType, StructField, FloatType, StringType
from pyspark.conf import SparkConf
from pyspark.sql import SQLContext
conf = SparkConf()
conf.setMaster(""local"").setAppName(""test"")
conf.set(""spark.sql.shuffle.partitions"", 3)
conf.set(""spark.default.parallelism"", 3)
conf.set(""spark.debug.maxToStringFields"", 100)
sc = pyspark.SparkContext(conf=conf)
sqlContext = SQLContext(sc)
hc = H2OContext.getOrCreate(sc, H2OConf(sc).set_internal_cluster_mode())
schema = StructType([
    StructField(""sepal_length"", FloatType(), True),
    StructField(""sepal_width"", FloatType(), True),
    StructField(""petal_length"", FloatType(), True),
    StructField(""petal_width"", FloatType(), True),
    StructField(""class"", StringType(), True)])
iris_df = sqlContext.read \
        .format('com.databricks.spark.csv') \
        .option('header', 'false') \
        .option('delimiter', ',') \
        .schema(schema) \
        .load('../../../../Downloads/iris.data')



If I try to follow 
this page of H2O docs
 and just translate to python


gbm_params = {'learnRate': [0.01, 0.1],
              'ntrees': [100 , 200, 300, 500]}
gbm_grid = H2OGridSearch()\
    .setLabelCol(""class"") \
    .setHyperParameters(gbm_params)\
    .setAlgo(H2OGBM().setMaxDepth(30))

model_pipeline = Pipeline().setStages([gbm_grid])
model = model_pipeline.fit(iris_df)



I get an internal NullPointerException, I guess there's something missing in the configuration.


Py4JJavaError: An error occurred while calling o111.fit.
: java.lang.NullPointerException
    at ai.h2o.sparkling.ml.algos.H2OGridSearch.extractH2OParameters(H2OGridSearch.scala:352)
    at ai.h2o.sparkling.ml.algos.H2OGridSearch.fit(H2OGridSearch.scala:64)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Unknown Source)



If I try to rewrite it in a different way, I get a different error,


gbm_grid = H2OGridSearch(algo=H2OGBM().setMaxDepth(30),
                         hyperParameters={'learnRate': [0.01, 0.1]},
                         withDetailedPredictionCol=True,
                         labelCol='class',
                         stoppingMetric=""AUC"")
model_pipeline = Pipeline().setStages([gbm_grid])
model = model_pipeline.fit(iris_df)



This is the output, no matter how do I change the hyperparameters,


Py4JJavaError: An error occurred while calling o1817.fit.
: java.lang.NoSuchFieldException: learnRate
    at java.lang.Class.getField(Unknown Source)
    at ai.h2o.sparkling.ml.algos.H2OGridSearch.findField(H2OGridSearch.scala:170)
    at ai.h2o.sparkling.ml.algos.H2OGridSearch.processHyperParams(H2OGridSearch.scala:154)
    at ai.h2o.sparkling.ml.algos.H2OGridSearch.fit(H2OGridSearch.scala:71)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Unknown Source)



The following works, however it is not useful since there is no grid,


gbm_grid = H2OGridSearch(algo=H2OGBM().setMaxDepth(30),
                         #hyperParameters=gbm_params,
                         withDetailedPredictionCol=True,
                         labelCol='class',
                         stoppingMetric=""AUC"")
model_pipeline = Pipeline().setStages([gbm_grid])
model = model_pipeline.fit(iris_df)
model.stages[0].transform(iris_df).head()



And finally, just to be sure that 
learnRate
 is a parameter of H2OGBM, this also works,


gbm_model = H2OGBM(labelCol='class',
                   withDetailedPredictionCol=True).setLearnRate(0.01).setMaxDepth(5).setNtrees(100)

model_pipeline = Pipeline().setStages([gbm_model])
model = model_pipeline.fit(iris_df)
model.stages[0].transform(iris_df).head()





EDIT: missing imports


from pyspark.ml.pipeline import Pipeline
from ai.h2o.sparkling.ml.algos import H2OGridSearch
from ai.h2o.sparkling.ml.algos import H2OGBM



and sparking water version


h2o-pysparkling-2-4       3.28.0.1-1               pypi_0    pypi





EDIT after comments for Spark/H2O/Java versions


Spark: 2.4.4


H2O: 3.28.0.3


Java: 1.8.0_232




EDIT java -version


openjdk version ""1.8.0_242""
OpenJDK Runtime Environment (build 1.8.0_242-8u242-b08-0ubuntu3~16.04-b08)
OpenJDK 64-Bit Server VM (build 25.242-b08, mixed mode)





Get the same error if I use 
learn_rate
 instead of 
learnRate
.


gbm_grid = H2OGridSearch(algo=H2OGBM().setMaxDepth(30),
                         hyperParameters={'learn_rate': [0.01, 0.1]},
                         withDetailedPredictionCol=True,
                         labelCol='class',
                         stoppingMetric=""AUC"")
model_pipeline = Pipeline().setStages([gbm_grid])
model = model_pipeline.fit(iris_df)



...


Py4JJavaError: An error occurred while calling o1376.fit.
: java.lang.NoSuchFieldException: learn_rate
    at java.lang.Class.getField(Class.java:1703)
    at ai.h2o.sparkling.ml.algos.H2OGridSearch.findField(H2OGridSearch.scala:170)
    at ai.h2o.sparkling.ml.algos.H2OGridSearch.processHyperParams(H2OGridSearch.scala:154)
    at ai.h2o.sparkling.ml.algos.H2OGridSearch.fit(H2OGridSearch.scala:71)
    at ai.h2o.sparkling.ml.algos.H2OGridSearch.fit(H2OGridSearch.scala:52)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)","['machine-learning', 'pyspark', 'h2o', 'sparkling-water']",Unknown,,N/A
60046449,60046449,2020-02-03T20:09:16,2020-02-04 13:15:03Z,0,"How do I format a custom function to pass variables into an h2o function? I can't figure out the proper quo/expr/ensym syntax. 


Here is a small example of the syntax that I can't figure out:  


suppressMessages(library(h2o))
#> Warning: package 'h2o' was built under R version 3.6.2

suppressMessages(library(rlang))
h2o.init()
#>  Connection successful!

data_h2o <- as.h2o(iris)

h2o.cor(data_h2o$Sepal.Length, data_h2o$Sepal.Width, use = ""everything"", method = ""Pearson"")
#> [1] -0.1175698



# function to take two variables and return the correlation

## in a larger data set I only care about how the target variable
## relates to the dependent variables
cor_function <- function(var1, var2) {
  var_1 = deparse(substitute(var1))
  var_2 = deparse(substitute(var2))
  r = h2o::h2o.cor(data_h2o[[var_1]], data_h2o[[var_2]], use = ""complete.obs"", na.rm = TRUE, method = ""spearman"")
  out <-  tibble::enframe(r, name = NULL)
  out$var1 = var_1
  out$var2 = var_2
  return(r)
}

# this works
cor_function(Sepal.Length, Sepal.Width)
#> [1] -0.1795433

params_to_run <- expand.grid(var1 = ""Sepal.Length"", var2 = c(""Sepal.Width"", ""Petal.Width""))

suppressMessages(library(purrr))

purrr::map(params_to_run, cor_funtion)
#> Error in if ((nrow(x) == 1L || (ncol(x) == 1L && ncol(y) == 1L))) .eval.scalar(expr) else .fetch.data(expr, : missing value where TRUE/FALSE needed



Created on 2020-02-03 by the 
reprex package
 (v0.3.0)


This is similar to other questions w/o answers:




How to pass variable name to custom function having h2o function


How to pass dynamic column name to h2o arrange function","['r', 'purrr', 'h2o', 'rlang']",Unknown,,N/A
60024798,60024798,2020-02-02T09:31:07,2020-02-05 16:10:13Z,0,"I cannot figure out why my random forest grid search hangs. I tried many things suggested on Stackoverflow, but nothing works.
First of all, here is my code:


library(data.table)
library(h2o)
library(dplyr)

# Initialise H2O
localH2O = h2o.init(nthreads = -1, min_mem_size = ""9240M"", max_mem_size = ""11336M"")

h2o.removeAll()

# Specify some dirs, inputs etc. (not shown)
laufnummer  <- 10
set.seed(laufnummer)
maxmodels   <- 500

# Convert to h2o
h2o_input <- as.h2o(input)

# Split: 80% = train; 0 = valid; rest = 20% = test
splits <- h2o.splitFrame(h2o_input, c(0.80,0))
train  <- h2o.assign(splits[[1]], ""train"") # 80%
test   <- h2o.assign(splits[[3]], ""test"")  # 10%



Set parameters:


# Select range of ntrees
min_ntrees      <- 10
max_ntrees      <- 2500
stepsize_ntrees <- 20
ntrees_opts <- seq(min_ntrees,max_ntrees, stepsize_ntrees)

# Select range of tries
min_mtries      <- 1
max_mtries      <- 12
stepsize_mtries <- 1
mtries_opts <- seq(min_mtries,max_mtries, stepsize_mtries)

# Cross-validation number of folds
nfolds <- 5

hyper_params_dl = list(ntrees = ntrees_opts,
                       mtries = mtries_opts)
search_criteria_dl = list(
  strategy = ""RandomDiscrete"",
  max_models = maxmodels)



Finally, the random grid search (this is where it hangs, almost always at 25%)


rf_grid <- h2o.grid(seed = laufnummer,
                    algorithm = ""randomForest"", 
                    grid_id = ""dlgrid"",
                    x = predictors, 
                    y = response, 
                    training_frame = train,
                    nfolds = nfolds,
                    keep_cross_validation_predictions = TRUE,
                    model_id = ""rf_grid"",
                    hyper_params = hyper_params_dl,
                    search_criteria = search_criteria_dl
)



Here is what I already tried:




Did not set nthreads in init: no effect.


Set nthreads to 4: no effect.


Set lower memory (I have 16 GB): no effect.


Added parallelism = 0 in grid search: no effect


Did not use h2o.removeAll(): no effect


Always used h2o.shutdown(prompt = FALSE) at end: no effect


Used different version of JDK, R and h2o. (now using the latest ones for all)




The problem is that the grid search progress stops at around 25%, sometimes less.


What does help is to switch the code to GBM instead of RF,
but it sometimes hangs there as well (and I need RF!).
What also helped was to reduce the number of models to 500 instead of 5000, but only for NN and GBM, not RF.


After trying for some weeks now, I would appreciate any help very much!! Thank you!


UPDATE:
Thanks for your suggestions, here is what I tried:
1. Imported already split files with h2o.importfile(): no effect
No surprise, because it is such a small data set and loading takes a few secs.
2. Set nthreads to 1: no effect
3. Do not use xgboost: I am not aware that I use it.
4. Do not use RF: Not possible, because I try to compare machine learning algorithms.
5. h2o.init(jvm_custom_args = c(""-XX:+PrintGCDetails"", ""-XX:+PrintGCTimeStamps"")): Did not work, because h2o would not start with this parameter added.
6. Bought an additional 8 GB of RAM and set max_mem_size to 18 and 22 GB respectively: effect = stops at about 65% and 80% instead of 25%. What is interesting is that the progress bar gets slower and slower until it stops completely. Then something like a hard reset takes place since I use a different keyboard layout (Win10) and that is set to the default...
Note: 500 GBM or NN run fine with the same data set.
7. Reduced number of models to 300: no effect.


So, my conclusion is that it is definitely a memory issue, but I cannot really monitor it. The RAM in the Task manager is not at 100%, but at the allocated max_mem_size.
Any help that can help me to pinpoint the problem further is greatly appreciated - thanks guys!!","['r', 'random-forest', 'h2o', 'freeze']",Unknown,,N/A
59991017,59991017,2020-01-30T17:04:50,2020-01-31 01:44:24Z,0,"I am working through section 19.2.3 of 
HOML/autoencoders.html
 and found an error message pop up.


In section: 19.2.3 Visualizing the reconstruction, I found an error associated with the line:


# Predict reconstructed pixel values  
best_model_id <- grid_perf@model_ids[[1]]



after this line I get:


Error: object 'grid_perf' not found



Up to this point, I have followed the code from the Autoencoder section, should I look at my setup or is this a change in H2O.ai and code?


HTH","['r', 'h2o']",mccurcio,https://stackoverflow.com/users/851043/mccurcio,"1,323"
59887618,59887618,2020-01-23T21:54:13,2020-01-24 09:31:35Z,268,"I'm solving a Scala data science problem in Intellij using maven. I noticed that MLFlow spark (
https://mvnrepository.com/artifact/org.mlflow/mlflow-spark/1.5.0
) is dependent on scala 2.12 while h2o.ai sparkling water is dependent on scala 2.11 (
https://mvnrepository.com/artifact/ai.h2o/sparkling-water-core
). Is there any way to use both of these together using Scala?","['scala', 'apache-spark', 'h2o', 'sparkling-water', 'mlflow']",proselotis,https://stackoverflow.com/users/12771832/proselotis,11
59833263,59833263,2020-01-21T02:05:11,2020-01-27 17:45:01Z,0,"I am trying to use 
h2o.glm
 to find the optimal penalty 
lambda
 by cross-validation. This is a multinomial model.


However, I see that it is optimizing according to the multinomial deviance. Can I do cross-validation with respect to some other metric, such as misclassification error? 


The parameter 
custom_metric_func
 is mentioned in the docs, but I am not clear on its description. Is this metric used as the cross-validation score? If yes, the docs also state that it is only available in the 
Python
 API. Is this really true?","['python', 'r', 'h2o']",James Hirschorn,https://stackoverflow.com/users/1349673/james-hirschorn,"7,886"
59812521,59812521,2020-01-19T17:20:03,2022-01-03 22:10:37Z,0,"I made a grid search that contains 36 models.


For each model the confusion matrix is available with : 


grid_search.get_grid(sort_by='a_metrics', decreasing=True)[index].confusion_matrix(valid=valid_set)


My problematic is I only want to access some parts of this confusion matrix in order to make my own ranking, which is not natively available with 
h2o
.


Let's say we have the 
confusion_matrix
 of the first model of the 
grid_search
 below:


+---+-------+--------+--------+--------+------------------+
|   |   0   |   1    | Error  |  Rate  |                  |
+---+-------+--------+--------+--------+------------------+
| 0 | 0     |  766.0 | 2718.0 | 0.7801 | (2718.0/3484.0)  |
| 1 | 1     |  351.0 | 6412.0 | 0.0519 | (351.0/6763.0)   |
| 2 | Total | 1117.0 | 9130.0 | 0.2995 | (3069.0/10247.0) |
+---+-------+--------+--------+--------+------------------+



Actually, the only things that really interest me is the precision of the class 
0
 as 
766/1117 = 0,685765443
. While 
h2o
 consider 
precision
 metrics for all the classes and it is done to the detriment of what I am looking for.


I tried to convert it in dataframe with:


model = grid_search.get_grid(sort_by='a_metrics', decreasing=True)[0]
model.confusion_matrix(valid=valid_set).as_data_frame()



Even if some topics on internet suggest it works, actually it does not (or doesn't anymore):


AttributeError: 'ConfusionMatrix' object has no attribute 'as_data_frame'



I search a way to return a list of attributes of the 
confusion_matrix
 without success.","['python', 'h2o']",AvyWam,https://stackoverflow.com/users/10314460/avywam,970
59690508,59690508,2020-01-11T00:07:22,2020-01-11 00:44:13Z,91,"Seeing error message 




Job setup failed : org.apache.hadoop.security.AccessControlException: Permission denied: user=airflow, access=WRITE, inode=""/"":hdfs:hdfs:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399) at ...




when trying to connect to start the 
h2o
 cluster (h2o-3.28.0.1-hdp3.1). Ie it appears that it does not like that the root hdfs dir 
hdfs:///
 does not have write permissions for my user (and giving write access to my user via ranger 
does
 appear to fix the problem), but this seems wrong. 


From 
past experience
, I've seen this for case where the launching user does not have write permissions the their own 
hdfs:///user/<username>
 folder, but seems odd to me that h2o wants the user to have write access over the entire top level hdfs dir. Is this normal? Can I change this?




Possibly related: Finding that after starting the cluster, can't manually kill in YARN ResourceManager UI or killing the PID, rather need to go to the h2o cluster url and use the admin tab to shutdown the cluster. Any ideas why this would happen?",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
59685087,59685087,2020-01-10T16:01:27,2020-01-13 19:43:06Z,72,"I'm looking for equivalent functionality to scikit-learn 
cross_val_predict
 function within h2o. 


I tried to use keep_cross_validation_predictions=True option with autoML, but aml.leader.cross_validation_predictions() returns None. 


I followed 
link
 and was able to extract CV predictions for the full training set, but not the only out-of-sample validation set. I'm thinking of using the 5-fold models from checkpoint to do prediction. I have values 0-4 in ""fold_column"", but the CV numbers in the model names are assigned 1-5. I'm not sure what is the mapping. 


Can anyone shed some light?


Thanks!","['python', 'cross-validation', 'h2o', 'automl']",Unknown,,N/A
59640204,59640204,2020-01-08T06:04:11,2020-01-18 04:05:53Z,201,I have around 2GB data in my local MongoDB database with one collection in the database. I want to ingest all these data from MongoDB database to standalone H2o cluster for building machine learning model. I am using python for data analysis in H2o. Could you please advise how can I proceed ?,"['mongodb', 'h2o']",arkaprova,https://stackoverflow.com/users/12672700/arkaprova,1
59605695,59605695,2020-01-06T01:30:30,2020-01-06 19:05:27Z,191,"I believe I've uncovered a bug (or limitation) in the h2o.ai AutoML StackedEnsemble validation metrics. 


When running AutoML with only one model type (in this case XGBoost) and n-fold cross validation, I was surprised to see that the BestOfFamily StackedEnsemble scored better than any of the individual XGBoost models. Which should not be possible, since the BestOfFamily StackedEnsemble in this scenario contains only one model, the leading XGBoost model, and therefore should have identical validation metrics to it. I confirmed by checking the StackedEnsemble did indeed only contain the best XGBoost model, yet had different and superior validation metrics.


My best hypothesis for this is that the metalearner algorithm (at least the default GLM one) does not take into account the weights I had assigned to the training data. Some of the observations in the training data are related, and I needed to reduce their weight relative to more unique observations (if that doesn't make sense or is wrong, feel free to correct as I'm fairly amateur, but it's beside the point). When I discovered this anomaly, I was only using XGBoost, but I do use AutoML with multiple model categories and am concerned the problem will affect those Stacked Ensembles and their rankings as well. 


So unfortunately, unless this can be explained or corrected, I will not be able to use Stacked Ensembles in my current endeavors, since I can't trust the validation metrics. Does anyone have such an explanation or method of fixing the problem?


h2o Version: 3.28.0.1


Used in conjunction with R Version: 3.6.1


As requested, below is some quickly cobbled together R code to generate some synthetic data and apply AutoML that should reproduce the problem.


Further edit: When I make all the weights the same in this example, the validation metric for the Best of Family Stacked Ensemble are closer to, but still not identical to, the single XGBoost model is contains. I don't understand how this can be true, as a Stacked Ensemble of one model should be have outputs identical to that one model, correct?


h2o.init()

DF<-data.frame(c(rep(T,1000), rep(F,1000)))
colnames(DF)<-""RESULT""

DF$WEIGHT<-rep(c(rep(1,500), rep(2,500)), 2)

DF[which(DF$RESULT & DF$WEIGHT==1), ""VAR_1""]<-rnorm(length(which(DF$RESULT & DF$WEIGHT==1)), mean = 1, sd = 1)
DF[which(DF$RESULT & DF$WEIGHT==2), ""VAR_1""]<-rnorm(length(which(DF$RESULT & DF$WEIGHT==2)), mean = 1, sd = 2)
DF[which(!DF$RESULT & DF$WEIGHT==1), ""VAR_1""]<-rnorm(length(which(!DF$RESULT & DF$WEIGHT==1)), mean = 2, sd = 1)
DF[which(!DF$RESULT & DF$WEIGHT==2), ""VAR_1""]<-rnorm(length(which(!DF$RESULT & DF$WEIGHT==2)), mean = 2, sd = 2)

DF[which(DF$RESULT & DF$WEIGHT==1), ""VAR_2""]<-rnorm(length(which(DF$RESULT & DF$WEIGHT==1)), mean = 1, sd = 1)
DF[which(DF$RESULT & DF$WEIGHT==2), ""VAR_2""]<-rnorm(length(which(DF$RESULT & DF$WEIGHT==2)), mean = 1, sd = 2)
DF[which(!DF$RESULT & DF$WEIGHT==1), ""VAR_2""]<-rnorm(length(which(!DF$RESULT & DF$WEIGHT==1)), mean = 2, sd = 1)
DF[which(!DF$RESULT & DF$WEIGHT==2), ""VAR_2""]<-rnorm(length(which(!DF$RESULT & DF$WEIGHT==2)), mean = 2, sd = 2)

DF[which(DF$RESULT & DF$WEIGHT==1), ""VAR_3""]<-rnorm(length(which(DF$RESULT & DF$WEIGHT==1)), mean = 1, sd = 1)
DF[which(DF$RESULT & DF$WEIGHT==2), ""VAR_3""]<-rnorm(length(which(DF$RESULT & DF$WEIGHT==2)), mean = 1, sd = 2)
DF[which(!DF$RESULT & DF$WEIGHT==1), ""VAR_3""]<-rnorm(length(which(!DF$RESULT & DF$WEIGHT==1)), mean = 2, sd = 1)
DF[which(!DF$RESULT & DF$WEIGHT==2), ""VAR_3""]<-rnorm(length(which(!DF$RESULT & DF$WEIGHT==2)), mean = 2, sd = 2)

TRAIN<-as.h2o(DF, ""TRAIN"")

AUTOML<-h2o.automl(project_name = ""ERROR_TEST"",
                   training_frame = ""TRAIN"",
                   y=""RESULT"",
                   weights_column = ""WEIGHT"",
                   stopping_metric = ""logloss"",
                   modeling_plan = list(list(name=""XGBoost"", alias='defaults'), ""StackedEnsemble""),
                   sort_metric = ""logloss"",
                   verbosity = ""info"")

print(AUTOML@leaderboard)",['h2o'],Unknown,,N/A
59552280,59552280,2020-01-01T11:55:13,2020-01-02 12:11:53Z,0,"I have to run the same R script in parallel (via batches) with different parameters. The R script builds and scores a H2O model. In this case should I, 




Set up an individual cluster for each batch run of the R script? 




(OR)




Create a common cluster and set the scripts to use it?




I would prefer the latter solution, but I am not sure how to automate initialization and shutting down of the H2O cluster for so many batches. The first batch has to create the cluster (H2O.init() and the last batch has to shut it down)","['r', 'h2o']",Unknown,,N/A
59536276,59536276,2019-12-30T20:04:03,2020-01-01 18:36:21Z,52,"I want to convert the leaf value into the probability. According to my current understanding, these predicted leaf values are not logodds, but are corrections. I want to know how to convert these corrections to probabilities (non-negative numbers).
H2o decision tree","['probability', 'decision-tree', 'h2o']",Bilal Munawar,https://stackoverflow.com/users/12627525/bilal-munawar,1
59514439,59514439,2019-12-28T20:02:37,2019-12-29 11:19:14Z,218,"base) stephen@stephen-Aspire-5250:~$ java --version
java 13.0.1 2019-10-15
Java(TM) SE Runtime Environment (build 13.0.1+9)
Java HotSpot(TM) 64-Bit Server VM (build 13.0.1+9, mixed mode, sharing)
(base) stephen@stephen-Aspire-5250:~$ find . -name h2o.jar
./R/x86_64-pc-linux-gnu-library/3.4/h2o/java/h2o.jar



then from R


> h2o.init()

H2O is not running yet, starting it now...
Error in .h2o.checkJava() : 
  Cannot find Java. Please install the latest JRE from
http://www.oracle.com/technetwork/java/javase/downloads/index.html



I have a feeling conda is messing up the landscape, but don't know how to fix.
I started h2o from terminal with:


java -jar ~/R/x86_64-pc-linux-gnu-library/3.4/h2o/java/h2o.jar



and then the h2o.init() from R works. Still, don't why h2o cannot find java, which is on my PATH.",['h2o'],Unknown,,N/A
59510832,59510832,2019-12-28T12:21:02,2020-01-03 11:31:23Z,0,"How does 
h2o
 create trees when we set 
binomial_double_trees
 to 
TRUE
?! If I can find the optimal number of trees using 
h2o.grid
, then why do we need 
binomial_double_trees = TRUE
(also one question more, what is the difference between internal trees and simple trees )


Reference","['python', 'r', 'h2o']",NelsonGon,https://stackoverflow.com/users/10323798/nelsongon,13.3k
59495896,59495896,2019-12-27T04:12:32,2019-12-27 05:03:08Z,299,"I have a dataset with 20 enum features (each of which takes 2 distinct string values), one int feature for a binary classification problem. H2o Xgboost is dropping all 20 columns with the warning 




Warning message:
  In .h2o.startModelJob(algo, params, h2oRestApiVersion) :
    Dropping bad and constant columns




I have tried setting all columns as strings and also as a factor (enum) but H2o drops all the columns every time. Any suggestions?","['enums', 'h2o', 'xgboost']",Lin Du,https://stackoverflow.com/users/6463558/lin-du,101k
59471469,59471469,2019-12-24T16:38:50,2019-12-29 16:48:25Z,0,"I am running 
gbm
 and 
glm
 with 
offset_column
 as base learners in 
h2o
. My response variable is 
binary
 and the 
offset_column
 is a positive constant. Base learners worked. Here is the code:


train[""offset""]<-train[""log_hazard""] # offset column in the training set

my_gbm <- h2o.gbm(x = x, y = y, training_frame = train,
                  fold_column = ""fold_id"",
                  keep_cross_validation_predictions = TRUE,
                  offset_column = ""offset"",
                  seed = 1) 

my_glm <- h2o.glm(x = x, y = y, training_frame = train,
              fold_column = ""fold_id"",
              keep_cross_validation_predictions = TRUE,
              offset_column = ""offset"",
              seed = 1,family = ""binomial"")



Then I am passing the 
offset_column
 in 
h2o.stackedEnsemble()
 through 
metalerner_params
. Here is the code:


stack_model <- h2o.stackedEnsemble(x = x,
                             y = y,
                             training_frame = train,
                             base_models = list(my_gbm, my_glm),
                             metalearner_params = list(offset_column = ""offset""))



But I received the following error:




ERRR on field: _offset_column: Offset column 'offset' not found in the training frame




The 
offset_column
 is in the training data. I am not sure why I am receiving this error message. 


Then I tried running 
h2o.stackedEnsemble()
 without the 
metalerner_params
 option. Here is the code:


stack_model <- h2o.stackedEnsemble(x = x,
                               y = y,
                               training_frame = train,
                               base_models = list(my_gbm, my_glm))



and received the following warning message:




Warning message:
  In .h2o.startModelJob(algo, params, h2oRestApiVersion) :
    Dropping bad and constant columns: [offset].




I am not sure whether it ran properly. Can anyone please help me with this issue?","['r', 'h2o']",Unknown,,N/A
59434658,59434658,2019-12-21T08:25:55,2019-12-21 14:20:35Z,0,"I have been struggling to get a Multi Node H2O cluster up and running using AWS EC2 instances.
I have followed the advice from 
this
 thread, but still struggle with the nodes not seeing each other. The EC2 instances all use the same AMI that I have pre-built, so the same 
h2o.jar
 file is on all of them,


I have also tried the following troubleshooting advice:




Name cluster 
-name


Rather use 
-network
 flag


Open port 54321 on security group as 
0.0.0.0




Here are my steps:


1) Start AWS EC2 in same availability zone and get private IPs and network cidr (
172.31.0.0/20
). Put ip addresses into 
flatfile.txt


172.31.8.210:54321
172.31.9.207:54321
172.31.13.136:54321



2) Copy the 
flatfile.txt
 to all servers to which I want to connect as nodes and start H2O


# cluster_run
library(h2oEnsemble)
library(ssh)

ips <- gsub(""(.*):.*"", ""\\1"", readLines(""flatfile.txt""))

start_cluster <- function(ip){
  # Copy flatfile across
  session <- ssh_connect(paste0(""ubuntu@"", ip), keyfile = ""mykey.pem"")
  scp_upload(session, ""flatfile.txt"")

  # Ensure no h2o instance is already running
  out <- ssh_exec_wait(session, ""sudo pkill java"")

  # Start H2O cluster
  cmd <- gsub(""\\s+"", "" "", paste0(""ssh -i mykey.pem -o 'StrictHostKeyChecking no' ubuntu@"", ip, 
         "" 'java -Xmx20g 
         -jar /home/rstudio/R/x86_64-pc-linux-gnu-library/3.5/h2o/java/h2o.jar
         -name mycluster
         -network 172.31.0.0/20
         -flatfile flatfile.txt 
         -port 54321 &'""))
  system(cmd, wait = FALSE)

}
start_cluster(ips[3])
start_cluster(ips[2])
start_cluster(ips[1])



3) Once this has been done, I now want to connect 
R
 to my new Multi Node cluster


 h2o.init(startH2O = F)
 h2o.shutdown(prompt = FALSE)



This is where I see that the nodes aren't being picked up:



I have also seen that when I start the H2O cluster on the different nodes, it isnt picking up the other machines within the network:","['r', 'amazon-ec2', 'h2o']",Hanjo Odendaal,https://stackoverflow.com/users/5620975/hanjo-odendaal,"1,431"
59409214,59409214,2019-12-19T11:50:42,2019-12-19 11:50:42Z,221,"I am making a classification model using H2O in python. I am able to build a GBM model and make predictions on training and test dataset whereas when I build an XGBoost model and try to make predictions.


Below is the GBM code: (Runs perfectly fine)


from h2o.estimators.gbm import H2OGradientBoostingEstimator

model_rf_v3 = H2OGradientBoostingEstimator(model_id='mojo_test_v4', ntrees=259, max_depth=6, categorical_encoding = 'OneHotExplicit', learn_rate = 0.1)

model_rf_v3.train(y = myResponse_rf,x = myCat + myNum, training_frame=hf_train_h2o,
            validation_frame = hf_test_h2o)

pred = model_rf_v3.predict(hf_test_h2o)[:,2]



XGBoost code: (Fails)


from h2o.estimators.xgboost import H2OXGBoostEstimator

model_rf_vn = H2OXGBoostEstimator(ntrees=259, learn_rate = 0.05, stopping_metric = ""misclassification"", categorical_encoding = 'OneHotExplicit', tree_method=""hist"", grow_policy=""lossguide"", max_depth = 9)

model_rf_vn.train(y = myResponse_rf,x = myCat + myNum, training_frame=hf_train_h2o,
            validation_frame = hf_test_h2o)

pred = model_rf_vn.predict(hf_test_h2o)[:,2] ## Error at this point



Error:


job with key $0300ffffffff$_9cacec5e7e4540e343f43ac2ce3e459e failed with an exception: DistributedException from ha880datanode-14.fab4.prod.booking.com/10.220.205.163:54321: '63', caused by java.lang.ArrayIndexOutOfBoundsException: 63""



If the error would have been in dataset, I think GBM should also given an error. Does the predict function for XGBoost work in a different way or am I missing something?


Thanks in advance,


Vishal","['python', 'h2o', 'xgboost', 'sparkling-water', 'xgbclassifier']",Vishal Gupta,https://stackoverflow.com/users/6599069/vishal-gupta,3
59398842,59398842,2019-12-18T19:25:21,2019-12-19 07:08:48Z,245,"I'm looking into H2O sparkling water AutoML using scala. I'm running it on my laptop on localhost. Even though I'm not adding any restrictions on H2OAutoML() class using setMaxModels or setMaxRuntimeSecs method. The model.fit method fails with an exception asking me to ease restrictions on setMaxModels or setMaxRuntimeSecs.


Exception in thread ""main"" ai.h2o.sparkling.ml.algos.H2OAutoML$$anon$1: No model returned from H2O AutoML. For example, try to ease your 'excludeAlgo', 'maxModels' or 'maxRuntimeSecs' properties.


Update: The dataset which I'm using diabeties.csv is a dataset for classification. But if I set metric using setSortMetric to AUTO then it works fine. It doesn't throw any exception but instead of classification it does regression on that dataset.


Here is the code:






def main(args: Array[String]): Unit = {
    println(""H2O AutoML"")
    println(""Creating Spark Session.."")
    val sparkConf = new SparkConf().setAppName(""H2OAutoML"").setMaster(""local[*]"")
      .set(""spark.ext.h2o.repl.enabled"",""false"")
      .set(""spark.driver.host"",""localhost"")
    val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    val hc = H2OContext.getOrCreate(sparkSession.sparkContext)
    val df = sparkSession.read.option(""header"",true).
      option(""inferschema"",true)
      .csv(""/Datasets/diabeties.csv"")

    df.show()

    df.schema.fields.foreach(x => println(x.dataType))
    val Array(trainingDF, testingDF) = df.randomSplit(Array(0.8, 0.2))
    val automl = new H2OAutoML()
    automl.setLabelCol(""diabetes"")
    automl.setSortMetric(""logloss"")
    val model = automl.fit(trainingDF)
    println(model.getModelDetails())

  }","['scala', 'apache-spark', 'h2o', 'automl', 'sparkling-water']",Unknown,,N/A
59397951,59397951,2019-12-18T18:16:01,2020-01-01 15:23:58Z,0,"[Edited for clarity] I'd like to create a H2O data frame in Python with a given number of rows and columns. I can see that 
there is a nice function in R
. Is there a corresponding one in Python?
Just to be precise, let's say I want to create a H2O dataframe of zeros with 10 rows and 5 columns. How do I do that? Something like this doesn't work:


h2o.createFrame(rows = 10, cols = 5, value = 0)","['python', 'dataframe', 'h2o']",Unknown,,N/A
59366199,59366199,2019-12-17T00:52:17,2019-12-18 02:31:38Z,0,"I am able to convert dataframe to h2oframe but how can I convert back to a dataframe? If this is possible not can I convert it to a python list?


import pandas as pd
import h2o
df = pd.DataFrame({'1': [2838, 3222, 4576, 5665, 5998], '2': [1123, 3228, 3587, 5678, 6431]})
data = h2o.H2OFrame(df)","['python', 'pandas', 'h2o']",JaredDudley04,https://stackoverflow.com/users/12548876/jareddudley04,123
59360013,59360013,2019-12-16T15:51:09,2019-12-17 07:28:13Z,489,"My dependencies:




python3.7


h2o==3.24.0.5




I serialized trained H2O autoencoder to Mojo format by means of:


autoencoder_model.download_mojo(path = './custom_path')



to use it later in another place for some sort of novelty detection.


Then I try to load this serialized model by the following methods:


model = h2o.upload_mojo('./custom_path/DeepLearning_model_python_1575291468476_40.zip')



It fails with:


generic Model Build progress: |▏ (failed)                                                 

    |   0%
*** OSError: Job with key $03017f00000132d4ffffffff$_9fccdffd1c011bdec61e5733e58f47e7 failed with an exception: java.lang.IllegalStateException: This is not a JSON Primitive.
stacktrace: 
java.lang.IllegalStateException: This is not a JSON Primitive.
        at com.google.gson.JsonElement.getAsJsonPrimitive(JsonElement.java:122)
        at hex.genmodel.descriptor.JsonModelDescriptorReader.extractTableFromJson(JsonModelDescriptorReader.java:71)
        at hex.genmodel.ModelMojoReader.readModelDescriptor(ModelMojoReader.java:194)
        at hex.genmodel.ModelMojoReader.readAll(ModelMojoReader.java:186)
        at hex.genmodel.ModelMojoReader.readFrom(ModelMojoReader.java:59)
        at hex.generic.Generic$MojoDelegatingModelDriver.computeImpl(Generic.java:64)
        at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:222)
        at water.H2O$H2OCountedCompleter.compute(H2O.java:1425)
        at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
        at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
        at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
        at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
        at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



Then I try another method from the documentation:


model = h2o.import_mojo('./custom_path/DeepLearning_model_python_1575291468476_40.zip')



It fails with the same exception:


generic Model Build progress: |▏ (failed)                                                     |   0%
*** OSError: Job with key $03017f00000132d4ffffffff$_8640c4218f00c561935274c25fe5429f failed with an exception: java.lang.IllegalStateException: This is not a JSON Primitive.
stacktrace: 
java.lang.IllegalStateException: This is not a JSON Primitive.
        at com.google.gson.JsonElement.getAsJsonPrimitive(JsonElement.java:122)
        at hex.genmodel.descriptor.JsonModelDescriptorReader.extractTableFromJson(JsonModelDescriptorReader.java:71)
        at hex.genmodel.ModelMojoReader.readModelDescriptor(ModelMojoReader.java:194)
        at hex.genmodel.ModelMojoReader.readAll(ModelMojoReader.java:186)
        at hex.genmodel.ModelMojoReader.readFrom(ModelMojoReader.java:59)
        at hex.generic.Generic$MojoDelegatingModelDriver.computeImpl(Generic.java:64)
        at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:222)
        at water.H2O$H2OCountedCompleter.compute(H2O.java:1425)
        at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
        at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
        at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
        at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
        at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



One mode method:


h2o.load_model('./custom_path/DeepLearning_model_python_1575291468476_40.zip')



Fails with:


*** h2o.exceptions.H2OResponseError: Server error java.lang.IllegalArgumentException:
  Error: Missing magic number 0x1CED at stream start
  Request: POST /99/Models.bin/



So none of these methods work, then I try another method which always works well for me:


h2o.mojo_predict_pandas(data_to_reconstruct, mojo_zip_path='./custom_path/DeepLearning_model_python_1575291468476_40.zip')



and it works but it returns reconstruction error values per every column and adds 'reconstr_' prefix at the beginning of every column name, not common reconstruction error as it's done by autoencoder_model.anomaly() method which returns frame with Reconstruction.MSE column.
The order of those values is different from what is presented in ""Reconstruction.MSE"".


How can I compute the same reconstruction error as it's done by model.anomaly() or can I fix the problem with loading of the serialized model by one of the methods?","['python-3.x', 'deep-learning', 'h2o', 'autoencoder']",Unknown,,N/A
59359438,59359438,2019-12-16T15:15:47,2019-12-18 02:30:02Z,0,"I have a machine learning problem: 88 instances, 2 classes (40 instances of ""FR"" class, 48 instances of ""RF"" class). I tried, by myself, several different algorithms and, evaluating the results with both cross-validation and leave-one-out, I could not reach more than 0.6 of accuracy. Here is the link to the dataset in csv format: 
https://drive.google.com/open?id=1lhCOP3Aywk4kGDEStAwL6Uq1H3twSJWS


Trying H2O AutoML with a 10-folds cross validation I reached more or less the same results:
cross-validation-leaderbord
.
But when I tried leave-one-out I had unexpectedly too much better results: 
leave-one-out-leaderboard


I performed the leave-one-out validation through the 
fold_column
 parameter by assigning to each instance a different fold, here is the code:


train <- read.csv(""training_set.csv"", header = TRUE)
train$ID <- seq.int(nrow(train))

# Identify predictors and response
y <- ""class""
x <- setdiff(setdiff(names(train), y), ""ID"")

# For binary classification, response should be a factor
train[,y] <- as.factor(train[,y])

# Run AutoML for 20 base models 
aml <- h2o.automl(x = x, y = y,
                  fold_column = ""ID"",
                  keep_cross_validation_predictions = TRUE,
                  keep_cross_validation_fold_assignment = TRUE,
                  sort_metric = ""logloss"",
                  training_frame = as.h2o(train),
                  max_models = 20,
                  seed = 1)

# View the AutoML Leaderboard
lb <- aml@leaderboard
print(lb, n = nrow(lb))



First of all I do not know if this is the proper way to perform leave-one-out, I tried also to set the n_folds to 88 but I had more or less the same results.
Here the information found in 
aml@leader@model[[""cross_validation_metrics""]]
:


H2OBinomialMetrics: stackedensemble
** Reported on cross-validation data. **
** 88-fold cross-validation on training data (Metrics computed for combined holdout predictions) **

MSE:  0.1248958
RMSE:  0.353406
LogLoss:  0.4083967
Mean Per-Class Error:  0.075
AUC:  0.8635417
pr_auc:  0.7441933
Gini:  0.7270833

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
       FR RF    Error   Rate
FR     34  6 0.150000  =6/40
RF      0 48 0.000000  =0/48
Totals 34 54 0.068182  =6/88

Maximum Metrics: Maximum metrics at their respective thresholds
                        metric threshold    value idx
1                       max f1  0.712894 0.941176  53
2                       max f2  0.712894 0.975610  53
3                 max f0point5  0.712894 0.909091  53
4                 max accuracy  0.712894 0.931818  53
5                max precision  0.712894 0.888889  53
6                   max recall  0.712894 1.000000  53
7              max specificity  0.739201 0.975000   0
8             max absolute_mcc  0.712894 0.869227  53
9   max min_per_class_accuracy  0.715842 0.850000  46
10 max mean_per_class_accuracy  0.712894 0.925000  53



Although this information seems consistent, another thing which leads me to think there is something wrong is the difference between the above confusion matrix and the one obtained by 
h2o.confusionMatrix(aml@leader)
:


Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.117307738035598:
       FR RF    Error    Rate
FR     18 22 0.550000  =22/40
RF      3 45 0.062500   =3/48
Totals 21 67 0.284091  =25/88



Why are the two confusion matrices different? Should not they find the same F1-optimal threshold?


Is there something wrong or is just the Stacked Ensemble that is so much better?","['r', 'machine-learning', 'cross-validation', 'h2o', 'automl']",Joseph,https://stackoverflow.com/users/12545147/joseph,23
59309659,59309659,2019-12-12T17:11:59,2019-12-12 18:12:06Z,0,"Using the 
h2o
 package, I am meeting an unexpected error. While using 
as.h2o
 on a dataset, it returned the error 
Cannot determine file type
. After a binary search, I found the problematic character: A dollar sign.


Here is a reproducible example:


library(h2o)

h2o.init()

tmp <- data.frame(text = ""$"", stringsAsFactors = FALSE)
data <- as.h2o(tmp)



And the error:


ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/3/ParseSetup)

water.exceptions.H2OIllegalArgumentException
 [1] ""water.exceptions.H2OIllegalArgumentException: Cannot determine file type. for /tmp/RtmpseoDZt/file36123d495362.csv_sid_ae2a_2""
 [2] ""    water.api.ParseSetupHandler.guessSetup(ParseSetupHandler.java:46)""                                                        
 [3] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                              
 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                                            
 [5] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                                    
 [6] ""    java.lang.reflect.Method.invoke(Method.java:498)""                                                                         
 [7] ""    water.api.Handler.handle(Handler.java:60)""                                                                                
 [8] ""    water.api.RequestServer.serve(RequestServer.java:462)""                                                                    
 [9] ""    water.api.RequestServer.doGeneric(RequestServer.java:295)""                                                                
[10] ""    water.api.RequestServer.doPost(RequestServer.java:221)""                                                                   
[11] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                                             
[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                             
[13] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                                   
[14] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)""                                               
[15] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                                       
[16] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)""                                                
[17] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                                        
[18] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                            
[19] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                    
[20] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                          
[21] ""    water.webserver.jetty8.Jetty8ServerAdapter$LoginHandler.handle(Jetty8ServerAdapter.java:119)""                             
[22] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                    
[23] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                          
[24] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                                  
[25] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                           
[26] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                            
[27] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)""                                 
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)""                 
[29] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)""                                                         
[30] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)""                                                    
[31] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                                   
[32] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                             
[33] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                                         
[34] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                          
[35] ""    java.lang.Thread.run(Thread.java:748)""                                                                                    

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Cannot determine file type. for /tmp/RtmpseoDZt/file36123d495362.csv_sid_ae2a_2



Any idea how to solve this issue? I would like to fit a Word2Vec model, so characters are actually important.


Thanks.


Edit


I keep investigating about this and found some extra information. Apparently, when the dataset reaches a 
big enough
 size, the problem doesn't appear anymore.


I found the sweet spot at about 7000 rows. For example, with my dataset:


tmp <- as.h2o(data[392949:399994])
 doesn't return an error.


But 
tmp <- as.h2o(data[392950:399994])
 does return an error.


Note that the 
$
 character appear at index 399994. In other examples, I also found the sweet spot to be around 7000 rows (not exactly the same number all the time, just close to 7000). I couldn't build a reproducible for this by adding only 
""a""
 or random letters in the data.frame.","['r', 'h2o']",Unknown,,N/A
59286816,59286816,2019-12-11T13:24:51,2019-12-17 07:39:38Z,0,"According to the H2O documentation, the threshold used at prediction is the max F1 threshold from train.
The performance function, 


h2o.performance(model, newdata = test)



actually run the prediction on the test set in order to compute the confusion matrix.


Strangely I am getting different confusion matrix while predicting the same test set using :


h2o.predict(object, newdata=test).



It means that 
h2o.performance()
 is using a different threshold from 
h2o.predict()
.
I am wondering how can i dictate the threshold upon prediction.","['r', 'h2o']",Nakx,https://stackoverflow.com/users/8112247/nakx,"1,590"
59281114,59281114,2019-12-11T07:55:56,2020-04-24 00:59:00Z,0,"I am getting different predictions for the same test data set from both 
h2o.predict
 and 
h2o.mojo_predict_df
. When compared - roughtly 50% of records have same probabilities but 50% are different with some where probabilities change drastically  =e.g. 0.88 to 0.55 for the same class. 


The modelling algorithm used is 
h2o.gbm
 and 
h2o.download_mojo(gbm_model,get_genmodel_jar = T)


I am trying to research and have found a few more posts with similar questions but no solution :


Reproduce predictions with MOJO file of a H2O GBM model


GLM model: h2o.predict gives very different results depending on number of rows used in the validation data


Why do I get different predictions with MOJO?


The codes used so far are as below :


# h2o start the cluster


h2o.init(nthreads=10,min_mem_size = '80g')

# variables 

predictors=c(1:76,78:681)
response=77

# getting datasets ready 

model_ready_df = model_ready_df %>% mutate_if(is.character,as.factor)
train.h2o = as.h2o(model_ready_df)
poc_test = poc_test %>% mutate_if(is.character,as.factor)
test.h2o <- as.h2o(poc_test)


# build model 

gbm_model <- h2o.gbm(x = predictors, y =response, training_frame = train.h2o , seed = 0xDECAF,ntrees = 1000, max_depth = 4,
                     learn_rate = 0.1,stopping_rounds=50,min_rows = 50,distribution = ""bernoulli"",ignore_const_col=F,
                     histogram_type='QuantilesGlobal',sample_rate=0.7,col_sample_rate=0.7,keep_cross_validation_models = T)


# save model object

h2o.download_mojo(gbm_model,get_genmodel_jar = T)

# predict 

preds=as.data.frame(h2o.predict(gbm_model,test.h2o))
preds2=h2o.mojo_predict_df(poc_test, 'GBM_model_R_1576045840818_1.zip',genmodel_jar_path = 'h2o-genmodel.jar',verbose = F)

# save 

fwrite(preds,""pred_usual.csv"")
fwrite(preds2,""pred_mojo.csv"")



example","['java', 'r', 'prediction', 'h2o', 'gbm']",Unknown,,N/A
59273268,59273268,2019-12-10T18:22:15,2019-12-11 18:28:41Z,603,"Using balance_classes on AutoML H2O generated error ""java.lang.IllegalArgumentException: Error during sampling - too few points?""


I am trying to train a multiclass problem using an AutoML H2O model with nfolds=5 and balance_classes enabled:


There are three diffent labels on the dataframe:


target           Count
-------------  -------
não conhecido     3789
não provido      11039
provido           3225

[3 rows x 2 columns]



All the models failed with the message ""java.lang.IllegalArgumentException: Error during sampling - too few points?"".


I do not think there are too few points. Can someone explain the problem?


Parameters used:


        include_algos = [""DRF"", ""GBM"", ""StackedEnsemble""],
        seed=1234,
        nfolds = nfolds,
        balance_classes = True,
        max_runtime_secs = 86400,
        max_models=8,
        max_runtime_secs_per_model = 1200,
        keep_cross_validation_predictions = True,
        verbosity = ""debug"",



Logs:


Executando o treinamento do modelo do problema < tipo_decisao >...
AutoML progress: |
02:51:01.681: Project: automl_py_488_sid_932d
02:51:01.681: AutoML job created: 2019.12.10 02:51:01.680
02:51:01.681: Disabling Algo: DeepLearning as requested by the user.
02:51:01.682: Disabling Algo: XGBoost as requested by the user.
02:51:01.682: Disabling Algo: GLM as requested by the user.
02:51:01.682: Build control seed: 1234
02:51:01.706: training frame: Frame key: automl_training_py_488_sid_932d    cols: 1225    rows: 18053  chunks: 200    size: 192349542  checksum: 7379304490974335888
02:51:01.706: validation frame: NULL
02:51:01.706: leaderboard frame: NULL
02:51:01.706: blending frame: NULL
02:51:01.706: response column: target
02:51:01.706: fold column: null
02:51:01.706: weights column: null
02:51:01.737: Setting stopping tolerance adaptively based on the training frame: 0.007442610801832542
02:51:01.799: AutoML build started: 2019.12.10 02:51:01.799

█
02:51:04.812: Default Random Forest build failed: java.lang.IllegalArgumentException: Error during sampling - too few points?

██
02:51:07.831: GBM 1 failed: java.lang.IllegalArgumentException: Error during sampling - too few points?

██
02:51:10.844: GBM 2 failed: java.lang.IllegalArgumentException: Error during sampling - too few points?

██████
02:51:14.878: GBM 3 failed: java.lang.IllegalArgumentException: Error during sampling - too few points?

███
02:51:18.897: GBM 4 failed: java.lang.IllegalArgumentException: Error during sampling - too few points?

███
02:51:19.915: GBM 5 failed: java.lang.IllegalArgumentException: Error during sampling - too few points?

███
02:51:22.954: Extremely Randomized Trees (XRT) Random Forest build failed: java.lang.IllegalArgumentException: Error during sampling - too few points?
02:51:22.954: AutoML: starting GBM hyperparameter search

████████████████████████████████████| 100%

02:51:41.57: No models were built, due to timeouts or the exclude_algos option. StackedEnsemble builds skipped.
02:51:41.57: AutoML build stopped: 2019.12.10 02:51:41.57
02:51:41.57: AutoML build done: built 0 models
02:51:41.57: AutoML duration: 39.258 sec","['h2o', 'sampling']",Gustavo Orair,https://stackoverflow.com/users/1008690/gustavo-orair,106
59219818,59219818,2019-12-06T20:19:04,2019-12-22 16:30:03Z,587,"Very different Model Performance using XGBoost on H2O


I am training a XGBoost model using 5-fold croos validation on a very imbalanced binary classification problem. The dataset has 1200 columns (multi-document word2vec document embeddings).


The only parameters specified to train the XGBoost model were:




min_split_improvement = 1e-5


seed=1


nfolds = 5




The reported performance on train data was extremely high (probably overfitting!!!):


Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.2814398407936096: 
       A      D    Error    Rate
-----  -----  ---  -------  -------------
A      16858  2    0.0001   (2.0/16860.0)
D      0      414  0        (0.0/414.0)
Total  16858  416  0.0001   (2.0/17274.0)

AUC: 0.9999991404060721



The performance on cross validation data was terrible:


Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.016815993119962513: 
       A      D    Error    Rate
-----  -----  ---  -------  ----------------
A      16003  857  0.0508   (857.0/16860.0)
D      357    57   0.8623   (357.0/414.0)
Total  16360  914  0.0703   (1214.0/17274.0)

AUC: 0.6015883863129724



I know H2O cross validation generates an extra model using the whole data available and different performances are expected.
But, could be the cause that generated too bad performance on the resulting model?


Ps: XGBoost on a multi node H2O cluster with OMP


Model Type: classifier
Performance do modelo < XGBoost_model_python_1575650180928_617 >: 

ModelMetricsBinomial: xgboost
** Reported on train data. **

MSE: 0.0008688085383330077
RMSE: 0.029475558320971762
LogLoss: 0.00836528606162877
Mean Per-Class Error: 5.931198102016033e-05
AUC: 0.9999991404060721
pr_auc: 0.9975495622569983
Gini: 0.9999982808121441

Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.2814398407936096: 
       A      D    Error    Rate
-----  -----  ---  -------  -------------
A      16858  2    0.0001   (2.0/16860.0)
D      0      414  0        (0.0/414.0)
Total  16858  416  0.0001   (2.0/17274.0)

Maximum Metrics: Maximum metrics at their respective thresholds
metric                       threshold    value     idx
---------------------------  -----------  --------  -----
max f1                       0.28144      0.99759   195
max f2                       0.28144      0.999035  195
max f0point5                 0.553885     0.998053  191
max accuracy                 0.28144      0.999884  195
max precision                0.990297     1         0
max recall                   0.28144      1         195
max specificity              0.990297     1         0
max absolute_mcc             0.28144      0.997534  195
max min_per_class_accuracy   0.28144      0.999881  195
max mean_per_class_accuracy  0.28144      0.999941  195
max tns                      0.990297     16860     0
max fns                      0.990297     413       0
max fps                      0.000111383  16860     399
max tps                      0.28144      414       195
max tnr                      0.990297     1         0
max fnr                      0.990297     0.997585  0
max fpr                      0.000111383  1         399
max tpr                      0.28144      1         195

Gains/Lift Table: Avg response rate:  2.40 %, avg score:  2.42 %
    group    cumulative_data_fraction    lower_threshold    lift     cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain     cumulative_gain
--  -------  --------------------------  -----------------  -------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  -------  -----------------
    1        0.0100151                   0.873526           41.7246  41.7246            1                0.907782     1                           0.907782            0.417874        0.417874                   4072.46  4072.46
    2        0.0200301                   0.776618           41.7246  41.7246            1                0.834968     1                           0.871375            0.417874        0.835749                   4072.46  4072.46
    3        0.0300452                   0.0326301          16.4004  33.2832            0.393064         0.303206     0.797688                    0.681985            0.164251        1                          1540.04  3228.32
    4        0.0400023                   0.0224876          0        24.9986            0                0.0263919    0.599132                    0.518799            0               1                          -100     2399.86
    5        0.0500174                   0.0180858          0        19.9931            0                0.0201498    0.479167                    0.418953            0               1                          -100     1899.31
    6        0.100035                    0.0107386          0        9.99653            0                0.0136044    0.239583                    0.216279            0               1                          -100     899.653
    7        0.149994                    0.00798337         0        6.66692            0                0.00922284   0.159784                    0.147313            0               1                          -100     566.692
    8        0.200012                    0.00629476         0        4.99971            0                0.00709438   0.119826                    0.112249            0               1                          -100     399.971
    9        0.299988                    0.00436827         0        3.33346            0                0.00522157   0.0798919                   0.0765798           0               1                          -100     233.346
    10       0.400023                    0.00311204         0        2.49986            0                0.00370085   0.0599132                   0.0583548           0               1                          -100     149.986
    11       0.5                         0.00227535         0        2                  0                0.00267196   0.0479333                   0.0472208           0               1                          -100     100
    12       0.599977                    0.00170271         0        1.66673            0                0.00197515   0.039946                    0.0396813           0               1                          -100     66.6731
    13       0.700012                    0.00121528         0        1.42855            0                0.00145049   0.0342375                   0.034218            0               1                          -100     42.8548
    14       0.799988                    0.000837358        0        1.25002            0                0.00102069   0.0299588                   0.0300692           0               1                          -100     25.0018
    15       0.899965                    0.000507632        0        1.11115            0                0.000670878  0.0266306                   0.0268033           0               1                          -100     11.1154
    16       1                           3.35288e-05        0        1                  0                0.00033002   0.0239667                   0.0241551           0               1                          -100     0


Performance da validação cruzada (xval) do modelo < XGBoost_model_python_1575650180928_617 >: 

ModelMetricsBinomial: xgboost
** Reported on cross-validation data. **

MSE: 0.023504756648164406
RMSE: 0.15331261085822134
LogLoss: 0.14134815775808462
Mean Per-Class Error: 0.4160864407653825
AUC: 0.6015883863129724
pr_auc: 0.04991836222189148
Gini: 0.2031767726259448

Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.016815993119962513: 
       A      D    Error    Rate
-----  -----  ---  -------  ----------------
A      16003  857  0.0508   (857.0/16860.0)
D      357    57   0.8623   (357.0/414.0)
Total  16360  914  0.0703   (1214.0/17274.0)

Maximum Metrics: Maximum metrics at their respective thresholds
metric                       threshold    value      idx
---------------------------  -----------  ---------  -----
max f1                       0.016816     0.0858434  209
max f2                       0.00409934   0.138433   318
max f0point5                 0.0422254    0.0914205  127
max accuracy                 0.905155     0.976323   3
max precision                0.99221      1          0
max recall                   9.60076e-05  1          399
max specificity              0.99221      1          0
max absolute_mcc             0.825434     0.109684   5
max min_per_class_accuracy   0.00238436   0.572464   345
max mean_per_class_accuracy  0.00262155   0.583914   341
max tns                      0.99221      16860      0
max fns                      0.99221      412        0
max fps                      9.60076e-05  16860      399
max tps                      9.60076e-05  414        399
max tnr                      0.99221      1          0
max fnr                      0.99221      0.995169   0
max fpr                      9.60076e-05  1          399
max tpr                      9.60076e-05  1          399

Gains/Lift Table: Avg response rate:  2.40 %, avg score:  0.54 %
    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain       cumulative_gain
--  -------  --------------------------  -----------------  --------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  ---------  -----------------
    1        0.0100151                   0.0540408          4.34129   4.34129            0.104046         0.146278     0.104046                    0.146278            0.0434783       0.0434783                  334.129    334.129
    2        0.0200301                   0.033963           2.41183   3.37656            0.0578035        0.0424722    0.0809249                   0.094375            0.0241546       0.0676329                  141.183    237.656
    3        0.0300452                   0.0251807          2.17065   2.97459            0.0520231        0.0292894    0.0712909                   0.0726798           0.0217391       0.089372                   117.065    197.459
    4        0.0400023                   0.02038            2.18327   2.77762            0.0523256        0.0225741    0.0665702                   0.0602078           0.0217391       0.111111                   118.327    177.762
    5        0.0500174                   0.0174157          1.92946   2.60779            0.0462428        0.0188102    0.0625                      0.0519187           0.0193237       0.130435                   92.9463    160.779
    6        0.100035                    0.0103201          1.59365   2.10072            0.0381944        0.0132217    0.0503472                   0.0325702           0.0797101       0.210145                   59.3649    110.072
    7        0.149994                    0.00742152         1.06366   1.7553             0.0254925        0.00867473   0.0420687                   0.0246112           0.0531401       0.263285                   6.3664     75.5301
    8        0.200012                    0.00560037         1.11073   1.59411            0.0266204        0.00642966   0.0382055                   0.0200645           0.0555556       0.318841                   11.0725    59.4111
    9        0.299988                    0.00366149         1.30465   1.49764            0.0312681        0.00452583   0.0358935                   0.0148859           0.130435        0.449275                   30.465     49.7642
    10       0.400023                    0.00259159         1.13487   1.40692            0.0271991        0.00306994   0.0337192                   0.0119311           0.113527        0.562802                   13.4872    40.6923
    11       0.5                         0.00189            0.579844  1.24155            0.0138969        0.00220612   0.0297557                   0.00998654          0.057971        0.620773                   -42.0156   24.1546
    12       0.599977                    0.00136983         0.990568  1.19972            0.0237406        0.00161888   0.0287534                   0.0085922           0.0990338       0.719807                   -0.943246  19.9724
    13       0.700012                    0.000980029        0.676094  1.1249             0.0162037        0.00116698   0.02696                     0.0075311           0.0676329       0.78744                    -32.3906   12.4895
    14       0.799988                    0.00067366         0.797286  1.08395            0.0191083        0.000820365  0.0259787                   0.00669244          0.0797101       0.86715                    -20.2714   8.39529
    15       0.899965                    0.000409521        0.797286  1.05211            0.0191083        0.000540092  0.0252155                   0.00600898          0.0797101       0.94686                    -20.2714   5.21072
    16       1                           2.55768e-05        0.531216  1                  0.0127315        0.000264023  0.0239667                   0.00543429          0.0531401       1                          -46.8784   0","['machine-learning', 'cross-validation', 'h2o', 'xgboost']",Gustavo Orair,https://stackoverflow.com/users/1008690/gustavo-orair,106
59186548,59186548,2019-12-05T00:34:18,2023-03-02 20:27:24Z,0,"I'm working with h2o (latest version 3.26.0.10) on a Hadoop cluster. I've read in a parquet file from HDFS and have performed some manipulation on it, built a model, etc. 


I've stored some important results in an 
H2OFrame
 that I wish to export to local storage, instead of HDFS. Is there a way to export this file as a parquet?


I tried using 
h2o.exportFile
, documentation here: 
http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/h2o.exportFile.html
 but all the examples are for writing .csv. I tried using the a file path with 
.parquet
 as an extension and that didn't work. It wrote a file but I think it was basically a .csv as it was identical file size to the .csv.


example: 
h2o.exportFile(iris_hf, path = ""/path/on/h2o/server/filesystem/iris.parquet"")


On a related note, if I were to export my 
H2OFrame
 to HDFS instead of local storage, would it be possible to write that in parquet format? I could at least then move that to local storage.","['r', 'hadoop', 'parquet', 'h2o']",Hutch3232,https://stackoverflow.com/users/9244371/hutch3232,428
59161989,59161989,2019-12-03T17:00:10,2019-12-03 18:54:25Z,92,"I am trying to export a trained model as a MOJO, but when I run: 


aml.leader().getMojo().writeTo(os);



I get 


water.exceptions.H2ONotFoundArgumentException
water.api.SchemaServer.schema(SchemaServer.java:281)
Failed to find schema for version: 3 and type: GLMModel



I am running H2O for a Java project, so after starting the H2OApp, I also registered the schemas.","['java', 'h2o']",Kevin Hernandez,https://stackoverflow.com/users/5875610/kevin-hernandez,"1,390"
59155659,59155659,2019-12-03T11:04:12,2019-12-03 15:31:10Z,174,"I'm planning to use H2O with sparkling water (vesion 3.26.0.2) in production.

Thus it is reuired to disable Flow UI that is available via port 54321.

I was unable to find a configuration property to achieve this.


Is it possible or do I have to restrict access via firewall?","['h2o', 'sparkling-water']",Gandras,https://stackoverflow.com/users/2029041/gandras,23
59146506,59146506,2019-12-02T20:54:11,2020-01-24 03:23:22Z,147,"When predicting values in a multiclass classification problem, I would like to get the probability of the predicted value.


I tried to solve this by using H2O's apply function:


predicted_df = modelo_assessor.predict(to_predict_h2o_frame)
predicted_df.apply((lambda x: x.max()), axis=1)



But it does not work:




'ValueError: unimpl bytecode instr: CALL_METHOD'




Maybe it doesn't work because h2o.max does not have axis parameter as h2o.mean does???
I couldn't find the documentation of which operations are supported on apply function.


I would like to solve the problem using h2o data manipulation similarly to this pandas code:


    predicted_df = modelo_assessor.predict(to_predict_h2o_frame).as_data_frame()
    predicted_df['PROB_PREDICTED']=predicted_df.iloc[:,1:].max(axis=1)","['h2o', 'multiclass-classification']",Gustavo Orair,https://stackoverflow.com/users/1008690/gustavo-orair,106
59146086,59146086,2019-12-02T20:19:20,2019-12-02 20:27:19Z,0,"Quite new to h2o. I updated my h2o-R package to 3.26 and I also installed the h2o cluster one by running 




java -jar h2o.jar
'




After running both and the h2o flow was accessible on the browser, when I run h2o.init(), it's still telling me that my h2o cluster is at 3.24 and h2o-R package is at 3.26.


Do I need to remove a previous h2o.jar version? How would I go about doing that?


I have forced it to work by initializing h2o by having the strict version check be false but, I want both h2o and h2o-R to be on the same version.


Thanks a lot!


Zarni


P.S. I have attached screenshots of the errors.


h2o.init() Error!




.jar run 01




.jar run 02","['r', 'h2o']",Zarni,https://stackoverflow.com/users/962885/zarni,109
59138734,59138734,2019-12-02T12:14:15,2019-12-10 06:48:43Z,0,"How can I subset a h2o frame in python based on a list, rather than 
a single value
?
For example, in R one would use 
%in%
, or in panda 
I can see
 one can use 
.isin()
.
I'd like to do something like this, if it makes sense:


df.loc[df['column name'].isin(pickTheseValues)]



How does it work in h2o? Alternatively, can I cast the h2o data frame into panda and do the operation as above?","['python', 'dataframe', 'subset', 'h2o']",Nonancourt,https://stackoverflow.com/users/1651933/nonancourt,559
59077793,59077793,2019-11-27T20:10:08,2019-12-09 14:53:19Z,183,"We are trying to run coxph model using h2o,Rsparkling for large data set with 6 GB with 300 columns, whatever the configuration we take for spark, we are getting memory issues. 


As per h2o, we should only have 4 times data size bigger cluster, but we took even 128GB 4 worker nodes with a 128 master node. But still its raising issues. 


Please help us to choose the spark configuration needed to run h2o with our current data set. We are able to run the same code for 50,000 records. 


we have 300 columns for X and 2  pairs of interaction terms. offset column and weights as well. 


You can find the sample code here but it doesnt have 300 column. I don't know how I can give the perfect input file and full code to replicate the issue. Please let me know if you prefer to see the actual code with 300 columns. 


`# Load the libraries used to analyze the data
 library(survival)
 library(MASS)
 library(h2o)


 # Create H2O-based model
 predictors <- c(""HasPartner"", ""HasSingleLine"", ""HasMultipleLines"",
            ""HasPaperlessBilling"", ""HasAutomaticBilling"", 
 ""MonthlyCharges"",
            ""HasOnlineSecurity"", ""HasOnlineBackup"", ""HasDeviceProtection"",
            ""HasTechSupport"", ""HasStreamingTV"", ""HasStreamingMovies"")

 h2o_model <- h2o.coxph(x = predictors,
                   event_column = ""HasChurned"",
                   stop_column = ""tenure"",
                   stratify_by = ""Contract"",
                   training_frame = churn_hex)

  print(summary(h2o_model))'","['h2o', 'sparkling-water']",Jacek Laskowski,https://stackoverflow.com/users/1305344/jacek-laskowski,74.5k
59002720,59002720,2019-11-22T22:44:25,2021-03-09 13:33:12Z,666,"""The lasso method requires initial standardization of the regressors,
so that the penalization scheme is fair to all regressors. For
categorical regressors, one codes the regressor with dummy variables
and then standardizes the dummy variables"" (p. 394).


Tibshirani, R. (1997). The lasso method for variable selection in the Cox model.

Statistics in medicine
, 
16
(4), 385-395. 
http://statweb.stanford.edu/~tibs/lasso/fulltext.pdf




H2O:


Similar to package ‘glmnet,’ the h2o.glm function includes a ‘standardize’ parameter that is true by default. However, if predictors are stored as factors within the input H2OFrame, H2O does not appear to standardize the automatically encoded factor variables (i.e., the resultant dummy or one-hot vectors). I've confirmed this experimentally, but references to this decision also show up in the source code:


For instance, method denormalizeBeta (
https://github.com/h2oai/h2o-3/blob/553321ad5c061f4831c2c603c828a45303e63d2e/h2o-algos/src/main/java/hex/DataInfo.java#L359
) includes the comment ""denormalize only the numeric coefs (categoricals are not normalized)."" It also looks like means (variable _normSub) and standard deviations (inverse of variable _normMul) are only calculated for the numerical variables, and not the categorical variables, in the setTransform method (
https://github.com/h2oai/h2o-3/blob/553321ad5c061f4831c2c603c828a45303e63d2e/h2o-algos/src/main/java/hex/DataInfo.java#L599
).


GLMnet:


In contrast, package 'glmnet' seems to expect categorical variables to be dummy-coded prior to fitting a model, using a function like model.matrix. The dummy variables are then standardized along with the continuous variables. It seems like the only way to avoid this would be to pre-standardize the continuous predictors only, concatenate them with the dummy variables, and then run glmnet with standardize=FALSE.


Statistical Considerations:


For a dummy variable or one-hot vector, the mean is the proportion of TRUE values, and the SD is directly proportional to the mean. The SD reaches its maximum when the proportion of TRUE and FALSE values is equal (i.e., 
σ
 = 0.5), and the sample SD (
s
) approaches 0.5 as 
n
 → ∞. Thus, if continuous predictors are standardized to have SD = 1, but dummy variables are left unstandardized, the continuous predictors will have at least twice the SD of the dummy predictors, and more than twice the SD for imbalanced dummy variables.


It seems like this could be a problem for regularization (LASSO, ridge, elastic net), because the scale/variance of predictors is expected to be equal so that the regularization penalty (λ) applies evenly across predictors. If two predictors A and B have the same standardized effect size, but A has a smaller SD than B, A will necessarily have a larger unstandardized coefficient than B. This means that, if left unstandardized, the regularization penalty will erroneously be more severe to A than B. In a regularized regression with a mixture of standardized continuous predictors and unstandardized categorical predictors, it seems like this could lead to systematic over-penalization of categorical predictors.


A commonly expressed concern is that standardizing dummy variables removes their normal interpretation. To avoid this issue, while still placing continuous and categorical predictors on an equal footing, Gelman (2008) suggested standardizing continuous predictors by dividing by 2 SD, rather than 1, resulting in standardized predictors with SD = 0.5. However, it seems like this would still be biased for class-imbalanced dummy variables, for which the SD might be substantially less than 0.5.




Gelman, A. (2008). Scaling regression inputs by dividing by two
standard deviations. Statistics in medicine, 27(15), 2865-2873.

http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf




Question:


Is H2O's approach of not standardizing one-hot vectors for regularized regression correct? Could this lead to a bias toward over-penalizing dummy variables / one-hot vectors? Or has Tibshirani (1997)'s recommendation since been revised for some reason?","['h2o', 'glmnet', 'lasso-regression', 'regularized', 'standardized']",Community,https://stackoverflow.com/users/-1/community,1
59000235,59000235,2019-11-22T19:01:34,2020-05-05 09:09:43Z,0,"I used H2O version 3.26.0.5 to train a GBM model in a binary problem, to predict the probability of positive class. I saved the model file as MOJO and used this file to generate predictions in new data:


## first, restart R session ##

# load the model
library(h2o)

h2o.init(nthreads = -1)

model <- h2o.import_mojo(""path_to_mojo_file"")

# load the new data input
input <- read_csv(""path_to_new_data"")

input_h2o <- as.h2o(input)

# predictions
predictions <- predict(model, input_h2o)



When I run this in my computer I get different predictions than when I use the same MOJO file to predict in a production environment.


Does this should happen with the MOJO file? I believed that once the model was saved in MOJO format, you could make predictions in any environment and get the same results. Does anyone knows why this is happening?","['r', 'machine-learning', 'prediction', 'h2o', 'gbm']",jessicalfr,https://stackoverflow.com/users/7111951/jessicalfr,79
58997372,58997372,2019-11-22T15:34:23,2019-11-26 09:32:20Z,0,"Stratified sampling is old, and very significant.  




Donald Knuth (high priest of computer science) uses it for evaluating the work of his PhD students, and for teaching his deeply and sincerely held religious beliefs. (
link
)


Royal Society article from 1934 on the topic. (
link
)




In the r-interface to h2o.ai they have a method to split frames ""h2o.splitframe"".  Is there a way to make a stratified split along the distinct elements of another column?  


Here are R packages that do not do this in h2o:    




https://www.rdocumentation.org/packages/fifer/versions/1.0/topics/stratified","['h2o', 'subsampling']",Unknown,,N/A
58966949,58966949,2019-11-21T03:41:16,2020-10-06 07:16:02Z,118,"Closed
. This question needs to be more 
focused
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Update the question so it focuses on one problem only by 
editing this post
.






Closed 
4 years ago
.















                        Improve this question
                    








Can we do Image Classification on own dataset using H2o Driverless-ai latest stable 1.8 version","['h2o', 'driverless-ai']",Vishal Meshram,https://stackoverflow.com/users/12399026/vishal-meshram,1
58916070,58916070,2019-11-18T13:49:11,2019-11-27 13:00:35Z,216,"I am seeing strange behaviour between file types when parsing the data. I am using a small dataset (200 rows, 34 columns) that is in both parquet and CSV format.


When I parse the CSV file I can see that boolean values are correctly identified as an enum see below (male/female):


print(h2o_frame_csv.types)

{'C1': 'int', 'userId': 'int', 'itemId': 'int', 'rating': 'int', 'timestamp': 'int', 'movieId': 'int', 'movieTitle': 'enum', 'releaseDate': 'time', 'videoReleaseDate': 'int', 'imdbUrl': 'enum', 'unknown': 'int', 'Action': 'int', 'Adventure': 'int', 'Animation': 'int', 'Childrens': 'int', 'Comedy': 'int', 'Crime': 'int', 'Documentary': 'int', 'Drama': 'int', 'Fantasy': 'int', 'FilmNoir': 'int', 'Horror': 'int', 'Musical': 'int', 'Mystery': 'int', 'Romance': 'int', 'SciFi': 'int', 'Thriller': 'int', 'War': 'int', 'Western': 'int', 'age': 'int', 'occupation': 'enum', 'zipCode': 'int', 'male': 'enum', 'female': 'enum'}



However, when I used the parquet version of the files I am seeing the same values being treated as int values 


print(h2o_frame_parquet.types)

{'Unnamed: 0': 'int', 'userId': 'int', 'itemId': 'int', 'rating': 'int', 'timestamp': 'int', 'movieId': 'int', 'movieTitle': 'enum', 'releaseDate': 'time', 'videoReleaseDate': 'int', 'imdbUrl': 'enum', 'unknown': 'int', 'Action': 'int', 'Adventure': 'int', 'Animation': 'int', 'Childrens': 'int', 'Comedy': 'int', 'Crime': 'int', 'Documentary': 'int', 'Drama': 'int', 'Fantasy': 'int', 'FilmNoir': 'int', 'Horror': 'int', 'Musical': 'int', 'Mystery': 'int', 'Romance': 'int', 'SciFi': 'int', 'Thriller': 'int', 'War': 'int', 'Western': 'int', 'age': 'int', 'occupation': 'enum', 'zipCode': 'int', 'male': 'int', 'female': 'int', '__index_level_0__': 'int'}



This becomes an issue when trying to train a classifier model. Some metrics are not available as h2o deems this be regressor rather than binomial. See below


print(f""For {file_type} dataset the metric class is {type(xgb.model_performance(xval=True))}"")

For csv dataset the metric class is <class 'h2o.model.metrics_base.H2OBinomialModelMetrics'>

For parquet dataset the metric class is <class 'h2o.model.metrics_base.H2ORegressionModelMetrics'>



What is the reason for treating boolean values as numeric (int) when parsing the parquet file? Are booleans not considered categorical enums like in the CSV file?","['python', 'csv', 'parquet', 'h2o']",Unknown,,N/A
58912175,58912175,2019-11-18T10:12:33,2019-11-18 11:31:16Z,999,"I have 2 instances running for H2O Python on Port 54321 as well as 54322. So, when, script which is using cluster on port 54321 completes execution, it closes the session. But, it also turn off cluster on Port 54322. Not sure, why it is happening. Is it expected behavior?


Thanks",['h2o'],Shivkumar Agrawal,https://stackoverflow.com/users/7873546/shivkumar-agrawal,41
58906453,58906453,2019-11-18T00:09:32,2019-11-18 00:20:23Z,0,"I am trying to build a ML model in Azure Machine Learning using H2o AutoML and could successfully create the model and do prediction.
What I am struggling with is to download the result as csv (ideally to my local PC).


The code I used is :


#Predict on the whole dataset
pred = best_model.predict(data)
data_pred = data.cbind(pred)

# Download as csv
h2o.download_csv(data_pred,'data_pred .csv')



The above code runs without any error & shows 
'/mnt/azmnt/code/Users/SA/data_pred.csv'
 as the result message. I assume the csv has been created succesfully.


But I don't know where to locate it.
I searched in AzureML datasets but there is none. Appreciate if someone can help me with this. Thanks","['python', 'download', 'h2o', 'automl', 'azure-machine-learning-service']",Math Lover,https://stackoverflow.com/users/5428238/math-lover,177
58813442,58813442,2019-11-12T06:52:26,2019-11-12 11:28:02Z,341,"I have tried h2o in one dataset with the exact same code, and now trying to try with another dataset. 
But I keep getting 'Unexpected HTTP error'


The code sample is as follows:


import h2o
h2o.init()
train_data = h2o.import_file(""pathtofile.csv"")
x = train_data.columns
y = ""Class""
x.remove(y)
train_data[y] = train_data[y].asfactor()
from h2o.automl import H2OAutoML
aml = H2OAutoML(max_models=10, seed=1,  max_runtime_secs=57600)
aml.train(x=x, y=y, training_frame=train_data)



The error at this point is:


---------------------------------------------------------------------------
H2OConnectionError                        Traceback (most recent call last)
<ipython-input-14-435d6f31b64e> in <module>()
      1 from h2o.automl import H2OAutoML
      2 aml = H2OAutoML(max_models=10, seed=1,  max_runtime_secs=57600)
----> 3 aml.train(x=x, y=y, training_frame=train_data)

/opt/anaconda3/envs/ege/lib/python2.7/site-packages/h2o/automl/autoh2o.pyc in train(self, x, y, training_frame, fold_column, weights_column, validation_frame, leaderboard_frame, blending_frame)
    443         poll_updates = ft.partial(self._poll_training_updates, verbosity=self._verbosity, state={})
    444         try:
--> 445             self._job.poll(poll_updates=poll_updates)
    446         finally:
    447             poll_updates(self._job, 1)

/opt/anaconda3/envs/ege/lib/python2.7/site-packages/h2o/job.pyc in poll(self, poll_updates)
     55             pb = ProgressBar(title=self._job_type + "" progress"", hidden=hidden)
     56             if poll_updates:
---> 57                 pb.execute(self._refresh_job_status, print_verbose_info=ft.partial(poll_updates, self))
     58             else:
     59                 pb.execute(self._refresh_job_status)

/opt/anaconda3/envs/ege/lib/python2.7/site-packages/h2o/utils/progressbar.pyc in execute(self, progress_fn, print_verbose_info)
    169                 # Query the progress level, but only if it's time already
    170                 if self._next_poll_time <= now:
--> 171                     res = progress_fn()  # may raise StopIteration
    172                     assert_is_type(res, (numeric, numeric), numeric)
    173                     if not isinstance(res, tuple):

/opt/anaconda3/envs/ege/lib/python2.7/site-packages/h2o/job.pyc in _refresh_job_status(self)
     92     def _refresh_job_status(self):
     93         if self._poll_count <= 0: raise StopIteration("""")
---> 94         jobs = h2o.api(""GET /3/Jobs/%s"" % self.job_key)
     95         self.job = jobs[""jobs""][0] if ""jobs"" in jobs else jobs[""job""][0]
     96         self.status = self.job[""status""]

/opt/anaconda3/envs/ege/lib/python2.7/site-packages/h2o/h2o.pyc in api(endpoint, data, json, filename, save_to)
    102     # type checks are performed in H2OConnection class
    103     _check_connection()
--> 104     return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
    105 
    106 

/opt/anaconda3/envs/ege/lib/python2.7/site-packages/h2o/backend/connection.pyc in request(self, endpoint, data, json, filename, save_to)
    439             else:
    440                 self._log_end_exception(e)
--> 441                 raise H2OConnectionError(""Unexpected HTTP error: %s"" % e)
    442         except requests.exceptions.Timeout as e:
    443             self._log_end_exception(e)

H2OConnectionError: Unexpected HTTP error: ('Connection aborted.', error(104, 'Connection reset by peer'))



I have tried 
h2o.cluster().shutdown()
 and killing the process but I keep getting the above error.","['python', 'h2o']",Unknown,,N/A
58764235,58764235,2019-11-08T09:56:33,2019-11-08 09:56:33Z,188,"I'm trying myself in creating a ML-based app. Server side in on python with the help of h2o, Client side is on nodejs + frontend.  


The server side part has the next structure:

 - run.sh (script that takes parameters from client side and runs the needed python script)

 - preprocess.py

 - analyse.py

 - visualize.py

 - load.py

 - upload.py  


Each script contains h20 cluster initialization:  


h2o.init()
h2o.no_progress()
...  
...  
h2o.cluster().shutdown()  
exit(0)



It works fine if only one process is ran. If I start another process - I will soon face cluster overload since all the computations are stored inside the cluster on the machine with h2o.  


Currently I tried to split processes via using Docker containers so that each script runs in separate docker container, but I'm starting to think that I chose the wrong way.


If there a way to split processes? Like running clusters on different ports for each call. So that shutting down of one process won't affect another running process.


Or maybe I should use some additional technologies? 


Any advises are welcome.","['python', 'h2o']",Unknown,,N/A
58736225,58736225,2019-11-06T18:13:42,2019-11-19 21:59:27Z,0,"I have followed this tutorial and was able to reproduce the results. However, the last graph confuses me. I understand most of the time it's probability, but why are there 
negative
 numbers? Since the response is Survived, how to interpret the numbers in the predictions? How to convert those numbers to Yes and No? 


https://www.h2o.ai/blog/finally-you-can-plot-h2o-decision-trees-in-r/


EIDT 11/19/2019:
 by the way, I did find a similar post on Cross Validated. The answer was not certain since it ended with a question mark.

https://stats.stackexchange.com/questions/374569/may-somebody-help-with-interpretation-of-trees-from-h2o-gbm-see-as-photo-attach
 


I filtered the data using the logic in the tree and looked at the unique prediction of the subset. I was able to find the threshold for 'yes' and 'no' predictions. I also changed the original code (starting line 34) so that the leaf shows the ultimate result of the numbers. However, this is just a way to hack the plot. If someone can tell me how the numbers are derived, that would be great.


    if(class(left_node)[[1]] == 'H2OLeafNode')
      leftLabel = ifelse(left_node@prediction >= threshold, 'Yes', 'No')
  else
    leftLabel = left_node@split_feature

  if(class(right_node)[[1]] == 'H2OLeafNode')
    rightLabel = ifelse(right_node@prediction >= threshold, 'Yes', 'No')
  else
    rightLabel = right_node@split_feature","['r', 'machine-learning', 'decision-tree', 'h2o']",Unknown,,N/A
58712625,58712625,2019-11-05T13:43:36,2022-04-15 17:22:42Z,100,"I have a large frame and used h2o flow run automl with a deep learning algo. However, the training metrics are calculated on a “temporary sample frame”. I could not find any info to this. I am not sure if the automl has been run on the full frame or just thus temp frame. Can someone help to understand or give a pointer? BTW, I don’t find this feature convenient.","['frame', 'h2o', 'temporary', 'automl']",General Grievance,https://stackoverflow.com/users/4294399/general-grievance,"4,967"
58691256,58691256,2019-11-04T10:13:54,2019-11-06 08:37:07Z,129,"I am trying to find the hour in a column which has the format of ""hhmmss"" i.e. ""90205"" where 9 indicates the hour. Some rows may not include seconds so it can be ""902"" and I need to still get the ""9"". 
An example of the column is as follows:


REQ_TIME
195426
508
140315
141432
203344
214103
63202
101807
110730
115052


I can do this in a regular dataframe as such:


df[""DATE""]=pd.to_datetime(df.REQ_DATE, format='%Y%m%d')
df[""TIME""]=df[""REQ_TIME""].apply(lambda x: str(x).zfill(6))
df['DATE_TIME']=df[['REQ_DATE','TIME']].apply(lambda x : '{} {}'.format(x[0],x[1]), axis=1)
df['DATE_TIME']=pd.to_datetime(df.DATE_TIME,infer_datetime_format=True)
df[""HOUR""]=df.DATE_TIME.dt.hour
df['YEAR'] = df.DATE.dt.year
df['MONTH'] = df.DATE.dt.month
df['DAY'] = df.DATE.dt.day
df['DAY_OF_WEEK']=df.DATE.dt.dayofweek



But my data is in an H2OFrame so I am not able to use regular python methods. I do not want to convert it to dataframe as well since it takes a long time. How can I do this in an H2OFrame?","['python', 'h2o']",Ege,https://stackoverflow.com/users/1043686/ege,941
58675365,58675365,2019-11-02T21:13:21,2019-11-02 22:55:27Z,320,"I'm trying to use autoML in the flow interface for a classification problem.


My response column is a enum data type with values of 1 and 0.


My data set is really imbalanced, around 0.5% of rows have a 1 response. 


I want to try the balance classes option, but every time I try it, the program ends up throwing errors.


If I check the balance classes option, am I required to also input values in the class_sampling_factors input box? If so, what do I put in? 


The documentation says:


""class_sampling_factors: (DRF, GBM, DL, Naive-Bayes, AutoML) Specify the per-class (in lexicographical order) over/under-sampling ratios. By default, these ratios are automatically computed during training to obtain the class balance. This option is only applicable for classification problems and when balance_classes is enabled.""


But it seems like the function fails to run unless I put something in. 


I've tried putting in 200.0, 1 and also 1.0,200.0 but neither seemed to work well.",['h2o'],Bob,https://stackoverflow.com/users/10643932/bob,41
58665502,58665502,2019-11-01T20:27:16,2019-11-18 13:13:18Z,0,"I'm doing a random forest with the next code: 


rf_md <- h2o.randomForest(training_frame = train_h,
                          nfolds = 5,
                          y = y,
                          ntrees = 500,
                          stopping_rounds = 10,
                          stopping_metric = ""RMSE"",
                          score_each_iteration = TRUE,
                          stopping_tolerance = 0.0001,
                          seed = 1234)



The output is an object with type: 
h2oregressionmodel
. I want to get a single tree from the forest, so I found out that the function 
getTree()
 works out very well with 
randomForest
 objects. How can I convert my actual output to a 
randomForest
 object? or Can I get the same random forest with a different function which its output is a 
randomForest
 object?


Appreciate your  help","['r', 'machine-learning', 'random-forest', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
58653362,58653362,2019-11-01T02:21:17,2019-11-01 15:37:01Z,0,"I run h2o automl function using R as described in the help and documentation page (accessed by 
typing ?h2o.automl
). The script is as follows:


library(h2o)
h2o.init()
votes_path <- system.file(""extdata"", ""housevotes.csv"", package = ""h2o"")
votes_hf <- h2o.uploadFile(path = votes_path, header = TRUE)
aml <- h2o.automl(y = ""Class"", training_frame = votes_hf, max_runtime_secs = 30)



After h2o finished its learning phase, I could retrieve the auc metrics of its models using


auc <- as.vector(aml@leaderboard[,""auc""])



Then I saved my session for later usage using


save.image(""automl_models.RData"")



The problem is, the auc retrieval did not succeed anymore after the h2o instance changed because, for example, h2o shut down function had been triggered or R session had been closed. This still happened even after I reactivate h2o instance using 
h2o.init()
. What I do now, is saving every model 
h2o.automl()
 has provided just after it finishes learning. 


The error messages when I tried to access auc are:


ERROR: Unexpected HTTP Status code: 400 Bad Request (url = http://localhost:54321/99/Rapids)
Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Name lookup of 'RTMP_sid_978c_5' failed



I would like to know if it is the right way or there is still a chance that I can have access to 
h2o.automl()
 results by only loading h2o variables saved in R environment.
FYI, my systems are:


H2O cluster version:        3.23.0.4468
R Version:                  R version 3.6.0 (2019-04-26)



Thank you.","['r', 'h2o', 'automl']",h45,https://stackoverflow.com/users/2986932/h45,246
58631145,58631145,2019-10-30T17:51:15,2019-10-30 18:15:58Z,0,"Here is my code (simple classification problem with h2o)


library(h2o)
h2o.init()
df_h2o <- as.h2o(Titanic)
y <- ""Survived""
x <- setdiff(names(df_h2o), y)
model_test <- h2o.gbm(training_frame = df_h2o, x = x, y = y)
pred_model_test <- h2o.predict(object = model_test, newdata = df_h2o)
as.data.frame(pred_model_test)



Here partial output of the last line:


predict        No       Yes
1       No 0.6665519 0.3334481
2       No 0.7618396 0.2381604
3      Yes 0.3836010 0.6163990
4       No 0.6665519 0.3334481
5       No 0.6665519 0.3334481
6       No 0.7618396 0.2381604
7      Yes 0.3836010 0.6163990
8       No 0.6665519 0.3334481
9      Yes 0.4391064 0.5608936
10     Yes 0.5561055 0.4438945
11     Yes 0.5684065 0.4315935



In row 11 predict column has 
Yes
, while 
Yes
 probability is only 0.4315935. What are the values in predict column then?","['r', 'classification', 'h2o', 'predict']",user1700890,https://stackoverflow.com/users/1700890/user1700890,"7,672"
58630426,58630426,2019-10-30T17:02:51,2019-11-01 13:30:38Z,0,"Hi I am currently building some models using R connected to a remote h2o instance running in a docker container.


I am at a point where I would like to iterate through a leaderboard of models and h2o.saveModel() each model to my local computer.


However, since saveModel() saves to the computer running h2o, my models are being saved within the docker container.


The solution I have come up with is to scp all the model files to my computer after h2o has finished running them, however I was wondering if anyone out there had run into this problem and had come up with a better solution.


Note : Running a local h2o instance for building the models is not an option


Thank you for the help","['r', 'docker', 'model', 'save', 'h2o']",V1cst3r,https://stackoverflow.com/users/10382081/v1cst3r,57
58615510,58615510,2019-10-29T21:13:27,2019-10-29 21:17:49Z,0,"I am running the following code in R


library(h2o)
h2o.init()  

df <- as.h2o(diamonds)



and get back the following error:


ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/3/Parse)

water.exceptions.H2OIllegalArgumentException
 [1] ""water.exceptions.H2OIllegalArgumentException: Provided column type ordered is unknown.  Cannot proceed with parse due to invalid argument.""
 [2] ""    water.parser.ParseSetup.strToColumnTypes(ParseSetup.java:240)""                                                                         
 [3] ""    water.api.ParseHandler.parse(ParseHandler.java:21)""                                                                                    
 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                                           
 [5] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                                                         
 [6] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                                                 
 [7] ""    java.lang.reflect.Method.invoke(Method.java:498)""                                                                                      
 [8] ""    water.api.Handler.handle(Handler.java:60)""                                                                                             
 [9] ""    water.api.RequestServer.serve(RequestServer.java:462)""                                                                                 
[10] ""    water.api.RequestServer.doGeneric(RequestServer.java:295)""                                                                             
[11] ""    water.api.RequestServer.doPost(RequestServer.java:221)""                                                                                
[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                                                          
[13] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                                          
[14] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                                                
[15] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)""                                                            
[16] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                                                    
[17] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)""                                                             
[18] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                                                     
[19] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                                         
[20] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                                 
[21] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                                       
[22] ""    water.webserver.jetty8.Jetty8ServerAdapter$LoginHandler.handle(Jetty8ServerAdapter.java:119)""                                          
[23] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                                 
[24] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                                       
[25] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                                               
[26] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                                        
[27] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                                         
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:984)""                                              
[29] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1045)""                              
[30] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)""                                                                      
[31] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:236)""                                                                 
[32] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                                                
[33] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                                          
[34] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                                                      
[35] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                                       
[36] ""    java.lang.Thread.run(Thread.java:748)""                                                                                                 

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Provided column type ordered is unknown.  Cannot proceed with parse due to invalid argument.



It is not very clear what is wrong.","['r', 'dataframe', 'h2o']",user1700890,https://stackoverflow.com/users/1700890/user1700890,"7,672"
58601802,58601802,2019-10-29T05:21:46,2019-10-29 05:21:46Z,127,"I want to train and deploy a h2o model using mlfow spark as mentioned in the diagram: 
https://res.infoq.com/presentations/mlflow-databricks/en/slides/sl21-1566324281761.jpg

I found 2 ways of training the model:
1. 
https://docs.databricks.com/_static/notebooks/h2o-sparkling-water-python.html

2. 
https://www.h2o.ai/blog/h2os-automl-in-spark


Using the first approach, I am able to train the H2o model but unable to deploy using mlflow spark as it expects a spark model.
When I try to use second approach, it throws below error during training while running sample example given in the link.
""pyspark.sql.utils.AnalysisException: ""cannot resolve '
label
' given input columns:""


Can anyone help and let me know which approach should I use for training the model in sparkling water and deploy the same in spark.","['apache-spark', 'pyspark', 'h2o']",ATUL AGARWAL,https://stackoverflow.com/users/11935985/atul-agarwal,101
58584899,58584899,2019-10-28T01:46:03,2019-10-28 01:46:03Z,36,"I'm not a skilled programmer. Have been using the flow user interface to try building some models in autoML.


I understand that the open source software doesn't do much in terms of adjusting features since that function is left to the commercial product.


I was wondering, are there easy steps I can take with the flow interface to help with improving features?


For example, I have one data set with something like 800 features. I think many of them are duplicative / correlated.


Would it be possible to reduce/collapse these by using the PCA model built into flow?  


If so, could someone offer some tips / steps on how to do it? I'm unsure of how to pull this off?


I assume it would be something like:


load train data set -> create pca model -> use pca model to reduce number of features -> run autoML on resulting data set


then


how to apply the same to score a testing data set?


does this make sense?


Thanks!",['h2o'],Bob,https://stackoverflow.com/users/10643932/bob,41
58581866,58581866,2019-10-27T17:48:37,2019-10-27 23:11:36Z,167,"I get the error below when I select to run 
XGBOOST
 via the menu in 
H2O-3
 flow UI or when I include it when running all architectures (also via the ""AutoML"" options in 
H2O-3
 flow). I only select the train + validation frames + response column and the click ""build model"".


Btw: I have set the following 2 enviroment variables:


Name: JRE_HOME + JAVA_HOME
Value: C:\Program Files\Java\jre1.8.0_211



I'm running 
Windows 10


java.lang.UnsatisfiedLinkError: ml.dmlc.xgboost4j.java.XGBoostJNI.XGDMatrixCreateFromMat([FIIF[J)I

10-24 21:05:31.201 192.168.0.126:54321 30544 FJ-1-7 INFO: Rebalancing train dataset into 8 chunks.
10-24 21:05:31.208 192.168.0.126:54321 30544 FJ-1-7 INFO: Completing model xgboost-ca6b679e-bed4-41a0-9129-9a28f38c8cc5
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: java.lang.UnsatisfiedLinkError: ml.dmlc.xgboost4j.java.XGBoostJNI.XGDMatrixCreateFromMat([FIIF[J)I
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at ml.dmlc.xgboost4j.java.XGBoostJNI.XGDMatrixCreateFromMat(Native Method)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at ml.dmlc.xgboost4j.java.DMatrix.(DMatrix.java:183)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at hex.tree.xgboost.XGBoost.hasGPU_impl(XGBoost.java:600)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at hex.tree.xgboost.XGBoost.hasGPU(XGBoost.java:581)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at hex.tree.xgboost.XGBoost.access$000(XGBoost.java:32)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at hex.tree.xgboost.XGBoost$XGBoostDriver.buildModel(XGBoost.java:242)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at hex.tree.xgboost.XGBoost$XGBoostDriver.computeImpl(XGBoost.java:237)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:222)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at water.H2O$H2OCountedCompleter.compute(H2O.java:1417)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
10-24 21:05:31.209 192.168.0.126:54321 30544 FJ-1-7 ERRR: at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)","['python', 'h2o', 'xgboost', 'xgbclassifier']",ashwin agrawal,https://stackoverflow.com/users/7702120/ashwin-agrawal,"1,611"
58576117,58576117,2019-10-27T02:06:44,2019-10-28 07:11:19Z,44,"I would like to know what the entries in the h2o logs mean. The logs came from a test (glm, poisson, y=testcolA) that ran for hours long (>= 6 hours and counting) that we had to stop it. A similar test (glm, poisson, y=testcolB) finished within an hour or so. Trainingset size is 131GB.


Cluster info:


R is connected to the H2O cluster:
   H2O cluster uptime:         17 seconds 996 milliseconds
   H2O cluster timezone:       Etc/UTC
   H2O data parsing timezone:  UTC
   H2O cluster version:        3.24.0.3
   H2O cluster version age:    5 months and 18 days !!!
   H2O cluster name:           h2o-ca60e2ab-9e9b-4154-bb35-a52fc092abdd
   H2O cluster total nodes:    12
   H2O cluster total memory:   372.00 GB
   H2O cluster total cores:    96
   H2O cluster allowed cores:  96
   H2O cluster healthy:        TRUE
   H2O Connection ip:          localhost
   H2O Connection port:        3000
   H2O Connection proxy:       NA
   H2O Internal Security:      FALSE
   H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4
   R Version:                  R version 3.6.0 (2019-04-26)



Log snippet:


...snip...
10-25 22:05:30.111 172.31.20.64:3000     206    FJ-1-23   INFO: Building H2O GLM model with these parameters:
10-25 22:05:30.122 172.31.20.64:3000     206    FJ-1-23   INFO: {""_train"":{""name"":""RTMP_sid_9e49_4"",""type"":""Key""},""_valid"":null,""_nfolds"":0,""_keep_cross_validation_models"":true,""_keep_cross_validation_predictions"":false,""_keep_cross_validation_fold_assignment"":false,""_parallelize_cross_validation"":true,""_auto_rebalance"":true,""_seed"":-1,""_fold_assignment"":""AUTO"",""_categorical_encoding"":""AUTO"",""_max_categorical_levels"":10,""_distribution"":""AUTO"",""_tweedie_power"":1.5,""_quantile_alpha"":0.5,""_huber_alpha"":0.9,""_ignored_columns"":[...snip...],""_ignore_const_cols"":true,""_weights_column"":null,""_offset_column"":null,""_fold_column"":null,""_check_constant_response"":true,""_is_cv_model"":false,""_score_each_iteration"":false,""_max_runtime_secs"":0.0,""_stopping_rounds"":3,""_stopping_metric"":""deviance"",""_stopping_tolerance"":1.0E-4,""_response_column"":snip,""_balance_classes"":false,""_max_after_balance_size"":5.0,""_class_sampling_factors"":null,""_max_confusion_matrix_size"":20,""_checkpoint"":null,""_pretrained_autoencoder"":null,""_custom_metric_func"":null,""_export_checkpoints_dir"":null,""_standardize"":true,""_family"":""poisson"",""_link"":""family_default"",""_solver"":""AUTO"",""_tweedie_variance_power"":0.0,""_tweedie_link_power"":1.0,""_theta"":1.0E-10,""_invTheta"":1.0E10,""_alpha"":null,""_lambda"":null,""_missing_values_handling"":""MeanImputation"",""_prior"":-1.0,""_lambda_search"":false,""_nlambdas"":-1,""_non_negative"":false,""_exactLambdas"":false,""_lambda_min_ratio"":-1.0,""_use_all_factor_levels"":false,""_max_iterations"":-1,""_intercept"":true,""_beta_epsilon"":1.0E-4,""_objective_epsilon"":-1.0,""_gradient_epsilon"":-1.0,""_obj_reg"":-1.0,""_compute_p_values"":false,""_remove_collinear_columns"":false,""_interactions"":null,""_interaction_pairs"":null,""_early_stopping"":true,""_beta_constraints"":null,""_max_active_predictors"":-1,""_stdOverride"":false}
10-25 22:05:30.127 172.31.20.64:3000     206    FJ-1-23   INFO: Dropping ignored columns: [..snip..]
10-25 22:05:30.132 172.31.20.64:3000     206    FJ-1-23   INFO: train dataset already contains 489 (non-empty)  chunks. No need to rebalance. [desiredChunks=8, rebalanceRatio=1.0]
10-25 22:05:35.360 172.31.20.64:3000     206    FJ-1-23   INFO: Starting model MyModel
10-25 22:05:35.557 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=0 lmb=.14E2 obj=530570.0 imp=.1E1 bdf=.11E2] Got 2151 active columns out of 2163 total
10-25 22:05:35.558 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=0 lmb=.14E2 obj=530570.0 imp=.1E1 bdf=.11E2] picked solver IRLSM
10-25 23:20:14.377 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver stopped after 10000 iterations. (max_iter=10000)
10-25 23:20:14.377 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver finished with gerr = 0.27587151573955293 >  eps = 1.0E-4
10-25 23:20:14.378 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=0 lmb=.14E2 obj=530570.0 imp=.1E1 bdf=.11E2] computed in 4383038+95782=4478820ms, step = 1, l1solver iter = 10000, gerr = 0.27587151573955293
10-26 00:31:02.569 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=1 lmb=.14E2 obj=525370.0 imp=.98E-2 bdf=.94E0] Scoring after 8732476ms
10-26 00:31:20.993 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=1 lmb=.14E2 obj=525370.0 imp=.98E-2 bdf=.94E0] Training metrics computed in 18424ms
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=1 lmb=.14E2 obj=525370.0 imp=.98E-2 bdf=.94E0] Model Metrics Type: RegressionGLM
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  Description: N/A
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  model id: MyModel
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  frame id: RTMP_sid_9e49_4
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  MSE: 3.63462328E11
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  RMSE: 602878.4
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  mean residual deviance: 525228.0
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  mean absolute error: 130539.59
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  root mean squared log error: 10.968928
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  null DOF: 1.0043978E7
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  residual DOF: 1.0042488E7
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  null deviance: 5.3290043E12
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  residual deviance: 5.275379E12
10-26 00:31:20.994 172.31.20.64:3000     206    FJ-1-23   INFO:  AIC: 5.2753832E12
10-26 00:31:21.088 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=1 lmb=.14E2 obj=525370.0 imp=.98E-2 bdf=.94E0] computed in 4182012+1+84696=4266709ms, step = 0.33, l1solver iter = 10000, gerr = 0.27587151573955293
10-26 01:37:31.864 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver stopped after 10000 iterations. (max_iter=10000)
10-26 01:37:31.864 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver finished with gerr = 55571.13056411838 >  eps = 1.0E-4
10-26 01:38:38.508 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=2 lmb=.14E2 obj=524670.0 imp=.13E-2 bdf=.65E0] Scoring after 4037421ms
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=2 lmb=.14E2 obj=524670.0 imp=.13E-2 bdf=.65E0] Training metrics computed in 17184ms
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=2 lmb=.14E2 obj=524670.0 imp=.13E-2 bdf=.65E0] Model Metrics Type: RegressionGLM
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  Description: N/A
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  model id: MyModel
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  frame id: RTMP_sid_9e49_4
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  MSE: 3.48753723E11
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  RMSE: 590553.75
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  mean residual deviance: 524416.7
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  mean absolute error: 130637.945
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  root mean squared log error: 10.955455
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  null DOF: 1.0043978E7
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  residual DOF: 1.0042486E7
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  null deviance: 5.3290043E12
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  residual deviance: 5.26723E12
10-26 01:38:55.693 172.31.20.64:3000     206    FJ-1-23   INFO:  AIC: 5.2672342E12
10-26 01:38:55.704 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=2 lmb=.14E2 obj=524670.0 imp=.13E-2 bdf=.65E0] computed in 3914318+56458+83839=4054615ms, step = 0.33, l1solver iter = 10000, gerr = 55571.13056411838
10-26 02:44:41.208 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver stopped after 10000 iterations. (max_iter=10000)
10-26 02:44:41.208 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver finished with gerr = 5768.096703827045 >  eps = 1.0E-4
10-26 02:46:20.563 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=3 lmb=.14E2 obj=523790.0 imp=.17E-2 bdf=.11E0] Scoring after 4044860ms
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=3 lmb=.14E2 obj=523790.0 imp=.17E-2 bdf=.11E0] Training metrics computed in 19030ms
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=3 lmb=.14E2 obj=523790.0 imp=.17E-2 bdf=.11E0] Model Metrics Type: RegressionGLM
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  Description: N/A
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  model id: MyModel
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  frame id: RTMP_sid_9e49_4
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  MSE: 3.48649226E11
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  RMSE: 590465.25
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  mean residual deviance: 523514.56
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  mean absolute error: 130344.89
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  root mean squared log error: 10.952192
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  null DOF: 1.0043978E7
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  residual DOF: 1.0042381E7
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  null deviance: 5.3290043E12
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  residual deviance: 5.2581693E12
10-26 02:46:39.594 172.31.20.64:3000     206    FJ-1-23   INFO:  AIC: 5.2581735E12
10-26 02:46:39.601 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=3 lmb=.14E2 obj=523790.0 imp=.17E-2 bdf=.11E0] computed in 3889387+56117+118392=4063896ms, step = 0.10890000000000001, l1solver iter = 10000, gerr = 5768.096703827045
10-26 03:52:19.527 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver stopped after 10000 iterations. (max_iter=10000)
10-26 03:52:19.527 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver finished with gerr = 4784.969933090491 >  eps = 1.0E-4
10-26 03:53:25.773 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=4 lmb=.14E2 obj=523120.0 imp=.13E-2 bdf=.46E0] Scoring after 4006173ms
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=4 lmb=.14E2 obj=523120.0 imp=.13E-2 bdf=.46E0] Training metrics computed in 20423ms
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=4 lmb=.14E2 obj=523120.0 imp=.13E-2 bdf=.46E0] Model Metrics Type: RegressionGLM
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  Description: N/A
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  model id: MyModel
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  frame id: RTMP_sid_9e49_4
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  MSE: 4.97980703E11
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  RMSE: 705677.5
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  mean residual deviance: 522778.22
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  mean absolute error: 130602.62
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  root mean squared log error: 10.941541
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  null DOF: 1.0043978E7
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  residual DOF: 1.0042353E7
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  null deviance: 5.3290043E12
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  residual deviance: 5.2507737E12
10-26 03:53:46.196 172.31.20.64:3000     206    FJ-1-23   INFO:  AIC: 5.2507779E12
10-26 03:53:46.201 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=4 lmb=.14E2 obj=523120.0 imp=.13E-2 bdf=.46E0] computed in 3886191+53735+86674=4026600ms, step = 0.33, l1solver iter = 10000, gerr = 4784.969933090491
10-26 04:59:31.210 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver stopped after 10000 iterations. (max_iter=10000)
10-26 04:59:31.211 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver finished with gerr = 44821.17005179174 >  eps = 1.0E-4
10-26 05:01:09.199 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=5 lmb=.14E2 obj=522540.0 imp=.11E-2 bdf=.98E-1] Scoring after 4042998ms
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=5 lmb=.14E2 obj=522540.0 imp=.11E-2 bdf=.98E-1] Training metrics computed in 20907ms
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=5 lmb=.14E2 obj=522540.0 imp=.11E-2 bdf=.98E-1] Model Metrics Type: RegressionGLM
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  Description: N/A
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  model id: MyModel
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  frame id: RTMP_sid_9e49_4
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  MSE: 4.94325006E11
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  RMSE: 703082.5
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  mean residual deviance: 522180.72
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  mean absolute error: 130517.93
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  root mean squared log error: 10.93738
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  null DOF: 1.0043978E7
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  residual DOF: 1.004234E7
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  null deviance: 5.3290043E12
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  residual deviance: 5.2447722E12
10-26 05:01:30.107 172.31.20.64:3000     206    FJ-1-23   INFO:  AIC: 5.2447763E12
10-26 05:01:30.118 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=5 lmb=.14E2 obj=522540.0 imp=.11E-2 bdf=.98E-1] computed in 3888934+56076+118907=4063917ms, step = 0.10890000000000001, l1solver iter = 10000, gerr = 44821.17005179174
10-26 06:07:41.261 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver stopped after 10000 iterations. (max_iter=10000)
10-26 06:07:41.261 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver finished with gerr = 56532.11099690192 >  eps = 1.0E-4
10-26 06:08:47.588 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=6 lmb=.14E2 obj=521080.0 imp=.28E-2 bdf=.28E0] Scoring after 4037470ms
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=6 lmb=.14E2 obj=521080.0 imp=.28E-2 bdf=.28E0] Training metrics computed in 21232ms
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=6 lmb=.14E2 obj=521080.0 imp=.28E-2 bdf=.28E0] Model Metrics Type: RegressionGLM
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  Description: N/A
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  model id: MyModel
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  frame id: RTMP_sid_9e49_4
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  MSE: 5.12239665E11
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  RMSE: 715709.2
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  mean residual deviance: 520660.66
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  mean absolute error: 130576.516
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  root mean squared log error: 10.925282
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  null DOF: 1.0043978E7
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  residual DOF: 1.0042325E7
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  null deviance: 5.3290043E12
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  residual deviance: 5.2295049E12
10-26 06:09:08.822 172.31.20.64:3000     206    FJ-1-23   INFO:  AIC: 5.2295091E12
10-26 06:09:08.827 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=6 lmb=.14E2 obj=521080.0 imp=.28E-2 bdf=.28E0] computed in 3911274+59869+87566=4058709ms, step = 0.33, l1solver iter = 10000, gerr = 56532.11099690192
10-26 07:15:06.827 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver stopped after 10000 iterations. (max_iter=10000)
10-26 07:15:06.827 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver finished with gerr = 60485.20087029953 >  eps = 1.0E-4
10-26 07:16:10.428 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=7 lmb=.14E2 obj=520960.0 imp=.22E-3 bdf=.19E0] Scoring after 4021601ms
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=7 lmb=.14E2 obj=520960.0 imp=.22E-3 bdf=.19E0] Training metrics computed in 22090ms
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=7 lmb=.14E2 obj=520960.0 imp=.22E-3 bdf=.19E0] Model Metrics Type: RegressionGLM
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  Description: N/A
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  model id: MyModel
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  frame id: RTMP_sid_9e49_4
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  MSE: 5.692472E11
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  RMSE: 754484.75
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  mean residual deviance: 520506.1
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  mean absolute error: 131249.19
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  root mean squared log error: 10.91393
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  null DOF: 1.0043978E7
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  residual DOF: 1.004232E7
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  null deviance: 5.3290043E12
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  residual deviance: 5.2279525E12
10-26 07:16:32.518 172.31.20.64:3000     206    FJ-1-23   INFO:  AIC: 5.2279567E12
10-26 07:16:32.523 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=7 lmb=.14E2 obj=520960.0 imp=.22E-3 bdf=.19E0] computed in 3892837+65163+85696=4043696ms, step = 0.33, l1solver iter = 10000, gerr = 60485.20087029953
10-26 08:22:34.254 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver stopped after 10000 iterations. (max_iter=10000)
10-26 08:22:34.254 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver finished with gerr = 87496.09341881957 >  eps = 1.0E-4
10-26 08:24:11.293 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=8 lmb=.14E2 obj=520840.0 imp=.24E-3 bdf=.42E-1] Scoring after 4058770ms
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=8 lmb=.14E2 obj=520840.0 imp=.24E-3 bdf=.42E-1] Training metrics computed in 21569ms
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=8 lmb=.14E2 obj=520840.0 imp=.24E-3 bdf=.42E-1] Model Metrics Type: RegressionGLM
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  Description: N/A
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  model id: MyModel
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  frame id: RTMP_sid_9e49_4
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  MSE: 5.503653E11
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  RMSE: 741866.06
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  mean residual deviance: 520372.72
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  mean absolute error: 131350.28
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  root mean squared log error: 10.910981
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  null DOF: 1.0043978E7
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  residual DOF: 1.0042317E7
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  null deviance: 5.3290043E12
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  residual deviance: 5.2266129E12
10-26 08:24:32.863 172.31.20.64:3000     206    FJ-1-23   INFO:  AIC: 5.2266171E12
10-26 08:24:32.868 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=8 lmb=.14E2 obj=520840.0 imp=.24E-3 bdf=.42E-1] computed in 3904152+57579+118614=4080345ms, step = 0.10890000000000001, l1solver iter = 10000, gerr = 87496.09341881957
10-26 09:30:37.337 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver stopped after 10000 iterations. (max_iter=10000)
10-26 09:30:37.337 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver finished with gerr = 46878.36427637116 >  eps = 1.0E-4
10-26 09:31:42.658 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=9 lmb=.14E2 obj=519810.0 imp=.2E-2 bdf=.11E0] Scoring after 4029790ms
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=9 lmb=.14E2 obj=519810.0 imp=.2E-2 bdf=.11E0] Training metrics computed in 21189ms
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=9 lmb=.14E2 obj=519810.0 imp=.2E-2 bdf=.11E0] Model Metrics Type: RegressionGLM
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  Description: N/A
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  model id: MyModel
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  frame id: RTMP_sid_9e49_4
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  MSE: 4.78805721E11
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  RMSE: 691957.9
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  mean residual deviance: 519314.25
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  mean absolute error: 131280.0
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  root mean squared log error: 10.902149
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  null DOF: 1.0043978E7
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  residual DOF: 1.0042314E7
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  null deviance: 5.3290043E12
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  residual deviance: 5.2159814E12
10-26 09:32:03.847 172.31.20.64:3000     206    FJ-1-23   INFO:  AIC: 5.2159856E12
10-26 09:32:03.853 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=9 lmb=.14E2 obj=519810.0 imp=.2E-2 bdf=.11E0] computed in 3909384+55085+86516=4050985ms, step = 0.33, l1solver iter = 10000, gerr = 46878.36427637116
10-26 10:38:06.545 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver stopped after 10000 iterations. (max_iter=10000)
10-26 10:38:06.545 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver finished with gerr = 153149.48474907267 >  eps = 1.0E-4
10-26 10:39:13.512 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=10 lmb=.14E2 obj=519560.0 imp=.48E-3 bdf=.75E-1] Scoring after 4029658ms
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=10 lmb=.14E2 obj=519560.0 imp=.48E-3 bdf=.75E-1] Training metrics computed in 21204ms
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=10 lmb=.14E2 obj=519560.0 imp=.48E-3 bdf=.75E-1] Model Metrics Type: RegressionGLM
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  Description: N/A
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  model id: MyModel
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  frame id: RTMP_sid_9e49_4
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  MSE: 5.12623084E11
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  RMSE: 715977.0
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  mean residual deviance: 519042.94
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  mean absolute error: 131338.34
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  root mean squared log error: 10.894209
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  null DOF: 1.0043978E7
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  residual DOF: 1.0042305E7
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  null deviance: 5.3290043E12
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  residual deviance: 5.2132567E12
10-26 10:39:34.716 172.31.20.64:3000     206    FJ-1-23   INFO:  AIC: 5.2132609E12
10-26 10:39:34.720 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=10 lmb=.14E2 obj=519560.0 imp=.48E-3 bdf=.75E-1] computed in 3906292+56400+88175=4050867ms, step = 0.33, l1solver iter = 10000, gerr = 153149.48474907267
10-26 11:45:29.369 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver stopped after 10000 iterations. (max_iter=10000)
10-26 11:45:29.369 172.31.20.64:3000     206    FJ-1-23   WARN: ADMM solver finished with gerr = 44089.34572744647 >  eps = 1.0E-4
10-26 11:46:35.949 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=11 lmb=.14E2 obj=519210.0 imp=.66E-3 bdf=.83E-1] Scoring after 4021229ms
10-26 11:46:58.380 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=11 lmb=.14E2 obj=519210.0 imp=.66E-3 bdf=.83E-1] Training metrics computed in 22431ms
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=11 lmb=.14E2 obj=519210.0 imp=.66E-3 bdf=.83E-1] Model Metrics Type: RegressionGLM
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  Description: N/A
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  model id: MyModel
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  frame id: RTMP_sid_9e49_4
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  MSE: 4.48644973E11
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  RMSE: 669809.6
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  mean residual deviance: 518686.6
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  mean absolute error: 131270.0
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  root mean squared log error: 10.888633
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  null DOF: 1.0043978E7
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  residual DOF: 1.0042303E7
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  null deviance: 5.3290043E12
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  residual deviance: 5.2096774E12
10-26 11:46:58.381 172.31.20.64:3000     206    FJ-1-23   INFO:  AIC: 5.2096816E12
10-26 11:46:58.385 172.31.20.64:3000     206    FJ-1-23   INFO: GLM[dest=MyModel, iter=11 lmb=.14E2 obj=519210.0 imp=.66E-3 bdf=.83E-1] computed in 3896878+57770+89016=4043664ms, step = 0.33, l1solver iter = 10000, gerr = 44089.34572744647
10-26 11:54:31.824 172.31.20.64:3000     206    #191:3000 ERRR: Got IO error when sending batch UDP bytes: java.io.IOException: Connection reset by peer
10-26 11:54:31.829 172.31.20.64:3000     206    #106:3000 ERRR: Got IO error when sending batch UDP bytes: java.io.IOException: Connection reset by peer","['h2o', 'poisson']",chrism,https://stackoverflow.com/users/844623/chrism,95
58524136,58524136,2019-10-23T13:35:46,2019-10-25 08:46:30Z,282,"After training a model with autoML tool of H2O, I can see the variable importance with 
saved_model.varimp_plot()
. I am curious about the feature engineering part whic H2O claims to do.


I'm trying simple lines of code sapmles in the documentation of H2O.


import h2o
h2o.init()

train_data = h2o.import_file(""../full_data.csv"")
test_data = h2o.import_file(""../201810_pca.csv"")

from h2o.automl import H2OAutoML
y = ""Label""
x = ['feature0','feature1','feature2','feature3','feature4','feature5','feature6','feature7','feature8','feature9','feature10',
'feature11','feature12','feature13','feature14','feature15','feature16','feature17','feature18','feature19','feature20',
'feature21','feature22','feature23','Amount','DateTime']


aml = H2OAutoML(max_models = 100, max_runtime_secs=100000, seed = 1)
aml.train(x = x, y = y, training_frame = train_data)

lb = aml.leaderboard
lb.head()
lb.head(rows=lb.nrows) # Entire leaderboard

preds = aml.predict(test_data)
h2o.save_model(aml.leader, path = ""./Saved_Models"")


saved_model = h2o.load_model(""./Saved_Models/XGBoost_2_AutoML_20191018_174201"")

training_frame = your_model.actual_params['training_frame'] #The part gives error
print(training_frame)



How do I see which features are being used in the trained model? I'd like to see if H2O is extracting and adding new features or not. 


I've used 
my_training_frame = your_model.actual_params['training_frame']
 as stated in 
another question
 but it gives error: ""TypeError: 'property' object has no attribute 'getitem'"".","['h2o', 'feature-extraction', 'automl']",Unknown,,N/A
58515602,58515602,2019-10-23T04:39:38,2020-03-28 17:44:34Z,0,"I'm trying to run a function on AWS Lambda that loads my H2O MOJO model from S3 and makes a prediction based on function input. My plan is to make a prediction endpoint using API Gateway.


However when I call 
ai.h2o.mojos.runtime.MojoPipeline.loadFrom(...)
 I am met with the following error:


java.lang.RuntimeException: Cannot find MOJO implementation backend!
at ai.h2o.mojos.runtime.MojoPipelineFactoryService.getMojoPipelineFactory(MojoPipelineFactoryService.java:46)
at ai.h2o.mojos.runtime.MojoPipeline.loadFrom(MojoPipeline.java:58)
...



Below is my method throwing the exception:


  private static MojoPipeline loadMojoPipelineFromS3(LambdaLogger logger)
      throws IOException, LicenseException {
    try (S3Object s3Object = s3Client.getObject(DEPLOYMENT_S3_BUCKET_NAME, MOJO_S3_OBJECT_KEY)) {
        logger.log(
          String.format(
              ""Loading Mojo pipeline from S3 object %s/%s"",
              DEPLOYMENT_S3_BUCKET_NAME, MOJO_S3_OBJECT_KEY));

        S3ObjectInputStream s3is = s3Object.getObjectContent();
        FileOutputStream fos = new FileOutputStream(new File(""/tmp/model""));
        byte[] read_buf = new byte[1024];
        int read_len = 0;

        while ((read_len = s3is.read(read_buf)) > 0) {
            fos.write(read_buf, 0, read_len);
        }

        s3is.close();
        fos.close();

      MojoReaderBackend mojoReaderBackend =
          MojoPipelineReaderBackendFactory.createReaderBackend(""/tmp/model"");
      MojoPipeline mojoPipeline = MojoPipeline.loadFrom(mojoReaderBackend);

      logger.log(String.format(""Mojo pipeline successfully loaded (%s)."", mojoPipeline.getUuid()));

      return mojoPipeline;
    }
  }




Can anyone help me debug this error? I haven't been able to find anything online.","['java', 'aws-lambda', 'h2o', 'automl']",brice,https://stackoverflow.com/users/6472849/brice,"1,881"
58482743,58482743,2019-10-21T08:46:57,2019-10-21 09:08:00Z,675,"I have two h2o frames. The both h2o frames have common site_id and timestamp columns. I need to merge these frames by applying left join. site_id column is in type of int whereas timestamp is in type of time. I confirm that when I run the describe() command.


df = h2o.H2OFrame.merge(df1, df2, by_x = [""site_id"", ""timestamp""], by_y=[""site_id"", ""timestamp""])
df.head()



This returns the following error.




H2OResponseError: Server error java.lang.IllegalArgumentException:

  Error: Merging columns must be the same type, column building_id found
  types Time and Numeric   Request: POST /99/Rapids
      data: {'ast': ""(tmp= py_7_sid_aff9 (merge py_4_sid_aff9 weather_train.hex False False [2 4] [0 1] 'auto'))"", 'session_id':
  '_sid_aff9'}




The data set I am using can be accessed from the link 
https://www.kaggle.com/c/ashrae-energy-prediction","['python', 'h2o']",sefiks,https://stackoverflow.com/users/7846405/sefiks,"1,585"
58482180,58482180,2019-10-21T08:05:37,2019-11-09 14:56:23Z,0,"I have made an H2O model to predict the values of 
varToBePredicted
:


data <- h2o.importFile(file)
split <- h2o.splitFrame(data, ratios = c(.70, .15))

gbm <- h2o.gbm(
  training_frame = split[[1]],
  validation_frame = split[[2]],
  x = c(setdiff(names(data), allExceptThis)),
  y = 'varToBePredicted',
  ntrees = 1000,
  max_depth = 2)

model_path <- h2o.saveModel(object = gbm, path = getwd(), force=TRUE)



When I print the 
R2
 of this program by 


print(h2o.r2(h2o.performance(gbm, newdata=split[[3]])))



I get a 
R2
 value of 
0.85
. But my question is: How can I add the predicted values of varToBePredicted to 
data
 (which is a 
data.table
)? I want this so that I can plot the observed values vs the predicted values.","['r', 'data.table', 'h2o', 'gbm']",jordinec,https://stackoverflow.com/users/6178478/jordinec,85
58469273,58469273,2019-10-20T01:08:30,2020-02-28 16:33:41Z,0,"I'm trying to train a model using H2O.ai's H2O-3 Automl Algorithm on AWS SageMaker using the console.


My model's goal is to predict if an arrest will be made based upon the year, type of crime, and location.


My data has 8 columns:




primary_type
: enum


description
: enum


location_description
: enum


arrest
: enum (true/false), this is the target column


domestic
: enum (true/false)


year
: number


latitude
: number


longitude
: number




When I use the SageMaker console on AWS and create a new training job using the H2O-3 Automl Algorithm, I specify the 
primary_type
, 
description
, 
location_description
, and 
domestic
 columns as categorical.


However in the logs of the training job I always see the following two lines:


Converting specified columns to categorical values:
[]



This leads me to believe the 
categorical_columns
 attribute in the 
training
 hyperparameter is not being taken into account.


I have tried the following hyperparameters with the same output in the logs each time:


{'classification': 'true', 'categorical_columns':'primary_type,description,location_description,domestic', 'target': 'arrest'}



{'classification': 'true', 'categorical_columns':['primary_type','description','location_description','domestic'], 'target': 'arrest'}



I thought the list of categorical columns was supposed to be delimited by comma, which would then be split into a list.


I expected the list of categorical column names to be output in the logs instead of an empty list, like so:


Converting specified columns to categorical values:
['primary_type','description','location_description','domestic']



Can anyone help me figure out how to get these categorical columns to apply to the training of my model?


Also-
I 
think
 this is the code that's running when I train my model but I have yet to confirm that: 
https://github.com/h2oai/h2o3-sagemaker/blob/master/automl/automl_scripts/train#L93-L151","['machine-learning', 'h2o', 'amazon-sagemaker', 'automl']",brice,https://stackoverflow.com/users/6472849/brice,"1,881"
58467247,58467247,2019-10-19T19:24:19,2021-02-20 16:31:38Z,114,"I have a very simple question. I recently started working on python.


Here is the R codes for H2O Automl


aml <- h2o.automl(x = x, y = y, project_name =gtp,max_runtime_secs = 99, max_runtime_secs_per_model = 3600,
                  leaderboard_frame = test,
                  training_frame = train, validation_frame = test,nfolds =0,
                  max_models = 1000,exclude_algos = c(""GLM"", ""DeepLearning"", ""GBM"",""DRF"",""StackedEnsemble""),
                  seed =  22)



How can I write these in Python?


aml = H2OAutoML(max_runtime_secs = 600, exclude_algos = ""GLM"", ""DeepLearning"", ""GBM"",""DRF"",""StackedEnsemble"" ,
                seed = 42,project_name =gtp)

aml.train(x = X, 
          y = y, validation_frame =hf_v
          training_frame = hf_train,
          leaderboard_frame = hf_test,)","['h2o', 'automl']",Rajath Rao - Software Engineer,https://stackoverflow.com/users/9928905/rajath-rao-software-engineer,"1,177"
58326351,58326351,2019-10-10T15:25:32,2019-10-16 00:24:34Z,880,"I'm using xgboost within H2O for a binary classification task. The dataset has several categorical features, to which the model applies a one-hot encoding during training. 


Now I want to use SHAP (
https://github.com/slundberg/shap
) to locally interpret the predictions. For this, it would be nice to have the dataframe with the one-hot encoded columns and values. However, I seem to find no way to get this from the H2O model. 


I could probably manually recreate the one-hot encoding, but maybe someone know a quicker solution?","['python', 'machine-learning', 'h2o', 'shap']",Thomas Märki,https://stackoverflow.com/users/12196044/thomas-m%c3%a4rki,13
58293412,58293412,2019-10-08T20:10:08,2019-10-08 20:21:32Z,0,"I'm translating a random forest using h20 and r into a random forest using SciKit Learn's Random Forest Classifier with python.  H2o's randomForest model has an argument 'stopping_rounds'.  Is there a way to do this in python using the SKLearn Random Forest Classifier model? I've looked through the documentation, so I'm afraid I might have to hard code this.","['scikit-learn', 'random-forest', 'h2o']",Harrison Kane,https://stackoverflow.com/users/12184444/harrison-kane,21
58284800,58284800,2019-10-08T10:43:31,2019-10-08 20:29:10Z,292,"I am fairly new to h2o and trying to get my head around it. I am currently using automl and from the models on my leaderboard 
I have decided to use the 3rd model not the leader model
.


I do that using the code below and then get the parameters of that specific model. I then manually check the actual parameters that are different from the default ones and use them to define my model. 


#choosing the 3rd model from the leaderboard
chosen_model = h2o.get_model(aml.leaderboard.as_data_frame()['model_id'][2])

#getting the model parameters
chosen_model.params

# example result including only some of the parameters 

{'model_id': {'default': None,
  'actual': {'__meta': {'schema_version': 3,
    'schema_name': 'ModelKeyV3',
    'schema_type': 'Key<Model>'},
   'name': 'GBM_grid_1_AutoML_20191007_170602_model_12',
   'type': 'Key<Model>',
   'URL': '/3/Models/GBM_grid_1_AutoML_20191007_170602_model_12'}},
 'training_frame': {'default': None,
  'actual': {'__meta': {'schema_version': 3,
    'schema_name': 'FrameKeyV3',
    'schema_type': 'Key<Frame>'},
   'name': 'automl_training_py_13_sid_ac88',
   'type': 'Key<Frame>',
   'URL': '/3/Frames/automl_training_py_13_sid_ac88'}},
 'validation_frame': {'default': None,
  'actual': {'__meta': {'schema_version': 3,
    'schema_name': 'FrameKeyV3',
    'schema_type': 'Key<Frame>'},
   'name': 'py_15_sid_ac88',
   'type': 'Key<Frame>',
   'URL': '/3/Frames/py_15_sid_ac88'}},
 'nfolds': {'default': 0, 'actual': 5},
 'keep_cross_validation_models': {'default': True, 'actual': False},
 'keep_cross_validation_predictions': {'default': False, 'actual': True},
 'keep_cross_validation_fold_assignment': {'default': False, 'actual': False},  etc.


# pasting the actual parameters on my model

model = H2OGradientBoostingEstimator(nfolds=5, keep_cross_validation_models=False, keep_cross_validation_predictions= True, score_tree_interval=5, fold_assignment= 'Modulo', ntrees=51, max_depth=12, min_rows=5.0, stopping_metric='deviance', stopping_tolerance = 0.04867923835112355, seed = 47, distribution='gaussian', learn_rate=0.1, sample_rate=0.5, col_sample_rate = 0.7) 



This is a process I have to repeat many times, as I am running many automls for a project I am currently wokring on.


Is there a code already available on h2o that lets you do that automatically? Or does anyone know a more efficient way?


Thanks a lot in advance!","['python', 'h2o', 'automl']",Marina,https://stackoverflow.com/users/12164059/marina,3
58253197,58253197,2019-10-05T23:53:01,2020-06-16 01:56:50Z,271,"This is very similar to 
H2O MOJO thread safe?
 but for 
xgboost
. 


h2o document 
Productionizing H2O
 does not seem to mention anything about it.
Is it thread safe to do this in a thread without a lock?


BinomialModelPrediction p = model.predictBinomial(row);



Or a more general question, 
How to view a MOJO model
 this document give me the impression the mojos of all these different types of models are essentially (if-else) decision tree and difference is about just how they are trained. is it true? 


UPDATE 4
:


Productionizing H2O FAQ




Are MOJOs thread safe?


Yes, all of H2O-3 MOJOs are thread safe.




UPDATE 3
:


It's like h2o has its own java prediction implementation and likely threadsafe looking at the numbers 
xgboost-predictor
.   


UPDATE 2
:


This 
thread
 cast some shadow, that xgboost may not be threadsafe for prediction. 
XGBoostJavaMojoModel


UPDATE 1
: 




Transform ML models into a native code (Java, C, Python, Go, JavaScript) with zero dependencies




m2cgen
 make me think the answer to the generalized question is likely true.","['java', 'machine-learning', 'thread-safety', 'h2o', 'xgboost']",Unknown,,N/A
58191320,58191320,2019-10-01T19:42:29,2019-10-01 19:42:29Z,0,"I installed the package many times and different versions too. But it still says 

Error in library(h2osteam) : there is no package called ‘h2osteam’


After installation it says 

** building package indices
** testing if installed package can be loaded
* DONE (h2osteam)


What am I doing wrong here? help!","['r', 'h2o']",trougc,https://stackoverflow.com/users/9670371/trougc,405
58107832,58107832,2019-09-25T23:46:03,2019-10-03 19:55:05Z,176,"Is there any documentation on how to install/setup Driverless AI on a High-Performance Computing (HPC) environment, so I can request few nodes (with GPU each one) and have DAI take advantage of it?","['h2o', 'hpc', 'driverless-ai']",Guillermo E Ponce-Campos,https://stackoverflow.com/users/2873150/guillermo-e-ponce-campos,115
58106762,58106762,2019-09-25T21:28:22,2020-08-29 10:01:08Z,511,"I'm trying to run some sample code that I found from the link below.


http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html#automl-interface


I modified the code a bit.  My actual code is below.


import h2o
from h2o.automl import H2OAutoML
import pandas as pd
from sklearn.model_selection import train_test_split

h2o.init()

df = pd.read_csv('C:\\my_path\\taxi.csv', header=0, encoding = 'unicode_escape')
pd.set_option('display.max_columns', None)
df.head(10)

df = df[:100000]

y = df.fare_amount

# create training and testing vars
X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)

# Identify predictors and response
y = df['fare_amount']
del df['fare_amount']
x = df

# Run AutoML for 20 base models (limited to 1 hour max runtime by default)
train = h2o.H2OFrame(df)

aml = H2OAutoML(max_models=20, seed=1)
aml.train(x=x, y=y, training_frame=train)

# View the AutoML Leaderboard
lb = aml.leaderboard
lb.head(rows=lb.nrows)



I set 'fare_amount' as my target variable.  When I run the code all I get is this message: 'This H2OFrame is empty.'  I may be missing something pretty basic, but I can't see what it is.  Any thoughts, anyone?","['python', 'python-3.x', 'machine-learning', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
58106200,58106200,2019-09-25T20:37:08,2019-09-25 20:41:06Z,398,"Closed
. This question needs to be more 
focused
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Update the question so it focuses on one problem only by 
editing this post
.






Closed 
5 years ago
.















                        Improve this question
                    








i am building a model using H2O. is there a way H2O can produce Jupyter notebook instead of seeing the predicted model using H2O flow",['h2o'],Ezana Aimero,https://stackoverflow.com/users/12121496/ezana-aimero,3
58099388,58099388,2019-09-25T13:19:37,2019-09-25 13:19:37Z,0,"i'm using h2o to build a classification model and I observed that target encoding of my categorical variables with high cardinality helps to improve performance ( lower false positive).


I'm using R api, and as the doc specfied 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/target-encoding.html
 i created the mapping with 
h2o.target_encode_fit
 and apply to train, test and validation with 
h2o.target_encode_transform
.


Now I'd like to put my model in production.


From the documentation of 
h2o.target_encode_transform
 it seems that the 
y
 arg of the function is mandatory. But in production phase I don't know my target variable.


Can I safely apply the encoding of the training to the new data? 
In absence of any other alternative I'd extract from my training dataframe categorical columns along with encoding , then join on new data on categorical columns.


Hope I was clear enough.","['r', 'h2o']",Marco Fumagalli,https://stackoverflow.com/users/5654564/marco-fumagalli,"2,437"
58092586,58092586,2019-09-25T06:47:39,2019-09-25 22:21:58Z,0,"I want to use: 
h2o.targetencoder
 function, then I do:


> install.packages(""h2o"")
> library(""h2o"")
> h2o.targetencoder



but I get the following error:


Error: object 'h2o.targetencoder' not found



At the same time, I see that function is defined here:


https://github.com/h2oai/h2o-3/blob/jenkins-rel-yau-5/h2o-r/h2o-package/R/targetencoder.R


Any idea on how can I use function: 
h2o.targetencoder
?


Thanks!","['r', 'machine-learning', 'rstudio', 'h2o']",Viewsonic,https://stackoverflow.com/users/10541642/viewsonic,947
58091045,58091045,2019-09-25T04:11:50,2019-09-26 02:45:09Z,78,"I am confused by the output of H2ORandomForestEstimator model. 


So my task is predicting house price, which is a regression task.


training:


model = H2ORandomForestEstimator(ntrees=100, max_depth=20,mtries=-1, seed=42, score_each_iteration=True)
model.train(x=features, y=target, training_frame=train_frame)



the data has around 80 features


and i predict on the test data, which contain 1495 rows (aka. data points)


log_predict = model.predict(test_frame)



but the shape of log_predict is (1459, 656), which is really confusing, and this is a screenshot of the output:
picture

(sorry i am not able to embed picture yet...)


What does the number 656 mean? And is there any convenient way to transform it to a (1459,1) array?


Thanks in advance!","['python', 'machine-learning', 'h2o']",EpicJ,https://stackoverflow.com/users/12050661/epicj,13
58051707,58051707,2019-09-22T17:24:47,2019-10-10 20:43:19Z,0,"Using H2O DeepLearning and a Tanh activation function, is it acceptable/valid to get a predicted (probability) value greater than 1? If so, doesn't that skew predictions to the first class?


Details:
I am using H2O for Deep Learning Artificial Neural Networks in R to predict 2 classes. My 
y
 data (
actualResults
) are the actual classifications of only 0s and 1s. The 
x
 independent variables are all numerical and the training frame excludes the 
y
 (
actualResults
). When I do a max on the predicted values, I get values greater than 1, never less than -1, though Tanh is suppose to be limited to [-1, 1].


Questions:




Are these predicted values greater than 1 acceptable/valid?


Why am I getting predicted values greater than 1 for Tanh?


Does this occurrence of values greater than 1 skew/bias positive (class 1) predictions?




Note: In the code below, the 
training_set
 and 
testing_set
's first column is the actual classification, so -c(1) removes it for the network's input.


ANN <- h2o.deeplearning(y = ""actualResult"",
                              x = independentVariableColumns,
                              training_frame = as.h2o(training_set[-c(1)]),
                              activation = ""Tanh"",
                              hidden = rep(3, 3),
                              epochs = 100)


prediction <- h2o.predict(ANN, newdata = as.h2o(test_set[-c(1)]))
maxPrediction <- max(prediction)","['r', 'machine-learning', 'neural-network', 'deep-learning', 'h2o']",Unknown,,N/A
58051432,58051432,2019-09-22T16:54:54,2019-10-06 23:15:17Z,0,"I would like to import a csv file from my Google Cloud Storage bucket into H2O running in R locally (h2o.init(ip = ""localhost"")). 


I tried following the instructions at 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/cloud-integration/gcs.html?highlight=environment
. 


I can already upload csv files from R to GCS and vice-versa using the R package cloudml. So I am reasonably sure I have the authorizations set correctly. 


I have tried using 
Sys.setenv(GOOGLE_APPLICATION_CREDENTIALS = ""/full/path/to/auth.json"")
. I tried using the terminal from Rstudio to do the same thing: 
export GOOGLE_APPLICATION_CREDENTIALS=""/full/path/to/auth.json""
. I also tried 
gcloud auth application-default login
 using the terminal from Rstudio.


But in every case, I could not make this work, from within Rstudio:


h2o.init()
h2o::h2o.importFile(path = ""gs://[gcs_bucket]/[tbl.csv], 
                    destination_frame = ""tbl_from_gcs"")



H2O throws the error:


Error in h2o.importFolder(path, pattern = """", destination_frame = destination_frame,  : 
   all files failed to import



If I turn on logging (
h2o::h2o.startLogging(""logfile"")
), it shows:


GET       http://localhost:54321/3/ImportFiles?path=gs%3A%2F%2F[gcs_bucket]%2F[tbl.csv]&pattern=
postBody: 

curlError:         FALSE
curlErrorMessage:  
httpStatusCode:    200
httpStatusMessage: OK
millis:            182

{""__meta"":{""schema_version"":3,""schema_name"":""ImportFilesV3"",""schema_type"":""ImportFiles""},""_exclude_fields"":"""",""path"":""gs://[gcs_bucket]/[tbl.csv]"",""pattern"":"""",""files"":[],""destination_frames"":[],""fails"":[""gs://[gcs_bucket]/[tbl.csv]""],""dels"":[]}



(Obviously, I changed the bucket name and table name, but hopefully you get the idea.)


I am running h2o version 3.26.0.2 in R 3.6.1 and Rstudio 1.2.1578. (I am running Rstudio server in Docker on my local server using rocker/tidyverse:latest, FYI.)


If anyone could walk me through the steps to authenticate H2O so it can access GCS buckets directly, I would appreciate it. I know I could use cloudml or googleCloudStorageR as a workaround, but I would like to be able to use H2O directly so I can more easily switch from a local H2O cluster to a cloud H2O cluster.","['google-cloud-storage', 'h2o']",caewok,https://stackoverflow.com/users/5786863/caewok,91
58047102,58047102,2019-09-22T07:43:35,2019-09-22 14:20:15Z,313,"when run 
h20.init()
 raise error like this:




Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.
  Attempting to start a local H2O server...
  ; Java HotSpot(TM) 64-Bit Server VM (build 13+33, mixed mode, sharing)
    Starting server from d:\python\lib\site-packages\h2o\backend\bin\h2o.jar
    Ice root: C:\Users\NIEFAN~1\AppData\Local\Temp\tmp4vq2d_7d
    JVM stdout: C:\Users\NIEFAN~1\AppData\Local\Temp\tmp4vq2d_7d\h2o_Niefangchao_started_from_python.out
    JVM stderr: C:\Users\NIEFAN~1\AppData\Local\Temp\tmp4vq2d_7d\h2o_Niefangchao_started_from_python.err
  Traceback (most recent call last):
    File ""<ipython-input-2-95453bf1556d>"", line 1, in <module>
      h2o.init()
    File ""d:\python\lib\site-packages\h2o\h2o.py"", line 278, in init
      bind_to_localhost=bind_to_localhost)
    File ""d:\python\lib\site-packages\h2o\backend\server.py"", line 138, in start
      bind_to_localhost=bind_to_localhost, log_dir=log_dir, log_level=log_level)
    File ""d:\python\lib\site-packages\h2o\backend\server.py"", line 346, in _launch_server
      raise H2OServerError(""Server process terminated with error code %d"" % proc.returncode)
  H2OServerError: Server process terminated with error code 1","['python', 'h2o']",Trenton McKinney,https://stackoverflow.com/users/7758804/trenton-mckinney,62k
58015854,58015854,2019-09-19T17:03:50,2019-09-21 12:38:14Z,55,"I have a data frame with four columns: 'a', 'b', 'c', 'target', where target is the sum of the first three values.
I train H2ONaiveBayesEstimator with this data frame, and try to predict the result for a similar data frame. But the result is unexpected.


I tried the same data frames with LinearRegression, and it gives the expected predictions.


Why H2ONaiveBayesEstimator gives such a prediction result, value 27 for 20 rows?


Code:


import pandas as pd
import h2o
from h2o.estimators import H2ONaiveBayesEstimator
from sklearn.linear_model import LinearRegression

train_df = pd.DataFrame(
    {
        ""a"": list(range(10)),
        ""b"": list(range(10)),
        ""c"": list(range(10)),
        ""target"": [i + i + i for i in list(range(10))]
    }
)

pred_df = pd.DataFrame(
    {
        ""a"": list(range(10, 30)),
        ""b"": list(range(10, 30)),
        ""c"": list(range(10, 30)),
    }
)

train_data = train_df.values
features_data = train_df.drop('target', axis=1).values
target_data = train_df['target'].values

pred_data = pred_df.values

lr = LinearRegression()
lr.fit(features_data, target_data)
predictions = lr.predict(pred_data)
print(predictions)

print('\n----\n')

h2o.init()
train_frame = h2o.H2OFrame(train_data, column_names=list(train_df.columns))
nbm = H2ONaiveBayesEstimator()
train_frame['target'] = train_frame['target'].asfactor()
nbm.train(y='target', training_frame=train_frame)

pred_frame = h2o.H2OFrame(pred_data, column_names=list(pred_df.columns))
predictions = nbm.predict(pred_frame)
print('\n----\n')
print(predictions.as_data_frame())



train data frame:


   a  b  c  target
0  0  0  0       0
1  1  1  1       3
2  2  2  2       6
3  3  3  3       9
4  4  4  4      12
5  5  5  5      15
6  6  6  6      18
7  7  7  7      21
8  8  8  8      24
9  9  9  9      27



pred data frame:


0   10  10  10
1   11  11  11
2   12  12  12
3   13  13  13
4   14  14  14
5   15  15  15
6   16  16  16
7   17  17  17
8   18  18  18
9   19  19  19
10  20  20  20
11  21  21  21
12  22  22  22
13  23  23  23
14  24  24  24
15  25  25  25
16  26  26  26
17  27  27  27
18  28  28  28
19  29  29  29



Result for H2ONaiveBayesEstimator:


    predict             p0             p3  ...           p21           p24       p27
0        27   3.180305e-65   7.583358e-53  ...  6.076669e-06  1.098688e-02  0.989007
1        27   6.040575e-77   2.893040e-63  ...  1.522156e-08  5.527786e-04  0.999447
2        27   1.135940e-88   1.092736e-73  ...  3.775031e-11  2.753569e-05  0.999972
3        27  2.135088e-100   4.125332e-84  ...  9.357610e-14  1.370957e-06  0.999999
4        27  4.012965e-112   1.557370e-94  ...  2.319523e-16  6.825603e-08  1.000000
5        27  7.542484e-124  5.879283e-105  ...  5.749522e-19  3.398268e-09  1.000000
6        27  1.417632e-135  2.219508e-115  ...  1.425164e-21  1.691898e-10  1.000000
7        27  2.664479e-147  8.378943e-126  ...  3.532629e-24  8.423464e-12  1.000000
8        27  5.007966e-159  3.163164e-136  ...  8.756511e-27  4.193796e-13  1.000000
9        27  9.412616e-171  1.194137e-146  ...  2.170522e-29  2.087968e-14  1.000000
10       27  1.769128e-182  4.508027e-157  ...  5.380186e-32  1.039538e-15  1.000000
11       27  3.325128e-194  1.701841e-167  ...  1.333615e-34  5.175555e-17  1.000000
12       27  6.249673e-206  6.424678e-178  ...  3.305701e-37  2.576757e-18  1.000000
13       27  1.174644e-217  2.425402e-188  ...  8.194013e-40  1.282892e-19  1.000000
14       27  2.207777e-229  9.156221e-199  ...  2.031093e-42  6.387142e-21  1.000000
15       27  4.149581e-241  3.456597e-209  ...  5.034575e-45  3.179971e-22  1.000000
16       27  7.799257e-253  1.304912e-219  ...  1.247946e-47  1.583214e-23  1.000000
17       27  1.465893e-264  4.926217e-230  ...  3.093350e-50  7.882360e-25  1.000000
18       27  2.755188e-276  1.859713e-240  ...  7.667648e-53  3.924396e-26  1.000000
19       27  5.178455e-288  7.020668e-251  ...  1.900620e-55  1.953842e-27  1.000000

[20 rows x 11 columns]



Result for LinearRegression:


[30. 33. 36. 39. 42. 45. 48. 51. 54. 57. 60. 63. 66. 69. 72. 75. 78. 81.
 84. 87.]","['python', 'h2o']",Unknown,,N/A
58011829,58011829,2019-09-19T13:12:38,2022-12-17 03:04:42Z,98,"I would like to fit a model by group in h2o using some type of distributed apply function.


I tried the following but it doesn't work. Probably due to the fact I cannot pipe the sc object through.


df%>%
  spark_apply(function(e)
        h2o.coxph(x = predictors,
                       event_column = ""event"",
                       stop_column = ""time_to_next"",
                       training_frame = as_h2o_frame(sc, e, strict_version_check = FALSE))      
  group_by = ""id""
  )



I receive a pretty generic spark error like this:


error : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 :","['h2o', 'sparklyr']",DataTx,https://stackoverflow.com/users/2962786/datatx,"1,869"
58011382,58011382,2019-09-19T12:48:00,2019-09-19 16:43:24Z,259,"I'm using h2o.gbm, and i'm specifying a tweedie distribution.  The response should logged, and I think by specifying a tweedie distribution that h2o will log the response.  Given the following from the documentation:




When specifying the distribution, the loss function is automatically
  selected as well. For exponential families (such as Poisson, Gamma, and
  Tweedie), the canonical logarithmic link function is used.




However, tweedie distributions have a point mass of 0.  So if h2o is logging the response, is it actually logging the response variables when the value is 0 or is there some other transformation?  such as: 



data[,""new_response""] <- h2o.if_else(data$response == 0, 0, log(data$response))",['h2o'],Will.I.am,https://stackoverflow.com/users/8695616/will-i-am,25
57998383,57998383,2019-09-18T17:55:34,2019-09-18 18:50:47Z,343,"I'm trying to understand how H2ONaiveBayesEstimator works. This my code:


import pandas as pd
import h2o
from h2o.estimators import H2ONaiveBayesEstimator


df = pd.DataFrame(
    {
        ""a"": list(range(10)),
        ""b"": list(range(10)),
        ""c"": list(range(10))
    }
)

data = df.values

h2o.init()
train_frame = h2o.H2OFrame(data)
nbm = H2ONaiveBayesEstimator()
y = train_frame.columns[-1]
nbm.train(y=y, training_frame=train_frame)



This is the error i get:


h2o.exceptions.H2OResponseError: ModelBuilderErrorV3  (water.exceptions.H2OModelBuilderIllegalArgumentException):
    timestamp = 1568829213744
    error_url = '/3/ModelBuilders/naivebayes'
    msg = 'Illegal argument(s) for NaiveBayes model: NaiveBayes_model_python_1568829211691_1.  Details: ERRR on field: _response: Response must be a categorical column'
    dev_msg = 'Illegal argument(s) for NaiveBayes model: NaiveBayes_model_python_1568829211691_1.  Details: ERRR on field: _response: Response must be a categorical column'
    http_status = 412
    values = {'messages': [{'_log_level': 5, '_field_name': '_keep_cross_validation_models', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_keep_cross_validation_predictions', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_keep_cross_validation_fold_assignment', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_fold_assignment', '_message': 'Only for cross-validation.'}, {'_log_level': 5, '_field_name': '_tweedie_power', '_message': 'Only for Tweedie Distribution.'}, {'_log_level': 5, '_field_name': '_tweedie_power', '_message': 'Tweedie power is only used for Tweedie distribution.'}, {'_log_level': 5, '_field_name': '_quantile_alpha', '_message': 'Quantile (alpha) is only used for Quantile regression.'}, {'_log_level': 5, '_field_name': '_max_after_balance_size', '_message': 'Balance classes is false, hide max_after_balance_size'}, {'_log_level': 5, '_field_name': '_balance_classes', '_message': 'Balance classes is only applicable to classification problems.'}, {'_log_level': 5, '_field_name': '_class_sampling_factors', '_message': 'Class sampling factors is only applicable to classification problems.'}, {'_log_level': 5, '_field_name': '_max_after_balance_size', '_message': 'Max after balance size is only applicable to classification problems.'}, {'_log_level': 5, '_field_name': '_max_confusion_matrix_size', '_message': 'Max confusion matrix size is only applicable to classification problems.'}, {'_log_level': 5, '_field_name': '_max_hit_ratio_k', '_message': 'Max K-value for hit ratio is only applicable to multi-class classification problems.'}, {'_log_level': 5, '_field_name': '_max_confusion_matrix_size', '_message': 'Only for multi-class classification problems.'}, {'_log_level': 5, '_field_name': '_max_after_balance_size', '_message': 'Only used with balanced classes'}, {'_log_level': 5, '_field_name': '_class_sampling_factors', '_message': 'Class sampling factors is only applicable if balancing classes.'}, {'_log_level': 1, '_field_name': '_response', '_message': 'Response must be a categorical column'}, {'_log_level': 5, '_field_name': '_balance_classes', '_message': 'Balance classes is not applicable to NaiveBayes.'}, {'_log_level': 5, '_field_name': '_class_sampling_factors', '_message': 'Class sampling factors is not applicable to NaiveBayes.'}, {'_log_level': 5, '_field_name': '_max_after_balance_size', '_message': 'Max after balance size is not applicable to NaiveBayes.'}], 'algo': 'NaiveBayes', 'parameters': {'_train': {'name': 'Key_Frame__upload_9cb9251216ede45dd909d3e9af81477a.hex', 'type': 'Key'}, '_valid': None, '_nfolds': 0, '_keep_cross_validation_models': True, '_keep_cross_validation_predictions': False, '_keep_cross_validation_fold_assignment': False, '_parallelize_cross_validation': True, '_auto_rebalance': True, '_seed': -1, '_fold_assignment': 'AUTO', '_categorical_encoding': 'AUTO', '_max_categorical_levels': 10, '_distribution': 'AUTO', '_tweedie_power': 1.5, '_quantile_alpha': 0.5, '_huber_alpha': 0.9, '_ignored_columns': None, '_ignore_const_cols': True, '_weights_column': None, '_offset_column': None, '_fold_column': None, '_check_constant_response': True, '_is_cv_model': False, '_score_each_iteration': False, '_max_runtime_secs': 0.0, '_stopping_rounds': 0, '_stopping_metric': 'AUTO', '_stopping_tolerance': 0.001, '_response_column': 'C3', '_balance_classes': False, '_max_after_balance_size': 5.0, '_class_sampling_factors': None, '_max_confusion_matrix_size': 20, '_checkpoint': None, '_pretrained_autoencoder': None, '_custom_metric_func': None, '_custom_distribution_func': None, '_export_checkpoints_dir': None, '_laplace': 0.0, '_eps_sdev': 0.0, '_min_sdev': 0.001, '_eps_prob': 0.0, '_min_prob': 0.001, '_compute_metrics': True}, 'error_count': 2}
    exception_msg = 'Illegal argument(s) for NaiveBayes model: NaiveBayes_model_python_1568829211691_1.  Details: ERRR on field: _response: Response must be a categorical column'
    stacktrace =
.
.
.
parameters = {'__meta': {'schema_version': 3, 'schema_name': 'NaiveBayesParametersV3', 'schema_type': 'NaiveBayesParameters'}, 'model_id': None, 'training_frame': {'__meta': {'schema_version': 3, 'schema_name': 'FrameKeyV3', 'schema_type': 'Key<Frame>'}, 'name': 'Key_Frame__upload_9cb9251216ede45dd909d3e9af81477a.hex', 'type': 'Key<Frame>', 'URL': '/3/Frames/Key_Frame__upload_9cb9251216ede45dd909d3e9af81477a.hex'}, 'validation_frame': None, 'nfolds': 0, 'keep_cross_validation_models': True, 'keep_cross_validation_predictions': False, 'keep_cross_validation_fold_assignment': False, 'parallelize_cross_validation': True, 'distribution': 'AUTO', 'tweedie_power': 1.5, 'quantile_alpha': 0.5, 'huber_alpha': 0.9, 'response_column': {'__meta': {'schema_version': 3, 'schema_name': 'ColSpecifierV3', 'schema_type': 'VecSpecifier'}, 'column_name': 'C3', 'is_member_of_frames': None}, 'weights_column': None, 'offset_column': None, 'fold_column': None, 'fold_assignment': 'AUTO', 'categorical_encoding': 'AUTO', 'max_categorical_levels': 10, 'ignored_columns': None, 'ignore_const_cols': True, 'score_each_iteration': False, 'checkpoint': None, 'stopping_rounds': 0, 'max_runtime_secs': 0.0, 'stopping_metric': 'AUTO', 'stopping_tolerance': 0.001, 'custom_metric_func': None, 'custom_distribution_func': None, 'export_checkpoints_dir': None, 'balance_classes': False, 'class_sampling_factors': None, 'max_after_balance_size': 5.0, 'max_confusion_matrix_size': 20, 'max_hit_ratio_k': 0, 'laplace': 0.0, 'min_sdev': 0.001, 'eps_sdev': 0.0, 'min_prob': 0.001, 'eps_prob': 0.0, 'compute_metrics': True, 'seed': -1}
    messages = [{'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'keep_cross_validation_models', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'keep_cross_validation_predictions', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'keep_cross_validation_fold_assignment', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'fold_assignment', 'message': 'Only for cross-validation.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'tweedie_power', 'message': 'Only for Tweedie Distribution.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'tweedie_power', 'message': 'Tweedie power is only used for Tweedie distribution.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'quantile_alpha', 'message': 'Quantile (alpha) is only used for Quantile regression.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_after_balance_size', 'message': 'Balance classes is false, hide max_after_balance_size'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'balance_classes', 'message': 'Balance classes is only applicable to classification problems.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'class_sampling_factors', 'message': 'Class sampling factors is only applicable to classification problems.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_after_balance_size', 'message': 'Max after balance size is only applicable to classification problems.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_confusion_matrix_size', 'message': 'Max confusion matrix size is only applicable to classification problems.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_hit_ratio_k', 'message': 'Max K-value for hit ratio is only applicable to multi-class classification problems.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_confusion_matrix_size', 'message': 'Only for multi-class classification problems.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_after_balance_size', 'message': 'Only used with balanced classes'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'class_sampling_factors', 'message': 'Class sampling factors is only applicable if balancing classes.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'ERRR', 'field_name': 'response', 'message': 'Response must be a categorical column'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'balance_classes', 'message': 'Balance classes is not applicable to NaiveBayes.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'class_sampling_factors', 'message': 'Class sampling factors is not applicable to NaiveBayes.'}, {'__meta': {'schema_version': 3, 'schema_name': 'ValidationMessageV3', 'schema_type': 'ValidationMessage'}, 'message_type': 'TRACE', 'field_name': 'max_after_balance_size', 'message': 'Max after balance size is not applicable to NaiveBayes.'}]
    error_count = 2



Can somebody help me figure it out what's wrong with my code?","['python', 'h2o']",Unknown,,N/A
57981830,57981830,2019-09-17T20:45:24,2019-09-18 08:10:44Z,111,"Running h2o (
http://h2o-release.s3.amazonaws.com/h2o/rel-yau/5/h2o-3.26.0.5-hdp3.1.zip
) on hdp 3.1.4 getting error at startup due to access restrictions to the 
hdfs:///user/hdfs
 folder


[root@HW005 h2o-3.26.0.5-hdp3.1]# hadoop jar h2odriver.jar -nodes 4 -mapperXmx 6g
Determining driver host interface for mapper->driver callback...
    [Possible callback IP address: 172.18.4.83]
    [Possible callback IP address: 127.0.0.1]
Using mapper->driver callback IP address and port: 172.18.4.83:37342
(You can override these with -driverif and -driverport/-driverportrange and/or specify external IP using -extdriverif.)
Memory Settings:
    mapreduce.map.java.opts:     -Xms6g -Xmx6g -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Dlog4j.defaultInitOverride=true
    Extra memory percent:        10
    mapreduce.map.memory.mb:     6758
Hive driver not present, not generating token.
19/09/17 10:38:17 INFO client.RMProxy: Connecting to ResourceManager at hw001.co.local/172.18.4.46:8050
19/09/17 10:38:17 INFO client.AHSProxy: Connecting to Application History server at hw002.co.local/172.18.4.47:10200
ERROR: Permission denied: user=root, access=WRITE, inode=""/user"":hdfs:hdfs:drwxr-xr-x




Seems odd that this would be a requirement, since I would like to run h2o as various different users depending on use case and I don't think it would be right to just give access the the hdfs user's (the HDP default HDFS admin user) HDFS home folder in order to do this. Can anyone explain what is going on here and how it would normally be handled?",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
57979819,57979819,2019-09-17T18:08:49,2019-09-18 18:30:21Z,0,"I created an cartesian grid of GBMs using 
h2o
 package in R and saved cross-validation metrics for each model in a data frame. So, for each model, I stored the results given in 
model@model$cross_validation_metrics_summary
.


What is the threshold used to calculate F1 and F2 scores, precision, recall and specificity in 
model@model$cross_validation_metrics_summary
? Is there a default value?","['r', 'cross-validation', 'h2o', 'gbm']",jessicalfr,https://stackoverflow.com/users/7111951/jessicalfr,79
57978333,57978333,2019-09-17T16:21:12,2019-09-19 15:16:28Z,0,"While training a job on a SageMaker instance using H2o AutoML a message ""This H2OFrame is empty"" has come up after running the code, what should I do to fix the problem?


/opt/ml/input/config/hyperparameters.json
All Parameters:
{'nfolds': '5', 'training': ""{'classification': 'true', 'target': 'y'}"", 'max_runtime_secs': '3600'}
/opt/ml/input/config/resourceconfig.json
All Resources:
{'current_host': 'algo-1', 'hosts': ['algo-1'], 'network_interface_name': 'eth0'}
Waiting until DNS resolves: 1
10.0.182.83
Starting up H2O-3
Creating Connection to H2O-3
Attempt 0: H2O-3 not running yet...
Connecting to H2O server at http://127.0.0.1:54321... successful.
-------------------------- ----------------------------------------

-------------------------- ----------------------------------------
Beginning Model Training
Parse progress: |█████████████████████████████████████████████████████████| 100%
Classification - If you want to do a regression instead, set ""classification"":""false"" in ""training"" params, inhyperparamters.json
Converting specified columns to categorical values:
[]
AutoML progress: |████████████████████████████████████████████████████████| 100%
This H2OFrame is empty.
Exception during training: Argument `model` should be a ModelBase, got NoneType None
Traceback (most recent call last):
File ""/opt/program/train"", line 138, in _train_model
h2o.save_model(aml.leader, path=model_path)
File ""/root/.local/lib/python3.7/site-packages/h2o/h2o.py"", line 969, in save_model
assert_is_type(model, ModelBase)
File ""/root/.local/lib/python3.7/site-packages/h2o/utils/typechecks.py"", line 457, in assert_is_type
skip_frames=skip_frames)
h2o.exceptions.H2OTypeError: Argument `model` should be a ModelBase, got NoneType None
H2O session _sid_8aba closed.



I'm wondering if it's a problem because of the max_runtime_secs, my data has around 500 rows and 250000 columns.","['h2o', 'amazon-sagemaker', 'automl']",Unknown,,N/A
57966245,57966245,2019-09-17T01:32:45,2019-09-17 13:30:43Z,0,"I'm trying to use de AWS SageMaker Training Jobs console to train a model with H2o.AutoMl.


I got stuck trying to set up Hyperparameters, specifically setting up the 'training' field.


{'classification': true, 'categorical_columns':'', 'target': 'label'}





I'm trying to set up a classification training job (1/0), and I believe that everything else on the setup page I can cope, but I don't know how to set up the 'training' field. My data is stored on S3 as a CSV file, as the algorithm requires.


My data has around 250000 columns, 4 out of them are categorical, one of them is the target, and the remainder is continuous variables (800 MB)


target column name = 'y'
categorical columns name = 'SIT','HOL','CTH','YTT'



I hope someone could help me.


Thank you!","['h2o', 'training-data', 'amazon-sagemaker', 'automl']",Marcel Mendes Reis,https://stackoverflow.com/users/8836963/marcel-mendes-reis,107
57951856,57951856,2019-09-16T07:04:33,2019-09-16 09:49:42Z,0,"I think, I have a datacasting problem using H2O platform in R.


this is the error:




Error: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_model_R_1568616391145_4.  Details: ERRR on field: _validation_frame: Test/Validation dataset has a categorical response column 'gold' with no levels in common with the model




and this is the code:


library(h2o)
kd_h2o = h2o.init(nthreads = -1)
data = readxl::read_excel(""C:\\Users\\frzd\\Desktop\\mtx.xlsx"")
data_order <- data[order(data$gold),]
data_order$gold=h2o.asfactor(data_order$gold)
Split_ts = .2
Split_vl = .1
indx <- 1:round(length(data$gold)*Split_ts)
ts <- max(indx)
ts <- round(indx*length(data$gold)/ts)
test = as.h2o(data_order[ts,])
train = data_order[-ts,]
indx <- 1:round(length(train$gold)*Split_vl)
ts <- max(indx)
ts <- round(indx*length(train$gold)/ts)
valid = as.h2o(train[ts,])
train = as.h2o(train[-ts,])

fit <- h2o.gbm(y = 15, 
              training_frame = train, 
              validation_frame=valid,
                    # cvControl = list(V = 5),
               )



could you guys help me out? :)","['r', 'deep-learning', 'h2o']",kath,https://stackoverflow.com/users/5892059/kath,"7,724"
57896105,57896105,2019-09-11T20:06:12,2019-09-13 18:44:46Z,255,"I'm trying to specify user split points for a partial dependence plot with H2o, but I can't figure out how to code for this.


The documentation says:




A two-level nested list containing user defined split points for pdp plots for each column. If there are two columns using user defined split points, there should be two lists in the nested list. Inside each list, the first element is the column name followed by values defined by the user.




I've tried looking for user tests on github, but couldn't find anything.


h2o.partialPlot(data
     , cols = ""change""
     , user_splits = list(list(""change""), list(0,.01,.02))
     , object = h2o.getModel(""gbm_model"")
     , plot = FALSE)





I get the following error:


Error in 
[.H2OFrame
(data, csname) : Row must be selected as an integer index, character, logical, or H2OFrame but got list


If i remove the user_splits line of code, I get no error.",['h2o'],Will.I.am,https://stackoverflow.com/users/8695616/will-i-am,25
57885860,57885860,2019-09-11T09:15:55,2019-09-11 10:12:59Z,0,"I'm trying to extract unique values from a column in h2o data frame from python where filter is applied, but it keeps extracting all possible unique values. Is this a bug? I think this problem occurs only on enum type of columns... Example code provided.
H2O version 3.26.0.2


df_example = h2o.H2OFrame({'time': ['M','M','M','D','D','M','M','D'],
              'amount': [1,4,5,0,0,1,3,0]})

df_example['amount'] = df_example['amount'].asfactor()

df_example[df_example['time']=='D', 'amount'].unique()



I expect to get one column data frame that has only 0 as value, but instead , I get one column data frame with values 0,1,2,3,4,5.


-edit-


What's even stranger, making new handle or deep copy of the dataframe does not solve the problem:


df_example_new = df_example[df_example['time']=='D'] # I want only 'D'
df_example_new['time'].unique()  # still producing 'M' and 'D'

df_example_newest = h2o.deep_copy(df_example[df_example['time']=='D'], 'df_example_newest')
df_example_newest.unique() # again 'M' and 'D'","['python', 'dataframe', 'h2o']",Unknown,,N/A
57885421,57885421,2019-09-11T08:48:10,2019-11-18 19:49:11Z,0,"I did some kind of data analitycs with the h2o.ai platform in R and I want to receive the AUCPR curve for the model / the prediction.


I already tried to use ""PRROC"" package, but it sees either not working or to slow for my dataset (1.4 Million instances). For the other available packages I don't really know how i can extract the data from h2o model.


pr <- h2o.predict(V_PUF_AGG1_NPI_ALLEX_BINAR.drf.tt.standard, data.test)



gives me a prediction matrix (which i can use for further proceeding?):


  predict        p1           p2
1       1 0.9999427 5.731940e-05
2       1 0.9999606 3.939748e-05
3       1 0.9999744 2.556443e-05
4       1 0.9999659 3.413081e-05
5       1 0.9999606 3.939748e-05
6       1 0.9999545 4.554749e-05

[987141 rows x 3 columns] 



So I'm searching for a quick solution to plot AUCPR curve.


It's easy to get the ROC curve, but there is no way to get an AUCPR curve directly from h2o:


plot(h2o.performance(V_PUF_AGG1_NPI_ALLEX_BINAR.drf.tt.standard, valid=T), type='roc')



Thanks - John","['r', 'plot', 'h2o', 'roc', 'auc']",pfohl,https://stackoverflow.com/users/12051758/pfohl,21
57877561,57877561,2019-09-10T20:07:20,2019-09-10 20:23:22Z,0,"i try to use 
R h2o.predict
 function to get prediction as data frame which consist 
p0 p1 predict
 columns, although even in this code i tried and get mentioned data frame,but now i get one column which consist predict. Below i will share piece of my code and result.


i tried to do some changes on my test frames (train_df and test_df) , but it does not work. Can anybody help me to get expected result which is mentioned below?


h2o.init()

h2o_data = as.h2o(train_df)
train = h2o_data


Survived = 'Survived'

aml = h2o.automl(y=Survived,
                training_frame = train,
                max_runtime_secs = 12)

str(test_df)
test = as.h2o(test_df)
predictions = as.data.frame(h2o.predict(object = aml, newdata = test))
predictions



i excepted something like


p0          p1              predict
0.124124    0.8752341         0
0.124124    0.8752341         0
0.124124    0.8752341         0
0.124124    0.8752341         0
.
.
.



but get



        predict
1   0.052932147
2   0.302577856
3   0.131041562
4   0.210355447
5   0.534559986
6   0.123824789
7   0.557775192



.
.
.","['r', 'data-science', 'h2o']",user1120,https://stackoverflow.com/users/7897666/user1120,119
57828268,57828268,2019-09-06T20:33:51,2019-10-18 14:15:35Z,248,"I am not able to understand how I can slice the h2o data frame based on particular rows specially with which function. can some one help me please.


Sample code I am trying:


iris_hf <- as.h2o(iris)


iris_hf[h2o.which(iris_hf[, 1] == 4.4),]


Similar login in R code:(reference)


data[which(data$col_1>0 &
              data$col_2 != 0 & data$col_3>0),]","['dataframe', 'apache-spark', 'h2o']",Divya Mereddy,https://stackoverflow.com/users/9311050/divya-mereddy,31
57820153,57820153,2019-09-06T10:19:13,2019-09-06 10:19:13Z,29,"I would like to know what is the algorithm that h2o uses in sampling. For example, does it use Latin Hyper-cube Sampling?","['h2o', 'sampling']",Alireza Ebrahimvandi,https://stackoverflow.com/users/6056224/alireza-ebrahimvandi,41
57815685,57815685,2019-09-06T04:22:02,2019-09-06 22:54:01Z,220,"I'm trying to launch H2O 3.26.0.3 through Python 3.6.8 using the command:




h2o.init(nthreads=-1)




On a 16-core virtual machine, H2O correctly initializes with this setting:


H2O cluster total cores:    16
H2O cluster allowed cores:  16



But on a 16-core Docker container, H2O initializes with this unusual setting:


H2O cluster total cores:    1
H2O cluster allowed cores:  1



Is there a reason why H2O doesn't recognize all the CPU cores in a container setting?


Edit

If I modify the initialization to explicitly list all cores:




h2o.init(nthreads=16)




Then the settings look like this:


H2O cluster total cores:    1
H2O cluster allowed cores:  16



It seems odd that the allowed cores can exceed the total cores. But after some testing, H2O does appear to use multiple cores instead of a single core. So even if H2O doesn't detect multiple cores, it can still make use of them.","['python', 'docker', 'h2o']",Unknown,,N/A
57811873,57811873,2019-09-05T19:25:33,2019-09-06 05:30:43Z,0,"I'm running some projects with H2o AutoML using Sagemaker notebook instances, and I would like to know if H2o AutoML can benefit from a GPU Sagemaker instance, if so, how should I configure the notebook?","['gpu', 'h2o', 'amazon-sagemaker', 'automl']",Marcel Mendes Reis,https://stackoverflow.com/users/8836963/marcel-mendes-reis,107
57778445,57778445,2019-09-03T20:38:31,2019-09-03 21:16:35Z,217,"Looking at the h2o docs, it says




Inside H2O, a Distributed Key/Value store is used to access and reference data, models, objects, etc., across all nodes and machines. The algorithms are implemented on top of H2O’s 
distributed Map/Reduce framework
 and utilize the Java Fork/Join framework for multi-threading. The data is read in parallel and is distributed across the cluster...




Looking at the h2o 
downloads page
, I see that there is a standalone version of h2o. Wondering what the difference(s) is between these versions? Eg. I assume that the h2o algorithms are intended to use MapReduce algorithm, so would ML training be slower on H2OFrame objs when using standalone mode even if the single host had same memory as if allocated as a YARN application?",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
57778138,57778138,2019-09-03T20:08:40,2019-09-03 20:23:15Z,362,"Can a docker image access hadoop resources? Eg. submit YARN jobs and access HDFS; something like 
MapR's Datasci. Refinery
, but for Hortonworks HDP 3.1. (May assume that the image will be launched on a hadoop cluster node).


Saw the hadoop 
docs
 for launching docker applications from hadoop nodes, but was interested in whether could go the ""other way"" (ie. being able to start a docker image with the conventional 
docker -ti ...
 command and have that application be able to run hadoop jars etc. (assuming that the docker image host is a hadoop node itself)). I understand that MapR hadoop has docker images for doing 
this
, but am interested in using Hortonworks HDP 3.1. Ultimately trying to run 
h2o
 hadoop in a docker container. 


Anyone know if this is possible or can confirm that this is not possible?","['docker', 'hadoop', 'h2o', 'hdp']",lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
57758996,57758996,2019-09-02T14:51:18,2019-09-02 22:51:12Z,398,"I run H2O on a docker image using Python 3.6.3 and H2O 3.26.0.3.


import h2o
from h2o.automl import H2OAutoML
h2o.init()



In this step, initialization is successful and it prints the following information.


H2O cluster uptime: 01 secs
H2O cluster timezone:   Europe/Istanbul
H2O data parsing timezone:  UTC
H2O cluster version:    3.26.0.3
H2O cluster version age:    9 days
H2O cluster name:   H2O_from_python_96273_8m5wyj
H2O cluster total nodes:    1
H2O cluster free memory:    26.67 Gb
H2O cluster total cores:    72
H2O cluster allowed cores:  72
H2O cluster status: accepting new members, healthy
H2O connection url: http://127.0.0.1:54321
H2O connection proxy:   None
H2O internal security:  False
H2O API Extensions: Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4
Python version: 3.6.3 final



Now, I will run AutoML but it is problematic.


hf = h2o.H2OFrame(x_train)
aml = H2OAutoML(max_runtime_secs=600)
aml.train(x = list(df.columns[:-1]), y = df.columns[-1], training_frame = hf)



I have the following error




ConnectionResetError                      Traceback (most recent call
  last) ~/.local/lib/python3.6/site-packages/urllib3/connectionpool.py
  in urlopen(self, method, url, body, headers, retries, redirect,
  assert_same_host, timeout, pool_timeout, release_conn, chunked,
  body_pos, **response_kw)
      599                                                   body=body, headers=headers,
  --> 600                                                   chunked=chunked)
      601 


~/.local/lib/python3.6/site-packages/urllib3/connectionpool.py in
  _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
      383                     # otherwise it looks like a programming error was the cause.
  --> 384                     six.raise_from(e, None)
      385         except (SocketTimeout, BaseSSLError, SocketError) as e:


~/.local/lib/python3.6/site-packages/urllib3/packages/six.py in
  raise_from(value, from_value)


~/.local/lib/python3.6/site-packages/urllib3/connectionpool.py in
  _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
      379                 try:
  --> 380                     httplib_response = conn.getresponse()
      381                 except Exception as e:


/opt/rh/rh-python36/root/usr/lib64/python3.6/http/client.py in
  getresponse(self)    1330             try:
  -> 1331                 response.begin()    1332             except ConnectionError:


/opt/rh/rh-python36/root/usr/lib64/python3.6/http/client.py in
  begin(self)
      296         while True:
  --> 297             version, status, reason = self._read_status()
      298             if status != CONTINUE:


/opt/rh/rh-python36/root/usr/lib64/python3.6/http/client.py in
  _read_status(self)
      257     def _read_status(self):
  --> 258         line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
      259         if len(line) > _MAXLINE:


/opt/rh/rh-python36/root/usr/lib64/python3.6/socket.py in
  readinto(self, b)
      585             try:
  --> 586                 return self._sock.recv_into(b)
      587             except timeout:


ConnectionResetError: [Errno 104] Connection reset by peer


During handling of the above exception, another exception occurred:


ProtocolError                             Traceback (most recent call
  last) ~/.local/lib/python3.6/site-packages/requests/adapters.py in
  send(self, request, stream, timeout, verify, cert, proxies)
      448                     retries=self.max_retries,
  --> 449                     timeout=timeout
      450                 )


~/.local/lib/python3.6/site-packages/urllib3/connectionpool.py in
  urlopen(self, method, url, body, headers, retries, redirect,
  assert_same_host, timeout, pool_timeout, release_conn, chunked,
  body_pos, **response_kw)
      637             retries = retries.increment(method, url, error=e, _pool=self,
  --> 638                                         _stacktrace=sys.exc_info()[2])
      639             retries.sleep()


~/.local/lib/python3.6/site-packages/urllib3/util/retry.py in
  increment(self, method, url, response, error, _pool, _stacktrace)
      367             if read is False or not self._is_method_retryable(method):
  --> 368                 raise six.reraise(type(error), error, _stacktrace)
      369             elif read is not None:


~/.local/lib/python3.6/site-packages/urllib3/packages/six.py in
  reraise(tp, value, tb)
      684         if value.
traceback
 is not tb:
  --> 685             raise value.with_traceback(tb)
      686         raise value


~/.local/lib/python3.6/site-packages/urllib3/connectionpool.py in
  urlopen(self, method, url, body, headers, retries, redirect,
  assert_same_host, timeout, pool_timeout, release_conn, chunked,
  body_pos, **response_kw)
      599                                                   body=body, headers=headers,
  --> 600                                                   chunked=chunked)
      601 


~/.local/lib/python3.6/site-packages/urllib3/connectionpool.py in
  _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
      383                     # otherwise it looks like a programming error was the cause.
  --> 384                     six.raise_from(e, None)
      385         except (SocketTimeout, BaseSSLError, SocketError) as e:


~/.local/lib/python3.6/site-packages/urllib3/packages/six.py in
  raise_from(value, from_value)


~/.local/lib/python3.6/site-packages/urllib3/connectionpool.py in
  _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
      379                 try:
  --> 380                     httplib_response = conn.getresponse()
      381                 except Exception as e:


/opt/rh/rh-python36/root/usr/lib64/python3.6/http/client.py in
  getresponse(self)    1330             try:
  -> 1331                 response.begin()    1332             except ConnectionError:


/opt/rh/rh-python36/root/usr/lib64/python3.6/http/client.py in
  begin(self)
      296         while True:
  --> 297             version, status, reason = self._read_status()
      298             if status != CONTINUE:


/opt/rh/rh-python36/root/usr/lib64/python3.6/http/client.py in
  _read_status(self)
      257     def _read_status(self):
  --> 258         line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
      259         if len(line) > _MAXLINE:


/opt/rh/rh-python36/root/usr/lib64/python3.6/socket.py in
  readinto(self, b)
      585             try:
  --> 586                 return self._sock.recv_into(b)
      587             except timeout:


ProtocolError: ('Connection aborted.', ConnectionResetError(104,
  'Connection reset by peer'))


During handling of the above exception, another exception occurred:


ConnectionError                           Traceback (most recent call
  last) ~/.local/lib/python3.6/site-packages/h2o/backend/connection.py
  in request(self, endpoint, data, json, filename, save_to)
      404                                     headers=headers, timeout=self._timeout, stream=stream,
  --> 405                                     auth=self._auth, verify=self._verify_ssl_cert, proxies=self._proxies)
      406             self._log_end_transaction(start_time, resp)


~/.local/lib/python3.6/site-packages/requests/api.py in
  request(method, url, **kwargs)
       59     with sessions.Session() as session:
  ---> 60         return session.request(method=method, url=url, **kwargs)
       61 


~/.local/lib/python3.6/site-packages/requests/sessions.py in
  request(self, method, url, params, data, headers, cookies, files,
  auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert,
  json)
      532         send_kwargs.update(settings)
  --> 533         resp = self.send(prep, **send_kwargs)
      534 


~/.local/lib/python3.6/site-packages/requests/sessions.py in
  send(self, request, **kwargs)
      645         # Send the request
  --> 646         r = adapter.send(request, **kwargs)
      647 


~/.local/lib/python3.6/site-packages/requests/adapters.py in
  send(self, request, stream, timeout, verify, cert, proxies)
      497         except (ProtocolError, socket.error) as err:
  --> 498             raise ConnectionError(err, request=request)
      499 


ConnectionError: ('Connection aborted.', ConnectionResetError(104,
  'Connection reset by peer'))


During handling of the above exception, another exception occurred:


H2OConnectionError                        Traceback (most recent call
  last)  in 
  ----> 1 aml.train(x = list(df.columns[:-1]), y = df.columns[-1], training_frame = hf)


~/.local/lib/python3.6/site-packages/h2o/automl/autoh2o.py in
  train(self, x, y, training_frame, fold_column, weights_column,
  validation_frame, leaderboard_frame, blending_frame)
      443         poll_updates = ft.partial(self._poll_training_updates, verbosity=self._verbosity, state={})
      444         try:
  --> 445             self._job.poll(poll_updates=poll_updates)
      446         finally:
      447             poll_updates(self._job, 1)


~/.local/lib/python3.6/site-packages/h2o/job.py in poll(self,
  poll_updates)
       55             pb = ProgressBar(title=self._job_type + "" progress"", hidden=hidden)
       56             if poll_updates:
  ---> 57                 pb.execute(self._refresh_job_status, print_verbose_info=ft.partial(poll_updates, self))
       58             else:
       59                 pb.execute(self._refresh_job_status)


~/.local/lib/python3.6/site-packages/h2o/utils/progressbar.py in
  execute(self, progress_fn, print_verbose_info)
      169                 # Query the progress level, but only if it's time already
      170                 if self._next_poll_time <= now:
  --> 171                     res = progress_fn()  # may raise StopIteration
      172                     assert_is_type(res, (numeric, numeric), numeric)
      173                     if not isinstance(res, tuple):


~/.local/lib/python3.6/site-packages/h2o/job.py in
  _refresh_job_status(self)
       92     def _refresh_job_status(self):
       93         if self._poll_count <= 0: raise StopIteration("""")
  ---> 94         jobs = h2o.api(""GET /3/Jobs/%s"" % self.job_key)
       95         self.job = jobs[""jobs""][0] if ""jobs"" in jobs else jobs[""job""][0]
       96         self.status = self.job[""status""]


~/.local/lib/python3.6/site-packages/h2o/h2o.py in api(endpoint, data,
  json, filename, save_to)
      102     # type checks are performed in H2OConnection class
      103     _check_connection()
  --> 104     return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
      105 
      106 


~/.local/lib/python3.6/site-packages/h2o/backend/connection.py in
  request(self, endpoint, data, json, filename, save_to)
      413             else:
      414                 self._log_end_exception(e)
  --> 415                 raise H2OConnectionError(""Unexpected HTTP error: %s"" % e)
      416         except requests.exceptions.Timeout as e:
      417             self._log_end_exception(e)


H2OConnectionError: Unexpected HTTP error: ('Connection aborted.',
  ConnectionResetError(104, 'Connection reset by peer'))




I suspect that proxy might be the reason of this exception. When I add the proxy information to the path, then exception message would be ""HTTP 500 INKApi Error""


import os
os.environ['http_proxy']= ...
os.environ['https_proxy']= ...



JVM stdout log file dumps the following exception.




[thread 140335217821440 also had an error][thread 140335320467200 also
  had an error] [thread 140335207294720 also had an error]


[thread 140335316256512 also had an error]# A fatal error has been detected by the Java Runtime Environment:


[thread 140335202031360 also had an error]
    SIGSEGV (0xb) at pc=0x00007fa3276cdb8d, pid=51986, tid=0x00007fa2575f5700


JRE version: OpenJDK Runtime Environment (8.0_212-b04) (build 1.8.0_212-b04)
   Java VM: OpenJDK 64-Bit Server VM (25.212-b04 mixed mode linux-amd64 compressed oops)
   Problematic frame:
   [thread 140335231506176 also had an error] C  [libc.so.6+0x39b8d][thread 140335341520640 also had an error]




JVM stderr log file contains interesting logs




libgomp: Thread creation failed: Resource temporarily unavailable
  
* Error in `/usr/bin/java': free(): corrupted unsorted chunks: 0x00007efe342f0240 *


libgomp: Thread creation failed: Resource temporarily unavailable




Funny but It runs successfully when I run same code on my local machine. I suspect that it might be because of docker configuration.","['python', 'h2o']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
57749892,57749892,2019-09-01T22:27:09,2022-02-10 05:18:50Z,0,"I need to install an older version of H2O because model loading doesn't work even if the versions are just one apart (3.26.0.2 vs. 3.26.0.3). I'm struggling to find a page from which I can find the download links. Why doesn't it exist? All software have an archive or older versions page for this. I also tried playing with the link to current version but no luck as it doesn't have a pattern you could guess.
So how can I install H2O 3.26.0.2 in Python (pip)?


Looked all over the web and documentation


model = h2o.load_model(""H2O.model_name.zip"")


  Error: Found version 3.26.0.2, but running version 3.26.0.3
  Request: POST /99/Models.bin/
    data: {'dir': 'H2O.model_name.zip'}```","['python', 'h2o']",Unknown,,N/A
57741388,57741388,2019-08-31T20:26:56,2020-07-29 15:57:50Z,468,"I wrote python code to return the count of each levels of a feature of a h2o dataframe, but the result always come back in scientific notation.  How do I get it to display using decimal?  


Code I used:  


print(all_propensity[""HasLoss""].table())



What it returns:

HasLoss       Count

0                        1.46457e+07

1                         35277


What I want it to return:

HasLoss       Count

0                        14,645,700

1                         35,277","['python', 'pandas', 'h2o']",floydn,https://stackoverflow.com/users/5745815/floydn,"1,131"
57732980,57732980,2019-08-30T20:38:55,2019-09-03 20:08:37Z,355,"I have a MOJO model that I want to explore for model metrics (rmse,roc, etc.) I understand all model metrics are available for a binary model, but I want to get these metrics from a MOJO model. 


Input - Mojo Model and training dataset 
Output - Extract the model metrics 


Tried extracting the metrics from a binary model using : 
ModelMetricsSupport.modelMetrics [ ModelMetricsSupervised] (binaryModel,valid)


But unable to do the same for a MOJO Model.","['scala', 'h2o', 'sparkling-water']",Unknown,,N/A
57708979,57708979,2019-08-29T11:16:11,2019-08-31 10:08:06Z,0,"Is there a quick way in H2O Flow to view a scatterplot of predicted vs actual variables on the Train or Validate data when modelling a continuous target variable?


Or is this only possible by calling a plot in R?


H2O Flow has many useful diagnostics when training a model, but it would be nice to be able to view predicted vs actual on a scatterplot. I can run code in R for this (train the model, use the model to predict on training / validation data, bind predictions to actual, plot predicted vs actual in ggplot), but does Flow offer a quick way to view a diagnostic of this type?","['r', 'scatter-plot', 'h2o']",Ferdi,https://stackoverflow.com/users/6359698/ferdi,560
57708579,57708579,2019-08-29T10:50:38,2019-08-29 10:50:38Z,0,"My data frame has 100 rows. I try to build binary generalized linear model:


model = h2o.glm(family=""binomial"", x=predictors, y=response,
                training_frame=training_df, 
                missing_values_handling=""Skip"",
                lambda=0, compute_p_values = TRUE, nfolds = 7)
var_imp = h2o.varimp(model)



Model #1
 (AUC= 0.826 ± 0.012) has the following variable importance order:


                    variable relative_importance scaled_importance
1                          Y           1.7976670         1.0000000
2                          C           1.3799875         0.7676547
3                        AGE           0.8155962         0.4536971
4                    V_4_sum           0.6452078         0.3589140
5                         T2           0.3446666         0.1917300
6                    V_3_sum           0.2452110         0.1364051
7                         T1           0.2028320         0.1128307



After some time I added new variable 
G
 and 
model #2
 (AUC= 0.943 ± 0.015) performance improved significantly (as I expected):


                    variable relative_importance scaled_importance
1                          G          8.25206428        1.00000000
2                          C          2.23826129        0.27123653
3                    V_4_sum          1.14390917        0.13862097
4                         T1          0.82906448        0.10046753
5                    V_3_sum          0.76380841        0.09255968
6                         T2          0.20089827        0.02434521
7                        AGE          0.19790613        0.02398262
8                          Y          0.09374866        0.01136063



As you can see variable importance order has changed. 
G
 is now the most important var, but 
Y
 is at the end (previously it was the first).

AGE
 rank has decreased in the new model too.


Is such effect well known property of h2o 
varimp
 function for GLM (logistic regression for binary classification)? Can I say something general about 
AGE
 or 
Y
 importance using 
varimp
 order? 


What alternative methods can be applied to generate variable importance order in my case? I would like to have more stable order of the top variables.","['r', 'classification', 'logistic-regression', 'h2o', 'glm']",MLearner,https://stackoverflow.com/users/11067633/mlearner,63
57704535,57704535,2019-08-29T06:49:44,2019-09-09 14:57:32Z,0,"I've used 
h2o package
 to run random forest. To figure out that my model is bad or good, I looked for 
RMSE
.(Models are sorted by 
RMSE
)


h2o.final1618.rf <- h2o.getModel(rf.sortedGrid.1618.2@model_ids[[1]])

h2o.final1618.rf@model$validation_metrics@metrics

h2o.final1618.rf@model$training_metrics@metrics



I could get the values of RMSE through validation metrics and training metrics. 
The value of 
RMSE
 of 
validation_metrics
 is 
0.4526.

The value of 
RMSE
 of 
training_metrics
 is 
0.4571.


But, I used 
h2o.performance
 function or 
h2o.predict
 function to get the values of 
RMSE
, the values by 
h2o.performance
 is 
0.2852
 and it's smaller than the values of validation and train metrics above.


perf <- h2o.performance(h2o.final1618.rf, newdata = h2o.incheon1618[[2]])



h2o.incheon1618[[2]]
 is validation data.(I split data set 0.7 and 0.2 and 0.1 ; train data, validation data, test data)


I couldn't understand why the difference is generated.","['r', 'h2o']",CHOI,https://stackoverflow.com/users/11809449/choi,1
57621947,57621947,2019-08-23T07:52:54,2019-08-23 07:58:16Z,0,"I am using h2o and R for a binary classification problem. I was wondering if there is any way to create a learning curve in h2o? 


I coded some splits myself and I am plotting the curve alright, but I'd like to know if there is a quick recipe provided by h2o? H2O is known to make the things easier for data people.


Here's what I've been doing based on the answer here

Plot learning curves with caret package and R


I customized the loop:


library(data.table)
library(dplyr)
library(h2o)
#
sizes <- seq(0.05, 1.0, 0.05)
#
learnCurve <- data.frame(mid = character(length(sizes)),
                         m = integer(length(sizes)),
                         n1 = integer(length(sizes)),
                         n0 = integer(length(sizes)),
                         trainRMSE = double(length(sizes)),
                         cvRMSE = double(length(sizes)),
                         trainAUC = double(length(sizes)),
                         cvAUC = double(length(sizes)),
                         trainTPR = double(length(sizes)),
                         cvTPR = double(length(sizes)),
                         trainFPR = double(length(sizes)),
                         cvFPR = double(length(sizes))
                         )
#
h2o.init()
for (i in 1:length(sizes)) {
  set.seed(3)
  ind <- sample(nrow(ddf1s), sizes[i]*nrow(ddf1s))
  tr <- ddf1s[ind, ]
  dd <- ddf1 %>% filter(id %in% tr$id)
  #
  setDT(dd)
  #
  dmh2o <- dd[, c(response, predictors), with = FALSE]
  setDT(dmh2o)
  setnames(dmh2o, predictors, newNames)
  #
  ddhex <- as.h2o(dmh2o)
  #
  splitsx <- h2o.splitFrame(data= ddhex, ratios = .7, seed = 1234)
  trainx <- splitsx[[1]]
  validx <- splitsx[[2]]
  #
  gbmTreex <- h2o.gbm(x = newNames, y = response,
                      training_frame = trainx,
                      validation_frame = validx,
                      max_depth = 8,
                      seed = 1234)
  #
  learnCurve$mid[i] <- gbmTreex@model_id
  learnCurve$m[i] <- nrow(dmh2o)
  learnCurve$n0[i] <- nrow(filter(dmh2o, target == 0))
  learnCurve$n1[i] <- nrow(filter(dmh2o, target == 1))
  learnCurve$trainRMSE[i] <- gbmTreex@model$training_metrics@metrics$RMSE
  learnCurve$cvRMSE[i] <- gbmTreex@model$validation_metrics@metrics$RMSE
  learnCurve$trainAUC[i] <- gbmTreex@model$training_metrics@metrics$AUC
  learnCurve$cvAUC[i] <- gbmTreex@model$validation_metrics@metrics$AUC
  learnCurve$trainTPR[i] <- gbmTreex@model$training_metrics@metrics$cm$table$`1`[[2]] / gbmTreex@model$training_metrics@metrics$cm$table$`1`[[3]]
  learnCurve$cvTPR[i] <- gbmTreex@model$validation_metrics@metrics$cm$table$`1`[[2]] / gbmTreex@model$validation_metrics@metrics$cm$table$`1`[[3]]
  learnCurve$trainFPR[i] <- gbmTreex@model$training_metrics@metrics$cm$table$Error[[2]]
  learnCurve$cvFPR[i] <- gbmTreex@model$training_metrics@metrics$cm$table$Error[[2]]
  #
  mstemp <- gbmTreex@model$model_summary
  if(i == 1) {
    ms <- mstemp
  }
  #
  if (i > 1) {
    ms <- rbind(ms, mstemp)
  }
  #
  print(i)#
  rm(gbmTreex, dmh2o, ddhex, splitsx, trainx, validx)
  gc()
}
#
h2o.shutdown()



I did not include the data since the main question is obtaining the model metrics by varying the sizes of the data. I'd like to know if there is another (quicker) way provided by H2O?","['r', 'machine-learning', 'h2o', 'supervised-learning']",Unknown,,N/A
57595703,57595703,2019-08-21T16:17:08,2019-08-27 07:09:45Z,266,"I'm using H2O init to specify the Snowflake JDBC driver as extra_classpath when connecting to external H2O instance however, getting the following error (H2O connects successfully to external instance), when attempting to access Snowflake DB: 


H2OServerError: HTTP 500 Server Error:
Server error java.lang.RuntimeException:
  Error: SQLException: No suitable driver found for jdbc:snowflake:..


It 
works fine when starting a standalone H2O
 instance with nothing else changed.


Here is the init code:


h2o.init(ip='<ip>', 
         port=54321,
         username='**',
         password='**',
         extra_classpath = [""snowflake-jdbc-3.8.0.jar""])



H2O version: 3.22.1.1
Python 3","['h2o', 'snowflake-cloud-data-platform']",Vaibhav,https://stackoverflow.com/users/1578274/vaibhav,"2,547"
57585109,57585109,2019-08-21T05:41:51,2019-08-21 22:11:03Z,462,"I am using H2O Flow when I upload the time series data, it convert the date to a number such as 1.07317E+12.I have used 2004-02-01 and 01/02/2004. I don't know how to fix this problem. I saw some note given blow but it did not help.


h2o.ai - Flow UI not detecting date formatting to convert to Time


Second issues, h20 flow split frame shuffle the data automatically. Is there anyway to stop it?. I know in R we can put shuffle= FALSE. But how can do it in H2O Flow as it create the problem for times series data when splitting into train and test.",['h2o'],shafi Q,https://stackoverflow.com/users/11489179/shafi-q,77
57580680,57580680,2019-08-20T19:51:54,2019-08-28 21:03:21Z,0,"I am trying to import a 
h2o
 model as a 
.zip
 file exporter as 
POJO
 with 
R
. The following error is all I get:


model_file <- ""/Users/bernardo/Desktop/DRF_1_AutoML_20190816_133251.zip""
m <- h2o.importFile(model_file)



Error: DistributedException from localhost/127.0.0.1:54321: 'Cannot determine file type. for nfs://Users/bernardo/Desktop/DRF_1_AutoML_20190816_133251.zip', caused by water.parser.ParseDataset$H2OParseException: Cannot determine file type. for nfs://Users/bernardo/Desktop/DRF_1_AutoML_20190816_133251.zip



I already ran 
file.exists(model_file)
 and that returns 
TRUE
, so the file exists. Did the same with 
normalizePath(model_file)
 and same result. When I try to import it into my R session, it seems that 
h2o
 finds the file but can't import it for some reason. 


Here's my R Session info:


R version 3.6.0 (2019-04-26)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Mojave 10.14.6

Matrix products: default
BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] h2o_3.26.0.2      lares_4.7         data.table_1.12.2 lubridate_1.7.4   forcats_0.4.0    
 [6] stringr_1.4.0     dplyr_0.8.3       purrr_0.3.2       readr_1.3.1       tidyr_0.8.3      
[11] tibble_2.1.3      ggplot2_3.2.1     tidyverse_1.2.1  



Hope you guys can help me import my POJO model into R. Thanks!","['java', 'r', 'h2o']",Bernardo,https://stackoverflow.com/users/7227825/bernardo,498
57576662,57576662,2019-08-20T15:10:42,2019-08-24 07:08:57Z,0,"I am using H2O and R for a binary classification problem. 
The dataset has over 800 features and some of them include non-english names and characters, for example 'ö'. 


I am getting the following error message:


Error in .verify_dataxy(params$training_frame, x, y): Invalid column names



Then the list of columns with the problematic characters.


I have already googled and searched SO for a documentation about the settings regarding accepted languages in H2O. 


Here is a sample code:


library(h2o)
h2o.init()
sodata <- data.frame(Erklärung = sample(c(0,1), 50, replace = TRUE),
                 isPot = sample(c(0,1), 50, replace = TRUE),
                 target = sample(c(0,1), 50, replace = TRUE))
#
tar <- ""target""
pr <- setdiff(colnames(sodata), tar)
sohex <- as.h2o(sodata)
spl <- h2o.splitFrame(data = sohex, ratios = .7, seed = 1)
training <- spl[[1]]
testing <- spl[[2]]
#
gbm1 <- h2o.gbm(x = pr, 
                y = tar, 
                training_frame = training, 
                validation_frame = testing)
#
#h2o.shutdown()



The error message is 


Error in .verify_dataxy(training_frame, x, y):
  Invalid column names: Erklärung



Is there a way to change the accepted language in H2O?


Edit
: session and environment info,


sessionInfo()
R version 3.5.1 (2018-07-02)
Platform: x86_64_w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

Matrix products: default

locale:
[1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252    LC_MONETARY=German_Germany.1252 LC_NUMERIC=C                   
[5] LC_TIME=German_Germany.1252    




Under the displayed settings after 
Sys.getenv()
 there is nothing language related.","['r', 'h2o']",Unknown,,N/A
57571722,57571722,2019-08-20T10:33:50,2019-09-02 23:07:36Z,0,"I have fairly small dataset: 15 columns, 3500 rows and I am consistenly seeing that xgboost in h2o trains better model than h2o AutoML. I am using H2O 3.26.0.2 and Flow UI.


H2O XGBoost finishes in a matter of seconds while AutoML takes as long as it needs (20 mins) and always gives me worse performance.


I admit dataset might not be perfect but I would expect that AutoML with gridsearch would be as good (or better) than h2o XGBoost. My thinking is that AutoML will train multiple XGBoost model and do gridsearch on hyperparameters so it should be similar, right?


For both AutoML and XGBoost I use same training dataset and same response column.


Code for running experiment with XGBoost is:


import h2o
from h2o.estimators.xgboost import H2OXGBoostEstimator

h2o_frame = h2o.import_file(path=""myFile.csv"")

feature_columns = h2o_frame.columns
label_column = ""responseColumn""
feature_columns.remove(label_column)

xgb = H2OXGBoostEstimator(nfolds=10, seed=1)

xgb.train(x=feature_columns, y=label_column, training_frame=h2o_frame)

# now export metrics to file
MRD = xgb.mean_residual_deviance()
RMSE= xgb.rmse()
MSE= xgb.mse()
MAE= xgb.mae()
RMSLE= xgb.rmsle()

header = ['model','mean_residual_deviance','rmse','mse','mae','rmsle']

with open('metrics.out', mode='w') as result_file:
    writer = csv.writer(result_file, delimiter=',', quotechar='""', quoting=csv.QUOTE_MINIMAL)
    writer.writerow(header)
    writer.writerow(['H2O_XGBoost', MRD, RMSE, MSE, MAE, RMSLE])



Code for running experiment with AutoML is:


import h2o
from h2o.automl import H2OAutoML

h2o_frame = h2o.import_file(path=""myFile.csv"")

feature_columns = h2o_frame.columns
label_column = ""responseColumn""
feature_columns.remove(label_column)

aml = H2OAutoML(seed=1, nfolds=10, exclude_algos=[""StackedEnsemble""], max_models=20)

aml.train(x=feature_columns, y=label_column, training_frame=h2o_frame)

# now export metrics to file
h2o.export_file(aml.leaderboard, ""metrics.out"", force = True, parts = 1)



Tried using different nfold, more models for AutoML, increasing early stopping rounds. I tried excluding all algorithms from AutoML (except XGBoost) and I still get same results.


Here are the differences in results:


H2O XGBoost:


model   xgboost-5a8f9766-940c-4e5c-b57d-62b186f4c058
model_checksum  7409831159060775248
frame   train_set_v01.hex
frame_checksum  6864971999838167226
description ·
model_category  Regression
scoring_time    1566296468447
predictions ·
MSE 252.265021
RMSE    15.882853
nobs    3476
custom_metric_name  ·
custom_metric_value 0
r2  0.726871
mean_residual_deviance  252.265021
mae 10.709369
rmsle   NaN



XGBoost native params for xgboost-5a8f9766-940c-4e5c-b57d-62b186f4c058:


name    value
silent  true
eta 0.3
colsample_bylevel   1
objective   reg:linear
min_child_weight    1
nthread 8
seed    -1058380797
max_depth   6
colsample_bytree    1
lambda  1
gamma   0
alpha   0
booster gbtree
grow_policy depthwise
nround  50
subsample   1
max_delta_step  0
tree_method auto



H2O AutoML (winning model):


model   StackedEnsemble_AllModels_AutoML_20190819_235446
model_checksum  -6727284429527535576
frame   automl_training_train_set_v01.hex
frame_checksum  6864971999838167226
description ·
model_category  Regression
scoring_time    1566256209073
predictions ·
MSE 332.146239
RMSE    18.224880
nobs    3476
custom_metric_name  ·
custom_metric_value 0
r2  0.640383
mean_residual_deviance  332.146239
mae 12.927023
rmsle   1.225650
residual_deviance   1154540.326762
null_deviance   3210476.302359
AIC 30070.640602
null_degrees_of_freedom 3475
residual_degrees_of_freedom 3464



And the best rated XGBoost model from same AutoML (third in the leaderboard):


model   XGBoost_grid_1_AutoML_20190819_235446_model_5
model_checksum  8047828446507408480
frame   automl_training_train_set_v01.hex
frame_checksum  6864971999838167226
description ·
model_category  Regression
scoring_time    1566255442068
predictions ·
MSE 616.910151
RMSE    24.837676
nobs    3476
custom_metric_name  ·
custom_metric_value 0
r2  0.332068
mean_residual_deviance  616.910151
mae 17.442629
rmsle   1.325149



XGBoost native params (for XGBoost_grid_1_AutoML_20190819_235446_model_5 in AutoML):


name    value
silent  true
normalize_type  tree
eta 0.05
objective   reg:linear
colsample_bylevel   0.8
nthread 8
seed    940795529
min_child_weight    15
rate_drop   0
one_drop    0
sample_type uniform
max_depth   20
colsample_bytree    1
lambda  100
gamma   0
alpha   0.1
booster dart
grow_policy depthwise
skip_drop   0
nround  120
subsample   0.8
max_delta_step  0
tree_method auto","['python', 'h2o', 'automl']",Unknown,,N/A
57570144,57570144,2019-08-20T09:01:03,2019-08-22 09:54:35Z,0,"I would like to tune ""classif.h2o.deeplearning"" learner via 
mlr
. During the tuning I have several architectures I would like explored. For each of these architectures I would like to specify a dropout space. However I am struggling with this.


Example:


library(mlr)
library(h2o)

ctrl <- makeTuneControlRandom(maxit = 10) 

lrn <- makeLearner(""classif.h2o.deeplearning"", predict.type = ""prob"")



I define two architectures ""a"" and ""b"" via the ""hidden"" DiscreteParam, for each of them I would like to create a NumericVectorParam of ""hidden_dropout_ratios""


par_set <- makeParamSet(
  makeDiscreteParam(""hidden"", values = list(a = c(16L, 16L),
                                            b = c(16L, 16L, 16L))),
  makeDiscreteParam(""activation"", values = ""RectifierWithDropout"", tunable = FALSE),
  makeNumericParam(""input_dropout_ratio"", lower = 0, upper = 0.4, default = 0.1),
  makeNumericVectorParam(""hidden_dropout_ratios"", len = 2, lower = 0, upper = 0.6, default = rep(0.3, 2),
                         requires = quote(length(hidden) == 2)),
  makeNumericVectorParam(""hidden_dropout_ratios"", len = 3, lower = 0, upper = 0.6, default = rep(0.3, 3),
                         requires = quote(length(hidden) == 3)))



this produces an error:


Error in makeParamSet(makeDiscreteParam(""hidden"", values = list(a = c(16L,  : 
All parameters must have unique names!



Setting just one of them results in dropout being applied only on architectures of appropriate number of hidden layers. 


When I attempt to use the same dropout for all hidden layers:


par_set <- makeParamSet(
  makeDiscreteParam(""hidden"", values = list(a = c(16L, 16L),
                                            b = c(16L, 16L, 16L))),
  makeDiscreteParam(""activation"", values = ""RectifierWithDropout"", tunable = FALSE),
  makeNumericParam(""input_dropout_ratio"", lower = 0, upper = 0.4, default = 0.1),
  makeNumericParam(""hidden_dropout_ratios"", lower = 0, upper = 0.6, default = 0.3))

tw <- makeTuneWrapper(lrn,
                      resampling = cv3,
                      control = ctrl,
                      par.set = par_set,
                      show.info = TRUE,
                      measures = list(auc,
                                      bac))

perf_tw <- resample(tw, 
                     task = sonar.task,
                     resampling = cv5,
                     extract = getTuneResult,
                     models = TRUE,
                     show.info = TRUE,
                     measures = list(auc,
                                     bac))



I get the error:


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
ERROR MESSAGE:

Illegal argument(s) for DeepLearning model: DeepLearning_model_R_1566289564965_2.  Details: ERRR on field: _hidden_dropout_ratios: Must have 3 hidden layer dropout ratios.



Perhaps I could overcome this by creating a separate learner for each architecture and then combining with 
makeModelMultiplexer
?


I would like your help in overcoming this. Thanks.


EDIT: I was able to overcome this using 
makeModelMultiplexer
 and by creating a learner for each architecture (number of hidden layers). 


base_lrn <- list(
  makeLearner(""classif.h2o.deeplearning"",
              id = ""h20_2"",
              predict.type = ""prob""),
  makeLearner(""classif.h2o.deeplearning"",
              id = ""h20_3"",
              predict.type = ""prob""))

mm_lrn <- makeModelMultiplexer(base_lrn)

par_set <- makeParamSet(
  makeDiscreteParam(""selected.learner"", values = extractSubList(base_lrn, ""id"")),
  makeDiscreteParam(""h20_2.hidden"", values = list(a = c(16L, 16L),
                                                  b = c(32L, 32L)),
                    requires = quote(selected.learner == ""h20_2"")),
  makeDiscreteParam(""h20_3.hidden"", values = list(a = c(16L, 16L, 16L),
                                                  b = c(32L, 32L, 32L)),
                    requires = quote(selected.learner == ""h20_3"")),
  makeDiscreteParam(""h20_2.activation"", values = ""RectifierWithDropout"", tunable = FALSE,
                    requires = quote(selected.learner == ""h20_2"")),
  makeDiscreteParam(""h20_3.activation"", values = ""RectifierWithDropout"", tunable = FALSE,
                    requires = quote(selected.learner == ""h20_3"")),
  makeNumericParam(""h20_2.input_dropout_ratio"", lower = 0, upper = 0.4, default = 0.1,
                   requires = quote(selected.learner == ""h20_2"")),
  makeNumericParam(""h20_3.input_dropout_ratio"", lower = 0, upper = 0.4, default = 0.1,
                   requires = quote(selected.learner == ""h20_3"")),
  makeNumericVectorParam(""h20_2.hidden_dropout_ratios"", len = 2, lower = 0, upper = 0.6, default = rep(0.3, 2),
                         requires = quote(selected.learner == ""h20_2"")),
  makeNumericVectorParam(""h20_3.hidden_dropout_ratios"", len = 3, lower = 0, upper = 0.6, default = rep(0.3, 3),
                         requires = quote(selected.learner == ""h20_3"")))

tw <- makeTuneWrapper(mm_lrn,
                      resampling = cv3,
                      control = ctrl,
                      par.set = par_set,
                      show.info = TRUE,
                      measures = list(auc,
                                      bac))

perf_tw <- resample(tw, 
                    task = sonar.task,
                    resampling = cv5,
                    extract = getTuneResult,
                    models = TRUE,
                    show.info = TRUE,
                    measures = list(auc,
                                    bac))



Is there a more elegant solution?","['r', 'h2o', 'mlr']",Unknown,,N/A
57526304,57526304,2019-08-16T14:20:24,2019-08-16 15:23:09Z,81,"I'm curious. Do we know the ranges of the hyperparameters searched in H2O AutoML ?


I checked this link 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html
 that provides the list of hyperparameters searched for each model but the ranges are not specified.


Thank you for your help !","['python', 'machine-learning', 'h2o']",BP34500,https://stackoverflow.com/users/11616033/bp34500,178
57524459,57524459,2019-08-16T12:12:48,2023-02-14 22:21:21Z,0,"I have trained and saved my H2O AutoML model. after reloading, while I am using predict method, I am getting below error:
java.lang.IllegalArgumentException: Test/Validation dataset has a non-categorical column 'response' which is categorical in the training data


I have not specified any encoding while model creation but I am getting this error now. Can anyone help me on this issue.


Any help will be highly appreciated.","['h2o', 'predict', 'automl']",ATUL AGARWAL,https://stackoverflow.com/users/11935985/atul-agarwal,101
57520867,57520867,2019-08-16T07:56:20,2019-08-19 12:43:01Z,59,"I'm trying to use results of two models built in h2o driverless ai. One of them was built in 1.6.0 version and the other one was built in latest 1.7.0 version.
When I try to load the pipeline.mojo of these two models(One of them will be in pb format and the other one in toml format) in my java application, the first model file is read fine where as it throws IllegalArgumentException when reading the second model file in the same JVM instance.


The mojo2-runtime jar that is being used is from 1.7.0 version.


String toml_fileName = ""/usr/toml/pipeline.mojo"";
MojoPipeline toml_model = MojoPipeline.loadFrom(toml_fileName);

String pb_fileName = ""/usr/pb/pipeline.mojo"";
MojoPipeline pb_model = MojoPipeline.loadFrom(pb_fileName);



I get the following exception while trying to load the second model file which is pb_model in my case.


Can someone help me in finding out what's going wrong?


java.lang.IllegalArgumentException: wrong number of arguments
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at ai.h2o.mojos.runtime.b.aj.a(SourceFile:62)
    at ai.h2o.mojos.runtime.b.J.a(SourceFile:35)
    at ai.h2o.mojos.runtime.readers.MojoReader.read(SourceFile:133)
    at ai.h2o.mojos.runtime.readers.MojoTransformerReader.readExecPipeTransformer(SourceFile:311)
    at ai.h2o.mojos.runtime.readers.MojoTransformerReader.read(SourceFile:41)
    at ai.h2o.mojos.runtime.readers.MojoReader.read(SourceFile:121)
    at ai.h2o.mojos.runtime.MojoPipelineFactoryImpl.loadFrom(SourceFile:59)
    at ai.h2o.mojos.runtime.MojoPipelineFactoryImpl.loadFrom(SourceFile:22)
    at ai.h2o.mojos.runtime.MojoPipeline.loadFrom(SourceFile:41)","['h2o', 'driverless-ai']",Ajay R,https://stackoverflow.com/users/11935751/ajay-r,3
57469702,57469702,2019-08-13T00:20:17,2020-01-21 17:16:28Z,0,"I am deploying a model using the H2O package with R on an Azure Machine Learning Studio notebook. 


I understand that when requesting my predict function the data goes in json format, and when the as.h2o () command inside the mypred function tries to convert json to h2o format it can't and occurs an error


Train the model


logistica_h2o <- h2o.glm(x = X, y = Y, 
                         training_frame = treino.h2o, 
                         family = ""binomial)



get workspace ID and token and


workspace_id <- """"
authorization_token <- """"
ws <- workspace(workspace_id, authorization_token)



create my predict function


newdata <- dados[,-32] #Remove response variable 

mypredict <- function(newdata){
  library(h2o)
  newdata <- as.h2o(newdata)
  as.data.frame(h2o.predict(logistica_h2o, newdata))
}



here I publish the service


ep <- publishWebService(ws = ws,
 fun = mypredict, 
 name = ""PredicaoDeEntradaDeRonda"", 
 inputSchema = dados[,-32], 
 data.frame=T) 



The problems occurs here


ewdata <-treino.h2o[1,-32]

pred <- consume(ep, newdata)



Error: No method asJSON S3 class: H2OFrame Traceback:

consume(ep, newdata)
callAPI(apiKey, requestUrl, requestsLists, globalParam, retryDelay, . .retry = .retry)
charToRaw(paste(toJSON(req, auto_unbox = TRUE, digits = 16), . collapse = ""\n""))
paste(toJSON(req, auto_unbox = TRUE, digits = 16), collapse = ""\n"")
toJSON(req, auto_unbox = TRUE, digits = 16)
asJSON(x, dataframe = dataframe, Date = Date, POSIXt = POSIXt, . factor = factor, complex = complex, raw = raw, matrix = matrix, . auto_unbox = auto_unbox, digits = digits, na = na, null = null, . force = force, indent = indent, ...)
asJSON(x, dataframe = dataframe, Date = Date, POSIXt = POSIXt,","['r', 'h2o', 'azure-machine-learning-service', 'azure-notebooks']",Rob,https://stackoverflow.com/users/162698/rob,15.1k
57466654,57466654,2019-08-12T18:47:41,2019-08-12 19:56:55Z,0,"I'm running into this error whenever I try to run h2o gbm: 


Error in h2o::h2o.gbm(y = y, x = x, training_frame = h2of, weights_column = wcol,  : 
  object 'wcol' not found



I have tried reinstalling h2o but I have just been running into this error. 


data(BostonHousing, package = ""mlbench"")
regr.task = makeRegrTask(id = ""bh"", data = BostonHousing, target = ""medv"")

lrn = makeLearner(""regr.h2o.gbm"")

outer = makeResampleDesc(""Holdout"")
r = resample(
  learner = lrn,
  task = regr.task,
  resampling = outer,
  show.info = TRUE
)","['r', 'h2o', 'mlr']",fabla,https://stackoverflow.com/users/11409379/fabla,"1,816"
57454662,57454662,2019-08-12T01:00:46,2019-08-19 08:00:01Z,304,"I'm trying to connect H2O and kerberized Hive following this document: 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/getting-data-into-h2o.html#connecting-to-hive-in-a-kerberized-hadoop-cluster


I'm sure that hiveHost, hivePrincipal, principal, keytab are all well configured.


Here is my environment:




Hadoop 2.7


Hive 2.2


h2o-3.26.0.2-hdp2.6.zip and h2o-3.26.0.2-hdp3.0.zip (Tried both hdp2.6 and hdp3.0)




Following is the error message:


08-08 14:08:51.819 192.168.6.33:54321    15189  #dex.html INFO: GET /flow/index.html, parms: {}
08-08 14:08:51.819 192.168.6.33:54321    15189  #dex.html INFO: Trying to load resource /flow/index.html via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:51.870 192.168.6.33:54321    15189  #flow.css INFO: Trying to load resource /flow/css/flow.css via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:51.933 192.168.6.33:54321    15189  #/flow.js INFO: Trying to load resource /flow/js/flow.js via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:52.054 192.168.6.33:54321    15189  #/h2o.png INFO: Trying to load resource /flow/img/h2o.png via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:52.083 192.168.6.33:54321    15189  #ar.woff2 INFO: GET /flow/fonts/Lato-Regular.woff2, parms: {}
08-08 14:08:52.083 192.168.6.33:54321    15189  #ar.woff2 INFO: Trying to load resource /flow/fonts/Lato-Regular.woff2 via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:52.094 192.168.6.33:54321    15189  #00.woff2 INFO: GET /flow/fonts/fa-solid-900.woff2, parms: {}
08-08 14:08:52.095 192.168.6.33:54321    15189  #00.woff2 INFO: Trying to load resource /flow/fonts/fa-solid-900.woff2 via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:52.101 192.168.6.33:54321    15189  #tf.woff2 INFO: GET /flow/fonts/SourceCodePro-Regular.ttf.woff2, parms: {}
08-08 14:08:52.101 192.168.6.33:54321    15189  #tf.woff2 INFO: Trying to load resource /flow/fonts/SourceCodePro-Regular.ttf.woff2 via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:52.597 192.168.6.33:54321    15189  #dex.html INFO: GET /flow/index.html, parms: {}
08-08 14:08:52.597 192.168.6.33:54321    15189  #dex.html INFO: Trying to load resource /flow/index.html via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:52.654 192.168.6.33:54321    15189  #ndpoints INFO: GET /3/Metadata/endpoints, parms: {}
08-08 14:08:52.859 192.168.6.33:54321    15189  #notebook INFO: GET /3/NodePersistentStorage/notebook, parms: {}
08-08 14:08:52.862 192.168.6.33:54321    15189  #s/exists INFO: GET /3/NodePersistentStorage/categories/environment/names/clips/exists, parms: {}
08-08 14:08:52.862 192.168.6.33:54321    15189  #log.json INFO: GET /flow/help/catalog.json, parms: {}
08-08 14:08:52.862 192.168.6.33:54321    15189  #log.json INFO: Trying to load resource /flow/help/catalog.json via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:52.863 192.168.6.33:54321    15189  #/3/About INFO: GET /3/About, parms: {}
08-08 14:08:52.867 192.168.6.33:54321    15189  #Builders INFO: GET /3/ModelBuilders, parms: {}
08-08 14:08:52.889 192.168.6.33:54321    15189  #00.woff2 INFO: GET /flow/fonts/fa-regular-400.woff2, parms: {}
08-08 14:08:52.889 192.168.6.33:54321    15189  #00.woff2 INFO: Trying to load resource /flow/fonts/fa-regular-400.woff2 via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:52.891 192.168.6.33:54321    15189  #flow.png INFO: Trying to load resource /flow/img/h2o-flow.png via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:53.035 192.168.6.33:54321    15189  #ic.woff2 INFO: GET /flow/fonts/Lato-Italic.woff2, parms: {}
08-08 14:08:53.035 192.168.6.33:54321    15189  #ic.woff2 INFO: Trying to load resource /flow/fonts/Lato-Italic.woff2 via classloader sun.misc.Launcher$AppClassLoader@7e32c033
08-08 14:08:58.454 192.168.6.33:54321    15189  #SQLTable INFO: POST /99/ImportSQLTable, parms: <hidden>
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR: java.lang.RuntimeException: SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://hive.linktime.cloud:10000/default;auth=delegationToken: java.lang.RuntimeException: class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback not org.apache.hive.org.apache.hadoop.security.GroupMappingServiceProvider
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR: Failed to connect and read from SQL database with connection_url: jdbc:hive2://hive.linktime.cloud:10000/default;auth=delegationToken
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at water.jdbc.SQLManager$SQLImportDriver.compute2(SQLManager.java:225)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at water.H2O$H2OCountedCompleter.compute(H2O.java:1417)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR: Caused by: java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://hive.linktime.cloud:10000/default;auth=delegationToken: java.lang.RuntimeException: class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback not org.apache.hive.org.apache.hadoop.security.GroupMappingServiceProvider
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:210)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at java.sql.DriverManager.getConnection(DriverManager.java:664)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at java.sql.DriverManager.getConnection(DriverManager.java:247)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at water.jdbc.SQLManager.getConnectionSafe(SQLManager.java:486)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at water.jdbc.SQLManager.access$000(SQLManager.java:13)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at water.jdbc.SQLManager$SQLImportDriver.compute2(SQLManager.java:134)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     ... 6 more
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR: Caused by: java.lang.RuntimeException: java.lang.RuntimeException: class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback not org.apache.hive.org.apache.hadoop.security.GroupMappingServiceProvider
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2248)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.org.apache.hadoop.security.Groups.<init>(Groups.java:106)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.org.apache.hadoop.security.Groups.<init>(Groups.java:102)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:450)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:309)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:276)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:832)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:802)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:675)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.org.apache.hadoop.hive.shims.Utils.getTokenStrForm(Utils.java:91)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.jdbc.HiveConnection.getClientDelegationToken(HiveConnection.java:539)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.jdbc.HiveConnection.createBinaryTransport(HiveConnection.java:464)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:226)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:183)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     ... 12 more
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR: Caused by: java.lang.RuntimeException: class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback not org.apache.hive.org.apache.hadoop.security.GroupMappingServiceProvider
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     at org.apache.hive.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2242)
08-08 14:08:58.472 192.168.6.33:54321    15189  FJ-1-15   ERRR:     ... 25 more
08-08 14:09:07.241 192.168.6.33:54321    15189  #les/info INFO: GET /3/Logs/nodes/self/files/info, parms: {}
08-08 14:09:07.334 192.168.6.33:54321    15189  #les/info INFO: GET /3/Logs/nodes/192.168.6.33:54321/files/info, parms: {}




Did I miss some configuration?","['hadoop', 'hive', 'kerberos', 'h2o']",Peter Zhao,https://stackoverflow.com/users/2298397/peter-zhao,"7,546"
57420788,57420788,2019-08-08T21:23:36,2019-08-08 22:14:46Z,0,"I saved H2O model using 
model.download_mojo(path=""path"", get_genmodel_jar=True)
.
I want retrieve that model to use in jupyter notebook again.
How can I do that? Thanks.","['python', 'h2o', 'mojo']",Unknown,,N/A
57382531,57382531,2019-08-06T19:09:19,2019-08-07 09:34:24Z,45,"Trying to run H2o AutoML from IBMwatson. But not able to connect to H2o cluster (using h2o.init(ip=""127.0.0.1"", port = 8080)


Able to perform in anaconda notebook installed in desktop PC.


Running H2o version 3.26 and Java version 1.8.


Please help me to understand, if this is the problem with port? Do I need to enable port? Or is this due to non-compatibility?  


Installing H2o


h2o and java version


H2o init and error details","['python', 'ibm-watson', 'h2o']",Unknown,,N/A
57371062,57371062,2019-08-06T07:35:03,2019-08-06 15:00:05Z,0,"I have a H2O cluster running on Google Cloud Platform Compute Engine which has been deployed through Marketplace and I want to send some data for analysis.


Data are ideally in the form of a Pandas dataframe, loaded in memory on my local machine. Alternatively I have the same data stored in a BigQuery table or in a csv file.


I tried the following code (illustrative example):


import h2o
import pandas as pd

def connect_and_send_data(my_dataframe)
  username = ""xxx""
  password = ""yyy""

  h2o.connect(url=""http://xx.xx.xx.xx:443"", auth(username,password), verify_ssl_certificates=False)

  h2o.H2OFrame(my_dataframe, destination_frame = ""train_df"") 



For dataframes bigger than, let's say, 200 rows, I get the following error message:


H2OServerError: HTTP 413 Request Entity Too Large:
'<html>\r\n<head><title>413 Request Entity Too Large</title></head>\r\n<body bgcolor=""white"">\r\n<center><h1>413 Request Entity Too Large</h1></center>\r\n<hr><center>nginx/1.10.3 (Ubuntu)</center>\r\n</body>\r\n</html>\r\n'



Note that the code above works perfectly if I pass a small pandas dataframe, or on a H2O cluster created on my local machine, regardless the size of data, to which I connect by using


h2o.init()



instead of 


h2o.connect()



I tried also to upload a csv file from the flow or using the function 


h2o.upload_file(path=""somecsvfile.csv"")



but still, no success.


Any idea on how I can quickly sort out this issue?","['python', 'google-cloud-platform', 'google-compute-engine', 'h2o']",siamsot,https://stackoverflow.com/users/8304665/siamsot,"1,559"
57340746,57340746,2019-08-03T17:45:17,2019-08-27 06:36:23Z,156,"I'm hoping someone can give me some advise for taking models built in h2o flow on a windows machine and exporting/using them on an AIX server.


I've been able to use the flow graphical interface to build some models. I'm not a java coder / programmer so have some trouble understanding the instructions on the pojo/mojo files in the documentation and really looking for some step-by-step help on using them.


I have h2o running on a windows desktop machine. The place I want to use it is within SAS programs running on an AIX server. The AIX server has java.


I'm assuming I can do something like: 




build model in h2o on my windows desktop


download the pojo or mojo files


do something????


put resulting file(s) from step 3 on the AIX server


use SAS to write out a CSV file in the same format I used to build the model in h2o


use SAS to issue unix command line statements that feed the .csv file into the exported files from step 3 using java and generate a new output .csv file with scored records


read the scores back into my SAS program and continue on my way




Is this do-able? 


If not, can someone give me a ""for dummies"" explanation of how I might do something similar? I don't have admin rights on the AIX server so can't install software.


Thanks for your help.",['h2o'],Bob,https://stackoverflow.com/users/10643932/bob,41
57309381,57309381,2019-08-01T12:47:31,2019-08-01 20:08:45Z,0,"I am using 
h2o
 to develop machine learning models. I'm done and have my predictions, now I want to transform my H2OFrame back to a data.frame to do some error analysis.


I tried the 
as.data.frame
 function to transform my H2OFrame to the standard R data.frame.


Here is a minimal reproducible example:


library(h2o)
h2o.init()
tmp <- data.frame(ngram = c(""SIRET:417 653 698"",
                            ""SIRET:417 653 698 00031"",
                            ""Sans"",
                            ""Sans esc."",
                            ""Sans esc. jusqu\""au"",
                            ""Sans esc. jusqu\""au 15.11.2018""))
tmp <- as.h2o(tmp)
tmp <- as.data.frame(tmp)
print(tmp)

#                                             ngram
# 1                               SIRET:417 653 698
# 2                         SIRET:417 653 698 00031
# 3                                            Sans
# 4                                       Sans esc.
# 5 Sans esc. jusquau\nSans esc. jusquau 15.11.2018



There is no error message, but as you can see, we started with 6 rows, and only 5 are left in the final output.


The last 2 ones have been merged.


It's a huge issue for my current project since it basically deletes half of the rows of my data. I have isolated this example, but there are many others. From the other examples I have seen (not a lot), the quotes are always involved.","['r', 'h2o']",Charles,https://stackoverflow.com/users/3437438/charles,150
57308335,57308335,2019-08-01T11:46:28,2019-08-05 16:42:14Z,279,"I am newbie using the H2o framework for machine learning. I trying it from python, (jupyter notebooks). When I print the output of the model in the notebook the columns overlap to each other.


Do someone know how to fix it?","['jupyter-notebook', 'h2o']",Unknown,,N/A
57273916,57273916,2019-07-30T14:20:55,2019-08-01 08:36:56Z,180,"I like h2o.ai for machine learning using R.

https://cran.r-project.org/web/packages/h2o/h2o.pdf


I like random forests, but I'm making a few thousand predictions in a loop.
It is spamming up my memory with things like this:




I can't afford to keep them all in memory.  I'm making my very nice computer work very hard.  That means it doesn't have the capacity to hold all the balls in the air at once.


If I could assign a destination frame name to the prediction then each new one would overwrite the old ones.


How do I assign a destination frame name when I am performing ""h2o.predict"" on an object?


Things that I have tried that did not work:


h2o.predict(object = rf.hex, newdata = test.hex, predictions_frame = ""predict.hex"")
h2o.predict(object = rf.hex, newdata = test.hex, destination_frame = ""predict.hex"")
h2o.predict(object = rf.hex, newdata = test.hex, model_id = ""predict.hex"")","['variable-assignment', 'h2o', 'predict']",EngrStudent,https://stackoverflow.com/users/2259468/engrstudent,"1,972"
57231558,57231558,2019-07-27T11:14:12,2020-12-22 11:52:27Z,0,"I'm trying to manually update some of the coefficients my H2O GLM produces for sparse segments. Regularisation is not an option unfortunately.


I'm trying to use GLMs using the H2O package in R to model amounts using several predictor variables. On some of the sparse predictors, I end up with inconsistent coefficients which I could manually update to consistent values in the object in base R. With this I could use the predict function quite neatly.


However, when I update them in H2O the h2o.predict function ignores the new coefficients and solves as per the original values. Is there another way to achieve what I'm trying to do? Dropping the variables is not an option.


library(h2o)
h2o.init()

insurance <- h2o.importFile(""https://s3.amazonaws.com/h2o-public-test-data/smalldata/glm_test/insurance.csv"")

predictors <- colnames(insurance)[1:4]
response <- 'Claims'

insurance['Group'] <- as.factor(insurance['Group'])
insurance['Age'] <- as.factor(insurance['Age'])

head(insurance)

claims.glm<-h2o.glm(family=""tweedie"",x=predictors,y=response, training_frame = insurance, lambda=0, compute_p_values = TRUE)
print(claims.glm)

#sample row for members age>35 for testing
sample<-insurance[4,]

sample

h2o.predict(claims.glm,sample)

#change Age>35 coefficient values in the model to 15 from 9.82 
claims.glm@model$coefficients_table[7,2]=15
claims.glm@model$coefficients[7]=15

#same predict & stderr as before
h2o.predict(claims.glm,sample)","['r', 'h2o', 'glm']",Unknown,,N/A
57226758,57226758,2019-07-26T20:41:49,2019-08-30 20:21:23Z,756,"Getting confused when trying to run a YARN process and getting errors. Looking in ambari UI YARN section, seeing...




(note it says 60GB available).
Yet, when trying to run an YARN process, getting errors indicating that there are less resources available than is being reported in ambari, see...


➜  h2o-3.26.0.2-hdp3.1 hadoop jar h2odriver.jar -nodes 4 -mapperXmx 5g -output /home/ml1/hdfsOutputDir
Determining driver host interface for mapper->driver callback...
    [Possible callback IP address: 192.168.122.1]
    [Possible callback IP address: 172.18.4.49]
    [Possible callback IP address: 127.0.0.1]
Using mapper->driver callback IP address and port: 172.18.4.49:46721
(You can override these with -driverif and -driverport/-driverportrange and/or specify external IP using -extdriverif.)
Memory Settings:
    mapreduce.map.java.opts:     -Xms5g -Xmx5g -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Dlog4j.defaultInitOverride=true
    Extra memory percent:        10
    mapreduce.map.memory.mb:     5632
Hive driver not present, not generating token.
19/08/07 12:37:19 INFO client.RMProxy: Connecting to ResourceManager at hw01.ucera.local/172.18.4.46:8050
19/08/07 12:37:19 INFO client.AHSProxy: Connecting to Application History server at hw02.ucera.local/172.18.4.47:10200
19/08/07 12:37:19 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/ml1/.staging/job_1565057088651_0007
19/08/07 12:37:21 INFO mapreduce.JobSubmitter: number of splits:4
19/08/07 12:37:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1565057088651_0007
19/08/07 12:37:21 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/08/07 12:37:21 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/08/07 12:37:21 INFO impl.YarnClientImpl: Submitted application application_1565057088651_0007
19/08/07 12:37:21 INFO mapreduce.Job: The url to track the job: http://HW01.ucera.local:8088/proxy/application_1565057088651_0007/
Job name 'H2O_80092' submitted
JobTracker job ID is 'job_1565057088651_0007'
For YARN users, logs command is 'yarn logs -applicationId application_1565057088651_0007'
Waiting for H2O cluster to come up...
19/08/07 12:37:38 INFO client.RMProxy: Connecting to ResourceManager at hw01.ucera.local/172.18.4.46:8050
19/08/07 12:37:38 INFO client.AHSProxy: Connecting to Application History server at hw02.ucera.local/172.18.4.47:10200

----- YARN cluster metrics -----
Number of YARN worker nodes: 4

----- Nodes -----
Node: http://HW03.ucera.local:8042 Rack: /default-rack, RUNNING, 1 containers used, 5.0 / 15.0 GB used, 1 / 3 vcores used
Node: http://HW04.ucera.local:8042 Rack: /default-rack, RUNNING, 0 containers used, 0.0 / 15.0 GB used, 0 / 3 vcores used
Node: http://hw05.ucera.local:8042 Rack: /default-rack, RUNNING, 0 containers used, 0.0 / 15.0 GB used, 0 / 3 vcores used
Node: http://HW02.ucera.local:8042 Rack: /default-rack, RUNNING, 0 containers used, 0.0 / 15.0 GB used, 0 / 3 vcores used

----- Queues -----
Queue name:            default
    Queue state:       RUNNING
    Current capacity:  0.08
    Capacity:          1.00
    Maximum capacity:  1.00
    Application count: 1
    ----- Applications in this queue -----
    Application ID:                  application_1565057088651_0007 (H2O_80092)
        Started:                     ml1 (Wed Aug 07 12:37:21 HST 2019)
        Application state:           FINISHED
        Tracking URL:                http://HW01.ucera.local:8088/proxy/application_1565057088651_0007/
        Queue name:                  default
        Used/Reserved containers:    1 / 0
        Needed/Used/Reserved memory: 5.0 GB / 5.0 GB / 0.0 GB
        Needed/Used/Reserved vcores: 1 / 1 / 0

Queue 'default' approximate utilization: 5.0 / 60.0 GB used, 1 / 12 vcores used

----------------------------------------------------------------------

ERROR: Unable to start any H2O nodes; please contact your YARN administrator.

       A common cause for this is the requested container size (5.5 GB)
       exceeds the following YARN settings:

           yarn.nodemanager.resource.memory-mb
           yarn.scheduler.maximum-allocation-mb

----------------------------------------------------------------------

For YARN users, logs command is 'yarn logs -applicationId application_1565057088651_0007'



Note the 




ERROR: Unable to start any H2O nodes; please contact your YARN administrator.


A common cause for this is the requested container size (5.5 GB) exceeds the following YARN settings:


  yarn.nodemanager.resource.memory-mb
  yarn.scheduler.maximum-allocation-mb





Yet, I have YARN configured with 


yarn.scheduler.maximum-allocation-vcores=3
yarn.nodemanager.resource.cpu-vcores=3
yarn.nodemanager.resource.memory-mb=15GB
yarn.scheduler.maximum-allocation-mb=15GB



and we can see both container and node resource restrictions are higher than the requested container size.


Trying to do a heftier calculation with the default mapreduce pi example


[myuser@HW03 ~]$ yarn jar /usr/hdp/3.1.0.0-78/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 1000 1000
Number of Maps  = 1000
Samples per Map = 1000
....



and checking the RM UI, I can see that it is at least possible in some cases to use all of the RM's 60GB of resources (notice the 61440MBs in the bottom of the image)



So there are some things about the problem that I don't understand






Queue 'default' approximate utilization: 5.0 / 60.0 GB used, 1 / 12 vcores used 




I would like to use the full 60GB that YARN can ostensibly provide (or at least have the option to, rather than have errors thrown). Would think that there should be enough resources to have each of the 4 nodes provide 15GB (> requested 4x5GB=20GB) to the process. Am I missing something here? Note that I only have the default root queue setup for YARN?




----- Nodes -----


Node: 
http://HW03.ucera.local:8042
 Rack: /default-rack, RUNNING, 1 containers used, 5.0 / 15.0 GB used, 1 / 3 vcores used


Node: 
http://HW04.ucera.local:8042
 Rack: /default-rack, RUNNING, 0 containers used, 0.0 / 15.0 GB used, 0 / 3 vcores used


....




Why is only a single node being used before erroring out? 




From these two things, it seems that neither the 15GB node limit nor the 60GB cluster limit are being exceeded, so why are these errors being thrown? What about this situation am I misinterpreting here? What can be done to fix (again, would like to be able to use all of the apparent 60GB of YARN resources for the job without error)? Any debugging suggestions of fixes?


UPDATE
:


Problem appears to be related to 
How to properly change uid for HDP / ambari-created user?
 and the fact that having a user exist on 
a
 node and have a 
hdfs://user/<username>
 directory with correct permissions (as I was lead to believe from a 
Hortonworks forum post
) is not sufficient to be acknowledges as ""existing"" on the cluster. 


Running the hadoop jar command for a different user (in this case, the Ambari-created hdfs user) that exists on all cluster nodes (even though Ambari created this user having different uids across nodes (IDK if this is a problem)) and has a 
hdfs://user/hdfs
 dir, found that the h2o jar ran as expected. 


I was initially under the impression that users only needed to exist on whatever client machine was being used plus the need for a hdfs://user/ dir (see 
https://community.cloudera.com/t5/Support-Questions/Adding-a-new-user-to-the-cluster/m-p/130319/highlight/true#M93005
).
One concerning / confusing thing that has come from this is the fact Ambari apparently created the hdfs user on the various cluster nodes with differing uid and gid values, eg...


[root@HW01 ~]# clush -ab id hdfs
---------------
HW[01-04] (4)
---------------
uid=1017(hdfs) gid=1005(hadoop) groups=1005(hadoop),1003(hdfs)
---------------
HW05
---------------
uid=1021(hdfs) gid=1006(hadoop) groups=1006(hadoop),1004(hdfs)
[root@HW01 ~]# 
[root@HW01 ~]#
# wondering what else is using a uid 1021 across the nodes 
[root@HW01 ~]# clush -ab id 1021
---------------
HW[01-04] (4)
---------------
uid=1021(hbase) gid=1005(hadoop) groups=1005(hadoop)
---------------
HW05
---------------
uid=1021(hdfs) gid=1006(hadoop) groups=1006(hadoop),1004(hdfs)



This does not seem like that is how it is supposed to be (just my suspicion from having worked with MapR (which requires the uid and gids to be same across nodes) and looking here: 
https://www.ibm.com/support/knowledgecenter/en/STXKQY_BDA_SHR/bl1adv_userandgrpid.htm
). Note that HW05 was a node that was added later. If this is actually fine in HDP, I plan to just add the user I actually indent to use h2o with across all the nodes with whatever arbitrary uid and gid values. 
Any thoughts on this? Any docs to support either why this is right or wrong you could link me to?


Will look into this a bit more before posting as an answer. I think basically will need to look for a bit more clarification as to when HDP considers a user to ""exist"" on a cluster.","['hadoop-yarn', 'h2o', 'ambari', 'hdp']",Unknown,,N/A
57211993,57211993,2019-07-26T01:24:11,2020-09-04 16:48:06Z,0,"Attempting to run h2o on a HDP 3.1 cluster and running into error that appears to be about YARN resource capacity...


[ml1user@HW04 h2o-3.26.0.1-hdp3.1]$ hadoop jar h2odriver.jar -nodes 3 -mapperXmx 10g
Determining driver host interface for mapper->driver callback...
    [Possible callback IP address: 192.168.122.1]
    [Possible callback IP address: 172.18.4.49]
    [Possible callback IP address: 127.0.0.1]
Using mapper->driver callback IP address and port: 172.18.4.49:46015
(You can override these with -driverif and -driverport/-driverportrange and/or specify external IP using -extdriverif.)
Memory Settings:
    mapreduce.map.java.opts:     -Xms10g -Xmx10g -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Dlog4j.defaultInitOverride=true
    Extra memory percent:        10
    mapreduce.map.memory.mb:     11264
Hive driver not present, not generating token.
19/07/25 14:48:05 INFO client.RMProxy: Connecting to ResourceManager at hw01.ucera.local/172.18.4.46:8050
19/07/25 14:48:06 INFO client.AHSProxy: Connecting to Application History server at hw02.ucera.local/172.18.4.47:10200
19/07/25 14:48:07 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/ml1user/.staging/job_1564020515809_0006
19/07/25 14:48:08 INFO mapreduce.JobSubmitter: number of splits:3
19/07/25 14:48:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1564020515809_0006
19/07/25 14:48:08 INFO mapreduce.JobSubmitter: Executing with tokens: []
19/07/25 14:48:08 INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
19/07/25 14:48:08 INFO impl.YarnClientImpl: Submitted application application_1564020515809_0006
19/07/25 14:48:08 INFO mapreduce.Job: The url to track the job: http://HW01.ucera.local:8088/proxy/application_1564020515809_0006/
Job name 'H2O_47159' submitted
JobTracker job ID is 'job_1564020515809_0006'
For YARN users, logs command is 'yarn logs -applicationId application_1564020515809_0006'
Waiting for H2O cluster to come up...
ERROR: Timed out waiting for H2O cluster to come up (120 seconds)
ERROR: (Try specifying the -timeout option to increase the waiting time limit)
Attempting to clean up hadoop job...
19/07/25 14:50:19 INFO impl.YarnClientImpl: Killed application application_1564020515809_0006
Killed.
19/07/25 14:50:23 INFO client.RMProxy: Connecting to ResourceManager at hw01.ucera.local/172.18.4.46:8050
19/07/25 14:50:23 INFO client.AHSProxy: Connecting to Application History server at hw02.ucera.local/172.18.4.47:10200

----- YARN cluster metrics -----
Number of YARN worker nodes: 3

----- Nodes -----
Node: http://HW03.ucera.local:8042 Rack: /default-rack, RUNNING, 0 containers used, 0.0 / 15.0 GB used, 0 / 3 vcores used
Node: http://HW04.ucera.local:8042 Rack: /default-rack, RUNNING, 0 containers used, 0.0 / 15.0 GB used, 0 / 3 vcores used
Node: http://HW02.ucera.local:8042 Rack: /default-rack, RUNNING, 0 containers used, 0.0 / 15.0 GB used, 0 / 3 vcores used

----- Queues -----
Queue name:            default
    Queue state:       RUNNING
    Current capacity:  0.00
    Capacity:          1.00
    Maximum capacity:  1.00
    Application count: 0

Queue 'default' approximate utilization: 0.0 / 45.0 GB used, 0 / 9 vcores used

----------------------------------------------------------------------

ERROR: Unable to start any H2O nodes; please contact your YARN administrator.

       A common cause for this is the requested container size (11.0 GB)
       exceeds the following YARN settings:

           yarn.nodemanager.resource.memory-mb
           yarn.scheduler.maximum-allocation-mb

----------------------------------------------------------------------

For YARN users, logs command is 'yarn logs -applicationId application_1564020515809_0006'




Looking in the YARN configs in Ambari UI, these properties are nowhere to be found. But checking the YARN logs in the YARN resource manager UI and checking some of the logs for the killed application, I see what appears to be unreachable-host errors...


Container: container_e05_1564020515809_0006_02_000002 on HW03.ucera.local_45454_1564102219781
LogAggregationType: AGGREGATED
=============================================================================================
LogType:stderr
LogLastModifiedTime:Thu Jul 25 14:50:19 -1000 2019
LogLength:2203
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/11/mapreduce.tar.gz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/usercache/ml1user/appcache/application_1564020515809_0006/filecache/10/job.jar/job.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
log4j:WARN No appenders could be found for logger (org.apache.hadoop.mapred.YarnChild).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
java.net.NoRouteToHostException: No route to host (Host unreachable)
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    ....
    at java.net.Socket.<init>(Socket.java:211)
    at water.hadoop.EmbeddedH2OConfig$BackgroundWriterThread.run(EmbeddedH2OConfig.java:38)

End of LogType:stderr
***********************************************************************



Taking note of 
""java.net.NoRouteToHostException: No route to host (Host unreachable)""
. However, I can access all the other nodes from each other and they can all ping each other, so not sure what is going on here. Any suggestions for debugging or fixing?","['hadoop', 'hadoop-yarn', 'h2o']",lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
57203896,57203896,2019-07-25T14:03:04,2019-07-25 22:26:48Z,0,"I'm learning how to use h2o but I can't seem to get the latest version to work in Jupyter Notebooks. I can manage to get it running with a very old version though (about 1 year and a few months old). I keep getting stuck when I try to initialise it 


import h2o
h2o.init()



I found there were several versions of h2o that I could download online. Here's what I tried:


I had first downloaded h2o from here (
https://anaconda.org/anaconda/h2o
) but it could not find the module when I tried importing it over in Jupyter (I installed the package and opened up Jupyter with the same environment [Python 3.6]). This was fixed when I downloaded the additional h2o-py package (
https://anaconda.org/anaconda/h2o-py
). I could run H2O with these two packages installed, its just that its a pretty old version (3.18.0.2). 


Deleting the h2o and leaving only the h2o-py gives me this error: 




H2OStartupError: Cannot start local server: h2o.jar not found.




I also tried getting the latest version from the h2o website (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html
) - I am assuming this is the one. I followed the downloading instructions for both pip and conda but neither worked. 
I kept getting the H2O Connection Error and CalledProcess Error when I tried to initialise h2o.




H2OConnectionError: Could not establish link to the H2O cloud 
http://localhost:54321
 after 5 retries


CalledProcessError: Command '['/usr/bin/java', '-version']' returned non-zero exit status 2.




I tried this in combination with h2o-py, h2o and h2o together (latest one from the website), and just the h2o package alone but nothing is working. So far, the only thing that I got working was the h2o in combination with h2o-py, both of which are the older versions of h2o.


Is there something that I'm missing?


=======================================================================


TL;DR


With packages: 


1) h2o and h2o-py - 
works, but is an older version


2) h2o only - 
cannot find module when importing


3) h2o-py only - 
H2OStartupError: h2o.jar not found


4) h2o (latest version) and others (h2o (old version) or h2o-py)- 
Connection Error when initialising


5) all three packages together - 
Cannot find module when importing","['python', 'python-3.x', 'conda', 'h2o']",Khye,https://stackoverflow.com/users/11795515/khye,15
57196545,57196545,2019-07-25T07:30:23,2019-07-31 17:21:26Z,156,I am running H2O AutoML on data with 3000 observations (for binary classification) with 10% default. The AUC of the best model is very low (0.6 on the test data). How can I maximize it?,"['python', 'h2o']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
57154630,57154630,2019-07-22T22:45:57,2023-06-07 07:01:31Z,570,"I am trying to export an image generated in a Jupyter notebook using the H2O library to a PNG file. The image is the variable importance plot


I have tried using the matplotlib export functionality but it generates an empty PNG file.


cv_gbm.varimp_plot()","['python-3.x', 'h2o']",Adam Michalik,https://stackoverflow.com/users/466738/adam-michalik,"9,945"
57149718,57149718,2019-07-22T15:57:26,2022-01-11 22:19:08Z,0,"I am trying H2O AutoML for multi-class classification. Everything seems to be working fine except when I am trying to extract the variable importance from metalearner for the StackedEnsemble.


This is what I am getting from code:


# Get the ""BestOfFamily"" Stacked Ensemble model
se.best <- h2o.getModel(grep(""StackedEnsemble_BestOfFamily_AutoML"", model_ids, value = TRUE)[1])
#se.best
metalearner <- h2o.getModel(se.best@model$metalearner$name)
#as.data.frame(h2o.varimp(metalearner))
h2o.varimp_plot(metalearner, num_of_features = 20)



Variable importance from metalearner for ""StackedEnsemble_BestOfFamily_AutoML"":




The problem here is instead of showing the 20 model names that H2O AutoML picked (from ""max_models =20"") the above plot shows the names of the classes of the multi-class classification.


To me, this looks like a bug because I do not have this problem when I tried regression with H2O AutoML in R.","['r', 'h2o', 'automl']",lovalery,https://stackoverflow.com/users/13661061/lovalery,"4,652"
57147822,57147822,2019-07-22T14:06:29,2019-07-22 15:34:24Z,116,"In Python (H2O version 3.20.0.9) which function is equivalent to 
h2o.transform
 in R?


h2o.transform(modelo_w2v, words_h, aggregate_method = 'AVERAGE')","['python', 'transform', 'h2o']",louisfischer,https://stackoverflow.com/users/4559927/louisfischer,"2,084"
57127895,57127895,2019-07-20T19:25:34,2019-07-21 04:25:58Z,358,"I am trying create H2OContext in python note book on databricks cluster. Following is my environment specs:




Databricks runtime environment: 5.3 


Spark = 2.4 


Python = 3.5


colorama >= 0.3.8 


h2o-pysparkling-2.4




I am writing following code:






from pysparkling import *

from pyspark.sql import SparkSession

import h2o

spark = SparkSession.builder.appName(""SparklingWaterApp"").getOrCreate()

h2oConf = H2OConf(spark).set(""spark.ui.enabled"", ""false"")

hc = H2OContext.getOrCreate(spark, conf=h2oConf)`








I am getting this error:






 org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 4 times, most recent failure: Lost task 2.3 in stage 1.0 (TID 40, 10.139.64.7, executor 2): java.io.InvalidClassException: org.apache.spark.h2o.backends.internal.InternalBackendUtils$; local class incompatible: stream classdesc serialVersionUID = -279081412540759760, local class serialVersionUID = -4513453206774459154

Py4JJavaError Traceback (most recent call last)
in ()
4 spark = SparkSession.builder.appName(""SparklingWaterApp"").getOrCreate()
5 h2oConf = H2OConf(spark).set(""spark.ui.enabled"", ""false"")
----> 6 hc = H2OContext.getOrCreate(spark, conf=h2oConf)

/databricks/python/lib/python3.5/site-packages/pysparkling/context.py in getOrCreate(spark, conf, verbose, pre_create_hook, h2o_connect_hook, **kwargs)
159
160 # Create backing Java H2OContext
--> 161 jhc = jvm.org.apache.spark.h2o.JavaH2OContext.getOrCreate(jspark_session, selected_conf._jconf)
162 h2o_context._jhc = jhc
163 h2o_context._conf = selected_conf

/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in call(self, *args)
1255 answer = self.gateway_client.send_command(command)
1256 return_value = get_return_value(
-> 1257 answer, self.gateway_client, self.target_id, self.name)
1258
1259 for temp_arg in temp_args:








Please suggest.


Regards,
Bharat","['python', 'pyspark', 'h2o', 'databricks']",Unknown,,N/A
57115761,57115761,2019-07-19T15:43:32,2019-07-19 15:43:32Z,74,"I'm trying to understand what the H2O autoencoder layer architecture looks like in the case of even and odd numbers of layers. Let's take some concrete examples:




Suppose an H2O autoencoder model has 4 hidden layers, 10 neurons per layer ([10, 10, 10, 10]). How many layers would the decoder and encoder have, respectively? Does this mean that the encoder has 1 input layer and 2 hidden layers and decoder has 2 hidden and 1 output? 


Suppose an H2O autoencoder model has 3 hidden layers, 10 neurons per layer ([10, 10, 10]). How many layers would the encoder and decoder have, respectively?","['python', 'machine-learning', 'h2o', 'autoencoder']",stacy1991,https://stackoverflow.com/users/11692859/stacy1991,11
57075441,57075441,2019-07-17T12:06:59,2020-01-21 17:16:53Z,44,I need to provide user based access to H2O Flow UI as the similar group users can see each others Flow and other group user are not able to see it.,['h2o'],Rob,https://stackoverflow.com/users/162698/rob,15.1k
57068435,57068435,2019-07-17T04:15:07,2019-07-17 23:22:21Z,277,"I'm having more than 100 String Columns which I need to be converting into enum so that the ML model identifies theses columns as categories. 


In Pyspark, there is no Category type (as in Pandas) and hence I casted all categories as 'String'. I don't want to click 'convert to enum' > 100 times and I'm sure there is an easier way to perform this task. Any help would be greatly appreciated.","['python', 'pyspark', 'h2o']",Sukhi,https://stackoverflow.com/users/1367159/sukhi,14k
57046562,57046562,2019-07-15T20:12:01,2019-07-25 20:49:36Z,41,"If the goal is to use H2O Flow interface for scoring (which is great BTW), how to accomplish this after a reboot or a system crash?


Is there a way to save models and rerun it, 
within H2O Flow
, without having to train sets again?",['h2o'],mfulvio,https://stackoverflow.com/users/10200522/mfulvio,33
57044168,57044168,2019-07-15T16:56:05,2019-07-17 12:26:47Z,926,"I'm dealing with a binary classification problem and I tried using the performance metric AUCPR in H2O since I have an imbalanced dataset.


However, I get the following error when I try calling the aucpr metric.


# entrainement
aml_sans_class_balance.train(x=x_h2o, y=y_h2o, training_frame=train_h2o, validation_frame=valid_h2o)


# Score auc
prediction = aml_sans_class_balance.leader.model_performance(test_data=test_h2o)
tot_auc_score_sans_class_balance.append(prediction.aucpr())
print('score auc à itération', j+1,':',prediction.aucpr())

----------------------------------------------------------------------

AttributeError: type object 'H2OBinomialModelMetrics' has no attribute 'aucpr'




Thank you for your help.


Regards.","['python', 'machine-learning', 'h2o']",BP34500,https://stackoverflow.com/users/11616033/bp34500,178
57022066,57022066,2019-07-13T19:06:11,2019-07-14 03:33:22Z,444,"I have a train set of 1000 rows (1 row per day for example).
I get a prediction for a set of 5 futures (model.predict).
Over the next 5 days, I actually get the data for next 5 days (numbers (sales for example).
Now I want the model to be trained on those 5 actual real life data points instead of on (1005 rows i.e 1000 original plus 5 new).


Can this be done. Sorry for the ""basic"" question and all the help (including links if already answered) appreciated.


Code


import h2o
from h2o.automl import H2OAutoML
import pandas as pd

h2o.init()

data_path = ""./df.csv""

df = h2o.import_file(data_path)
y = ""c""

splits = df.split_frame(ratios = [0.8,0.19], seed = 1)

train = splits[0] #some part to train first
test = splits[1] # this is test set 1 (test later to become train set)
test2 = splits[2] # assume this to be the real world values 

aml = H2OAutoML(max_runtime_secs=120,project_name='try',  seed=1234)
aml.train(y = y, training_frame = train)

#First set of predictions

yy=aml.predict(test)

x=yy.as_data_frame(use_pandas=True) # predictions based on train set 
#print them
print(x) 

#the test set is now ""new real world data"" 
#to be added as incremental training of the model

aml.train(y = y, training_frame = test) 

#get the predictions again
yy=aml.predict(test2)

x=yy.as_data_frame(use_pandas=True) 
print(x)



I have tried to retrain on the ""new set""  ofdata (assuming that is what line 30 does) but get rather curious numbers.","['h2o', 'automl']",Community,https://stackoverflow.com/users/-1/community,1
57004020,57004020,2019-07-12T09:22:31,2019-07-12 20:55:58Z,111,"I have built a random-forest model with h2o. I am trying to plot it with h2otree. I am getting a TypeError: 'NoneType' object is not iterable


I can replicate the error. The class 
H2OTree
 calls a function 
__decode_categoricals(model, response['levels'])
 which iterates over reponse levels of type None.


from h2o.tree import H2OTree
first_tree = H2OTree(model = rf_model, tree_number = 0, tree_class = target_class_category)



#error replication
params = {""model"": model.model_id,
                  ""tree_number"": 0,
                  ""tree_class"": target_class_category}


response = h2o.api(endpoint=""GET /3/Tree"", data=params)
for lvl_index in response['levels']:

    if len(lvl_index) >0: #error, 'NoneType' has no len()
        print('yes')","['python', 'random-forest', 'h2o']",Unknown,,N/A
56965370,56965370,2019-07-10T07:19:16,2019-07-11 20:27:47Z,37,"I managed to plot my ROC with distributed random forest. I noticed that the ROC was plotted with 0 nfold. I'm puzzled how h2o plotted the ROC with 1 set of data. I did not select a validation frame for this experiment. If it is using the same set of dataset to self validate, shouldn't the AUC be 1.0?","['machine-learning', 'h2o', 'roc', 'auc']",user1342124,https://stackoverflow.com/users/9272452/user1342124,651
56959385,56959385,2019-07-09T19:25:37,2019-07-10 12:57:18Z,108,"I create random forrest model. mcc value are a list of two value. Why?


mRF=H2ORandomForestEstimator(nfolds=10,keep_cross_validation_models = True,seed=12345,model_id=""RF0"",ntrees=1000)
mRF.train(x=column_use, y=target, training_frame=train,validation_frame=valid)
print(""MCC valid"",mRF.model_performance(valid).mcc())
MCC valid [[0.35743618321170406, 0.21239407659849494]]","['machine-learning', 'h2o']",mau_who,https://stackoverflow.com/users/3354898/mau-who,335
56957790,56957790,2019-07-09T17:26:01,2019-07-10 21:44:54Z,371,"I currently have a 
df
 with both numerical and categorical values. The issue is that one of the columns has a list of categorical values:

Colors: [[red,blue,green],[red,black]...]
 


I tried converting the 
df
 into a 
h2o df
 with:


#convert df to h2o frame


data = h2o.H2OFrame(pantheon_data)


But got the following error:


ValueError: `python_obj` is not a list of flat lists!


Is there a way to keep the column ""Colors"" and to simply flatten the list somehow to just have the values separated with coma? and if it is possible, will the algorithm work? If it's not possible, what else can I do?","['python', 'machine-learning', 'data-science', 'decision-tree', 'h2o']",welo121,https://stackoverflow.com/users/11699693/welo121,47
56955913,56955913,2019-07-09T15:25:01,2019-07-09 15:59:20Z,308,"I have graphed an h2o decision tree:

I was following a lot of posts on SO and correct me if I'm wrong, but the values at the leaves are correlations, the levels are the count of categorical values, and tree 0 means that first tree that was created.

Now my problem is that

1. I can't figure out the ""greater or equal"" signs and the ""smaller than"" signs at the categorical values. For example, if we continue after 
Z<10.032598
, we have ""greater or equal"" sign on the right which implies what? Also, we have a ""smaller than"" sign on the left with 
NA
 which are the categorical variables but what does ""smaller than"" a categorical variable even means?

2. If we start at the top (
c
) and go right, we have the value 1, which I understand imply that 
c
 has 1 correlation. But if we go down 1 level to again 
Z<10.032598
 , the ""greater than or equal"" sign on the right imply 1 correlation again. What does that mean?","['python', 'machine-learning', 'decision-tree', 'h2o']",Unknown,,N/A
56938966,56938966,2019-07-08T16:20:39,2021-01-12 08:42:32Z,0,"I have df 
data_categorical
 and a model 
model
.

I converted my df to h2o frame with


data = h2o.H2OFrame(data_categorical)
 


and trained my model with


model = H2ORandomForestEstimator(ntrees=1, max_depth=20, nfolds=10)
# Train model
model.train(x=training_columns, y=response_column, training_frame=train) 



I'm trying to visualize the tree that is created (note that I only need one tree) but I can't seem to do that.

I downloaded the mojo file with


model.download_mojo(path,get_genmodel_jar=True)

But I don't know what to do next","['python', 'visualization', 'decision-tree', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
56937847,56937847,2019-07-08T15:09:44,2019-07-09 14:36:13Z,0,"Using the h2o R package (v 3.24.0.5) for some deeplearning, I need to import some big sparse matrix [2M * 10k] into it.
I've tried using fwrite but got a cholmod problem too large error, so went with svmlight.
Original matrix looks like this :


    Count    Dist    
1   nan     10.1266
2   859.124 10.8198
3   nan     10.1266



For this I took the sparsio package, writing goes ok but when reading the file with h2o.importFile I noticed something wrong :
I get the column indexes in front of every numbers as you can see below :


library(sparsio)
write_svmlight(HiC_mat.All, file=""Rdata/mat_kmer-NA.txt"")


HIC_df = h2o.importFile(""Rdata/mat_kmer-NA.txt"")

HIC_df[1:3,1:3]
  C1        C2        C3
1  0     0:nan 1:10.1266
2  0 0:859.124 1:10.8198
3  0     0:nan 1:10.1266




Any idea on how I can get rid of these ?


Data should look like this:


  C1        C2        C3
1  0       nan     10.1266
2  0    859.124    10.8198
3  0       nan     10.1266","['r', 'h2o', 'svmlight']",Unknown,,N/A
56930004,56930004,2019-07-08T07:16:03,2022-03-01 16:35:57Z,883,"Reference to 
https://dzone.com/articles/machine-learning-with-h2o-hands-on-guide-for-data


I was able to follow the example and plotted the ROC and AUC curve but when I did the same thing with my data, I have ""SCORING HISTORY - DEVIANCE"" instead of ""SCORING HISTORY - LOGLOSS"" and my ""ROC CURVE - TRAINING METRICS"" did not appear.


My dataset has 2 classes, 0, 1 instead of yes and no from the example. What determines whether the ROC curve will be portable?


Updated 10 Jul 2019 reference to Maurever's reply: 
I have ""convert to enum"" for my response ""label"" and change distribution to ""bernoulli"" but I still can't plot ROC. 



Update: For future users who has the same encounter as me. Seems like h2o cannot use response with class as ""0"" and ""1"" even if you convert to enum. My problem is solved by relabel of 0 and 1 to ""Normal"" and ""Malicious"" and the ROC curve will be plot.","['machine-learning', 'enums', 'random-forest', 'h2o', 'roc']",Unknown,,N/A
56928872,56928872,2019-07-08T05:28:08,2019-07-08 22:49:39Z,111,"I just set up h2o flow UI. I have a csv with the following labels.


Label | Count
0     | 9340
1     | 400
2     | 349



I have imported my file and parsed it. After I do split frame (by 80:20 ratio) I downloaded the 2 csv files to check the label count. 


But the split doesn't split to what I expected to be.


I was expecting the data to be split as follows:


Class | Expected 0.8 | Actual 0.8 | Expected 0.2 | Actual 0.2
0     | 7472         | 7418       | 1868         | 1882
1     | 320          | 610        | 80           | 159
2     | 279          | 15         | 69           | 5



How can I split my data into the expected value I wanted above so that I can use it as train and validate frame for model building?","['split', 'dataset', 'h2o']",user1342124,https://stackoverflow.com/users/9272452/user1342124,651
56907349,56907349,2019-07-05T17:44:54,2019-07-08 17:15:56Z,0,"I have a dataset with 200+ categorical variables (non-ordinal) and just a few continuous variables. I have tried to use one-hot encoding but that increases the dimensions by a lot and results in a poor score.

It seems like the regular scikit-learn tree can only be used with categorical variables that has been transformed into one-hot encoding (for non-ordinal vars) and I was if there's a way to create a tree without one-hot. I did some research and found that there's an API called h2o that might be useful, but I'm trying to find a way to run it on my local machine.","['machine-learning', 'data-science', 'decision-tree', 'h2o', 'categorical-data']",Unknown,,N/A
56893769,56893769,2019-07-04T20:05:21,2019-07-04 22:20:37Z,363,"At first I thought 
model_performance(train=True)
 gives the performance result of predicting on the same data that we trained the model. But this is not the case, because the number must have been the same as  
model.model_performance(test_data=train)
, but it isn't.  


Consider the following toy example:


# Make a dataframe
df = h2o.H2OFrame({'a':list(range(100)), 'b':list(range(100, 0, -1)), 'c':list(range(0, 200, 2))})

# Split the data
train, val, test = df.split_frame([.6, .2], seed=0)

# Build a model
from h2o.estimators.random_forest import H2ORandomForestEstimator
model = H2ORandomForestEstimator(seed=0)

# Train the model
model.train(x=train.names[:-1], y=train.names[-1], training_frame=train, validation_frame=val)

# Get performance results
print(model.model_performance(train=True)['mae'] 
      , model.model_performance(valid=True)['mae']
      , model.model_performance(test_data=test)['mae']
     )
# 1.3816 1.1968 1.4722



Compare the results with 


print(model.model_performance(test_data=train)['mae'] 
      , model.model_performance(test_data=val)['mae']
      , model.model_performance(test_data=test)['mae']
     )
# 0.5548 1.1968 1.4722



Note that the result of 
model_performance(train=True)
 and 
model_performance(test_data=train)
 are different, but the result of 
model_performance(valid=True)
 and 
model_performance(test_data=val)
 are the same.


So I'm wondering whether 
model_performance(train=True)
 and 
model.model_performance(test_data=train)
 should be the same (and there is a mistake in the calculation in H2O code), or the purpose of 
model_performance(train=True)
 is something else.


In the 
docs
 it says




train: boolean, optional

  Report the training metrics for the model.


valid: boolean, optional

  Report the validation metrics for the model.




But this is not very clear, given the above mentioned facts.","['python', 'machine-learning', 'h2o']",LoMaPh,https://stackoverflow.com/users/2445273/lomaph,"1,680"
56862368,56862368,2019-07-03T03:01:39,2019-07-03 15:25:25Z,0,"I have built a model with H2ORandomForestEstimator and the results shows something like this below. 


The threshold keeps changing (0.5 from traning and 0.313725489027 from validation) and I like to fix the threshold in H2ORandomForestEstimator for comparison during fine tuning. Is there a way to set the threshold? 


From 
http://h2o-release.s3.amazonaws.com/h2o/master/3484/docs-website/h2o-py/docs/modeling.html#h2orandomforestestimator
, there is no such parameter. 


If there is no way to set this, how do we know what threshold our model is built on?


rf_v1
** Reported on train data. **

MSE:    2.75013548238e-05  
RMSE:   0.00524417341664  
LogLoss:0.000494320913199  
Mean Per-Class Error: 0.0188802936476  
AUC: 0.974221763605  
Gini: 0.948443527211  
Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5:
       0       1    Error    Rate
-----  ------  ---  -------  --------------  
0      161692  1    0        (1.0/161693.0)  
1      3       50   0.0566   (3.0/53.0)  
Total  161695 51   0        (4.0/161746.0)  
Maximum Metrics: Maximum metrics at their respective thresholds

metric                       threshold    value     idx
---------------------------  -----------  --------  -----  
max f1                       0.5          0.961538  19  
max f2                       0.25         0.955056  21  
max f0point5                 0.571429     0.983936  18  
max accuracy                 0.571429     0.999975  18  
max precision                1            1         0  
max recall                   0            1         69  
max specificity              1            1         0  
max absolute_mcc             0.5          0.961704  19  
max min_per_class_accuracy   0.25         0.962264  21  
max mean_per_class_accuracy  0.25         0.98112   21  
Gains/Lift Table: Avg response rate:  0.03 %

** Reported on validation data. **

MSE:      1.00535766226e-05  
RMSE:     0.00317073755183  
LogLoss:  4.53885183426e-05  
Mean Per-Class Error: 0.0  
AUC: 1.0  
Gini: 1.0  
Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.313725489027:
       0      1    Error    Rate
-----  -----  ---  -------  -------------  
0      53715  0    0        (0.0/53715.0)  
1      0      16   0        (0.0/16.0)  
Total  53715  16   0        (0.0/53731.0)  
Maximum Metrics: Maximum metrics at their respective thresholds

metric                       threshold    value    idx
---------------------------  -----------  -------  -----  
max f1                       0.313725     1        5  
max f2                       0.313725     1        5  
max f0point5                 0.313725     1        5  
max accuracy                 0.313725     1        5  
max precision                1            1        0  
max recall                   0.313725     1        5  
max specificity              1            1        0  
max absolute_mcc             0.313725     1        5  
max min_per_class_accuracy   0.313725     1        5  
max mean_per_class_accuracy  0.313725     1        5","['performance', 'random-forest', 'h2o', 'threshold']",Unknown,,N/A
56842876,56842876,2019-07-01T21:49:53,2019-07-01 23:45:09Z,168,"Getting error


TypeError: set_params() takes exactly 1 argument (2 given)



even though I only appear to be providing a single argument...


HYPARAMS = {
            unicode(HYPER_PARAM): best_random_forest.params[unicode(HYPER_PARAM)][u'actual']
            for HYPER_PARAM in list_of_hyperparams_names
            }
assert isinstance(HYPARAMS, dict)
print 'Setting optimal params for full-train model...'
pp.pprint(HYPARAMS)
model = model.set_params(HYPARAMS)

#output
{   u'col_sample_rate_per_tree': 1.0,
    u'max_depth': 3,
    u'min_rows': 1024.0,
    u'min_split_improvement': 0.001,
    u'mtries': 5,
    u'nbins': 3,
    u'nbins_cats': 8,
    u'ntrees': 8,
    u'sample_rate': 0.25}
model = model.set_params(OPTIM_HYPARAMS)
TypeError: set_params() takes exactly 1 argument (2 given)



Looking at the 
source code
, 


def set_params(self, **parms):
    """"""Used by sklearn for updating parameters during grid search.

    Parameters
    ----------
      parms : dict
        A dictionary of parameters that will be set on this model.

    Returns
    -------
      Returns self, the current estimator object with the parameters all set as desired.
    """"""
    self._parms.update(parms)
    return self



there does not appear to be much going on that I see could go wrong. Anyone know what I'm missing here or what is happening to cause this error?",['h2o'],Unknown,,N/A
56813145,56813145,2019-06-28T20:58:24,2019-06-30 23:01:16Z,610,"I'm using H2O 3.24 with python 3.6 and the connection to the H2O server, which is on my local machine, will either freeze and/or crash with the error:


H2OConnectionError: Unexpected HTTP error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))


This has happened with previous versions of H2O, and isn't strictly reproducible, it will happen randomly. Sometimes the process will finish and sometimes it will crash. I see a little discussion of this online but there never seems to be a resolution.","['python', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
56793065,56793065,2019-06-27T14:16:55,2019-06-27 14:16:55Z,797,"I'm dealing with high dimensionality datasets and need to do some feature selection. I have used Random Forest in H2O and I would like to use LASSO regularization to see if it outperforms random forest.


Below is the code I created. When I specify alpha = 0 (RIDGE regularization), the code works fine and no error is raised. However, when I put alpha = 1 (LASSO) the error ""ZeroDivisionError: float division by zero"" is raised.


I followed the recommandation of this post for achieving LASSO : 
Attribute selection in h2o


Code :


from h2o.estimators.glm import H2OGeneralizedLinearEstimator
from h2o.grid.grid_search import H2OGridSearch

# feature columns et target column
x_lasso_h2o = list(feature_matrix.columns)
y_h2o = response_column

# select the values for lambda to grid over
hyper_params = {'lambda': list(arange(0.001,1,0.01))}

search_criteria_dim_reduction= {'strategy': 'RandomDiscrete',
                   'max_runtime_secs': 100,
                   'max_models': 5,
                   'stopping_metric': ""auto"",
                   'stopping_tolerance': 0.001,
                   'stopping_rounds': 5,
                   'seed': 1234}


# Train and validate a cartesian grid of GLMs
glm_grid_lasso = H2OGridSearch(model=H2OGeneralizedLinearEstimator(family= ""binomial"",nfolds = 5,alpha = 1,balance_classes = True),hyper_params=hyper_params,search_criteria=search_criteria_dim_reduction)

glm_grid_lasso.train(x=x_lasso_h2o, y=y_h2o,training_frame= train_h2o)

# Get the grid results, sorted by validation AUC
glm_lasso_gridperf = glm_grid_lasso.get_grid(sort_by='auc', decreasing=True)

best_lasso = glm_lasso_gridperf.model_ids[0]
best_lasso = h2o.get_model(best_lasso)
var_imp_pd_lasso = pd.DataFrame(best_lasso.varimp(True))



The error raised is : 


ZeroDivisionError                         Traceback (most recent call last)
<ipython-input-51-ca2e05533e82> in <module>
     32 best_lasso = glm_lasso_gridperf.model_ids[0]
     33 best_lasso = h2o.get_model(best_lasso)
---> 34 var_imp_pd_lasso = pd.DataFrame(best_lasso.varimp(True))

~\AppData\Local\Continuum\anaconda3\lib\site-packages\h2o\model\model_base.py in varimp(self, use_pandas)
    444                 vals = []
    445                 for item in tempvals:
--> 446                     tempT = (item[0], item[1], item[1]/maxVal, item[1]/sum)
    447                     vals.append(tempT)
    448                 header = [""variable"", ""relative_importance"", ""scaled_importance"", ""percentage""]

ZeroDivisionError: float division by zero



What I think : since LASSO reduces ""non important"" variables to 0 it explains why there is a division by 0 error. The output I expected was a list of all the variables with their respective importance. 


Thank you for your help.


Best regards.","['python', 'pandas', 'h2o', 'feature-selection', 'grid-search']",BP34500,https://stackoverflow.com/users/11616033/bp34500,178
56763772,56763772,2019-06-26T00:53:46,2019-06-26 11:57:36Z,980,"Trying to save an h2o model with some specific name that differs from the model's 
model_id
 field, but trying something like...


h2o.save_model(model=model,
               path='/some/path/then/filename',
               force=False)



just creates a dir/file structure like


some
   |__path
         |__then
              |__filename
                        |__<model_id>



as opposed to


some
   |__path
         |__then
              |__filename



Is this possible to do from the 
save_model
 method?


I can't / hesitate to simply change the 
model_id
 before calling the save method because the model names have timestamps appended to them to avoid name collisions with other models that may be on the h2o cluster (am trying to remove these timestamps when saving on disk and simplifying the name on the cluster before saving creates a time where naming collision can occur if other processes are also attempting to save such a model (of, say, a different timestamp)).


Any way to get this behavior or other common alternatives / workarounds?",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
56759854,56759854,2019-06-25T18:13:34,2019-06-25 19:38:06Z,0,"While running the following piece of code, I get an error converting mtcars_tbl to h2o dataframe


mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)  

Error: java.lang.AbstractMethodError
    at org.apache.spark.internal.Logging$class.initializeLogIfNecessary(Logging.scala:99)
    at org.apache.spark.h2o.H2OContext$.initializeLogIfNecessary(H2OContext.scala:360)
    at org.apache.spark.internal.Logging$class.log(Logging.scala:46)
    at org.apache.spark.h2o.H2OContext$.log(H2OContext.scala:360)
    at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)
    at org.apache.spark.h2o.H2OContext$.logWarning(H2OContext.scala:360)
    at org.apache.spark.h2o.H2OContext$.getOrCreate(H2OContext.scala:430)
    at org.apache.spark.h2o.H2OContext.getOrCreate(H2OContext.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at sparklyr.Invoke.invoke(invoke.scala:139)
    at sparklyr.StreamHandler.handleMethodCall(stream.scala:123)
    at sparklyr.StreamHandler.read(stream.scala:66)
    at sparklyr.BackendHandler.channelRead0(handler.scala:51)
    at sparklyr.BackendHandler.channelRead0(handler.scala:4)
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)
    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
    at java.lang.Thread.run(Unknown Source)



Here's my sessionInfo()


R version 3.5.3 (2019-03-11)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 14393)

Matrix products: default

locale:
[1] LC_COLLATE=Spanish_Ecuador.1252  LC_CTYPE=Spanish_Ecuador.1252    LC_MONETARY=Spanish_Ecuador.1252 LC_NUMERIC=C                    
[5] LC_TIME=Spanish_Ecuador.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] h2o_3.24.0.5         dplyr_0.8.1          sparklyr_1.0.1       rsparkling_2.2.42    RevoUtils_11.0.3     RevoUtilsMath_11.0.0

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.1        compiler_3.5.3    pillar_1.4.1      dbplyr_1.4.2      r2d3_0.2.3        bitops_1.0-6      base64enc_0.1-3  
 [8] tools_3.5.3       digest_0.6.19     packrat_0.5.0     jsonlite_1.6      tibble_2.1.3      pkgconfig_2.0.2   rlang_0.3.4      
[15] cli_1.1.0         DBI_1.0.0         rstudioapi_0.10   yaml_2.2.0        parallel_3.5.3    withr_2.1.2       httr_1.4.0       
[22] generics_0.0.2    htmlwidgets_1.3   askpass_1.1       rprojroot_1.3-2   tidyselect_0.2.5  glue_1.3.1        forge_0.2.0      
[29] R6_2.4.0          sessioninfo_1.1.1 purrr_0.3.2       magrittr_1.5      backports_1.1.4   htmltools_0.3.6   ellipsis_0.2.0   
[36] assertthat_0.2.1  config_0.3        RCurl_1.95-4.12   openssl_1.4       crayon_1.3.4     



And the full code


options(rsparkling.sparklingwater.version = ""2.4.99999-157"")

library(rsparkling)
library(sparklyr)
library(dplyr)
library(h2o)

sc <- spark_connect(master = ""local"", spark_home = ""C:/spark-2.4.3/"")
mtcars_tbl <- copy_to(sc, mtcars, ""mtcars"")

mtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)



If I run 
h2o_context()
 I get almost the same result as if no connection between h2o and Spark was made.","['java', 'r', 'apache-spark', 'rstudio', 'h2o']",Andre Dublin,https://stackoverflow.com/users/607055/andre-dublin,"1,158"
56706274,56706274,2019-06-21T15:23:04,2019-06-21 20:42:39Z,284,"I'm following a tutorial from 
https://github.com/h2oai/h2o-tutorials/blob/master/tutorials/gbm-randomforest/GBM_RandomForest_Example.py


I have been following the tutorial until I reached the line with hit_ratio_table. when I executed
""rf_v1.hit_ratio_table(valid=True)"", I encounter the error below. 


>>> rf_v1.hit_ratio_table(valid=True)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/h2oUser/sparkling-water-2.3.0/py/build/dist/h2o_pysparkling_2.3-2.3.0.zip/h2o/utils/backward_compatibility.py"", line 74, in __getattr__
  File ""/home/h2oUser/sparkling-water-2.3.0/py/build/dist/h2o_pysparkling_2.3-2.3.0.zip/h2o/utils/backward_compatibility.py"", line 49, in __getattribute__
AttributeError: type object 'H2ORandomForestEstimator' has no attribute 'hit_ratio_table'



I tried to search if hit_ratio_table is being deprecated or not, but I can't find any links. Does anyone know if this function has been changed?",['h2o'],user1342124,https://stackoverflow.com/users/9272452/user1342124,651
56693407,56693407,2019-06-20T20:23:20,2019-06-21 01:30:55Z,154,"Seeing error 




TypeError: unsupported operand type(s) for +: 'NoneType' and 'unicode'




when trying to use gridsearch for training a model in h2o and am unable to interpret the cause.


Here is the output that gets printed right before the error:


drf Grid Build progress: |████████████████████████████████████████████████| 100%
Errors/Warnings building gridsearch model

Hyper-parameter: col_sample_rate_per_tree, 0.75
Hyper-parameter: max_depth, 5
Hyper-parameter: min_rows, 4096.0
Hyper-parameter: min_split_improvement, 1e-08
Hyper-parameter: mtries, 8
Hyper-parameter: nbins, 8
Hyper-parameter: nbins_cats, 64
Hyper-parameter: ntrees, 96
Hyper-parameter: sample_rate, 0.6320000291
failure_details: None
failure_stack_traces: java.lang.NullPointerException
    at hex.tree.SharedTree.init(SharedTree.java:164)
    at hex.tree.drf.DRF.init(DRF.java:53)
    at hex.tree.SharedTree$Driver.computeImpl(SharedTree.java:207)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:222)
    at hex.ModelBuilder.trainModelNested(ModelBuilder.java:348)
    at hex.ModelBuilder$TrainModelNestedRunnable.run(ModelBuilder.java:383)
    at water.H2O.runOnH2ONode(H2O.java:1304)
    at water.H2O.runOnH2ONode(H2O.java:1297)
    at hex.ModelBuilder.trainModelNested(ModelBuilder.java:364)
    at hex.grid.GridSearch.buildModel(GridSearch.java:343)
    at hex.grid.GridSearch.gridSearch(GridSearch.java:220)
    at hex.grid.GridSearch.access$000(GridSearch.java:71)
    at hex.grid.GridSearch$1.compute2(GridSearch.java:138)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1416)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



and the code being used to create the gridsearch object


model = h2o.h2o.H2ORandomForestEstimator(
                response_column=configs['RESPONSE'],
                keep_cross_validation_models=False,
                keep_cross_validation_predictions=False
            )

random_forest_grid = h2o.h2o.H2OGridSearch(model=model, 
                     hyper_params=configs['HYPERPARAMETER_RANGES'], 
                     search_criteria=configs['SEARCH_CRITERIA'])
.
.
.

max_train_time_hrs = 8
# here is where the ERROR is thrown
random_forest_grid.train(x=training_features, y=training_response,
                         weights_column='weight',
                         training_frame=train_u, validation_frame=test_u,
                         max_runtime_secs=max_train_time_hrs * 60 * 60)



where the 
configs
 being referred to is a dictionary like...


configs = {
.
.
.
 'HYPERPARAMETER_RANGES': {
        'ntrees': [32, 64, 96, 128],  # default 50
        'nbins_cats': [16, 32, 64, 128, 512, 1024],  # default is 1024
        'nbins': [8, 13, 21, 34],  # default is 20
        'max_depth': [5, 8, 13],  # default is 20
        'mtries': [-1, 5, 8, 13],  # default is -1 for the square root of number of features
        'min_split_improvement': [1 * 10 ** -8,
                                  1 * 10 ** -5,
                                  1 * 10 ** -3],
        'min_rows': [16, 64, 256, 1024, 4096],  # this option specifies the number of observations for a split
        'col_sample_rate_per_tree': [0.75, 0.9, 1],  # default is 1
        'sample_rate': [0.5, 0.6320000291, 0.75]  # default is 0.6320000291
},
'SEARCH_CRITERIA': {
    'strategy': 'RandomDiscrete',
    'max_models': 24,
    'seed': 64,
    'stopping_metric': 'AUTO',  # log-loss
 }

}



Note that the gridsearch works for some other DRF models I am training (with exact same gridsearch hyper-parameter and criteria ranges) and can't seem to find any notable difference between these working versions and this erroring one. Any common reasons why this kind of error may be thrown in h2o? Any theories or further debugging suggestions would be appreciated.",['h2o'],Unknown,,N/A
56692248,56692248,2019-06-20T18:55:26,2019-06-28 15:07:33Z,226,"Problem Specific to Mac OS Mojave.


I am using the python library of h2o and while predicting the h2o cluster freezes. I am able to access the logs through flow but there are no failures in it. 


I have tried 
h2o.init(model,min_mem_size = ""16g"")
 also. 


I think it be an issue with h2o unable to access my mac lib folder?


I am basing the guess on:


$user qalex$ java -jar app/env/lib/python3.7/site-packages/h2o/backend/bin/h2o.jar

Cannot load library from path lib/osx_64/libxgboost4j_gpu.dylib
Cannot load library from path lib/libxgboost4j_gpu.dylib
Failed to load library from both native path and jar!
Cannot load library from path lib/osx_64/libxgboost4j_omp.dylib
Cannot load library from path lib/libxgboost4j_omp.dylib





My H2O version:


pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html 'h2o==3.22.1.1'



These log Files are empty:


h2o_127.0.0.1_54321-6-warn.log
h2o_127.0.0.1_54321-6-fatal.log
h2o_127.0.0.1_54321-6-error.log



Let me know if you need me to upload these files somewhere?


h2o_127.0.0.1_54321-6-info.log
h2o_127.0.0.1_54321-6-httpd.log
h2o_qalex_started_from_python.err (nevermind works only with linux)
h2o_qalex_started_from_python.out (nevermind works only with linux)","['python', 'h2o']",Unknown,,N/A
56688462,56688462,2019-06-20T14:41:56,2019-06-20 14:55:17Z,333,"I created a custom loss function for a binary (0/1) classification problem in h2o via Python as shown below. The idea is to minimize total cost based on true positive, true negative, false positive, and false negative. Here are the questions that I hope to get an answer on:




What is used to calculate custom loss function? I used the output of confusion matrix of training data and validation data and manually calculate the value of the loss function but it doesn't match the output. Note that the default metrics used to generate confusion matrix is using 
f1
 threshold


On h2o 
documentation
, 
custom_metric_func
 can be used in GLM, DRF, and GBM. However, it doesn't work in GLM (the loss function value is default to 0) although it works perfectly in GBM. Any idea on why that's the case?




Custom loss function:


class CustomLossFunc:
    def map(self, predicted, actual, weight, offset, model):
        import math
        cost_tp = -9
        cost_tn = 0
        cost_fp = 1
        cost_fn = 10

        y = actual[0]
        y_pred = predicted[0] # [class, p0, p1]

        if (y == 0) and (y_pred == 0):
            total_cost = cost_tn
        elif (y == 0) and (y_pred == 1):
            total_cost = cost_fp
        elif (y == 1) and (y_pred == 1):
            total_cost = cost_tp
        else:
            total_cost = cost_fn

        return [total_cost, 1]

    def reduce(self, left, right):
        return [left[0] + right[0], left[1] + right[1]]

    def metric(self, last):
        return last[0]



The loss function is uploaded using 
h2o.upload_custom_metric()
 then I run GLM and GBM for comparison:


# GLM
glm_fit_cost = H2OGeneralizedLinearEstimator(family='binomial',
                                             model_id='glm_fit_cost',
                                             #standardize=True,
                                             custom_metric_func= cost_loss_func)
glm_fit_cost.train(x=x_co,
                   y=y_co,
                   training_frame = train_co_h2o,
                   validation_frame = valid_co_h2o)

# GBM
gbm_mod = H2OGradientBoostingEstimator(model_id = ""gbm_mod"",
                                       custom_metric_func = cost_loss_func)
gbm_mod.train(y=y_co,
              x=x_co,
              training_frame=train_co_h2o,
              validation_frame = valid_co_h2o)



I have tried the following:




Debug via manual calculation using confusion matrix of the training set and validation set and compare to the output of the loss function




Reference for the example used to create my own loss function:




Resource1


Resource2",['h2o'],Unknown,,N/A
56685354,56685354,2019-06-20T11:50:23,2019-06-20 19:25:15Z,377,I have built a model in H2O.ai however I would like to now use this model in C#/.net.,"['.net', 'machine-learning', 'h2o']",ashley aberneithy,https://stackoverflow.com/users/586316/ashley-aberneithy,520
56677557,56677557,2019-06-20T00:37:00,2019-06-20 01:02:32Z,655,"Trying to do a simple 
set
 difference in python, and getting results that indicate the the difference operator is doing nothing. Eg. have code


python version 2.7.15+


    assert isinstance(left_frame, h2o.H2OFrame)
    assert isinstance(right_frame, h2o.H2OFrame)
    assert isinstance(left_key, str)
    assert isinstance(right_key, str)

    # ensure that the primary_key exists in both frames
    assert left_key in left_frame.columns, 'left_key: {} does not exist in left_frame'.format(left_key)
    assert right_key in right_frame.columns, 'right_key: {} does not exist in right_frame'.format(right_key)

    # ensure that the primary_key is the only common column between the left and right frame
    left_non_pk_cols = set(left_frame.columns) - set(left_key)
    assert left_on not in left_non_pk_cols, '%s' % left_key
    right_non_pk_cols = set(right_frame.columns) - set(right_key)
    assert right_on not in right_non_pk_cols, '%s' % right_key
    left_non_pk_cols_in_right = left_non_pk_cols.intersection(right_non_pk_cols)
    assert len(left_non_pk_cols_in_right) == 0,\
        'The primary_key is not the only common column between frames, h2o merge will not work as expected\n%s\n%s\n%s' \
        % (left_non_pk_cols, right_non_pk_cols, left_non_pk_cols_in_right)



I get the error


    assert left_key not in left_non_pk_cols, '%s' % left_key
AssertionError: <the left_key value>



This is really weird to me. Running in a terminal (with same python version) for a simplified example case


assert u'1' not in (set([u'1', u'2', u'3']) - set(u'1'))
# noting that the H2OFrames `.columns` field is a list of unicode strings



throws no error at all and completes as expected (when printing the resulting 
set
, everything looks as it should (no 
u'1'
 element)).


Using the 
.difference()
 method rather than the 
-
 operator does not result in any difference either.


Does anyone know what could be going on here or other things to do to get more debugging info?","['python', 'h2o']",Unknown,,N/A
56675904,56675904,2019-06-19T21:12:20,2019-06-19 21:12:20Z,0,"I created a MOJO file with H2O and try to predict based on the saved modell with R. However, I get an exit status 127 error and could not find any solution. 


I have updated to the latest H2O release. The issues only occurs with Windows, running the code on Kaggle gives no error at all. 


>   modelpath <- h2o.download_mojo(fileh, path=getwd(), get_genmodel_jar=TRUE)
>   modelpath



[1] ""DeepLearning_model_R_1560926535628_2.zip""


>   if(is.vector(X)) X <- t(X)
>   X <- as.matrix(X)
>   Ynew <-   h2o.mojo_predict_df(frame = X, mojo_zip_path=modelpath, verbose=T)



...


input_csv:      C:\Users\PAPERS~1\AppData\Local\Temp\Rtmpem4zDz/input.csv 
mojo_zip:C:\Users\paperspace\Documents\DeepLearning_model_R_1560926535628_2.zip genmodel_jar:   C:/Users/paperspace/Documents/h2o-genmodel.jar 
output_csv:    C:\Users\PAPERS~1\AppData\Local\Temp\Rtmpem4zDz/prediction.csv 
classpath:      C:/Users/paperspace/Documents/h2o-genmodel.jar 
java_options:   -Xmx4g -XX:ReservedCodeCacheSize=256m 
java cmd:       java -Xmx4g -XX:ReservedCodeCacheSize=256m -cp C:/Users/paperspace/Documents/h2o-genmodel.jar hex.genmodel.tools.PredictCsv --mojo C:\Users\paperspace\Documents\DeepLearning_model_R_1560926535628_2.zip --input C:\Users\PAPERS~1\AppData\Local\Temp\Rtmpem4zDz/input.csv --output C:\Users\PAPERS~1\AppData\Local\Temp\Rtmpem4zDz/prediction.csv --decimal 
Fehler in h2o.mojo_predict_csv(input_csv_path = input_csv_path, mojo_zip_path = mojo_zip_path,  : 
  SYSTEM COMMAND FAILED (exit status 127)
Zusätzlich: Warnmeldung:
In dir.create(tmp_dir) :
  'C:\Users\PAPERS~1\AppData\Local\Temp\Rtmpem4zDz' existiert bereits","['r', 'h2o', 'mojo']",Richie,https://stackoverflow.com/users/11667022/richie,1
56666876,56666876,2019-06-19T11:41:18,2019-06-19 20:27:26Z,0,"I am going through the book ""Hands-on Time series analysis with R"" and I am stuck at the example using machine learning h2o package. I don't get how to use h2o.predict function. In the example it requires newdata argument, which is test data in this case. But how do you predict future values of time series if you in fact don't know these values ?


If I just ignore newdata argument I get :   predictions with a missing 
newdata
 argument is not implemented yet.


library(h2o)

h2o.init(max_mem_size = ""16G"")


train_h <- as.h2o(train_df)
test_h <- as.h2o(test_df)
forecast_h <- as.h2o(forecast_df)


x <- c(""month"", ""lag12"", ""trend"", ""trend_sqr"")
y <- ""y""

rf_md <- h2o.randomForest(training_frame = train_h,
                          nfolds = 5,
                          x = x,
                          y = y,
                          ntrees = 500,
                          stopping_rounds = 10,
                          stopping_metric = ""RMSE"",
                          score_each_iteration = TRUE,
                          stopping_tolerance = 0.0001,
                          seed = 1234)

h2o.varimp_plot(rf_md)

rf_md@model$model_summary

library(plotly)

tree_score <- rf_md@model$scoring_history$training_rmse
plot_ly(x = seq_along(tree_score), y = tree_score,
        type = ""scatter"", mode = ""line"") %>%
  layout(title = ""Random Forest Model - Trained Score History"",
         yaxis = list(title = ""RMSE""),
         xaxis = list(title = ""Num. of Trees""))

test_h$pred_rf <- h2o.predict(rf_md, test_h)

test_1 <- as.data.frame(test_h)

mape_rf <- mean(abs(test_1$y - test_1$pred_rf) / test_1$y)
mape_rf","['r', 'machine-learning', 'h2o']",Alex Stepanov,https://stackoverflow.com/users/10890039/alex-stepanov,21
56649144,56649144,2019-06-18T12:26:25,2019-06-18 12:40:38Z,47,"I built an autoencoder (AE) in 
h2o ver. 3.24.0.3 python
 with 
""export_weights_biases""=TRUE
. After training, I tried to get my AE model weights by AE.weights(0). At first, it returns the weights matrix. But then I saved my AE model, and load it again after I shutdown my h2o cluster. I tried to get the weights again but it returns ""None"". Can someone help?


params={....
        'export_weights_and_biases':True,}

ae_model=H2OAutoEncoderEstimator(**params)
ae_model.train(x=X,training_frame=data)

w=ae_model.weights(0)



Expected: H2OFrame of weights with input*layer dimension
Actual: None","['python', 'h2o']",Jasurbek,https://stackoverflow.com/users/8342718/jasurbek,"2,966"
56648018,56648018,2019-06-18T11:23:35,2019-06-18 23:16:28Z,85,AutoKeras is built on Keras framework and Google automl is built on tensorflow. So on which framework does the h2o build ? or is it made from scratch ?,"['machine-learning', 'h2o', 'automl']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
56636206,56636206,2019-06-17T17:27:49,2019-06-17 20:35:55Z,146,"I have used the H2O function ""partial_plot"" to created the partial dependent plot for binary problem. But when I try to use the same function apply to multi-classification problem, it fails with error message.


failed with an exception: java.lang.RuntimeException: water.exceptions.H2OIllegalArgumentException: unimplemented



Does that mean we cannot use partial_plot function to run multi-classification problem? maybe some plot like the 
link",['h2o'],Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
56635755,56635755,2019-06-17T16:54:54,2019-06-19 16:46:52Z,0,"I know H2O can use 


model_perf = model.model_performance(input)
model_perf.confusion_matrix



to output the confusion matrix. But is there a way to get the confusion matrix table to create plot?",['h2o'],Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
56629764,56629764,2019-06-17T10:48:59,2019-10-04 20:37:29Z,0,"After using H2O Python Module AutoML, it is found that XGBoost is on the top of the Leaderboard. Then what I was trying to do is to extract the hyper-parameters from the H2O XGBoost and 
replicate
 it in the XGBoost Sklearn API. However, the performance is different between these 2 approaches:



from sklearn import datasets
from sklearn.model_selection import train_test_split, cross_val_predict
from sklearn.metrics import classification_report

import xgboost as xgb
import scikitplot as skplt
import h2o
from h2o.automl import H2OAutoML
import numpy as np
import pandas as pd

h2o.init()


iris = datasets.load_iris()
X = iris.data
y = iris.target

data = pd.DataFrame(np.concatenate([X, y[:,None]], axis=1)) 
data.columns = iris.feature_names + ['target']
data = data.sample(frac=1)
# data.shape

train_df = data[:120]
test_df = data[120:]

# Import a sample binary outcome train/test set into H2O
train = h2o.H2OFrame(train_df)
test = h2o.H2OFrame(test_df)

# Identify predictors and response
x = train.columns
y = ""target""
x.remove(y)

# For binary classification, response should be a factor
train[y] = train[y].asfactor()
test[y] = test[y].asfactor()

aml = H2OAutoML(max_models=10, seed=1, nfolds = 3,
                keep_cross_validation_predictions=True,
                exclude_algos = [""GLM"", ""DeepLearning"", ""DRF"", ""GBM""])
aml.train(x=x, y=y, training_frame=train)
# View the AutoML Leaderboard
lb = aml.leaderboard
lb.head(rows=lb.nrows)

model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])
m = h2o.get_model([mid for mid in model_ids if ""XGBoost"" in mid][0])
# m.params.keys()





Performance of H2O Xgboost




skplt.metrics.plot_confusion_matrix(test_df['target'], 
                                    m.predict(test).as_data_frame()['predict'], 
                                    normalize=False)








Replicate in XGBoost Sklearn API:




mapping_dict = {
        ""booster"": ""booster"",
        ""colsample_bylevel"": ""col_sample_rate"",
        ""colsample_bytree"": ""col_sample_rate_per_tree"",
        ""gamma"": ""min_split_improvement"",
        ""learning_rate"": ""learn_rate"",
        ""max_delta_step"": ""max_delta_step"",
        ""max_depth"": ""max_depth"",
        ""min_child_weight"": ""min_rows"",
        ""n_estimators"": ""ntrees"",
        ""nthread"": ""nthread"",
        ""reg_alpha"": ""reg_alpha"",
        ""reg_lambda"": ""reg_lambda"",
        ""subsample"": ""sample_rate"",
        ""seed"": ""seed"",

        # ""max_delta_step"": ""score_tree_interval"",
        #  'missing': None,
        #  'objective': 'binary:logistic',
        #  'scale_pos_weight': 1,
        #  'silent': 1,
        #  'base_score': 0.5,
}

parameter_from_water = {}
for item in mapping_dict.items():
    parameter_from_water[item[0]] = m.params[item[1]]['actual']
# parameter_from_water

xgb_clf = xgb.XGBClassifier(**parameter_from_water)
xgb_clf.fit(train_df.drop('target', axis=1), train_df['target'])





Performance of Sklearn XGBoost:


(always worse than H2O in all examples I tried.)




skplt.metrics.plot_confusion_matrix(test_df['target'], 
                                    xgb_clf.predict(test_df.drop('target', axis=1)  ), 
                                    normalize=False);





Anything obvious that I missed?","['python', 'machine-learning', 'scikit-learn', 'h2o', 'xgboost']",vlemaistre,https://stackoverflow.com/users/11492024/vlemaistre,"3,331"
56584254,56584254,2019-06-13T15:49:47,2019-07-08 16:52:36Z,53,"I have the following CSV files that I'm importing into H2OFrames. 


CSV 1(a):


year,manufacturer,model,salePrice
2010,HONDA,CIVIC,100
2011,TOYOTA,CAMRY,150
2010,HONDA,CIVIC,50
2011,TOYOTA,CAMRY,200
2010,HONDA,CIVIC,150
2011,TOYOTA,CAMRY,250
2012,SUZUKI,SWIFT,500
2012,SUZUKI,SWIFT,600



CSV 1(b):


manufacturer,model,year,salePrice
HONDA,CIVIC,2010,100
TOYOTA,CAMRY,2011,150
HONDA,CIVIC,2010,50
TOYOTA,CAMRY,2011,200
HONDA,CIVIC,2010,150
TOYOTA,CAMRY,2011,250
SUZUKI,SWIFT,2012,500
SUZUKI,SWIFT,2012,600



CSV 2:


year,manufacturer,model,bodyType
2010,HONDA,CIVIC,SEDAN
2011,TOYOTA,CAMRY,SEDAN
2012,SUZUKI,SWIFT,HATCHBACK



Notice that ""1a"" and ""1b"" are exactly the same data with just the columns ordered differently. Now I import all three of them into 
H2OFrame
 as follows:


import h2o
h2o.init()
df1a=h2o.import_file('csv1a.csv')
df1b=h2o.import_file('csv1b.csv')
df2=h2o.import_file('csv2.csv')



And then I try the following merge operations:


merge1=df1a.merge(df2, by_x=['year','manufacturer','model'], by_y=['year','manufacturer','model'])
merge2=df1b.merge(df2, by_x=['year','manufacturer','model'], by_y=['year','manufacturer','model'])



The first merge works as expected. But the second one fails with error ""Merging columns must be the same type, column salePrice found types Enum and Numeric""


When I checked the logs on the h2o server, I found that the 
by_x
 column indices sent to the server were exactly the same for both the merges(and ordered - 
[0 1 2]
), in spite of the different order in which the 
by_x
 columns occur in 
df1b
. From the server logs..


parms={ast=(tmp= py_5_sid_abdb (merge h2o_bug_df1b1.hex h2o_bug_df21.hex False False [0 1 2] [0 1 2] 'auto'))



Then I found line #1967 and #3422 in the code of the 
h2o.frame
 module:

https://github.com/h2oai/h2o-3/blob/jenkins-rel-yates-4/h2o-py/h2o/frame.py#L1967


https://github.com/h2oai/h2o-3/blob/jenkins-rel-yates-4/h2o-py/h2o/frame.py#L3422


For my second merge, the 
by_x
 column indices should have been 
[2 0 1]
, instead the python client sent 
[0 1 2]
 because of the 
list(set(tmp))
 statement in line #3422. 


So I'm wondering, wouldn't it be better to fail fast and throw an error if the supplied 
by_x
 or 
by_y
 column names are not unique, instead of collecting them into a 
set
 which obviously won't preserve the order?


Also the error message was misleading until I checked the code as the message had a reference to the 
salePrice
 column which was neither in my 
by_x
 nor 
by_y
.","['python', 'python-3.x', 'h2o']",Unknown,,N/A
56551679,56551679,2019-06-11T21:07:18,2019-06-11 21:35:19Z,141,"I'm trying to import data from a mysql table into a H2oFrame using 
h2o.import_sql_select()
. I'd like NULL or empty values in VARCHAR columns in the database to be recognized as NAs when imported into the  H2oFrame, but they are being considered as empty string literals. However, for numerical columns, NULL values are automatically recognized as NAs.


Here's the code I have:


select_query = 'SELECT * FROM my_table'
train_data = h2o.import_sql_select(""jdbc:mysql://localhost:3306/my_schema"", select_query, ""username"", ""password"", use_temp_table=False)



train_data['my_string_column'].isna()
 always results in zeros even for NULL or empty values coming from the database. 


However when I dump the data to CSV and import it using 
h2o.import_file('/path/to/file.csv', na_strings=[''])
 and then do 
train_data['my_string_column'].isna()
, I can see that the empty values are correctly recognized as NAs because of the 
na_strings
 parameter.


Is there some way of specifying 
na_strings
 or some other work around to achieve the expected behavior while importing data using 
h2o.import_sql_select()
?","['python', 'mysql', 'python-3.x', 'h2o']",Prashanth Sg,https://stackoverflow.com/users/4184039/prashanth-sg,3
56536080,56536080,2019-06-11T03:29:46,2019-06-11 03:29:46Z,0,"I am using h2o.deeplearning to build poisson models, so I am offsetting the exposure weights (I also need to use other offset values in the future). Because they are the log of numbers smaller than 1, all offset values are negative.


I noticed when scoring, negative offsets are not used.


I am using the latest h2o version on CRAN, 3.22.1.1. 


R version 3.6.0.


Reproducible example:


# Setup
set.seed(1234)
install.packages(""h2o"")
install.packages(""insuranceData"")
library(h2o)
library(insuranceData)
h2o.init()
h2o.getVersion()  # ""3.22.1.1

data(dataCar)
dataCar$log_exposure <- log(dataCar$exposure)
# log_exposure contains negative values
summary(dataCar$log_exposure)

# Build model
dataCar.h2o <- as.h2o(dataCar)
dl <- h2o.deeplearning(x = c(""veh_value"", ""veh_body"", ""veh_age"", ""gender"", ""area"", ""agecat""),
                       y = ""numclaims"",
                       training_frame = dataCar.h2o,
                       nfolds = 5,
                       offset_column = ""log_exposure"",
                       distribution = ""poisson"",
                       reproducible = TRUE)

# Prediction Scenarios
dataCar_offsetadj <- dataCar[1,]

# Offset 0
dataCar_offsetadj$log_exposure <- 0
pred1 <- h2o.predict(dl, as.h2o(dataCar_offsetadj))
print(pred1)
# 0.1058487

# Offset 1.5
dataCar_offsetadj$log_exposure <- 1.5
pred2 <- h2o.predict(dl, as.h2o(dataCar_offsetadj))
print(pred2)
# 0.4743808  # Different value, as expected

# Offset -1.5
dataCar_offsetadj$log_exposure <- -1.5
pred3 <- h2o.predict(dl, as.h2o(dataCar_offsetadj))
print(pred3)
# 0.1058487  # Same value as offset 0

# Offset -2
dataCar_offsetadj$log_exposure <- -2
pred4 <- h2o.predict(dl, as.h2o(dataCar_offsetadj))
print(pred4)
# 0.1058487  # again, same value

all.equal(pred1, pred3)  # TRUE
all.equal(pred1, pred4)  # TRUE



I would expect the predictions with the negative offsets to all be different. The positive offset has a different score as expected.


It seems like h2o is not applying negative offsets. Is this correct? What would be the reason for doing this? Does this also mean the negative offsets are not used when creating the model? How would I get around this problem? I will need to build more deep learning models with different distributions and negative offsets in the future.


Appreciate any advice, thanks.","['r', 'h2o']",stephanieR,https://stackoverflow.com/users/11612074/stephanier,1
56535161,56535161,2019-06-11T01:01:53,2019-06-12 22:38:20Z,0,"Unable to Split frame using split_frame(). The dataframe is able to show() but I cannot split it. Please help.


Below is a sample of the code I have used.


from h2o.estimators.random_forest import H2ORandomForestEstimator
from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o.estimators.deeplearning import H2ODeepLearningEstimator
from h2o.estimators.glm import H2OGeneralizedLinearEstimator
from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator
from __future__ import print_function

temp = spark.read.option(""header"",""true"").option(""inferSchema"",""true"").csv(""hdfs://bda-ns/user/august_week2.csv"")

train,test,valid = temp.split_frame(ratios=[.75, .15])



Expected: no error. Data split into test and train data frame.
Actual: 


Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/python/pyspark/sql/dataframe.py"", line 1182, in __getattr__
    ""'%s' object has no attribute '%s'"" % (self.__class__.__name__, name))
AttributeError: 'DataFrame' object has no attribute 'split_frame'
>>> train,test,valid = temp.split_frame(ratios=[.75, .15])
Traceback (most recent call last):
  File ""/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/python/pyspark/context.py"", line 234, in signal_handler",['h2o'],eyllanesc,https://stackoverflow.com/users/6622587/eyllanesc,243k
56534185,56534185,2019-06-10T22:16:47,2019-06-10 22:16:47Z,226,"After running a long process (~2 hours) process on an h2o instance (version 3.24.0.2 (Hadoop)), the python script appears to lose connection and errors out with message


...
File ""/home/osboxes/projects/ml1c/venv/local/lib/python2.7/site-packages/h2o/h2o.py"", line 274, in init
    raise H2OConnectionError('Can only start H2O launcher if IP address is localhost.')
H2OConnectionError: Can only start H2O launcher if IP address is localhost.



Checking the h2o instance that was started before running the script the terminal output after presssing 
enter
 shows


...
H2O node 172.18.4.62:54321 reports H2O cluster size 5 [leader is 172.18.4.62:54321]
H2O node 172.18.4.66:54321 reports H2O cluster size 5 [leader is 172.18.4.62:54321]
H2O node 172.18.4.65:54321 reports H2O cluster size 5 [leader is 172.18.4.62:54321]
H2O cluster (5 nodes) is up
(Note: Use the -disown option to exit the driver after cluster formation)

Open H2O Flow in your web browser: http://172.18.4.62:54321

(Press Ctrl-C to kill the cluster)
Blocking until the H2O cluster shuts down...
packet_write_wait: Connection to 172.18.4.62 port 22: Broken pipe



Anyone know what can be done to fix this?




For context I am processing a large pandas dataframe by rowwise-chunks, converting them into h2o dataframe, and putting them back together with row binding 
h2o.rbind
.",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
56523173,56523173,2019-06-10T08:48:08,2019-06-10 08:48:08Z,0,"I used h2o package before. But it can't do now.
I only have 64-bit java.




I have java.exe  D:\java\New Foder\java8\bin. Also added path. 
This is error message.


> h2o.init()

H2O is not running yet, starting it now...
<simpleError in system2(command, ""-version"", stdout = TRUE, stderr = TRUE): '""D:\java\New Folder\java8;D:\java;\bin\java.exe""' not found>
Error in value[[3L]](cond) : 
  You have a 32-bit version of Java. H2O works best with 64-bit Java.
Please download the latest Java SE JDK 8 from the following URL:
http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
In addition: Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :
  path[1]=""D:\\java\\New Folder\\java8;D:\java;/bin/java.exe"": 파일 이름, 디렉터리 이름 또는 볼륨 레이블 구문이 잘못되었습니다



I checked regedit. There seems to be no problem. How can it be solved?","['java', 'r', 'h2o']",Sang won kim,https://stackoverflow.com/users/9857061/sang-won-kim,522
56517949,56517949,2019-06-09T19:56:10,2019-06-12 22:33:41Z,77,"I have saved an optimised 
H2ORandomForestEstimator
 model using 
h2o.save_model()
 (python API). I now want to load this model and re-train it with different variants of my data, whilst maintaining the optimised hyperparams (e.g. ntrees, max_depth).


When I do this by calling 
train()
 on the loaded models however all the hyperparams appear to be reset to their defaults. What is the recommended way to achieve this?","['random-forest', 'h2o']",Simon Thorogood,https://stackoverflow.com/users/639530/simon-thorogood,75
56495634,56495634,2019-06-07T13:50:51,2019-06-08 22:18:51Z,0,"According to the docs, 
xgboost
 is not supported on 
h2o windows OS
. Is it still the case? Is there a workaround?


Really want to use on windows machine.


Thanks","['r', 'rstudio', 'h2o', 'xgboost']",Shery,https://stackoverflow.com/users/3107664/shery,"1,872"
56440110,56440110,2019-06-04T08:29:08,2019-06-06 08:16:08Z,0,"When cross-validating the elastic net 
lambda
 hyper-parameter using the 
lambda_search
 option, the algorithm may not pick the value of 
lambda
 from the specified grid that minimizes deviance on the validation sample. This occurs also when we set 
early_stopping = FALSE
, i.e., when one would expect H2O to evaluate all values of 
lambda
 in the grid.


This statement can be checked by cross-validating lambda first using 
lambda_search = TRUE
 in 
h2o.glm()
, then running a grid search over the same values of lambda using 
h2o.grid()
 and comparing the resulting hyperparameters and validation deviance values. See the R code below.  


The issue is closely related to the one pointed out 
here
 and mentioned 
here
. What this question adds is the documentation that the cross-validated value of 
lambda
 need not be the one that minimizes validation deviance. I.e., the problem can be more severe than H2O computing up-to the best lambda and then exiting, as stated in the comments 
here
. The issue occurred for me when tuning on one validation sample in a Tweedie glm with log link, I am not sure how specific it is to this setting.


Based on these results, I would tend to always use grid search to determine 
lambda
. Is this appropriate? Alternatively, is there some option in 
h2o.glm()
 that addresses the issue with 
lambda_search
?


rm(list = ls())
library(h2o)
library(tweedie)
library(tidyverse)

# Configuration -----------------------------------------------------------
# DGP:
n = 1000
k = 10
phi = 1
const = 0
bet = seq(-1, 1, length.out = k)
power = 1.5

# algorithm
alpha = 0.5

# Generate some data ------------------------------------------------------
set.seed(42)

x = rnorm(n * k) %>% 
  matrix(nrow = n, dimnames = list(NULL, paste0(""x"", seq(1, k))))
mu = as.numeric(exp(const + x %*% bet))

dat = x %>% 
  as_tibble() %>% 
  mutate(mu = mu,
         y  = rtweedie(n, 
                       mu = mu,
                       phi = phi, 
                       power = power),
         id = row_number(),
         sample = case_when(
           id <= n / 2 ~ ""train"",
           TRUE ~ ""valid""))

# Initialize H2O ----------------------------------------------------------
h2o.init()

df_h2o_train = dat %>% 
  filter(sample == ""train"") %>% 
  as.h2o()

df_h2o_valid = dat %>% 
  filter(sample == ""valid"") %>% 
  as.h2o()


# Tune lambda -------------------------------------------------------------
# 1. Lambda search
glm_warmstart = h2o.glm(
  x                      = paste0(""x"", seq(1, k)),
  y                      = ""y"",
  family                 = ""tweedie"",
  tweedie_variance_power = power,
  tweedie_link_power     = 0,
  training_frame         = df_h2o_train,
  validation_frame       = df_h2o_valid,
  alpha                  = alpha,
  lambda_search          = TRUE,
  early_stopping         = FALSE
)

lambda_warmstart = glm_warmstart@model$lambda_best 
print(lambda_warmstart) # 0.1501327

# 2. Grid search
hyper_params = list(lambda = glm_warmstart@model$scoring_history$lambda %>% 
                      h2o.asnumeric())

grid_search = h2o.grid(""glm"",
                       hyper_params           = hyper_params,
                       x                      = paste0(""x"", seq(1, k)),
                       y                      = ""y"",
                       family                 = ""tweedie"",
                       tweedie_variance_power = power,
                       tweedie_link_power     = 0,
                       training_frame         = df_h2o_train,
                       validation_frame       = df_h2o_valid,
                       alpha                  = alpha,
                       lambda_search          = FALSE)

lambda_grid_search = grid_search@summary_table %>% 
  as_tibble() %>%
  head(1) %>% 
  pull(lambda) %>% 
  stringr::str_sub(2, -2) %>% 
  as.numeric()
print(lambda_grid_search) # 0.013

glm_grid_search = h2o.glm(
  x                      = paste0(""x"", seq(1, k)),
  y                      = ""y"",
  family                 = ""tweedie"",
  tweedie_variance_power = power,
  tweedie_link_power     = 0,
  training_frame         = df_h2o_train,
  alpha                  = alpha,
  lambda                 = lambda_grid_search)

# Compare validation deviance ---------------------------------------------
dat %>% 
  filter(sample == ""valid"") %>% 
  mutate(pred_warmstart = as.vector(h2o.predict(glm_warmstart,
                                             newdata = df_h2o_valid)),
         pred_grid_search  = as.vector(h2o.predict(glm_grid_search,
                                             newdata = df_h2o_valid)),
         deviance_warmstart = tweedie.dev(y, pred_warmstart, power),
         deviance_grid_search = tweedie.dev(y, pred_grid_search, power)) %>% 
  summarise(
    mean_deviance_warmstart = mean(deviance_warmstart), # 1.16
    mean_deviance_grid_search = mean(deviance_grid_search) # 1.08
  )

# Close -------------------------------------------------------------------
h2o.shutdown(prompt = FALSE)","['r', 'cross-validation', 'h2o', 'glm']",Unknown,,N/A
56430009,56430009,2019-06-03T15:03:14,2019-06-08 22:25:08Z,181,"I am working on the visualization of a regression tree using H2O in Python. 
https://github.com/h2oai/h2o-tutorials/blob/master/tutorials/display_tree_mojo/DisplayTreeMojoH2O.ipynb


The example above provides a method of using PrintMOJO. However the output figure does not include the sample size of each node (and the average value of the node). In SKLearn the sample size and average values are included but I don't know how to integrate them in H2O.


This is what I am looking for:

http://www.netinstructions.com/content/images/2015/07/decision-tree-visualized.png


Any idea?


I looked for a lot of resources but I don't know which option or variable should I play with to make that happen. If I switch to R does that make things easier?","['python', 'h2o', 'mojo']",Hongyu Lu,https://stackoverflow.com/users/11594453/hongyu-lu,11
56424096,56424096,2019-06-03T09:00:35,2019-06-03 09:00:35Z,57,"I have a  problem. I want to make a ensemble model. The base models are gbm and neural network(nn). At the first time, I train these two base models and make ensemble model. And I save these two base models. The ensemble model can work. When I close the R Script and open a new file at the second day. First, I load these two base models. Howerver, I want two make ensemble model by using these two base models, which are saved in the first time. It will give the error: java.lang.NullPointerException. How can I solve this problem. Do I need to retrain these two base models before making ensemble model every time? This is so time-consuming.","['h2o', 'ensemble-learning']",Zhe.Y,https://stackoverflow.com/users/8545995/zhe-y,3
56382712,56382712,2019-05-30T17:11:11,2019-05-30 17:11:11Z,58,"I see h2o has model serving (as documented in ""Productionizing H2O"" at 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html#
) but I don't see any reference to serving the feature pipeline. 


Does h2o have a feature serving methodology? (for example, TensorFlow has feature serving via Apache Beam, which is served along with the model via TFServing.)",['h2o'],CoderOfTheNight,https://stackoverflow.com/users/2226322/coderofthenight,"1,112"
56346607,56346607,2019-05-28T16:33:34,2019-05-28 16:35:43Z,156,"I am trying out the distributed random forest implementation of H2O using sparkling-water. But I am facing the following error when I run the spark-submit command.


Exception in thread ""H2O Launcher thread"" java.lang.ExceptionInInitializerError
    at water.init.NetworkInit.initializeNetworkSockets(NetworkInit.java:77)
    at water.H2O.startLocalNode(H2O.java:1621)
    at water.H2O.main(H2O.java:2081)
    at water.H2OStarter.start(H2OStarter.java:22)
    at water.H2OStarter.start(H2OStarter.java:47)
    at org.apache.spark.h2o.backends.internal.InternalBackendUtils$$anonfun$6$$anon$1.run(InternalBackendUtils.scala:173)
Caused by: java.lang.IllegalStateException: HTTP Server cannot be loaded: No implementation of HttpServerFacade found on classpath. Please refer to https://0xdata.atlassian.net/browse/TN-13 for details.
    at water.webserver.iface.HttpServerLoader.<clinit>(HttpServerLoader.java:16)
    ... 6 more



I have tried out the solution mentioned at the location 
https://0xdata.atlassian.net/browse/TN-13


but for some reason it still isn't able to find the 
ai.h2o:h2o-jetty-8
 on the classpath.",['h2o'],user238607,https://stackoverflow.com/users/3238085/user238607,"2,428"
56334963,56334963,2019-05-28T04:22:21,2019-05-28 04:22:21Z,403,"In Python, while downloading MOJO file from h2o, how to rename the jar file.
if I run below code, the zip file and jar of the model being saved, 


modelfile = model.download_mojo(path=""c:/models/"", get_genmodel_jar=True)



but model file is not saved, I have to save it separately using h2o.save_model


rf_path = h2o.save_model(model=model, path=""c:/models/"", force=True)



jar file name is saving as ""h2o-genmodel.jar""


I want to rename the jar file while saving, I tried below code, this saves the zip file and model file, Not the jar file


modelfile = model.download_mojo(path=""c:/models/"", get_genmodel_jar=True, genmodel_name = ""my_jar"")



Is this an issue?


I except to rename my jar file?
And If I can save zip, model and jar file in one go may be better?","['python', 'h2o']",hanzgs,https://stackoverflow.com/users/11053801/hanzgs,"1,596"
56323306,56323306,2019-05-27T09:31:07,2019-05-29 10:49:05Z,350,"If the training_frame, validation_frame and nfolds exist at the same time. How gbm train and valid by using h2o.gbm","['validation', 'h2o', 'gbm']",Zhe.Y,https://stackoverflow.com/users/8545995/zhe-y,3
56297085,56297085,2019-05-24T17:20:45,2020-10-14 21:11:01Z,0,"When I train a model in R using autoML, I can view the leaderboard of the models via


automl_model@leaderboard



and I can get access to the best model via


automl_model@leader



However, I want also make experiments with the 2nd best model, 3rd best model, and so on. How can I access them?","['r', 'h2o', 'automl']",Dan,https://stackoverflow.com/users/1011724/dan,45.7k
56288957,56288957,2019-05-24T08:46:12,2019-05-24 10:34:39Z,0,"I would like to set UTF8 in H2O Flow UI.


In terminal, I can set UTF8 and open H2O Flow UI.


java -jar -Dfile.encoding=UTF-8 h2o.jar



but I normally use R.
How can I set UTF8 and start H2O Flow UI via R?


I tried that following.


options(encoding = ""utf-8"")
h2o.init()



and


h2o.init(extra_classpath = ""-Dfile.encoding=UTF-8"")



It runs, but doesn't set UTF8","['r', 'h2o']",JJJJ,https://stackoverflow.com/users/11525628/jjjj,5
56271717,56271717,2019-05-23T09:07:30,2019-05-25 19:22:42Z,0,"I've installed h2o in R and when I run the command:


h2o.init()



the R console completely hangs with no message at all being given. I have to restart R in order to get out of this.


If I run 
java -jar h2o.jar
 in the command prompt and it ends with 
cloud of size 2 formed [/172.17.132.30:54323, /172.17.132.30:54325]


so I then tried to run 
h2o.init(port = 54323)
 and it seems to work fine.


However, when I try to do demo's e.g. 
demo(h2o.kmeans)
 it starts the demo in 
localhost\54321
 which doesn't exist. My question is:


Why doesn't the usual port 54321 work? What does the message 
cloud of size 2 formed [/172.17.132.30:54323, /172.17.132.30:54325]
 mean and why is there 2 ports? How do I set it to default to a working port from R directly as it seems to be able to find the correct port using the 
java -jar h2o.jar
 command? How can I get the 
demo(h2o.kmeans)
 to work with the correct port?","['r', 'h2o']",user33484,https://stackoverflow.com/users/4504877/user33484,570
56265273,56265273,2019-05-22T21:30:48,2019-05-22 21:30:48Z,193,"We are producitonalizing a model that was created using h2o. However, during one of our load tests, we are getting a 

ERRR: water.AutoBuffer$AutoBufferException
 


What exactly is the AutoBufferException and what causes it to be thrown?


Previously, when we deployed the application, we did not provide any Java heap memory parameters and during load testing, that always caused the instance to terminate due to an OOM error. However, since adding the Java heap memory parameter and conducting load tests, we have been getting the AutoBufferException under heavy load. 


When we look at the Debug logs, we see that there is plenty of memory still available when this Exception is thrown. CPU usage seems to be pretty normal as well. Below is a snippet of our code and the stacktrace that the error produces.


According to the logs, h2o makes a series of web requests in order to complete the prediction. The exception only gets thrown during one of those calls: 
(Request Type: POST, Request Path: /99/Models.bin)
. Once the exception gets thrown, the request fails and the service picks up the next request.




h2o Version: 3.18.0.11


Python Version: 3.5.4


OS: RHEL 7.x




h2o.connect(ip = HOST, port = PORT)
loaded_model = h2o.load_model(MODEL_PATH)

def predict(to_be_scored):
    to_be_scored_hex = h2o.H2OFrame(to_be_scored)
    prediction = loaded_model.predict(to_be_scored_hex)
    return prediction



We would expect the code to return a prediction, however the following stacktrace is the output.


Stacktrace: [water.AutoBuffer$AutoBufferException,
water.AutoBuffer.getImpl(AutoBuffer.java:634),
water.AutoBuffer.getSp(AutoBuffer.java:610),
water.AutoBuffer.get1(AutoBuffer.java:749),
water.AutoBuffer.get1U(AutoBuffer.java:750),
water.AutoBuffer.getInt(AutoBuffer.java:829),
water.AutoBuffer.get(AutoBuffer.java:793),
water.AutoBuffer.getKey(AutoBuffer.java:811),
water.AutoBuffer.getKey(AutoBuffer.java:808),
hex.Model.readAll_impl(Model.java:1635),
hex.tree.SharedTreeModel.readAll_impl(SharedTreeModel.java:411),
water.AutoBuffer.getKey(AutoBuffer.java:814),
water.Keyed.readAll(Keyed.java:50),
hex.Model.importBinaryModel(Model.java:2256),
water.api.ModelsHandler.importModel(ModelsHandler.java:209),
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source),
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43),
java.lang.reflect.Method.invoke(Method.java:498),
water.api.Handler.handle(Handler.java:63),
water.api.RequestServer.serve(RequestServer.java:451),
water.api.RequestServer.doGeneric(RequestServer.java:296),
water.api.RequestServer.doPost(RequestServer.java:222),
javax.servlet.http.HttpServlet.service(HttpServlet.java:755),
javax.servlet.http.HttpServlet.service(HttpServlet.java:848),
org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684),
org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503),
org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086),
org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429),
org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020),
org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135),
org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154),
org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116),
water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:197),
org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154),
org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116),
org.eclipse.jetty.server.Server.handle(Server.java:370),
org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494),
org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53),
org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982),
org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043),
org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865),
org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240),
org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72),
org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264),
org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608),
org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543),
java.lang.Thread.run(Thread.java:748)];parms={dir=/opt/h2o/data/model, model_id=}",['h2o'],Shank,https://stackoverflow.com/users/11541663/shank,21
56215238,56215238,2019-05-20T06:16:08,2019-05-20 17:24:52Z,58,"I want to use other library like tensorflow in H2O flow UI.


how can i install and import new library?


I found this project

https://github.com/gdtm86/sparklingwater-examples/tree/master/h2o-examples-flow_ui


and I tried this code.


import org.apache.spark.sql.dataFrame



and this is error.


[stdin]:2:13: error: unexpected .
  import org.apache.spark.sql.dataFrame
            ^",['h2o'],JJJJ,https://stackoverflow.com/users/11525628/jjjj,5
56198009,56198009,2019-05-18T10:14:46,2019-08-27 14:41:03Z,237,"I am trying to use multiprocessing in python (version 3.6.8) where 2 tables are to be called as input in a function. Within the function I fit models through h2o. Here is my code:


def innerFold(params, feats, target, h2o_train_inner, h2o_test_inner, outer_fold, inner_fold):

    inner_scores = []

    for param in params:

        counter = param.get('counter')
        del param['counter']

        print('parameter combination: ', param)
        print('COUNTER: ', counter)

        #define model and fit
        gbm = H2OGradientBoostingEstimator(stopping_rounds = 5,
                                           stopping_metric = 'rmse',
                                           stopping_tolerance = 1e-4,
                                           seed = random_state,
                                           **param)

        print('TRAINING STARTS....')
        gbm.train(x = feats,
                  y = target,
                  training_frame = h2o_train_inner)


        score = gbm.model_performance(h2o_test_inner).r2()
        pd_scores = pd_scores.append({'outer_fold': int(outer_fold),
                                      'inner_fold': int(inner_fold),
                                      'score': score,
                                      'param_idx': int(counter)},
                                     ignore_index=True)

        inner_scores.append(gbm.model_performance(h2o_test_inner).r2())

    return pd_scores



I split 'params' argument into 5 as to be processed by 5 different cores and the rest of the arguments should be identical. To this end, I split 'params' as below:


df0 = np.array_split(param_combs,5)[0] 
df1 = np.array_split(param_combs,5)[1] 
df2 = np.array_split(param_combs,5)[2] 
df3 = np.array_split(param_combs,5)[3] 
df4 = np.array_split(param_combs,5)[4]



And introduce them as follows:


args = [(df0, feats, target, h2o_train_inner, h2o_test_inner, outer_fold, inner_fold), 
        (df1, feats, target, h2o_train_inner, h2o_test_inner, outer_fold, inner_fold), 
        (df2, feats, target, h2o_train_inner, h2o_test_inner, outer_fold, inner_fold),
        (df3, feats, target, h2o_train_inner, h2o_test_inner, outer_fold, inner_fold),
        (df4, feats, target, h2o_train_inner, h2o_test_inner, outer_fold, inner_fold)]



,where feats, target, h2o_train_inner, h2o_test_inner, outer_fold, inner_fold are type of list (containing dictionaries), string, h2o dataframe, h2o dataframe, int, int, respectively.


Eventually, I start the process as below:


p = mp.Pool(processes=5)
pool_results = p.starmap(innerFold, args)



And I get:




TypeError: 
new
() missing 1 required positional argument: 'keyvals'




It seems the number of arguments is OK. What am I missing here?


EDIT:
 Apparently the problem stems from H2O dataframes. If I convert them to pandas df, it works. Any idea how I can directly use H2O df?


EDIT2:
 As far as I understand, arguments sent to the function (e.g. innerFold above) are pickled. Since h2o object cannot be pickled, the function worked after it had been converted to pandas df.","['python-3.x', 'multiprocessing', 'h2o']",Unknown,,N/A
56181979,56181979,2019-05-17T07:51:27,2019-05-20 08:54:58Z,0,"According to the h2o documentation, I can set 
keep_cross_validation_predictions = T
 to get the cross validation predictions from my 
automl
 model.


But I cannot get it to work.


Using this example from the documentation


library(h2o)

h2o.init()

# Import a sample binary outcome train/test set into H2O
train <- h2o.importFile(""https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv"")
test <- h2o.importFile(""https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv"")

# Identify predictors and response
y <- ""response""
x <- setdiff(names(train), y)

# For binary classification, response should be a factor
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])

# Run AutoML for 20 base models (limited to 1 hour max runtime by default)
aml <- h2o.automl(x = x, y = y,
                  training_frame = train,
                  max_models = 20,
                  keep_cross_validation_predictions = TRUE,
                  seed = 1)



After running the model, I tried


h2o.cross_validation_predictions(aml)
h2o.cross_validation_predictions(aml@leader)

h2o.cross_validation_holdout_predictions(aml)
h2o.cross_validation_holdout_predictions(aml@leader)



but none of it works.


edit
 I am using the latest stable 3.24.02","['r', 'h2o', 'automl']",Unknown,,N/A
56140284,56140284,2019-05-15T00:27:43,2019-05-20 18:07:28Z,390,"Having problem where an H2O DRF model is treating a field type as an 
int
 when the field type that was set when the model was being trained was an 
enum
.


When using the H2O 
tree API
 to examine some of the individual trees in a trained DRF model, I can see that for some types that were 
explicitly set as 
enum
 when the model was trained (ie. the pandas dataframe was converted to an 
H2OFrame
 where certain fields were set to a particular type with a 
column_types
 map parameter), they 
appear to be being treated as 
int
s
 when doing something like


root_node.features
> observe that the feature being examined for this node is one of the features set to be categorical enum by the H2OFrame that the model was trained on
tree.root_node.features
> some_categorical
tree.root_node.levels
> []
root_node.threshold
> some number



More compactly


print(tree.root_node)

Node ID 0 
Left child node ID = 1 Right child node ID = 2 
Splits on column some_categorical 
Split threshold < 2562.5 to the left node, >= 2562.5 to the right node 
NA values go to the LEFT



yet for other nodes (for the same model) we (correctly) see


tree.root_node.features
> some_other_categorical
tree.root_node.levels
> ['cat1', ..., 'catn']
root_node.threshold
> na



Initially I had thought that this just appeared to be treated as an int because of how categorical values are internally represented in H2O 




enum or Enum: Leave the dataset as is, 
internally map the strings to
  integers
, and use these integers to make splits - either via ordinal
  nature when nbins_cats is too small to resolve all levels or via
  bitsets that do a perfect group split. Each category is a separate
  category; its name (or number) is irrelevant. For example, after the
  strings are mapped to integers for Enum, you can split {0, 1, 2, 3, 4,
  5} as {0, 4, 5} and {1, 2, 3}.




but looking at the fact that the informational output shows a greater-than threshold and no levels for determining left-right direction, you can see that there is some other problem here. 


Examining the 
column_types
 map used in the pandas-to-H2OFrame conversion and printing the types as well before training the model, we can see that the appropriate types are being set as 
enum
, so this output being seen now is confusing. Anyone know any other debugging steps that could be done here or what could be going on?",['h2o'],Unknown,,N/A
56133829,56133829,2019-05-14T15:21:35,2019-05-14 18:22:08Z,0,"I'm doing a binary classification with a GBM using the h2o package. I want to assess the predictive power of a certain variable, and if I'm correct I can do so by comparing the AUC of a model with the specific variable and a model without the specific variable.


I'm taking the titanic dataset as an example.


So my hypothesis is:
Age has significant predictive value whether someone will survive.


df <- h2o.importFile(path = ""http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv"")
response <- ""survived""
df[[response]] <- as.factor(df[[response]])           
## use all other columns (except for the name) as predictors
predictorsA <- setdiff(names(df), c(response, ""name"")) 
predictorsB <- setdiff(names(df), c(response, ""name"", ""age"")) 

splits <- h2o.splitFrame(
  data = df, 
  ratios = c(0.6,0.2),   ## only need to specify 2 fractions, the 3rd is implied
  destination_frames = c(""train.hex"", ""valid.hex"", ""test.hex""), seed = 1234
)
train <- splits[[1]]
valid <- splits[[2]]
test  <- splits[[3]]

gbmA <- h2o.gbm(x = predictorsA, y = response, distribution=""bernoulli"", training_frame = train)
gbmB <- h2o.gbm(x = predictorsB, y = response, distribution=""bernoulli"", training_frame = train)

## Get the AUC
h2o.auc(h2o.performance(gbmA, newdata = valid))
[1] 0.9631624
h2o.auc(h2o.performance(gbmB, newdata = test))
[1] 0.9603211



I know the pROC package has a 
roc.test
 function to compare AUC of two ROC curves and I would like to apply this function on the outcomes of my h2o model.","['r', 'h2o', 'roc', 'proc-r-package']",Calimo,https://stackoverflow.com/users/333599/calimo,"7,870"
56120443,56120443,2019-05-13T22:05:20,2019-05-16 04:15:04Z,0,"I am trying to launch h2o in Python.  I can get it to work in R, so I'm not sure why its not working within Python.  Below is the error message that I receive from Python.


#R
library(h2o)
h2o.init()
 everything goes great

#Python
import h2o
h2o.init()

error below






Checking whether there is an H2O instance running at http://localhost:54321..... not found.
Attempting to start a local H2O server...
  Java Version: java version ""10.0.1"" 2018-04-17; Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10); Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode)
  Starting server from /anaconda3/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar
  Ice root: /var/folders/lj/gf5ntj5j2lq9tqwjx5yhxtw8000ym1/T/tmpawb7odqt



H2OConnectionError                        Traceback (most recent call last)
/anaconda3/lib/python3.6/site-packages/h2o/h2o.py in init(url, ip, port, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, **kwargs)
    251                                      _msgs=(""Checking whether there is an H2O instance running at {url}"",
--> 252                                             ""connected."", ""not found.""))
    253     except H2OConnectionError:

/anaconda3/lib/python3.6/site-packages/h2o/backend/connection.py in open(server, url, ip, port, https, auth, verify_ssl_certificates, proxy, cookies, verbose, _msgs)
    317             conn._timeout = 3.0
--> 318             conn._cluster = conn._test_connection(retries, messages=_msgs)
    319             # If a server is unable to respond within 1s, it should be considered a bug. However we disable this

/anaconda3/lib/python3.6/site-packages/h2o/backend/connection.py in _test_connection(self, max_retries, messages)
    587             raise H2OConnectionError(""Could not establish link to the H2O cloud %s after %d retries\n%s""
--> 588                                      % (self._base_url, max_retries, ""\n"".join(errors)))
    589 

H2OConnectionError: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries
[18:49.71] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1189ba048>: Failed to establish a new connection: [Errno 61] Connection refused',))
[18:49.93] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1208284e0>: Failed to establish a new connection: [Errno 61] Connection refused',))
[18:50.13] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x120835da0>: Failed to establish a new connection: [Errno 61] Connection refused',))
[18:50.35] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x120835240>: Failed to establish a new connection: [Errno 61] Connection refused',))
[18:50.56] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x11fc01080>: Failed to establish a new connection: [Errno 61] Connection refused',))

During handling of the above exception, another exception occurred:

H2OServerError                            Traceback (most recent call last)
<ipython-input-198-95453bf1556d> in <module>()
----> 1 h2o.init()

/anaconda3/lib/python3.6/site-packages/h2o/h2o.py in init(url, ip, port, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, **kwargs)
    259             raise H2OConnectionError('Can only start H2O launcher if IP address is localhost.')
    260         hs = H2OLocalServer.start(nthreads=nthreads, enable_assertions=enable_assertions, max_mem_size=mmax,
--> 261                                   min_mem_size=mmin, ice_root=ice_root, port=port, extra_classpath=extra_classpath)
    262         h2oconn = H2OConnection.open(server=hs, https=https, verify_ssl_certificates=not insecure,
    263                                      auth=auth, proxy=proxy,cookies=cookies, verbose=True)

/anaconda3/lib/python3.6/site-packages/h2o/backend/server.py in start(jar_path, nthreads, enable_assertions, max_mem_size, min_mem_size, ice_root, port, extra_classpath, verbose)
    119         if verbose: print(""Attempting to start a local H2O server..."")
    120         hs._launch_server(port=port, baseport=baseport, nthreads=int(nthreads), ea=enable_assertions,
--> 121                           mmax=max_mem_size, mmin=min_mem_size)
    122         if verbose: print(""  Server is running at %s://%s:%d"" % (hs.scheme, hs.ip, hs.port))
    123         atexit.register(lambda: hs.shutdown())

/anaconda3/lib/python3.6/site-packages/h2o/backend/server.py in _launch_server(self, port, baseport, mmax, mmin, ea, nthreads)
    306         while True:
    307             if proc.poll() is not None:
--> 308                 raise H2OServerError(""Server process terminated with error code %d"" % proc.returncode)
    309             ret = self._get_server_info_from_logs()
    310             if ret:

H2OServerError: Server process terminated with error code 1



Can anyone give me some specific pointers on how to fix this ?  I am using the most recent stable release for Python.","['python', 'h2o']",runningbirds,https://stackoverflow.com/users/3788557/runningbirds,"6,565"
56115850,56115850,2019-05-13T15:50:46,2020-04-01 08:18:44Z,168,"I am using a static IP and are now accessing the server with Putty. So I try to use h2o-dai with putty, but it does not work with xxx.xxx.xx.xxx:12345.


I used tar_sh.sh when installing.


So, when I set up ./dai_env.sh, I see no_proxy is localhost, 127.0.0.1 and NO_PROXY is


What is this and why is not it?


and 
How do I resolve it?","['proxy', 'h2o', 'driverless-ai']",Unknown,,N/A
56086214,56086214,2019-05-11T00:17:39,2019-05-11 00:17:39Z,125,"I am looking to implement a constrained conditional model in H2O. Are there any resources that can help me get started on implementing a new framework in H2O ?


I have been looking at the github repo. Would I start by looking at implementations of hex.Model as a starting point to understand how to implement?


If there are any samples, etc., would be much appreciated.",['h2o'],raylite3,https://stackoverflow.com/users/9950406/raylite3,33
56064670,56064670,2019-05-09T17:29:55,2019-05-09 20:22:02Z,315,"I am trying to merge two datasets. The dataset on the left contains data about input variables. The dataset on the right contains data about output variable. The two datasets has a common column that contains data of type string. I am trying to merge them into a single dataset, suign the common column, in H2O Flow. When I call the merge operation I get following error:


ERROR MESSAGE: DistributedException from /10.151.9.92:54321: 'Operation not allowed on string vector.'


H2O is running on my local machine.",['h2o'],Ayondeep Datta,https://stackoverflow.com/users/8382591/ayondeep-datta,111
56036749,56036749,2019-05-08T08:41:04,2019-05-08 19:43:37Z,60,"I have a table of 5360*51200 size. Here, 5360 are the number of instances and 51200 are the number of features. I need to reduce the dimension of features. I was trying it by the help of stacked autoencoder in H2o, but it did not allow me to train to raise an error as:


Model is a large and large number of parameters



Here is the code:


library(h2o)
h2o.init(nthreads = -1)

check.deeplearning_stacked_autoencoder <- function() {
  # this function builds a vector of autoencoder models, one per layer
  #library(h2o)
  #h2o.init()
  get_stacked_ae_array <- function(training_data, layers, args) {
    vector <- c()
    index = 0
    for (i in 1:length(layers)) {
      index = index + 1
      ae_model <- do.call(h2o.deeplearning,
                          modifyList(
                            list(
                              x = names(training_data),
                              training_frame = training_data,
                              autoencoder = T,

                              hidden = layers[i]
                            ),
                            args
                          ))
      training_data = h2o.deepfeatures(ae_model, training_data, layer =
                                         3)

      names(training_data) <-
        gsub(""DF"", paste0(""L"", index, sep = """"), names(training_data))
      vector <- c(vector, ae_model)
    }
    cat(
      length(vector))
  }

  # this function returns final encoded contents
  apply_stacked_ae_array <- function(data, ae) {
    index = 0
    for (i in 1:length(ae)) {
      index = index + 1
      data = h2o.deepfeatures(ae[[i]], data, layer = 3)
      names(data) <-
        gsub(""DF"", paste0(""L"", index, sep = """"), names(data))
    }
    data
  }

  TRAIN <-
    ""E:/Chiranjibi file/Geometric features/Lu/Train/d_features.csv""
  TEST <-
    ""E:/Chiranjibi file/Geometric features/Lu/Test/d_features.csv""
  response <- 51201

  # set to T for RUnit
  # set to F for stand-alone demo
  if (T) {
    train_hex <- h2o.importFile((TRAIN))
    test_hex  <- h2o.importFile((TEST))
  } else 
  {
    library(h2o)
    h2o.init()
    homedir <-
      paste0(path.expand(""~""), ""/h2o-dev/"") #modify if needed
    train_hex <-
      h2o.importFile(path = paste0(homedir, TRAIN),
                     header = F,
                     sep = ',')
    test_hex  <-
      h2o.importFile(path = paste0(homedir, TEST),
                     header = F,
                     sep = ',')
  }
  train <- train_hex[, -response]
  test  <- test_hex [, -response]
  train_hex[, response] <- as.factor(train_hex[, response])
  test_hex [, response] <- as.factor(test_hex [, response])

  ## Build reference model on full dataset and evaluate it on the test set
  model_ref <-
    h2o.deeplearning(
      training_frame = train_hex,
      x = 1:(ncol(train_hex) - 1),
      y = response,
      hidden = c(67),
      epochs = 50
    )
  p_ref <- h2o.performance(model_ref, test_hex)
  h2o.logloss(p_ref)

  ## Now build a stacked autoencoder model with three stacked layer AE models
  ## First AE model will compress the 717 non-const predictors into 200
  ## Second AE model will compress 200 into 100
  ## Third AE model will compress 100 into 50
  layers <- c(50000,20000,10000,5000,2000, 1000, 500)
  args <- list(activation = ""Tanh"",
               epochs = 1,
               l1 = 1e-5)
  ae <- get_stacked_ae_array(train, layers, args)

  ## Now compress the training/testing data with this 3-stage set of AE models
  train_compressed <- apply_stacked_ae_array(train, ae)
  test_compressed <- apply_stacked_ae_array(test, ae)

  ## Build a simple model using these new features (compressed training data) and evaluate it on the compressed test set.
  train_w_resp <- h2o.cbind(train_compressed, train_hex[, response])
  test_w_resp <- h2o.cbind(test_compressed, test_hex[, response])
  model_on_compressed_data <-
    h2o.deeplearning(
      training_frame = train_w_resp,
      x = 1:(ncol(train_w_resp) - 1),
      y = ncol(train_w_resp),
      hidden = c(67),
      epochs = 1
    )
  p <- h2o.performance(model_on_compressed_data, test_w_resp)
  h2o.logloss(p)


}
#h2o.describe(train)

#doTest(""Deep Learning Stacked Autoencoder"", check.deeplearning_stacked_autoencoder)","['h2o', 'dimension', 'reduction']",Martin Evans,https://stackoverflow.com/users/4985733/martin-evans,46.7k
56027486,56027486,2019-05-07T17:10:06,2019-05-09 20:30:31Z,0,"I am trying to import redshift table to H20 flow and using the following URL as in other db editors ie.


jdbc:redshift://xxxx.xxxxzubx6zm.us-west-2.redshift.amazonaws.com:5439/dev in the URL string where it does mention use the jdbc format of url


But it is failing with : 


java.lang.RuntimeException: SQLException: No suitable driver found for jdbc://


How to add a suitable driver if required and is it possible to connect Redshift cluster to H2O?","['python', 'amazon-redshift', 'h2o']",Viv,https://stackoverflow.com/users/7529256/viv,"1,584"
56009244,56009244,2019-05-06T16:39:26,2019-05-14 19:24:03Z,0,"I've looked into the h2o.predict_contributions function that exposes the Shap values from xgb and gbm models.   Does this function also provide these metrics from cross validation predictions?  I can't seem to find them.


library(h2o)
library(mlbench)
data(Sonar)

Sonar.h2o = as.h2o(Sonar)

mdl = h2o.xgboost(x=names(Sonar), y='Class', training_frame = Sonar, nfolds=5, keep_cross_validation_predictions = TRUE)","['r', 'h2o']",runningbirds,https://stackoverflow.com/users/3788557/runningbirds,"6,565"
56008884,56008884,2019-05-06T16:12:38,2020-07-07 09:29:30Z,0,H2O Driverless AI serves its models using AWS Lambda Function. I'm wondering if there's a possible integration between H2O and Watson OpenScale.,"['aws-lambda', 'ibm-watson', 'h2o', 'watson-openscale']",Bufan Zeng,https://stackoverflow.com/users/11239116/bufan-zeng,33
55954142,55954142,2019-05-02T13:59:53,2019-05-03 01:53:14Z,91,"Often I'm unsure to what extent to preprocess my data while using DAI. Often you want to reduce the dimensionality, rid duplicate features, standardize/normalize, etc... for a production level model. Is there a rule at which I should stop personal preprocessing in favor of DAI (I.E. Only rid a binary classification algorithm of Nan's and DAI will do the rest). Will it explicitly explain which normalization technique it used, like a MinMaxScaler() from Sklearn for example?","['h2o', 'driverless-ai']",Unknown,,N/A
55941654,55941654,2019-05-01T19:32:05,2019-05-02 12:44:52Z,0,"i have a usecase of very imbalace data set , i undersampled the training dataset ,
and tried running the automl in h2o, but it gave me great AUC results (over 0.99) but very bad aup_pr results (0.09).
is it related to the imbalance issue ? 
i ran with weight_column option (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/weights_column.html
) 
but it didn't help.
should i use the balance_classes option instead (when i run both options it fails with ""h2oFrame is empty"" message) .
the train and test are splitted on date time range , and the test dataset has the proper ration between majority and minority classes.",['h2o'],user1450410,https://stackoverflow.com/users/1450410/user1450410,301
55930870,55930870,2019-05-01T02:15:54,2019-05-01 14:49:28Z,300,"I am reading through the example of calibrating probabilities from h2o documentation 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/calibrate_model.html


Since the example is poorly explained, my question is:




do we have to have weights (weights_column) in the training set?


if so, what do these weights do?




I also tried to leave out the weights. The code still runs but the results are very different. Any insight would be appreciated","['python', 'h2o']",Wai Ha Lee,https://stackoverflow.com/users/1364007/wai-ha-lee,"8,775"
55893482,55893482,2019-04-28T18:45:39,2019-06-21 08:33:07Z,859,"hi guys recently i've encounter this problem in google colab, and i dont know how to solve this problem


import h2o
h2o.init

data_train = ""https://drive.google.com/open?id=14OYzA93IkuKPlkdvXs9Od53HyopgSQ2u""
data_train_df = h2o.import_file(path=data_train)



i following a book that suggest these following codes, but somehow it wont work.","['import', 'h2o', 'google-colaboratory']",GGets,https://stackoverflow.com/users/2243682/ggets,416
55882841,55882841,2019-04-27T16:57:17,2019-04-27 16:57:17Z,0,"I'm at a loss as to why this seems so challenging, but isn't that what always separates us into two piles. 


Here's what I have;


    - Today April 27th 2019
    - MacOSX - High Sierra version 10.13.6
    -- older MacBook Air 2010 build
    - RStudio - Version 1.1.453
    - h2o downloaded from within RStudio
    -- from http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R/src/contrib/h2o_3.24.0.2.tar.gz
    -- installed in RStudio and library active



attempting to run 


h2o.init()

H2O is not running yet, starting it now...
Error in .h2o.startJar(ip = ip, port = port, name = name, nthreads = nthreads,  : 
  Your java is not supported: java version ""1.6.0_65""
Please download the latest Java SE JDK 8 from the following URL:
http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html



Slightly more elaborate version provides exact same result


h2o.init(ip = ""localhost"", port = 54321, name = NA_character_,
         startH2O = TRUE, forceDL = FALSE, enable_assertions = TRUE,
         license = NULL, nthreads = -1, max_mem_size = NULL,
         min_mem_size = NULL, ice_root = tempdir(), log_dir = NA_character_,
         log_level = NA_character_, strict_version_check = TRUE,
         proxy = NA_character_, https = FALSE, insecure = FALSE,
         username = NA_character_, password = NA_character_,
         cookies = NA_character_, context_path = NA_character_,
         ignore_config = FALSE, extra_classpath = NULL,
         jvm_custom_args = NULL, bind_to_localhost = TRUE)



from terminal


java -version
openjdk version ""1.8.0_212""
OpenJDK Runtime Environment (Zulu 8.38.0.13-CA-macosx) (build 1.8.0_212-b04)
OpenJDK 64-Bit Server VM (Zulu 8.38.0.13-CA-macosx) (build 25.212-b04, mixed mode)



On surface it seems that h2o and the Terminal are thinking two different version of Java are the current version. 


Using SDKMAN in terminal


sdk list java
================================================================================
Available Java Versions
================================================================================
     13.ea.18-open       10.0.2-zulu         1.0.0-rc-15-grl                    
     12.0.1-sapmchn      10.0.2-open         1.0.0-rc-14-grl                    
     12.0.1-zulu         9.0.7-zulu                                             
     12.0.1-open         9.0.4-open                                             
     12.0.1.j9-adpt  > * 8.0.212-zulu                                           
     12.0.1.hs-adpt      8.0.212-amzn                                           
     12.0.1-librca       8.0.212.j9-adpt                                        
     11.0.3-sapmchn      8.0.212.hs-adpt                                        
     11.0.3-zulu         8.0.212-librca                                         
     11.0.3-amzn         8.0.202-zulu                                           
     11.0.3.j9-adpt      8.0.202-amzn                                           
     11.0.3.hs-adpt      8.0.202-zulufx                                         
     11.0.3-librca       7.0.222-zulu                                           
     11.0.2-open         7.0.181-zulu                                           
     11.0.2-zulufx       1.0.0-rc-16-grl                                        

================================================================================
+ - local version
* - installed
> - currently in use
================================================================================



Sadly, I've tried so many things from various threads that it's probably not even worth going into them. Basically after countless changes, downloads, restarts, etc.... none of them have changed the end result when trying to initiate h2o in RStudio. 


Thoughts?","['java', 'jvm', 'rstudio', 'h2o', 'macos-high-sierra']",Ghetto Counselor,https://stackoverflow.com/users/11420050/ghetto-counselor,1
55873785,55873785,2019-04-26T19:20:50,2019-04-26 19:37:24Z,0,"Hi I'm getting this error when running the AmazonReviews tutorial from NLP with H2O Tutorial


http://docs.h2o.ai/h2o-tutorials/latest-stable/h2o-world-2017/nlp/index.html


when I try to run :


# Train Word2Vec Model
from h2o.estimators.word2vec import H2OWord2vecEstimator

# This takes time to run - left commented out
#w2v_model = H2OWord2vecEstimator(vec_size = 100, model_id = ""w2v.hex"")
#w2v_model.train(training_frame=words)

# Pre-trained model available on s3: https://s3.amazonaws.com/tomk/h2o-world/megan/w2v.hex
w2v_model = h2o.load_model(""https://s3.amazonaws.com/tomk/h2o-world/megan/w2v.hex"")



I get the following error :


---------------------------------------------------------------------------
H2OResponseError                          Traceback (most recent call last)
<ipython-input-22-a55d2503e18d> in <module>
      7 
      8 # Pre-trained model available on s3: https://s3.amazonaws.com/tomk/h2o-world/megan/w2v.hex
----> 9 w2v_model = h2o.load_model(""https://s3.amazonaws.com/tomk/h2o-world/megan/w2v.hex"")

~\Anaconda3\lib\site-packages\h2o\h2o.py in load_model(path)
    989     """"""
    990     assert_is_type(path, str)
--> 991     res = api(""POST /99/Models.bin/%s"" % """", data={""dir"": path})
    992     return get_model(res[""models""][0][""model_id""][""name""])
    993 

~\Anaconda3\lib\site-packages\h2o\h2o.py in api(endpoint, data, json, filename, save_to)
    101     # type checks are performed in H2OConnection class
    102     _check_connection()
--> 103     return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
    104 
    105 

~\Anaconda3\lib\site-packages\h2o\backend\connection.py in request(self, endpoint, data, json, filename, save_to)
    400                                     auth=self._auth, verify=self._verify_ssl_cert, proxies=self._proxies)
    401             self._log_end_transaction(start_time, resp)
--> 402             return self._process_response(resp, save_to)
    403 
    404         except (requests.exceptions.ConnectionError, requests.exceptions.HTTPError) as e:

~\Anaconda3\lib\site-packages\h2o\backend\connection.py in _process_response(response, save_to)
    723         # Client errors (400 = ""Bad Request"", 404 = ""Not Found"", 412 = ""Precondition Failed"")
    724         if status_code in {400, 404, 412} and isinstance(data, (H2OErrorV3, H2OModelBuilderErrorV3)):
--> 725             raise H2OResponseError(data)
    726 
    727         # Server errors (notably 500 = ""Server Error"")

H2OResponseError: Server error java.lang.IllegalArgumentException:
  Error: Cannot find persist manager for scheme https
  Request: POST /99/Models.bin/
    data: {'dir': 'https://s3.amazonaws.com/tomk/h2o-world/megan/w2v.hex'}","['python', 'nlp', 'h2o']",lol123,https://stackoverflow.com/users/8162260/lol123,9
55863711,55863711,2019-04-26T08:31:31,2019-04-26 08:31:31Z,87,"I am creating H2o Autoencoder Anomaly Detection model in h2o python.When calculating anomalies using (test_rec_error=model.anomaly(test.hex,per_feature=false) I am getting one reconstruction error for each record.But When I am trying to predict(finding anomaly) on any test data in H2o Flow I am getting reconstruction error per feature.Is there any option in h2o Flow to get only one reconstruction error(not per feature) in h2o flow?


Also what is the REST API endpoint for getting reconstruction error from anomaly model in h2o. Just like for scoring prediction on test data in classification models API(POST /3/Predictions/models/{model}/frames/{frame}) is there..So just wanted to know whats the REST API for getting reconstruction error from anomaly model in h2o.


Thanks in advance.","['machine-learning', 'h2o', 'predictive', 'h2o4gpu']",Sarvendra Singh,https://stackoverflow.com/users/10871102/sarvendra-singh,139
55845076,55845076,2019-04-25T08:35:52,2023-02-07 14:41:48Z,0,"I'm trying to parallelize the training of multiple ML models using the autoML feature provided by H2O. The core code I'm using is the following:


library(foreach)
library(doParallel)

project_folder <- ""/home/user/Documents/""

ncores <- parallel::detectCores(logical = FALSE)
nlogiccpu <- parallel::detectCores()
max_mem_size <- ""4G""

cl<-makeCluster(nlogiccpu)

registerDoParallel(cl)

df4 <-foreach(i = as.numeric(seq(1,length(divisions))), .combine=rbind) %dopar% {
  library(dplyr)
  library(h2o)
  h2o.init(nthreads = ncores, max_mem_size = max_mem_size)

  div <- divisions[i]

  df.h2o <- as.h2o(
    df %>% filter(code == div) )

  y <- ""TARGET""
  x <- names(df.train.x.discretized)

  automl.models.h2o <- h2o.automl(
    x = x,
    y = y,
    training_frame = df.h2o,
    nfolds = 10,
    seed = 111,
    project_name = paste0(""PRJ_"", div)
  )

  leader <- automl.models.h2o@leader

  div_folder <- file.path(project_folder, paste0(""Division_"", div))
  h2o.saveModel(leader,
                path = file.path(div_folder, ""TARGET_model_bin""))
  ...
}



Only a part of all the models are trained and saved in their folder, because at some point I got the following error:




water.exceptions.H2OIllegalArgumentException: Illegal argument:
  training_frame of function: grid: Cannot append new models to a grid
  with different training input




I suppose grids are used during the autoML phase, so I tried to find a parameter to pass the 
grid_id
 as I can do in the 
h2o.grid
 function as following:


grid <- h2o.grid(“gbm”,  grid_id = paste0(“gbm_grid_id”, div),
                 ...)



but I can't find the way to do that.
The H2O package version I'm using is the 3.24.0.2.


Any suggestion?","['r', 'h2o', 'automl']",lucazav,https://stackoverflow.com/users/416988/lucazav,878
55823004,55823004,2019-04-24T05:33:44,2019-04-24 06:31:42Z,0,"I have created a R model using mlr and h2o package as below


library(h2o)
rfh20.lrn = makeLearner(""classif.h2o.randomForest"", predict.type = ""prob"")



Done the model tunings and model initiates h2o JVM and connects R to h2o cluster, modelling is done and I saved the model as .rds file.


saveRDS(h2orf_mod, ""h2orf_mod.rds"")



I do the prediction as


pred_h2orf <- predict(h2orf_mod, newdata = newdata)



then i shutdown h2o


h2o.shutdown()



Later I re-call the saved model


h2orf_mod <- readRDS(""h2orf_mod.rds"")



Initiate h2o so JVM connects R to h2o cluster


h2o.init()



Now the model is from local saved location, cluster doesn't know the model, when i do prediction, I get error which is obvious


ERROR: Unexpected HTTP Status code: 404 Not Found (url = http://localhost:54321/4/Predictions/models/DRF_model_R_1553297204511_743/frames/data.frame_sid_b520_1)

water.exceptions.H2OKeyNotFoundArgumentException
 [1] ""water.exceptions.H2OKeyNotFoundArgumentException: Object 'DRF_model_R_1553297204511_743' not found in function: predict for argument: model""
Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page, : ERROR MESSAGE: Object 'DRF_model_R_1553297204511_743' not found in function: predict for argument: model



May I know how to handle this, whether the saved model uploaded to cluster or something else, as every time building the model is NOT the effective way.","['r', 'h2o']",hanzgs,https://stackoverflow.com/users/11053801/hanzgs,"1,596"
55768013,55768013,2019-04-19T21:23:01,2020-07-04 13:10:38Z,591,"I'm trying to build an H2o docker image, using 
https://github.com/h2oai/h2o-3/blob/master/Dockerfile
.


It gets the following error: 


E: Package 'oracle-java8-installer' has no installation candidate


The command '/bin/sh -c echo 'DPkg::Post-Invoke {""/bin/rm -f /var/cache/apt/archives/
.deb || true"";};' | tee /etc/apt/apt.conf.d/no-cache &&   echo ""deb 
http://mirror.math.princeton.edu/pub/ubuntu
 xenial main universe"" >> /etc/apt/sources.list &&   apt-get update -q -y &&   apt-get dist-upgrade -y &&   apt-get clean &&   rm -rf /var/cache/apt/
 &&   DEBIAN_FRONTEND=noninteractive apt-get install -y wget unzip python-pip python-sklearn python-pandas python-numpy python-matplotlib software-properties-common python-software-properties &&   add-apt-repository -y ppa:webupd8team/java &&   apt-get update -q &&   echo debconf shared/accepted-oracle-license-v1-1 select true | debconf-set-selections &&   echo debconf shared/accepted-oracle-license-v1-1 seen true | debconf-set-selections &&   DEBIAN_FRONTEND=noninteractive apt-get install -y oracle-java8-installer &&   apt-get clean &&   wget 
http://h2o-release.s3.amazonaws.com/h2o/latest_stable
 -O latest &&   wget --no-check-certificate -i latest -O /opt/h2o.zip &&   unzip -d /opt /opt/h2o.zip &&   rm /opt/h2o.zip &&   cd /opt &&   cd 
find . -name 'h2o.jar' | sed 's/.\///;s/\/h2o.jar//g'
 &&   cp h2o.jar /opt &&   /usr/bin/pip install 
find . -name ""*.whl""
 &&   cd / &&   wget 
https://raw.githubusercontent.com/h2oai/h2o-3/master/docker/start-h2o-docker.sh
 &&   chmod +x start-h2o-docker.sh &&   wget 
http://s3.amazonaws.com/h2o-training/mnist/train.csv.gz
 &&   gunzip train.csv.gz &&   wget 
https://raw.githubusercontent.com/laurendiperna/Churn_Scripts/master/Extraction_Script.py
  &&   wget 
https://raw.githubusercontent.com/laurendiperna/Churn_Scripts/master/Transformation_Script.py
 &&   wget 
https://raw.githubusercontent.com/laurendiperna/Churn_Scripts/master/Modeling_Script.py
' returned a non-zero code: 100


I'm assuming it may be related to the recent Oracle JDK License Update.


Is there a way to resolve this error in building the docker image?","['oracle-database', 'docker', 'java-8', 'h2o']",Parvin Dadgar,https://stackoverflow.com/users/7843792/parvin-dadgar,13
55701958,55701958,2019-04-16T06:35:58,2019-04-16 17:34:39Z,0,"im tring to calculate prediction accuracy for the test data simply using dep_delay (dep_delay>30) alone as predictor in h2o


i specify the response first:


response <- ""late_arrival""



than i specify the predictor:


predictors <- filter(flights, flights$dep_delay>30)



than i used formula to calculate glm:


> flights_test_delay_glm <- h2o.glm(training_frame=flights_test, x=predictors, y=response, family=""binomial"")



and im getting this error:


Error in .verify_dataxy(training_frame, x, y) : 
  `x` must be column names or indices



i did cross check the predictors values and they look ok:


summary(predictors)


    X               year          month             day           dep_time   
 Min.   :    86   Min.   :2013   Min.   : 1.000   Min.   : 1.00   Min.   :   1  
 1st Qu.:103457   1st Qu.:2013   1st Qu.: 4.000   1st Qu.: 9.00   1st Qu.:1428  
 Median :186217   Median :2013   Median : 6.000   Median :16.00   Median :1755  
 Mean   :178012   Mean   :2013   Mean   : 6.372   Mean   :15.79   Mean   :1676  
 3rd Qu.:253087   3rd Qu.:2013   3rd Qu.: 9.000   3rd Qu.:23.00   3rd Qu.:2028  
 Max.   :336764   Max.   :2013   Max.   :12.000   Max.   :31.00   Max.   :2400  

 sched_dep_time   dep_delay          arr_time    sched_arr_time   arr_delay      
 Min.   : 500   Min.   :  31.00   Min.   :   1   Min.   :   1   Min.   : -42.00  
 1st Qu.:1334   1st Qu.:  44.00   1st Qu.:1308   1st Qu.:1457   1st Qu.:  39.00  
 Median :1645   Median :  66.00   Median :1841   Median :1841   Median :  65.00  
 Mean   :1581   Mean   :  86.82   Mean   :1598   Mean   :1730   Mean   :  83.29  
 3rd Qu.:1910   3rd Qu.: 107.00   3rd Qu.:2134   3rd Qu.:2112   3rd Qu.: 108.00  
 Max.   :2359   Max.   :1301.00   Max.   :2400   Max.   :2359   Max.   :1272.00  
                                  NA's   :216                   NA's   :386      
    carrier          flight          tailnum      origin           dest      
 EV     :11655   Min.   :   1.0   N15910 :   84   EWR:19914   ORD    : 2653  
 B6     : 8411   1st Qu.: 619.5   N258JB :   79   JFK:15241   ATL    : 2268  
 UA     : 7617   Median :1692.0   N14573 :   78   LGA:13136   BOS    : 1840  
 DL     : 4982   Mean   :2250.0   N15980 :   77               MCO    : 1814  
 MQ     : 3730   3rd Qu.:4100.0   N725MQ :   77               SFO    : 1733  
 AA     : 3537   Max.   :8500.0   N12921 :   76               FLL    : 1708  
 (Other): 8359                    (Other):47820               (Other):36275  
    air_time        distance           hour           minute     
 Min.   : 20.0   Min.   :  80.0   Min.   : 5.00   Min.   : 0.00  
 1st Qu.: 77.0   1st Qu.: 483.0   1st Qu.:13.00   1st Qu.:10.00  
 Median :120.0   Median : 762.0   Median :16.00   Median :29.00  
 Mean   :140.7   Mean   : 971.2   Mean   :15.54   Mean   :27.57  
 3rd Qu.:171.0   3rd Qu.:1134.0   3rd Qu.:19.00   3rd Qu.:45.00  
 Max.   :666.0   Max.   :4983.0   Max.   :23.00   Max.   :59.00  
 NA's   :386                                                     
               time_hour    
 2013-08-08 19:00:00:   52  
 2013-08-08 17:00:00:   51  
 2013-07-22 17:00:00:   49  
 2013-03-08 17:00:00:   48  
 2013-06-25 17:00:00:   48  
 2013-07-28 19:00:00:   48  
 (Other)            :47995  



i need help to understand if i did wrong coding into predictors value, as its simply says that i need to use dep_delay, being large than 30 as predictor.
Thank you!","['r', 'prediction', 'h2o']",NataliaK,https://stackoverflow.com/users/11284204/nataliak,85
55672999,55672999,2019-04-14T07:33:43,2019-04-14 08:34:39Z,0,"I'm trying to add new categorical variable in the frame h2o.
I have created a new variable based on some requirements and I'm trying to get new values into h2o frame, but I'm getting error.


New variable to be added:


late_arrival <- with(flights,
 ifelse(arr_delay>=30,1,
 ifelse(arr_delay<30,0,NA)))
table(late_arrival)



I'm trying to mutate it with existing h2o frame to add this new variable:


 flights_new <- select(flights.hex) %>%
mutate(late_arrival)





Error in UseMethod(""select_"") : 
        no applicable method for 'select_' applied to an object of class ""H2OFrame""




I have also tried 
collect
 function:


flights_new <- select (flights.hex, late_arrival) %>% collect()





Error in UseMethod(""select_"") : 
        no applicable method for 'select_' applied to an object of class ""H2OFrame""




How can I add new categorical variable into h2o data frame?","['r', 'variables', 'h2o']",NelsonGon,https://stackoverflow.com/users/10323798/nelsongon,13.3k
55623803,55623803,2019-04-11T02:26:22,2019-04-11 02:34:02Z,0,"I've been trying to add data point weightings to my analyses with h2o.deeplearning. 


The following code


library(h2o)
x <- rnorm(1000)
y <- x-x^2+rnorm(1000,sd=0.2)
w <- vector(length=1000)      #weights vector
w[] <- 1
dfx <- data.frame(x,y,w)
h2o.init()
dfx <- as.h2o(dfx)

H <- h2o.deeplearning(x = 1, y = 2,training_frame=dfx,weights_column = 3, hidden=c(5,4))



gives 


Error in .h2o.checkAndUnifyModelParameters(algo = algo, allParams = ALL_PARAMS,  : 
""weights_column"" must be of type character, but got numeric.



Substituting the weights_column '3' for 'w' gives the same result. I tried as.character(w), but it definitely did not like that.


Substituting weights_column with dfx[,3] or dfx$w gives:


Error in args$x_ignore[!(weights_column == args$x_ignore)] : 
  invalid subscript type 'environment'



This error has left me scratching my head, as it appears to be different to how this sort of thing is implemented in any other model. I've found no references to this error by google, and the 
documentation
 doesn't give any further explanation.","['r', 'h2o']",Ingolifs,https://stackoverflow.com/users/8968617/ingolifs,311
55621457,55621457,2019-04-10T21:17:47,2019-06-19 19:01:42Z,554,"We have created and trained model using H2O libraries. Configured H2O in OpenShift container and deployed the trained model for getting realtime inference. It worked well when we have one container. We have to scale up to handle the increase in transaction volume. Encountered an issue with the statefull nature of the H2OFrame. Please see my sample code.



Step-1: Converts the JSON dictionary in to Pandas frame.

Step-2: Converts the Pandas frame in to H2O frame.

Step-3: Run the model with H2O frame as input.




Here step-2 is returning a handle to the data stored in the container. ""H2OFrame is similar to pandas’ DataFrame, or R’s data.frame. One of the critical distinction is that the data is generally not held in memory, instead it is located on a (possibly remote) H2O cluster, and thus H2OFrame represents a mere handle to that data."" So Step-3's request must go to the same container. If not it cannot find the H2O frame and throws error.


Step-1: convert JSON dictionary to data frame using Pandas dataFrame 


 ToBeScored = pd.DataFrame([jsonDictionary])



Step-2: convert panda data frame to H2o frame  


 ToBeScored_hex = h2o.H2OFrame(ToBeScored)



Step-3: run the model  


 outPredections = rf_model.predict(ToBeScored_hex)



If the H2OFrame can be returned as an in memory object in step-2 then the statefull nature can be avoided. Is there any way?
Or, Can the H2O clustering be configured to store the H2OFrame such a way that it can be accessible from any OpenShift container in the cluster? 
 

Useful links 
 H2O’s Predict function accepts data only in H2OFrame format.
Predict function - 
http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/model_categories.html#h2o.model.model_base.ModelBase.predict
 

H2O frame data type - 
http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html


Updated on 6/19/2019 continuation question to @ErinLeDell's clarification
 

We have upgraded to H2O 3.24 and used MOJO model. Removed step 2 and replaced step 3 with this function call.


import h2o as h 
result = h.mojo_predict_csv(input_csv_path=""PredictionDataRow.csv"",mojo_zip_path=""rf_model.zip"",
genmodel_jar_path=""h2o-genmodel.jar"", java_options='-Xmx512m -XX:ReservedCodeCacheSize=256m', verbose=True) 



Internally it executed the below command which initialized a new JVM and started H2O local server for every call. H2O local server is initialized to find the path to java.


java = H2OLocalServer._find_java()   // Find java path then creates below command line

C:\Program Files (x86)\Common Files\Oracle\Java\javapath\java.exe -Xmx512m -XX:ReservedCodeCacheSize=256m -cp h2o-genmodel.jar hex.genmodel.tools.PredictCsv --mojo C:\Users\admin\Documents\Code\python\rf_model.zip --input PredictionDataRow.csv --output C:\Users\admin\Documents\Code\python\prediction.csv --decimal 



Question-1: Is there any way to use an existing JVM and not always spawn a new one for every transaction? 

Question-2: Is there a way to pass the java path to avoid the H2O local server initialization? Is H2OLocalServer required for anything other than finding java path? If it cannot be avoided then, Is it possible to initialize local server once and direct new requests to existing H2O local server instead of starting a new H2O local server?","['openshift', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
55617205,55617205,2019-04-10T16:21:00,2019-04-18 17:56:22Z,0,"I'm trying to get the the 
VIF
 scores from an 
h2o
 regression. Is there a VIF like function or data stored within 
h2o
?


Here's my example:



library(ggplot2)
library(h2o, quietly = TRUE)
library(tibble)

#build h20 sessions
h2o::h2o.init()
#>  Connection successful!

mtcars.df <- as.h2o(mtcars)
#>                                                                
  |=================================================================| 100%

#set x & y vars
y <-  ""mpg""
x <-  setdiff(dput(names(mtcars)), ""mpg"")
#> c(""mpg"", ""cyl"", ""disp"", ""hp"", ""drat"", ""wt"", ""qsec"", ""vs"", ""am"", 
#> ""gear"", ""carb"")

dput(names(mtcars))
#> c(""mpg"", ""cyl"", ""disp"", ""hp"", ""drat"", ""wt"", ""qsec"", ""vs"", ""am"", 
#> ""gear"", ""carb"")
model <- h2o.glm( y = ""mpg"", x = setdiff(dput(names(mtcars)), ""mpg""), training_frame = mtcars.df)
#> c(""mpg"", ""cyl"", ""disp"", ""hp"", ""drat"", ""wt"", ""qsec"", ""vs"", ""am"", 
#> ""gear"", ""carb"")
#> 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%

model 
#> Model Details:
#> ==============
#> 
#> H2ORegressionModel: glm
#> Model ID:  GLM_model_R_1554907509984_6 
#> GLM Model: summary
#>     family     link                              regularization
#> 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 1.0132 )
#>   number_of_predictors_total number_of_active_predictors
#> 1                         10                           9
#>   number_of_iterations    training_frame
#> 1                    1 mtcars_sid_8128_1
#> 
#> Coefficients: glm coefficients
#>        names coefficients standardized_coefficients
#> 1  Intercept    26.298144                 20.090625
#> 2        cyl    -0.447375                 -0.798977
#> 3       disp    -0.005674                 -0.703231
#> 4         hp    -0.011042                 -0.757065
#> 5       drat     0.859638                  0.459630
#> 6         wt    -1.185114                 -1.159584
#> 7       qsec     0.000000                  0.000000
#> 8         vs     0.655750                  0.330509
#> 9         am     1.116929                  0.557338
#> 10      gear     0.123540                  0.091148
#> 11      carb    -0.350465                 -0.566071
#> 
#> H2ORegressionMetrics: glm
#> ** Reported on training data. **
#> 
#> MSE:  6.511253
#> RMSE:  2.551716
#> MAE:  2.00629
#> RMSLE:  0.113459
#> Mean Residual Deviance :  6.511253
#> R^2 :  0.8149633
#> Null Deviance :1126.047
#> Null D.o.F. :31
#> Residual Deviance :208.3601
#> Residual D.o.F. :22
#> AIC :172.7651

#formula
f <- as.formula(paste(y, paste(x, collapse = "" + ""), sep = "" ~ ""))

model_lm <- lm(f, data = mtcars)

#model output
model_lm
#> 
#> Call:
#> lm(formula = f, data = mtcars)
#> 
#> Coefficients:
#> (Intercept)          cyl         disp           hp         drat  
#>    12.30337     -0.11144      0.01334     -0.02148      0.78711  
#>          wt         qsec           vs           am         gear  
#>    -3.71530      0.82104      0.31776      2.52023      0.65541  
#>        carb  
#>    -0.19942

# package for vif variables
library(car)
#> Warning: package 'car' was built under R version 3.5.3
#> Loading required package: carData
#> 
#> Attaching package: 'car'
#> The following object is masked from 'package:dplyr':
#> 
#>     recode

# list of VIF values
car::vif(model_lm) %>% as_tibble(rownames = ""x_vars"") %>%  arrange(desc(value))
#> Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `enframe(name = NULL)` instead.
#> This warning is displayed once per session.
#> # A tibble: 10 x 2
#>    x_vars value
#>    <chr>  <dbl>
#>  1 disp   21.6 
#>  2 cyl    15.4 
#>  3 wt     15.2 
#>  4 hp      9.83
#>  5 carb    7.91
#>  6 qsec    7.53
#>  7 gear    5.36
#>  8 vs      4.97
#>  9 am      4.65
#> 10 drat    3.37



Created on 2019-04-10 by the 
reprex package
 (v0.2.1)","['r', 'regression', 'h2o']",Ryan John,https://stackoverflow.com/users/6497137/ryan-john,"1,430"
55582972,55582972,2019-04-08T23:08:33,2022-04-05 16:50:05Z,733,"I am trying to use XGBoost MOJO created and downloaded from H2O in runtime using java, but I am getting errors while compiling. I have tries multiple different versions of dependencies but couldn't get through it.   


Dependencies :


    <dependency>
        <groupId>ai.h2o</groupId>
        <artifactId>h2o-genmodel</artifactId>
        <version>3.22.1.6</version>
    </dependency>
    <!-- https://mvnrepository.com/artifact/ai.h2o/h2o-genmodel-ext-xgboost -->
    <dependency>
        <groupId>ai.h2o</groupId>
        <artifactId>h2o-genmodel-ext-xgboost</artifactId>
        <version>3.24.0.1</version>
        <type>pom</type>
    </dependency>



Main.java


import hex.genmodel.easy.EasyPredictModelWrapper;
import hex.genmodel.easy.RowData;
import hex.genmodel.easy.prediction.BinomialModelPrediction;

public class Main {
    public static void main(String[] args) throws Exception {
        EasyPredictModelWrapper model = new EasyPredictModelWrapper(
                MojoModel.load(""/Users/p0g0085/Downloads/grid_5dd10c7e_297d_42eb_b422_56c687ba85df_model_18.zip""));

        RowData row = new RowData();
        row.put(""brand"", ""Great Value"");
        row.put(""product_type"", ""Cheeses"");
        row.put(""duration"", ""21.0"");
        row.put(""quantity"", ""1.0"");
        row.put(""ghs_frequency"", ""11.3714"");

        BinomialModelPrediction p = model.predictBinomial(row);
        System.out.println(""Has penetrated the prostatic capsule (1=yes; 0=no): "" + p.label);
        System.out.print(""Class probabilities: "");
        for (int i = 0; i < p.classProbabilities.length; i++) {
            if (i > 0) {
                System.out.print("","");
            }
            System.out.print(p.classProbabilities[i]);
        }
        System.out.println("""");
    }
}



Error :


Exception in thread ""main"" java.lang.IllegalStateException: Algorithm `XGBoost` is not supported by this version of h2o-genmodel. If you are using an algorithm implemented in an extension, be sure to include a jar dependency of the extension (eg.: ai.h2o:h2o-genmodel-ext-xgboost)
    at hex.genmodel.ModelMojoFactory.getMojoReader(ModelMojoFactory.java:102)
    at hex.genmodel.ModelMojoReader.readFrom(ModelMojoReader.java:31)
    at hex.genmodel.MojoModel.load(MojoModel.java:37)
    at Main.main(Main.java:9)","['pojo', 'h2o', 'xgboost', 'mojo', 'machine-learning-model']",Prashant Gupta,https://stackoverflow.com/users/4428110/prashant-gupta,113
55566277,55566277,2019-04-08T04:29:40,2019-04-08 04:57:36Z,150,"I am new to H2o ai and I have tried few things using python api. It looks good to me. I know spark and I am very much interested in Sparkling Water. So my question is - In pySparkling, Can I use same python API applied for H2o ai ptatfom ?",['h2o'],ARKAPROVA SAHA,https://stackoverflow.com/users/11327040/arkaprova-saha,1
55546188,55546188,2019-04-06T05:29:28,2019-04-06 07:46:14Z,112,"I tried to use h2o's cross-validation. metrics, but I found the standard deviation is different from what I have calculated


  mean           sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid cv_5_valid        model
auc   0.9512599 0.0058884337 0.96505374  0.9521365  0.9406109  0.9451134 0.95338506          drf

sd(c(0.96505374,0.9521365,0.9406109,0.9451134,0.95338506) )
[1] 0.009310416
mean(c(0.96505374,0.9521365,0.9406109,0.9451134,0.95338506) )
[1] 0.9512599



Mean is the same but sd is different, is there other ways of calculating this?","['machine-learning', 'cross-validation', 'h2o']",Pierre.Vriens,https://stackoverflow.com/users/4640431/pierre-vriens,"2,117"
55538659,55538659,2019-04-05T15:19:04,2019-04-05 15:30:38Z,174,"I need to export an H2o model in Javascript or Excel. 


I know how to export models using JAVA, Python and R. I want to do the same for JS and Excel. 
An idea could be to load the exported model in mojo to the sever and then use the REST API to perform model evaluation. Unfortunately we cannot succeed in performing the loading (we have opened an issue here regarding this specific problem). 
Do you have any idea of how to solve this issue?


Thanks",['h2o'],Simone Silvetti,https://stackoverflow.com/users/10021039/simone-silvetti,11
55504026,55504026,2019-04-03T20:48:07,2019-04-03 21:17:43Z,0,"Is there a way to convert the types of specific columns in an h2o dataframe? For example if all the columns are type 
int
 in some dataframe df, but would like to use one of the columns C of 1's and 0's as the sample responses for training, is there a way to do something like 
df['C'].to_type('enum')
? Could find no such thing in the 
docs
.",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
55503702,55503702,2019-04-03T20:24:10,2019-04-04 22:39:12Z,318,"Looking at the docs for h2o's categorical_encoding 
enum
 type, it says 




enum or Enum: Leave the dataset as is, 
internally map the strings to
  integers
, and use these integers to make splits - either via ordinal
  nature when nbins_cats is too small to resolve all levels or via
  bitsets that do a perfect group split. Each category is a separate
  category; its name (or number) is irrelevant. For example, after the
  strings are mapped to integers for Enum, you can split {0, 1, 2, 3, 4,
  5} as {0, 4, 5} and {1, 2, 3}.




Does this mean that whenever I convert the same (in this case) pandas dataframe to an h2o dataframe, the internal representation for each of any of the categorical 
enum
 type column values will always be the same (or is it in some way random each time)? (I assume it would have to be in order to be useful for running any actual predictions on a model trained on the dataframe, but want to make sure my understanding is correct).




As a side note, what does the doc's description mean by ""to make splits""?",['h2o'],Unknown,,N/A
55485950,55485950,2019-04-03T01:59:20,2019-04-03 19:53:34Z,0,"Trying to train a DRF classifier in h2o (version 3.20.0.5), the error ""H2OServerError: HTTP 500 Server Error"" with no further explanation.


---------------------------------------------------------------------------
H2OServerError                            Traceback (most recent call last)
<ipython-input-44-f52d1cb4b77a> in <module>()
      4     training_frame=train_u, validation_frame=val_u,
      5     weights_column='weight',
----> 6     max_runtime_secs=max_train_time_hrs*60*60)
      7 
      8 

/home/mapr/python-virtual-envs/ml1c/venv/lib/python2.7/site-packages/h2o/estimators/estimator_base.pyc in train(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, max_runtime_secs, ignored_columns, model_id, verbose)
    224         rest_ver = parms.pop(""_rest_version"") if ""_rest_version"" in parms else 3
    225 
--> 226         model_builder_json = h2o.api(""POST /%d/ModelBuilders/%s"" % (rest_ver, self.algo), data=parms)
    227         model = H2OJob(model_builder_json, job_type=(self.algo + "" Model Build""))
    228 

/home/mapr/python-virtual-envs/ml1c/venv/lib/python2.7/site-packages/h2o/h2o.pyc in api(endpoint, data, json, filename, save_to)
    101     # type checks are performed in H2OConnection class
    102     _check_connection()
--> 103     return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
    104 
    105 

/home/mapr/python-virtual-envs/ml1c/venv/lib/python2.7/site-packages/h2o/backend/connection.pyc in request(self, endpoint, data, json, filename, save_to)
    400                                     auth=self._auth, verify=self._verify_ssl_cert, proxies=self._proxies)
    401             self._log_end_transaction(start_time, resp)
--> 402             return self._process_response(resp, save_to)
    403 
    404         except (requests.exceptions.ConnectionError, requests.exceptions.HTTPError) as e:

/home/mapr/python-virtual-envs/ml1c/venv/lib/python2.7/site-packages/h2o/backend/connection.pyc in _process_response(response, save_to)
    728         # Note that it is possible to receive valid H2OErrorV3 object in this case, however it merely means the server
    729         # did not provide the correct status code.
--> 730         raise H2OServerError(""HTTP %d %s:\n%r"" % (status_code, response.reason, data))
    731 
    732 

H2OServerError: HTTP 500 Server Error:
Server error java.lang.NullPointerException:
  Error: Caught exception: java.lang.NullPointerException
  Request: None



The code snippet in question is shown below:


max_train_time_hrs = 8 
drf_proc.train(
    x=train_features, y=train_response,
    training_frame=train_u, validation_frame=val_u,
    weights_column='weight',
    max_runtime_secs=max_train_time_hrs*60*60)



The output from running the 
h2o.init()
 command looks like


Checking whether there is an H2O instance running at http://172.18.4.62:54321. connected.
Warning: Your H2O cluster version is too old (7 months and 24 days)! Please download and install the latest version from http://h2o.ai/download/
H2O cluster uptime: 06 secs
H2O cluster timezone:   Pacific/Honolulu
H2O data parsing timezone:  UTC
H2O cluster version:    3.20.0.5
H2O cluster version age:    7 months and 24 days !!!
H2O cluster name:   H2O_88021
H2O cluster total nodes:    4
H2O cluster free memory:    15.34 Gb
H2O cluster total cores:    8
H2O cluster allowed cores:  8
H2O cluster status: accepting new members, healthy
H2O connection url: http://172.18.4.62:54321
H2O connection proxy:   None
H2O internal security:  False
H2O API Extensions: AutoML, XGBoost, Algos, Core V3, Core V4
Python version: 2.7.12 fin



While I realize that there is a warning that the version of h2o I am using is ""too old"", the version of the h2o python package I am using and the cluster I am connecting to still match and this cannot be upgraded due to other h2o applications that access this cluster and expect a certain version (all of these applications appear to have no problem running on the cluster). Meanwhile, any web browser is unable to connect to the H2O connection url.


Any ideas about what could be going on here or debugging steps that could be looked into?",['h2o'],Unknown,,N/A
55484531,55484531,2019-04-02T22:42:00,2019-04-04 20:52:34Z,422,"Is there a way to select only first rows in each h2o dataframe group_by group?


The reason for doing this is to merge some columns in an h2o dataframe into a 
group_by
'ed version of that dataframe that was created to get some stats. based on particular groupings in the original.


Example, suppose had two dataframes like


df1
receipt_key  b  c item_id
------------------------
a1           1  2 1
a2           3  4 1

and

df2
receipt_key  e  f  item_id
--------------------------
a1           5  6  1
a1           7  8  2
a2           9  10 1



would like to join them such that end up with dataframe


df3
receipt_key  b c e f  item_id
-----------------------------
a1           1 2 5 6  1
a2           3 4 9 10 1



Have tried doing something like 
df2.group_by('receipt_key').max('item_id')
 to merge into df1, but doing so only leaves the item_id column in the group's 
get_frame()
 dataframe (and even listing all of the columns in df2 to 
max()
 on would not give the right values as well as be cumbersome for my actual use case which has much more columns in df2). 


Any ideas on how this could be done? Would simply deleting duplicates be sufficient to get the desired dataframe (though there appears to be barriers to doing this in h2o, see 
https://0xdata.atlassian.net/browse/PUBDEV-3292
)?",['h2o'],Unknown,,N/A
55473526,55473526,2019-04-02T11:20:08,2019-04-03 14:42:57Z,0,"I'm using H2O target encoding to treat some of my factor variables. However, I think I stumble into a problem.


I can use the function easily, and had thus both created and applyed a target encoding ""model"". However, I need to be able to save said model and use it latter (as one does for example with actual H2O models). 


I have save the model object with R's save function, and now I can not use it after loading (Got 


ERROR: Unexpected HTTP Status code: 404 Not Found (url = http://localhost:54014/3/Frames/RTMP_sid_93aa_3?row_count=10)

water.exceptions.H2OKeyNotFoundArgumentException
 [1] ""water.exceptions.H2OKeyNotFoundArgumentException: Object 'RTMP_sid_93aa_3' not found for argument: key""             
 [2] ""    water.api.FramesHandler.getFromDKV(FramesHandler.java:135)""                                                     
 [3] ""    water.api.FramesHandler.doFetch(FramesHandler.java:225)""                                                        
 [4] ""    water.api.FramesHandler.doFetch(FramesHandler.java:220)""                                                        
 [5] ""    water.api.FramesHandler.fetch(FramesHandler.java:199)""                                                          
 [6] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                    
 [7] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                                  
 [8] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                          
 [9] ""    java.lang.reflect.Method.invoke(Method.java:497)""                                                               
[10] ""    water.api.Handler.handle(Handler.java:63)""                                                                      
[11] ""    water.api.RequestServer.serve(RequestServer.java:482)""                                                          
[12] ""    water.api.RequestServer.doGeneric(RequestServer.java:327)""                                                      
[13] ""    water.api.RequestServer.doGet(RequestServer.java:251)""                                                          
[14] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:735)""                                                   
[15] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                   
[16] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                         
[17] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)""                                     
[18] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                             
[19] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)""                                      
[20] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                              
[21] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                  
[22] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                          
[23] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                
[24] ""    water.webserver.jetty8.Jetty8ServerAdapter$LoginHandler.handle(Jetty8ServerAdapter.java:119)""                   
[25] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                          
[26] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                
[27] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                        
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                 
[29] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                  
[30] ""    org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:973)""                
[31] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1035)""
[32] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:641)""                                               
[33] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:231)""                                          
[34] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                         
[35] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                   
[36] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                               
[37] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                
[38] ""    java.lang.Thread.run(Thread.java:745)""                                                                          

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Object 'RTMP_sid_93aa_3' not found for argument: key



Two questions:


1 - Is there a way to still use the encoding I save this way?


2 - How should I save it and load it?","['r', 'save', 'h2o', 'feature-engineering']",Diogo Santos,https://stackoverflow.com/users/2501571/diogo-santos,854
55465319,55465319,2019-04-02T00:30:23,2019-04-03 21:10:40Z,326,"Wondering about what is happening when summing columns in an h2o dataframe GroupBy object when the column types are 
categorical
 (specifically h2o 
enum
 types). 


Have a pandas dataframe converted to an H2o dataframe. Am then grouping rows by a certain column and summing the other columns, eg.


location_id  price store
------------------
1            10    JCP
1            15    SBUX
3            20    HOL

then after grouping and summing; df.group_by('location_id').sum(['price', 'store'])

location_id  price store
------------------
1            25    <some number>
3            20    <some number>



Would like to know what is going on under the surface here when adding categorical column values together and can't seem to find the sum() source code for GroupBy objects in the h2o 
docs
.",['h2o'],Unknown,,N/A
55383781,55383781,2019-03-27T18:01:01,2019-04-04 23:10:09Z,620,"I have a set of coefficients from a trained model but I don't have access to the model itself or training dataset. I'd like to create an instance of 
H2OGeneralizedLinearEstimator
 and set the coefficients manually to use the model for prediction. 


The first thing I tried was (this is an example to reproduce the error):  


import h2o
from h2o.estimators.glm import H2OGeneralizedLinearEstimator
from h2o.frame import H2OFrame
h2o.init()

# creating some test dataset
test = {""x"":[0,1,2], ""y"":[0,0,1]}
df = H2OFrame(python_obj=test)
glm = H2OGeneralizedLinearEstimator(family='binomial', model_id='logreg')
# setting the coefficients
glm.coef = {'Intercept':0, 'x':1}
# predict
glm.predict(test_data=df)



This throws an error:




H2OResponseError: Server error
  water.exceptions.H2OKeyNotFoundArgumentException:   Error: Object
  'logreg' not found in function: predict for argument: model




I also tried to set 
glm.params
 keys based on the keys of a similar trained model:


for key in trained.params.keys():
    glm.params.__setitem__(key, trained.params[key])



but this doesn't populate 
glm.params
 (
glm.params = {}
).","['python', 'h2o']",Mahdi,https://stackoverflow.com/users/3349443/mahdi,"3,238"
55378660,55378660,2019-03-27T13:40:18,2019-03-29 04:34:53Z,0,"I would like to ensemble two already ensemble models made with the 
h2o::h2o.automl
 function.


# Fit models
all_models_h2o_s1 <- h2o::h2o.automl(y = ""sr_elec"",
                                  training_frame = df_train_d_h2o,
                                  max_models = 2,
                                  max_runtime_secs = 60*60*24, # 1 day
                                  stopping_metric = ""deviance"",
                                  sort_metric = ""deviance"",
                                  seed = 1, # this is the only diference
                                  nfolds = 5,
                                  keep_cross_validation_predictions = TRUE)
all_models_h2o_s2 <- h2o::h2o.automl(y = ""sr_elec"",
                                     training_frame = df_train_d_h2o,
                                     max_models = 2,
                                     max_runtime_secs = 60*60*24, # 1 day
                                     stopping_metric = ""deviance"",
                                     sort_metric = ""deviance"",
                                     seed = 2, # this is the only diference
                                     nfolds = 5,
                                     keep_cross_validation_predictions = TRUE)

# Get best models
model_l1_s1 <- all_models_h2o_s1@leader
model_l1_s2 <- all_models_h2o_s2@leader

# Model types
model_l1_s1@model_id # StackedEnsemble_AllModels_AutoML_20190327_141553
model_l1_s2@model_id # StackedEnsemble_AllModels_AutoML_20190327_142026

# Ensemble models
ensemble <- h2o::h2o.stackedEnsemble(y = ""sr_elec"",
                                     training_frame = df_train_d_h2o,
                                     base_models = list(model_l1_s1, model_l1_s2))



The error given is:


water.exceptions.H2OIllegalArgumentException: Base model does not use cross-validation: 0

water.exceptions.H2OIllegalArgumentException: Base model does not use cross-validation: 0
    at hex.StackedEnsembleModel.checkAndInheritModelProperties(StackedEnsembleModel.java:368)
    at hex.ensemble.StackedEnsemble$StackedEnsembleDriver.computeImpl(StackedEnsemble.java:234)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:218)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1395)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

Error: water.exceptions.H2OIllegalArgumentException: Base model does not use cross-validation: 0



Version:


packageVersion(pkg = ""h2o"")
[1] ‘3.22.1.1’



R.Version()
$platform
[1] ""x86_64-w64-mingw32""

$arch
[1] ""x86_64""

$os
[1] ""mingw32""

$system
[1] ""x86_64, mingw32""

$status
[1] """"

$major
[1] ""3""

$minor
[1] ""5.3""

$year
[1] ""2019""

$month
[1] ""03""

$day
[1] ""11""

$`svn rev`
[1] ""76217""

$language
[1] ""R""

$version.string
[1] ""R version 3.5.3 (2019-03-11)""

$nickname
[1] ""Great Truth""","['r', 'h2o']",sbcalaff,https://stackoverflow.com/users/11266610/sbcalaff,11
55378452,55378452,2019-03-27T13:30:37,2019-03-27 14:22:48Z,117,"I'm currently experimenting with the possibilities of Sparkling-Water. There are a few possible Use-Cases including Data-Munging in H2O/Spark, Model Building and Offline-Training and Online Stream Prediction. I was wondering whether it is also possible to use Sparkling-Water for Online-Training together with a Kafka Streaming Source?","['apache-spark', 'pyspark', 'h2o', 'sparkling-water']",dnks23,https://stackoverflow.com/users/5358418/dnks23,369
55365530,55365530,2019-03-26T20:12:09,2019-03-26 21:05:37Z,0,"I am running h2o automl in python on a windows machine. I noticed that none of the models which were being built were based on XGBoost.


I tried running the following code:


 import h2o
 from h2o.automl import H2OAutoML
 h2o.init()

 h2o.estimators.xgboost.H2OXGBoostEstimator.available()



I get the notification below:


Cannot build an XGBoost model - no backend found.



I know with other ML packages (Keras for example) you can set the config file to specify back ends or you can set the back end as an environment variable. Is there a way to do this for h20?


FYI I am running h2o v3.22.1.6","['python', 'h2o', 'xgboost']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
55359773,55359773,2019-03-26T14:35:56,2019-03-26 16:54:05Z,0,"Is it possible with AutoML (from H2O) to use only the Word2Vec algorithm and try out different values for the parameters to find out which parameter settings give me the most accurate vectors for my data set? So I don't want AutoML to apply the algorithms DeepLearning, GBM etc. to my dataset. Only the Word2Vec algorithm… How Do I do that?


So far I only managed to build a word2vec model with H2O. 


I would like to test different Settings of the hyperparameters of Word2Vec with AutoML to evaluate which Settings are optimal...","['word2vec', 'h2o', 'hyperparameters']",Mona Stebner,https://stackoverflow.com/users/11260978/mona-stebner,1
55347180,55347180,2019-03-25T22:13:56,2019-03-28 01:21:08Z,472,"Currently I am doing a random forest by H2O package and have plotted a sample tree for presentation purpose. The prediction value of each node is not quite the same as probability of positive class over the all instances of the node. 


Just wondering to know how H2O calculate the prediction value. I need a formula to derive this prediction! I know that random forest goes over the average of the trees' prediction. But how is this prediction calculated at each node of each tree?


Any help would be appreciated.","['random-forest', 'h2o']",ascripter,https://stackoverflow.com/users/3104974/ascripter,"6,141"
55346860,55346860,2019-03-25T21:42:21,2019-03-26 14:35:09Z,0,"I am using h2o.deeplearning to train a neural network on a classification task.


What I have


Y ~ x1 + x2... where all x variables are continuous and Y is binary. 


What I want


To be able to train a deeplearning object to predict the probability of a given row of being true or false. That is, a predicted(Y) restricted to between 0 and 1.


What I've tried

When Y is inputted as a numeric (i.e. 0 or 1), h2o deeplearning automatically treats it as a regression problem. This is fine, except the final layer of the NN is linear, not tanh, and the predicted values can be greater than 1 or less than 0. I've not been able to find a way to get the final layer to be a tanh.


When Y is inputted as categorical (i.e. TRUE or FALSE), h2o deeplearning automatically treats it as a classification problem. Instead of giving me the desired probability of Y being 1 or 0, it gives me its best guess of what Y is.


Is there a way around this? A trick, tweak or an overlooked parameter? I have noticed in the h2o.deeplearning documentation a 'distribution' parameter, but no further information on what that's for. My best guess is that it is some kind of link function in the same vein as GLM, but I'm not sure.","['r', 'regression', 'classification', 'h2o']",Ingolifs,https://stackoverflow.com/users/8968617/ingolifs,311
55310289,55310289,2019-03-23T03:18:10,2019-03-27 21:41:19Z,85,"I am learning the H2O TargetEncoder function from h2o.targetencoder. I am wondering, is there a way that we can export the mapping table for each level of the categorical variable?


For example, if we have 2 categorical variable for 50 US states and 5 weekdays. Can TargetEncoder export a results table with the mapped numeric value for the 50 states and 5 weekdays?","['python', 'h2o']",Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
55306686,55306686,2019-03-22T19:34:11,2019-05-13 10:46:25Z,644,"I am following the H2O example to run target mean encoding in Sparking Water (sparking water 2.4.2 and H2O 3.22.04). It runs well in all the following paragraph


from h2o.targetencoder import TargetEncoder

# change label to factor
input_df_h2o['label'] = input_df_h2o['label'].asfactor()

# add fold column for Target Encoding
input_df_h2o[""cv_fold_te""] = input_df_h2o.kfold_column(n_folds = 5, seed = 54321)

# find all categorical features
cat_features = [k for (k,v) in input_df_h2o.types.items() if v in ('string')]
# convert string to factor
for i in cat_features:
    input_df_h2o[i] = input_df_h2o[i].asfactor()

# target mean encode
targetEncoder = TargetEncoder(x= cat_features, y = y, fold_column = ""cv_fold_te"", blending_avg=True)
targetEncoder.fit(input_df_h2o)



But when I start to use the same data set used to fit Target Encoder to run the transform code (see code below):


ext_input_df_h2o = targetEncoder.transform(frame=input_df_h2o,
                                    holdout_type=""kfold"", # mean is calculating on out-of-fold data only; loo means leave one out
                                    is_train_or_valid=True,
                                    noise = 0, # determines if random noise should be added to the target average
                                    seed=54321)



I will have 
error
 like


Traceback (most recent call last):
  File ""/tmp/zeppelin_pyspark-6773422589366407956.py"", line 331, in <module>
    exec(code)
  File ""<stdin>"", line 5, in <module>
  File ""/usr/lib/envs/env-1101-ver-1619-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/h2o/targetencoder.py"", line 97, in transform
    assert self._encodingMap.map_keys['string'] == self._teColumns
AssertionError



I found the code in its source code 
http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/_modules/h2o/targetencoder.html



but how to fix this issue? It is the same table used to run the 
fit
.","['python-3.x', 'pyspark', 'h2o']",Unknown,,N/A
55303305,55303305,2019-03-22T15:46:06,2019-03-22 15:46:06Z,376,"I am trying to import a model that was trained and saved using 
H2O 3.22.0.2
.


The error is as follows:


INFO ERROR: Server error water.exceptions.H2OIllegalArgumentException:
  Error: Illegal argument: dir of function: importModel: /home/centos/models/model_20190202_1204/DeepLearning_model_python_1549126947138_1
  Request: POST /99/Models.bin/
    data: {'dir': '/home/centos/models/model_20190202_1204/DeepLearning_model_python_1549126947138_1'}



The version running in the server is 
h2o==3.22.0.2
.


I have run a prediction using this model file locally with no issues. 


Could anyone provide any clues as to what might be going on? Thanks.


UPDATE
: 


I am attempting to load the model using 
h2o.load_model(model_path)",['h2o'],Walter U.,https://stackoverflow.com/users/5030133/walter-u,331
55272948,55272948,2019-03-21T02:32:24,2019-03-21 10:22:42Z,513,"I am trying to merge two data frames after an operation.


import pandas as pd
import h2o
from h2o.automl import H2OAutoML
h2o.init()
import pandas as pd

import numpy as np



support = ""splvl.csv""
data = h2o.import_file(support)

df1 = data[data['X'] == 0]
df2 = data[data['X'] == 1]

df1.impute(""A"", method = ""mean"", by = [""B"", ""C""])
df1.impute(""Q"", method = ""mode"", by = [""B"", ""C""])

df2.impute(""A"", method = ""mean"", by = [""B"", ""C""])
df2.impute(""Q"", method = ""mode"", by = [""B"", ""C""])

df1[""X""].table()
df2[""X""].table()

df3 = df2.merge(df1)

h2o.export_file(df3, path = ""merged.csv"", force=True, parts=1)



I get the following error when I execute the export to CSV command,




H2OServerError: HTTP 500 Server Error:
  Server error water.util.DistributedException:
    Error: DistributedException from /127.0.0.1:54321: 'Operation not allowed on string vector.'
    Request: None




df3[""X""].table()





Server error water.exceptions.H2OKeyNotFoundArgumentException:
    Error: Object 'py_13_sid_95bb' not found for argument: key
    Request: GET /3/Frames/py_13_sid_95bb
      params: {'row_count': '10', 'row_offset': '0', 'column_count': '-1', 'full_column_count': '-1', 'column_offset': '0'}




this error when i try to print value count in merged data frame","['python', 'dataframe', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
55266642,55266642,2019-03-20T17:15:40,2019-03-20 17:15:40Z,0,"I have run a logistical regression model with both categorical and numerical variables. The model was trying to predict the number of website visits in a month based off the first week. Obviously the number of website visits in the first week was the strongest indicator. However when i ran h2o deep learning with various models and activation functions the model performs very poorly. Based off the var_imp function it gives importance to very non important variables(based off my logistical regression model, which is quite good, this is wrong), and only seems to have categorical subsets ranked with high importance. and the model does not perform well even on the training data, a real warning sign! So i just wanted to upload my code to check i am not doing anything to harm the model. It seems strange for logistical regression to get it quiteright but deep learning to get it so wrong, so i imagine its something i've done!


summary(data)
 $ VAR1: Factor w/ 8 levels ,..: 1 5 2 1 7 2 5 1 5 1 ...
  $ VAR2: Factor w/ 5 levels ,..: 1 4 1 1 4 4 4 1 1 4 ...
  $ VAR3: Factor w/ 2 levels ""F"",""M"": 2 2 2 1 2 2 2 2 2 2 ...
  $ VAR4: Factor w/ 2 levels : 2 1 2 2 1 1 1 2 2 1 ...
  $ VAR5         : num  1000 20 30 20 30 30 30 50 30 400 ...
  $ VAR6: Factor w/ 2 levels ""N"",""Y"": 1 2 2 1 2 2 2 2 1 2 ...
  $ VAR7: Factor w/ 2 levels ""N"",""Y"": 1 2 2 1 2 2 2 2 1 2 ...
  $ VAR8: num  0 0 0 0 0 0 0 0 0 0 ...
  $ VAR9: num  56 52 49 29 28 38 34 79 53 36 ...
  $ VAR10: num  3 2 1 3 2 2 3 4 2 2 ...
  $ VAR11: num  1 1 1 2 2 1 1 1 1 2 ...
  $ VAR12: Factor w/ 2 levels ""N"",""Y"": 1 1 1 1 2 1 1 1 1 1 ...
  $ VAR13: num  1 0 1 1 1 0 1 0 0 0 ...
  $ VAR14: Factor w/ 2 levels ""N"",""Y"": 2 1 1 1 1 1 1 1 1 1 ...
  $ VAR15: Factor w/ 2 levels ""N"",""Y"": 1 1 1 1 1 1 1 1 1 1 ...
  $ VAR16: num  1 0 0 1 0 0 0 1 1 0 ...
  $ VAR17: num  19 7 1 4 10 2 4 4 7 12 ...
  $ VAR18: Factor w/ 2 levels ""N"",""Y"": 1 2 2 2 2 2 2 1 2 1 ...
  $ VAR19: Factor w/ 2 levels ""0"",""Y"": 1 1 2 1 1 1 1 1 1 1 ...
  $ VAR20: Factor w/ 2 levels ""N"",""Y"": 1 1 2 1 1 1 1 1 1 1 ...
  $ VAR21: Factor w/ 2 levels ""N"",""Y"": 1 1 1 1 1 1 1 1 1 1 ...
  $ VAR22:    : num  0.579 0 0 0 0.4 ...
  $ VAR23: num  1.89 1 1 1 2.9 ...
  $ VAR24: num  0.02962 0.00691 0.05327 0.02727 0.01043 ...
  $ VAR25: Factor w/ 3 levels ..: 2 2 2 3 3 2 3 2 1 3 ...
  $ VAR26: num  3 2 1 2 3 1 2 1 2 4 ...
  $ VAR27: num  3 2 1 1 5 1 1 1 1 2 ...
  $ VAR_RESPONSE: num  7 24 4 3 8 12 5 48 2 7 ...

sapply(data,function(x) sum(is.na(x)))

colSums(is.na(data))
data[is.na(data)] = 0



 d.hex = as.h2o(data, destination_frame= ""d.hex"")

 Data_g.split = h2o.splitFrame(data = d.hex,ratios = 0.75)
 Data_train = Data_g.split[[1]]#75% training data
 Data_test = Data_g.split[[2]]

 activation_opt <- 
c(""Rectifier"",""RectifierWithDropout"",""Maxout"",""MaxoutWithDropout"", 
""Tanh"",""TanhWithDropout"")
 hidden_opt <- list(c(10,10),c(20,15),c(50,50,50),c(5,3,2),c(100,100),c(5),c(30,30,30),c(50,50,50,50),c(5,4,3,2))
 l1_opt <- c(0,1e-3,1e-5,1e-7,1e-9)
 l2_opt <- c(0,1e-3,1e-5,1e-7,1e-9)

 hyper_params <- list( activation=activation_opt,
                  hidden=hidden_opt,
                  l1=l1_opt,
                  l2=l2_opt )

 search_criteria <- list(strategy = ""RandomDiscrete"", max_models=30)

 dl_grid10 <- h2o.grid(""deeplearning""
                ,grid_id = ""deep_learn10""
                ,hyper_params = hyper_params
                ,search_criteria = search_criteria
                ,x = 1:27
                ,y = ""VAR_RESPONSE""
                ,training_frame = Data_train)
d_grid10 <- h2o.getGrid(""deep_learn10"",sort_by = ""mse"")

mn = h2o.deeplearning(x = 1:27,
                 y = ""VAR_RESPONSE"",
                 training_frame = Data_train,
                 model_id = ""mn"",
                 activation = ""Maxout"",
                 l1 = 0,
                 l2 = 1e-9,
                 hidden = c(100,100),)","['r', 'logistic-regression', 'h2o', 'categorical-data']",tfd,https://stackoverflow.com/users/11232908/tfd,1
55251381,55251381,2019-03-19T23:23:05,2019-03-19 23:28:54Z,85,"What is H2O.ai data parsing doing exactly?


Import h2o
h2o.init()
df = h2o.import_file(path=myfilepath)



When I run the above code I get the below output which takes a while ...


Parse progress: [###################################] 100%



When I run this on 2GB of data, my memory increases by roughly 2GB.  I thought that H2O.ai was supposed to use lazy evaluation?  It almost seems like a lot of information is being stored in memory (maybe even the entire dataset) and clearly this is not a lazy evaluation because it take a while to run.  


The H2O.ai documentation is not very helpful either.


http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/h2o.html#h2o.import_file


http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/importing-data.html


Does anyone understand exactly what is happening here?","['python', 'h2o']",yeamusic21,https://stackoverflow.com/users/9920026/yeamusic21,305
55222540,55222540,2019-03-18T13:30:16,2019-03-22 13:55:24Z,86,"I want to set up a Sparkling-Water app in IntelliJ. I found the droplet for a project at: 
Sparkling-Water-Droplet


But this has not been touched for a year and I was wondering whether there is a more recent version or any other template with newer version numbering for Spark, Scala and Sparkling-Water.


Thanks in Advance for any Hints!","['intellij-idea', 'h2o', 'sparkling-water']",dnks23,https://stackoverflow.com/users/5358418/dnks23,369
55218074,55218074,2019-03-18T09:23:17,2019-03-18 15:43:25Z,0,"I am using Jupyter notebooks with R i.e. by using R-essentials in Anaconda.
However, while installing H2O package in Jupyter notebook using 




install.packages(""h2o"")




it is giving an error and unable to install it. I also tried




conda install c -r anaconda h20




in terminal and it did installed h2o. But while exceuting the library function in the r notebook:




library(h2o)




I am getting the following error message:


Error in library(h2o): there is no package called ‘h2o’
Traceback:

1. library(h2o)
2. stop(txt, domain = NA)



Please let me know how to resolve this.","['r', 'anaconda', 'jupyter-notebook', 'h2o']",Unknown,,N/A
55214516,55214516,2019-03-18T03:46:53,2019-03-18 05:47:29Z,146,"I am running grid search using H2o and I am trying to find out where the model stats for the completed training models are stored with H2o - Hadoop. Say, it runs 10 models as part of grid search and it crashes after completing 5 models. Where do I find the scores (and parameters) for the completed models and where do I find the parameters for the model it was running when h2o crashed. I can't find this log anywhere. I am assuming it should persist the data somewhere to show in H2o flow. I just can't find it.",['h2o'],passionate,https://stackoverflow.com/users/4664842/passionate,533
55196441,55196441,2019-03-16T11:53:02,2019-03-18 18:55:57Z,243,"I'm experiencing an issue where the 
h2o.H2OFrame([1,2,3])
 command is creating a frame within h2o on an 
internal backend
, but not on an 
external backend
. Instead, the connection is not terminating (the frame is being created but the process hangs). 


It would appear that a post to 
/3/ParseSetup
 is not returning (where 
urllib3
 seems to get stuck). More specifically, from the h2o logs for a connection to the external backend, an example of this is (where I've shortened the date and IP):


* 10.*.*.15:56565 8120 #7003-141 INFO: Reading byte InputStream into Frame:
* 10.*.*.15:56565 8120 #7003-141 INFO: frameKey: upload_8a440dcf457c1e5deacf76a7ac1a4955
* 10.*.*.15:56565 8120 #7003-141 DEBUG: write-lock upload_8a440dcf457c1e5deacf76a7ac1a4955 by job null
* 10.*.*.15:56565 8120 #7003-141 INFO: totalChunks: 1
* 10.*.*.15:56565 8120 #7003-141 INFO: totalBytes:  21
* 10.*.*.15:56565 8120 #7003-141 DEBUG: unlock upload_8a440dcf457c1e5deacf76a7ac1a4955 by job null
* 10.*.*.15:56565 8120 #7003-141 INFO: Success.
* 10.*.*.15:56565 8120 #7003-135 INFO: POST /3/ParseSetup, parms: {source_frames=[""upload_8a440dcf457c1e5deacf76a7ac1a4955""], check_header=1, separator=44}



By comparison, the internal backend completes that call and the log files contain:


** 10.*.*.15:54444 2421 #0581-148 INFO: totalBytes:  21
** 10.*.*.15:54444 2421 #0581-148 INFO: Success.
** 10.*.*.15:54444 2421 #0581-149 INFO: POST /3/ParseSetup, parms: {source_frames=[""upload_b985730020211f576ef75143ce0e43f2""], check_header=1, separator=44}
** 10.*.*.15:54444 2421 #0581-150 INFO: POST /3/Parse, parms: {number_columns=1, source_frames=[""upload_b985730020211f576ef75143ce0e43f2""], column_types=[""Numeric""], single_quotes=False, parse_type=CSV, destination_frame=Key_Frame__upload_b985730020211f576ef75143ce0e43f2.hex, column_names=[""C1""], delete_on_done=True, check_header=1, separator=44, blocking=False, chunk_size=4194304}
...




There is a difference in the 
by job null
 lock that occurs, but it is released, so I suspect that it is not a critical issue. I've curled that endpoint unsuccessfully on both backends, and am reviewing the source code to determine why.


I am able to view the uploaded frame running 
h2o.ls()
, despite the hanging process, and I'm able to retrieve the frame using 
h2o.get_frame(frame_id=""myframe_id"")
 on the external backend.


I've tried/confirmed the following things:




Confirmed that the sparkling water version is correct with respect to the version of spark (i.e. h2o_pysparkling_2.3 - for Spark 2.3.x, as stated in 
docs.h2o.ai
 --- in my case sparkling water 2.3.12 - Spark 2.3.0.cloudera2);


Downloaded sparkling-water stable to the cluster and ran 
./get-extended-h2o.sh cdh5.14
, which gave me the 
h2odriver-sw2.3.0-cdh5.14-extended.jar
 jar;


Various permutations of parameters for the map reduce job. Interestingly, our cluster is quite busy and the base port setting was essential for stability. Also, our sub-nets span switches which messed with the multi-casting. Ultimately the following argument brought up the backend without fail:




    hadoop jar h2odriver-sw2.3.0-cdh5.14-extended.jar -Dmapreduce.job.queuename=root.users.myuser -jobname extback -baseport 56565 -nodes 10 -mapperXmx 10g -network 10.*.*.0/24





Confirmed that I could query the backend since 
h2o.ls()
 works;


Uploaded a spark dataframe instead of a plain list (same issue):




    sdf = session.createDataFrame([
    ('a', 1, 1.0), ('b', 2, 2.0)],
    schema=StructType([StructField(""string"", StringType()),
                       StructField(""int"", IntegerType()),
                       StructField(""float"", FloatType())])) 
    hc.as_h2o_frame(sdf)



From a YARN point of view, I attempted client and cluster mode submissions of the simple test app:


spark2-submit --master yarn --deploy-mode cluster --queue root.users.myuser --conf 'spark.ext.h2o.client.port.base=65656' extreboot.py



and without 
--master yarn
 and 
--deploy-mode cluster
 for the default client mode.


Lastly, the 
extreboot.py
 code is:


    from pyspark.conf import SparkConf
    from pyspark.sql import SparkSession
    from pysparkling import *
    import h2o

    conf = SparkConf().setAll([
    ('spark.ext.h2o.client.verbose', True),
    ('spark.ext.h2o.client.log.level', 'DEBUG'),
    ('spark.ext.h2o.node.log.level', 'DEBUG'),
    ('spark.ext.h2o.client.port.base', '56565'),
    ('spark.driver.memory','8g'),
    ('spark.ext.h2o.backend.cluster.mode', 'external')])

    session = SparkSession.builder.config(conf=conf).getOrCreate() 

    ip_addr='10.10.10.10'  
    port=56565

    conf = H2OConf(session).set_external_cluster_mode().use_manual_cluster_start().set_h2o_cluster(ip_addr, port).set_cloud_name(""extback"")
    hc = H2OContext.getOrCreate(session, conf)

    print(h2o.ls())
    h2o.H2OFrame([1,2,3])
    print('DONE')



Does anyone know why it may be hanging (in comparison to the internal backend), what I'm doing wrong, or which steps I can take to better debug this? Thanks!","['pyspark', 'hadoop-yarn', 'h2o', 'sparkling-water']",Unknown,,N/A
55194145,55194145,2019-03-16T06:40:22,2019-03-26 17:39:00Z,0,"prediction_h2o <- h2o.predict(automl_leader, testing)
when executing this syntax i got some weird error.


prediction_h2o <- h2o.predict(automl_leader, testing)

java.lang.IllegalArgumentException: Actual column must be integer class labels!

    java.lang.IllegalArgumentException: Actual column must be integer class labels!
        at hex.GainsLift.init(GainsLift.java:51)
        at hex.GainsLift.exec(GainsLift.java:124)
        at hex.glm.GLMMetricBuilder.makeModelMetrics(GLMMetricBuilder.java:217)
        at hex.glm.GLMModel.predictScoreImpl(GLMModel.java:1456)
        at hex.Model.score(Model.java:1381)
        at hex.ensemble.StackedEnsembleModel.predictScoreImpl(StackedEnsembleModel.java:150)
        at hex.Model.score(Model.java:1381)
        at water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:374)
        at water.H2O$H2OCountedCompleter.compute(H2O.java:1386)
        at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
        at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
        at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
        at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
        at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
    Error: java.lang.IllegalArgumentException: Actual column must be integer class labels!","['r', 'data-science', 'h2o']",Unknown,,N/A
55182284,55182284,2019-03-15T12:07:18,2019-10-04 21:32:33Z,897,"I have been using the command below to save my h2O model into a s3 bucket in python3 (I am using amazon EMR):


h2o.save_model(model=best_gbm1,path='s3://bucketname/folder1/folder2', force=False)



but I do get the following error:




H2OServerError: HTTP 500 Server Error:
  Server error java.lang.RuntimeException:
    Error: Not implemented
    Request: None




Do you know if it is possible to save a H2O model directly to a S3 bucket?",['h2o'],Unknown,,N/A
55177382,55177382,2019-03-15T07:13:08,2019-03-18 15:48:15Z,315,"The H2O had been installed in my PC but it's failed to import H2O before h2o.init() as the following screen shoot:

Fail to import h2o


Python is version 3.6.8 and h2o is version 3.18.0.2
Please advise me how to import h2o correctly. Thanks!","['python-3.x', 'h2o']",Tony KUO,https://stackoverflow.com/users/10210985/tony-kuo,35
55177163,55177163,2019-03-15T06:56:13,2019-03-15 10:04:41Z,81,"I am using h2o autoencoder anomaly for finding outlier data in my model but issue is autoencoder only accepts numerical predictors.
    My requirement is i have find outlier's based on CardNumber or merchant number.
    and Cardnumber is 12 digit(342178901244) and unique mostly So its nominal data and we can not do hot encoding as well as it will create many new fields as many as unique card no.
    So please suggest any way we can include categorical data as well and still we can run autoencoder


model=H2OAutoEncoderEstimator(activation=""Tanh"",
                              hidden=[70],
                              ignore_const_cols=False,
                              epochs=40)

model.train(x=predictors,training_frame=train.hex)

#Get anomalous values
test_rec_error=model.anomaly(test.hex,per_feature=True)
train_rec_error=model.anomaly(train.hex,per_feature=True)
recon_error_df['outlier'] = np.where(recon_error_df['Reconstruction.MSE'] > top_whisker, 'outlier', 'no_outlier')","['machine-learning', 'h2o', 'sklearn-pandas', 'predictive', 'h2o4gpu']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
55160293,55160293,2019-03-14T10:33:44,2019-03-18 15:27:32Z,301,"it's said 
Vertical : Actual
Across : Predicted


does it mean the rows are actual (i=1 for No,i=2 for Yes)
or the reverse","['h2o', 'confusion-matrix']",blute,https://stackoverflow.com/users/5180402/blute,21
55138112,55138112,2019-03-13T09:15:08,2019-03-14 07:45:32Z,276,"I have a target consisting of 5 classes where there is an ordinal relation. I want to implement an ordinal regression for this multi-class classification problem. Here what I tried:


train['target'] = train['target'].asfactor()
valid['target'] = valid['target'].asfactor()
test['target'] = test['target'].asfactor()

from h2o.estimators.glm import H2OGeneralizedLinearEstimator

hyper_params_glm = {'alpha': [0.001, 0.003, 0.005, 0.1, 0.3, 0.5, 0.7,0.75,0.8,0.85,0.9,0.95],
                   'missing_values_handling': [""skip"", ""mean_imputation""]}

ip_grid_glm = H2OGridSearch(model=H2OGeneralizedLinearEstimator(                 
                    standardize = True,
                    family='ordinal', keep_cross_validation_predictions=True, 
                    fold_assignment = ""Modulo"",lambda_search = True,
                    intercept = True,seed=2345, early_stopping = True, nfolds = 5)
                    ,hyper_params=hyper_params_glm)

ip_grid_glm.train(x=finalFeatures, y='AnsQ1', training_frame=train)



However I got the error: 




Argument 
family
 should be a ?Enum[""multinomial"", ""quasibinomial"",
  ""poisson"", ""gamma"", ""gaussian"", ""tweedie"", ""binomial""], got string
  ordinal




I have also changed target type as numeric using 
asnumeric()
, however again I got the same error.


How can I apply ordinal regression for my multi-class classification?


EDIT:
 If I understood correctly from the document 
link
, if I use multinomial family with more than 2 categorical values, then it will take into account the order of the categories?","['python-3.x', 'classification', 'h2o']",Unknown,,N/A
55109739,55109739,2019-03-11T20:17:44,2020-01-21 17:17:08Z,0,"Closed
. This question needs to be more 
focused
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Update the question so it focuses on one problem only by 
editing this post
.






Closed 
5 years ago
.















                        Improve this question
                    








The 
H2O R
 package is a great resource for building predictive models. But I am concerned with the security aspect of it.


Is it safe to use patient data with 
H2O
 in terms of security vulnerabilities ?","['r', 'h2o']",Rob,https://stackoverflow.com/users/162698/rob,15.1k
55081358,55081358,2019-03-09T19:52:57,2020-03-26 16:36:57Z,0,"I trained h2o automl and got a leader model with satisfying metrics. I want to retrain the model periodically but without using checkpoint. So, I guess all I need are the best parameters of the leader model to run it manually. I know automlmodels.leader.params but it gives a list of all parameters tried. How can I get the best ones as found in the leaderboard?","['python', 'parameters', 'h2o', 'automl']",Georgios Kourogiorgas,https://stackoverflow.com/users/8676754/georgios-kourogiorgas,351
55039924,55039924,2019-03-07T09:10:12,2019-03-07 14:35:46Z,248,"I am trying to comile and run simple h2o scala code. But when I do sbt package I get errors.
Am I missing something in the sbt file


This is my h2o scala code


import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.sql._

import ai.h2o.automl.AutoML
import ai.h2o.automl.AutoMLBuildSpec

import org.apache.spark.h2o._

object H2oScalaEg1 {

  def main(args: Array[String]): Unit = {

  val sparkConf1 = new SparkConf().setMaster(""local[2]"").setAppName(""H2oScalaEg1App"")

  val sparkSession1 = SparkSession.builder.config(conf = sparkConf1).getOrCreate()

  val h2oContext = H2OContext.getOrCreate(sparkSession1.sparkContext)

  import h2oContext._

  import java.io.File

  import h2oContext.implicits._

  import water.Key

  }

}



And this is my sbt file. 


name := ""H2oScalaEg1Name""

version := ""1.0""

scalaVersion := ""2.11.12""

scalaSource in Compile := baseDirectory.value / """"

libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""2.2.3""

libraryDependencies += ""org.apache.spark"" %% ""spark-sql"" % ""2.2.0""

libraryDependencies += ""org.apache.spark"" %% ""spark-hive"" % ""2.2.0""

libraryDependencies += ""ai.h2o"" % ""h2o-core"" % ""3.22.1.3"" % ""runtime"" pomOnly()



When I do sbt package I get these errors 


[error] /home/myuser1/h2oScalaEg1/H2oScalaEg1.scala:7:8: not found: object ai

[error] import ai.h2o.automl.AutoML

[error]        ^

[error] /home/myuser1/h2oScalaEg1/H2oScalaEg1.scala:8:8: not found: object ai

[error] import ai.h2o.automl.AutoMLBuildSpec

[error]        ^
[error] /home/myuser1/h2oScalaEg1/H2oScalaEg1.scala:10:25: object h2o is not a member of package org.apache.spark

[error] import org.apache.spark.h2o._
[error]                         ^

[error] /home/myuser1/h2oScalaEg1/H2oScalaEg1.scala:20:20: not found: value H2OContext
[error]   val h2oContext = H2OContext.getOrCreate(sparkSession1.sparkContext)
[error]                    ^


[error] /home/myuser1/h2oScalaEg1/H2oScalaEg1.scala:28:10: not found: value water
[error]   import water.Key
[error]          ^
[error] 5 errors found



How can I fix this problem.


My spark version in spark-2.2.3-bin-hadoop2.7


Thanks,


marrel","['scala', 'apache-spark', 'h2o']",Unknown,,N/A
55029227,55029227,2019-03-06T17:43:56,2019-03-18 18:23:54Z,0,"I am attempting to run h2o.glm in R but am encountering some strange behaviour. The same line of code sometimes works and sometimes errors with the following result


h2o.glm(x = Predictors.Revised, y = ""NN"", model_id = ""GLM_FREQ_INITIAL"", 
offset_column = ""Offset.To.Apply"", nfolds = 5, family = ""poisson"", 
link = ""log"", lambda_search = TRUE, training_frame = TrainDS.h2o, 
alpha = 1, standardize = TRUE)





java.lang.ArrayIndexOutOfBoundsException: 32


java.lang.ArrayIndexOutOfBoundsException: 32  at
  water.util.ArrayUtils.subtract(ArrayUtils.java:1334)  at
  hex.glm.GLM$GLMDriver.fitIRLSM(GLM.java:824)  at
  hex.glm.GLM$GLMDriver.fitModel(GLM.java:1080)     at
  hex.glm.GLM$GLMDriver.computeSubmodel(GLM.java:1169)  at
  hex.glm.GLM.cv_computeAndSetOptimalParameters(GLM.java:132)   at
  hex.ModelBuilder.cv_buildModels(ModelBuilder.java:595)    at
  hex.ModelBuilder.computeCrossValidation(ModelBuilder.java:431)    at
  hex.glm.GLM.computeCrossValidation(GLM.java:100)  at
  hex.ModelBuilder$1.compute2(ModelBuilder.java:309)    at
  water.H2O$H2OCountedCompleter.compute(H2O.java:1395)  at
  jsr166y.CountedCompleter.exec(CountedCompleter.java:468)  at
  jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)    at
  jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)     at
  jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)    at
  jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)






R Version: 3.3.1


Platform: x86_64-pc-linux-gnu (64-bit)


h2o Version: 3.22.1.5




Any ideas why? I am stumped.","['r', 'h2o']",Adrian LLoyd,https://stackoverflow.com/users/6474113/adrian-lloyd,21
55028999,55028999,2019-03-06T17:30:49,2019-03-21 22:21:41Z,0,"I'm using Target Encoding following theses steps: 
Target Encoding


Edit: example code


Notice the row count for the test data set has grown from 40k to 200k in record counts. Also from the sample data you can see ID 
2320
 has been duplicated 5 times.


library(h2o)
h2o.init()
#>  Connection successful!
#> 
#> R is connected to the H2O cluster: 


loan <- readr::read_csv(""/loan.csv"")
#> Parsed with column specification:
#> cols(
#>   loan_amnt = col_integer(),
#>   term = col_character(),
#>   int_rate = col_double(),
#>   emp_length = col_integer(),
#>   home_ownership = col_character(),
#>   annual_inc = col_double(),
#>   purpose = col_character(),
#>   addr_state = col_character(),
#>   dti = col_double(),
#>   delinq_2yrs = col_integer(),
#>   revol_util = col_double(),
#>   total_acc = col_integer(),
#>   bad_loan = col_integer(),
#>   longest_credit_length = col_integer(),
#>   verification_status = col_character()
#> )

loan$ID <- seq.int(nrow(loan))
dplyr::glimpse(loan)
#> Observations: 163,987
#> Variables: 16
#> $ loan_amnt             <int> 5000, 2500, 2400, 10000, 5000, 3000, 560...
#> $ term                  <chr> ""36 months"", ""60 months"", ""36 months"", ""...
#> $ int_rate              <dbl> 10.65, 15.27, 15.96, 13.49, 7.90, 18.64,...
#> $ emp_length            <int> 10, 0, 10, 10, 3, 9, 4, 0, 5, 10, 0, 3, ...
#> $ home_ownership        <chr> ""RENT"", ""RENT"", ""RENT"", ""RENT"", ""RENT"", ...
#> $ annual_inc            <dbl> 24000.00, 30000.00, 12252.00, 49200.00, ...
#> $ purpose               <chr> ""credit_card"", ""car"", ""small_business"", ...
#> $ addr_state            <chr> ""AZ"", ""GA"", ""IL"", ""CA"", ""AZ"", ""CA"", ""CA""...
#> $ dti                   <dbl> 27.65, 1.00, 8.72, 20.00, 11.20, 5.35, 5...
#> $ delinq_2yrs           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
#> $ revol_util            <dbl> 83.70, 9.40, 98.50, 21.00, 28.30, 87.50,...
#> $ total_acc             <int> 9, 4, 10, 37, 12, 4, 13, 3, 23, 34, 9, 1...
#> $ bad_loan              <int> 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0...
#> $ longest_credit_length <int> 26, 12, 10, 15, 7, 4, 7, 7, 13, 22, 7, 8...
#> $ verification_status   <chr> ""verified"", ""verified"", ""not verified"", ...
#> $ ID                    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1...
df <- as.h2o(loan)
df$bad_loan <- as.factor(df$bad_loan)
df$addr_state <- as.factor(df$addr_state)

# Split Frame into training and testing
splits <- h2o.splitFrame(df, seed = 1234,
                         destination_frames=c(""train.hex"", ""test.hex""),
                         ratios = 0.75)
train <- splits[[1]]
test <- splits[[2]]

response <- ""bad_loan""
predictors <- c(""loan_amnt"", ""int_rate"", ""emp_length"", ""annual_inc"", ""dti"",
                ""delinq_2yrs"", ""revol_util"", ""total_acc"", ""longest_credit_length"",
                ""verification_status"", ""term"", ""purpose"", ""home_ownership"",
                ""addr_state"")


train$fold <- h2o.kfold_column(train, 5, seed = 1234)
te_map <- h2o.target_encode_create(train, x = list(""addr_state""),
                                   y = response, fold_column = ""fold"")
head(te_map$addr_state)
#>   addr_state fold numerator denominator
#> 1         AK    0         7          52
#> 2         AK    1         8          55
#> 3         AK    2         7          56
#> 4         AK    3        13          68
#> 5         AK    4         8          70
#> 6         AL    0        57         297

ext_train <- h2o.target_encode_apply(train, x = list(""addr_state""), y = response,
                                     target_encode_map = te_map, holdout_type = ""KFold"",
                                     fold_column = ""fold"",
                                     blended_avg = TRUE, noise_level = 0, seed = 1234)
#> Warning in h2o.target_encode_apply(train, x = list(""addr_state""),
#> y = response, : The string columns: term, home_ownership, purpose,
#> verification_status were dropped from the dataset

head(ext_train[c(""addr_state"", ""fold"", ""TargetEncode_addr_state"")])
#>   addr_state fold TargetEncode_addr_state
#> 1         AK    0               0.1445783
#> 2         AK    0               0.1445783
#> 3         AK    0               0.1445783
#> 4         AK    0               0.1445783
#> 5         AK    0               0.1445783
#> 6         AK    0               0.1445783

nrow.H2OFrame(test)
#> [1] 40925
ext_test <- h2o.target_encode_apply(test, x = list(""addr_state""), y = response,
                                    target_encode_map = te_map, holdout_type = ""None"",
                                    #fold_column = ""fold"",
                                    blended_avg = FALSE, noise_level = 0)
#> Warning in h2o.target_encode_apply(test, x = list(""addr_state""),
#> y = response, : The string columns: term, home_ownership, purpose,
#> verification_status were dropped from the dataset
nrow.H2OFrame(ext_test)
#> [1] 204614

head(ext_test)
#>   addr_state loan_amnt int_rate emp_length annual_inc   dti delinq_2yrs
#> 1         AK     14000    12.42          9      72000 19.80           0
#> 2         AK     14000    12.42          9      72000 19.80           0
#> 3         AK     14000    12.42          9      72000 19.80           0
#> 4         AK     14000    12.42          9      72000 19.80           0
#> 5         AK     14000    12.42          9      72000 19.80           0
#> 6         AK     16000     7.90          3      35500  6.59           0
#>   revol_util total_acc bad_loan longest_credit_length   ID fold
#> 1       74.6        26        0                    17 2320    0
#> 2       74.6        26        0                    17 2320    1
#> 3       74.6        26        0                    17 2320    2
#> 4       74.6        26        0                    17 2320    3
#> 5       74.6        26        0                    17 2320    4
#> 6       18.1        26        0                    14 2574    0
#>   TargetEncode_addr_state
#> 1               0.1346154
#> 2               0.1454545
#> 3               0.1250000
#> 4               0.1911765
#> 5               0.1142857
#> 6               0.1346154



Created on 2019-03-14 by the 
reprex
package
 (v0.2.0).","['r', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
55009590,55009590,2019-03-05T18:50:15,2019-03-18 18:50:20Z,0,"I was wondering how to build h2o.glm regression with intercept only (Null Model)


The equivalent in lm would be 
lm(y~1, data = data)


Thanks!","['r', 'regression', 'h2o', 'intercept']",Chenying Gao,https://stackoverflow.com/users/8419421/chenying-gao,310
54975474,54975474,2019-03-04T01:23:44,2019-04-16 23:08:01Z,0,"I was trying to perform Autoencoder for anomaly detection. I used H2O R package to generate reconstruction MSE for a sample data using 
h2o.anomaly
 function. However, I have also tried to manually calculate it by myself according the the MSE formula from the documentation link below:

http://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html#mse-mean-squared-error


The training data consisting of three features and 5 rows that I used to build the model is as below:


head(train_dat)

  Feature1  Feature2 Feature3
1    68.18 0.1806535 3.871201
2    71.51 0.3987761 2.484907
3    67.77 0.4285304 3.332205
4    69.58 0.1823216 2.890372
5    70.98 0.4134333 1.791759



The test data consisting of three features and 5 rows that I used for prediction is as below:


head(test_dat)

  Feature1  Feature2 Feature3
1 68.33000 0.4350239 2.708050
2 73.98000 0.5550339 3.044522
3 67.11000 0.7323679 2.639057
4 69.90395 0.9999787 4.499810
5 71.28867 0.4882539 3.091042



After running training and prediction, the reconstructed features are as below:


head(mod.out)

  reconstr_Feature1 reconstr_Feature2 reconstr_Feature3
1          69.66297         0.4239244          2.346250
2          69.88329         0.3963843          2.381598
3          69.46544         0.4610502          2.233164
4          68.96117         0.4229165          2.676295
5          69.63208         0.3895452          2.530025



When I used the 
h2o.anomaly
 function for MSE calculation, I received MSE output as below:


head(mse.list)

  Reconstruction.MSE
1         0.05310159
2         0.57037600
3         0.54427385
4         2.08407248
5         0.14251951



However, when I tried to calculate the MSE by applying the function below, I obtained different MSE output:


mod.anon.validate <- apply((test_dat - mod.out)^2, 1, mean)
mse.list.validate <- as.data.frame(mod.anon.validate)
head(mse.list.validate)

  mod.anon.validate
1         0.6359438
2         5.7492281
3         1.9288268
4         1.5156829
5         1.0229217



I was wondering what I have done wrong in my manual MSE calculation? When it is called ""Reconstruction MSE"", is it different from the general MSE? The full R script is as below:


### H2O Autoencoder test run ###

#Load test and training data.
test_dat <- read.table(""sample.test.dat"", header=TRUE)
train_dat <- read.table(""sample.train.dat"", header=TRUE)

#Start H2O
library(h2o)
localH2O <- h2o.init(port =54321)

#Training and deep learning

feature_names <- names(train_dat[1:3])

unmod.hex <- as.h2o(train_dat, destination_frame=""train.hex"") ; mod.hex=as.h2o(test_dat, destination_frame=""test.hex"")

unmod.dl <- h2o.deeplearning(x=feature_names,
        training_frame=unmod.hex,
        autoencoder = TRUE,
        reproducible = T,
        hidden = c(3,2,3), epochs = 50,
        activation = ""Tanh"")

#Output result

mod.out <- as.data.frame(h2o.predict(unmod.dl,mod.hex,type=response))

mod.anon <- h2o.anomaly(unmod.dl, mod.hex, per_feature=FALSE)
mse.list <- as.data.frame(mod.anon)

mod.anon.validate <- apply((test_dat - mod.out)^2, 1, mean)
mse.list.validate <- as.data.frame(mod.anon.validate)



Thanks for your help.","['r', 'h2o', 'autoencoder', 'anomaly-detection']",bison72,https://stackoverflow.com/users/1594810/bison72,324
54968458,54968458,2019-03-03T11:45:43,2019-03-04 17:39:04Z,127,"I'm trying to set column names, but I encounter an error:




H2OValueError: Argument 
names




Code:


index_columns_names =  [""Date""]
generator_output_columns_names = [""GenOut""]
generator_v_columns_names = [""GenVar""]
turb_bearing_vib_columns_names =[""TurbBearingVib""+str(i) for i in range(1,6)]
gen_bearing_vib_columns_names = [""GenBearingVib""+str(i) for i in range(7,9)]
input_file_column_names = index_columns_names + generator_output_columns_names + generator_v_columns_names + turb_bearing_vib_columns_names + gen_bearing_vib_columns_names
data = h2o.upload_file(""data\Data_SLA_Unit_1_2018.csv"")
data.set_names(input_file_column_names);



How to fix this problem?","['python', 'h2o', 'names', 'valueerror']",Unknown,,N/A
54961216,54961216,2019-03-02T17:39:13,2019-03-03 08:39:06Z,0,"I am a newbie using H2O. I am trying to run H2OGridSearch with GBM to get my best hyper parameters. I am following the instructions given at 
H2O-AI Github repo
. It worked well when I was trying Regression but now when I am trying classification it is giving me an error.


Here is my code:


    import h2o
from h2o.automl import H2OAutoML
h2o.init(nthreads = -1, max_mem_size = 8)
data = h2o.import_file(""train.csv"")
y = ""target""
data[y] = data[y].asfactor()
X = data.columns
X.remove(y)
from h2o.grid.grid_search import H2OGridSearch
from h2o.estimators import H2OGradientBoostingEstimator

# hyper_params = {'max_depth' : range(1,30,2)}
hyper_params = {'max_depth' : [4,6,8,12,16,20]} ##faster for larger datasets

#Build initial GBM Model
gbm_grid = H2OGradientBoostingEstimator(
        ## more trees is better if the learning rate is small enough 
        ## here, use ""more than enough"" trees - we have early stopping
        ntrees=10000,
        ## smaller learning rate is better
        ## since we have learning_rate_annealing, we can afford to start with a 
        #bigger learning rate
        learn_rate=0.05,
        ## learning rate annealing: learning_rate shrinks by 1% after every tree 
        ## (use 1.00 to disable, but then lower the learning_rate)
        learn_rate_annealing = 0.99,
        ## sample 80% of rows per tree
        sample_rate = 0.8,
        ## sample 80% of columns per split
        col_sample_rate = 0.8,
        ## fix a random number generator seed for reproducibility
        seed = 1234,
        ## score every 10 trees to make early stopping reproducible 
        #(it depends on the scoring interval)
        score_tree_interval = 10, 
        ## early stopping once the validation AUC doesn't improve by at least 0.01% for 
        #5 consecutive scoring events
        stopping_rounds = 5,
        stopping_metric = ""AUC"",
        stopping_tolerance = 1e-4)

#Build grid search with previously made GBM and hyper parameters
grid = H2OGridSearch(gbm_grid,hyper_params,
                         grid_id = 'depth_grid',
                         search_criteria = {'strategy': ""Cartesian""})


#Train grid search
grid.train(x=X, 
           y=y,
           training_frame = data)



Here is the error:


`


H2OResponseError                          Traceback (most recent call last)
<ipython-input-14-cc0918796da0> in <module>()
     38 grid.train(x=X, 
     39            y=y,
---> 40            training_frame = data)

~\Anaconda3\lib\site-packages\h2o\grid\grid_search.py in train(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, **params)
    207         x = list(xset)
    208         parms[""x""] = x
--> 209         self.build_model(parms)
    210 
    211 

~\Anaconda3\lib\site-packages\h2o\grid\grid_search.py in build_model(self, algo_params)
    225             y = y if y in training_frame.names else training_frame.names[y]
    226             self.model._estimator_type = ""classifier"" if training_frame.types[y] == ""enum"" else ""regressor""
--> 227         self._model_build(x, y, training_frame, validation_frame, algo_params)
    228 
    229 

~\Anaconda3\lib\site-packages\h2o\grid\grid_search.py in _model_build(self, x, y, tframe, vframe, kwargs)
    247         rest_ver = kwargs.pop(""_rest_version"") if ""_rest_version"" in kwargs else None
    248 
--> 249         grid = H2OJob(h2o.api(""POST /99/Grid/%s"" % algo, data=kwargs), job_type=(algo + "" Grid Build""))
    250 
    251         if self._future:

~\Anaconda3\lib\site-packages\h2o\h2o.py in api(endpoint, data, json, filename, save_to)
    101     # type checks are performed in H2OConnection class
    102     _check_connection()
--> 103     return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
    104 
    105 

~\Anaconda3\lib\site-packages\h2o\backend\connection.py in request(self, endpoint, data, json, filename, save_to)
    405                                     auth=self._auth, verify=self._verify_ssl_cert, proxies=self._proxies)
    406             self._log_end_transaction(start_time, resp)
--> 407             return self._process_response(resp, save_to)
    408 
    409         except (requests.exceptions.ConnectionError, requests.exceptions.HTTPError) as e:

~\Anaconda3\lib\site-packages\h2o\backend\connection.py in _process_response(response, save_to)
    741         # Client errors (400 = ""Bad Request"", 404 = ""Not Found"", 412 = ""Precondition Failed"")
    742         if status_code in {400, 404, 412} and isinstance(data, (H2OErrorV3, H2OModelBuilderErrorV3)):
--> 743             raise H2OResponseError(data)
    744 
    745         # Server errors (notably 500 = ""Server Error"")

H2OResponseError: Server error water.exceptions.H2OIllegalArgumentException:
  Error: Illegal argument: training_frame of function: grid: Cannot append new models to a grid with different training input


  Request: POST /99/Grid/gbm
    data: {'search_criteria': ""{'strategy': 'Cartesian'}"", 'hyper_parameters': ""{'max_depth': [4, 6, 8, 12, 16, 20]}"", 'ntrees': '10000', 'learn_rate': '0.05', 'learn_rate_annealing': '0.99', 'sample_rate': '0.8', 'col_sample_rate': '0.8', 'seed': '1234', 'score_tree_interval': '10', 'stopping_rounds': '5', 'stopping_metric': 'AUC', 'stopping_tolerance': '0.0001', 'training_frame': 'py_1_sid_af08', 'response_column': 'target', 'grid_id': 'depth_grid'}



Can anyone help me with this?","['python', 'h2o', 'grid-search', 'gbm']",Rupesh Acharya,https://stackoverflow.com/users/11140923/rupesh-acharya,31
54943424,54943424,2019-03-01T11:09:35,2019-03-18 19:40:58Z,428,"I have created h20 random forest model for fraud prediction.now while scoring using predict function  for test data. I got below dataframe from predict function output.


Now for 2nd records it predicted 1 but probability of p1 is far less than p0. What's the correct probability scores (p0/1) and classification we can use for my fraud prediction model?


If these are not correct probabilities then calibrated probabilities calculated using parameters(calibrate_model = True) as mentioned below will give correct probability?


    nfolds=5
    rf1 = h2o.estimators.H2ORandomForestEstimator(
        model_id = ""rf_df1"", 
        ntrees = 200,
        max_depth = 4,
        sample_rate = .30,
       # stopping_metric=""misclassification"",
       # stopping_rounds = 2, 
        mtries = 6,
        min_rows = 12,
        nfolds=3,
        distribution = ""multinomial"",
        fold_assignment=""Modulo"",
        keep_cross_validation_predictions=True,
        calibrate_model = True,
        calibration_frame = calib,
        weights_column = ""weight"",
        balance_classes = True
      #  stopping_tolerance = .005)
       )

        predict p0          p1
    1   0   0.9986012   0.000896514
    2   1   0.9985695   0.000448676
    3   0   0.9981387   0.000477767","['machine-learning', 'h2o', 'predictive', 'h2o4gpu']",Rob,https://stackoverflow.com/users/162698/rob,15.1k
54927475,54927475,2019-02-28T14:02:35,2019-02-28 14:19:36Z,164,"hi guys so i'm currently trying to use h2o and encountering a problem for uploading the data


train = h2o.upload_file(""‪train_FD004.txt"")
test  = h2o.upload_file(""test_FD004.txt"")
train.set_names(input_file_column_names);
test.set_names(input_file_column_names);



how to fix this?","['python', 'h2o']",Michael Titian Adinata,https://stackoverflow.com/users/11105759/michael-titian-adinata,7
54919798,54919798,2019-02-28T06:38:39,2019-03-18 19:05:39Z,213,"My machine has 20 cores on its CPU, but when running Driverless AI, it uses only 4 of them.How can I make it use more cores for faster results?","['cpu', 'cpu-usage', 'h2o', 'multicore', 'driverless-ai']",Roshan Shah,https://stackoverflow.com/users/10983171/roshan-shah,13
54903280,54903280,2019-02-27T10:27:20,2019-03-19 18:06:39Z,166,"There are accessors such as 
h2o.logloss
 and 
h2o.residual_deviance
 but I want the penalized likelihood which is the value optimized during training. Is there a way to access this value without re-computing it by hand?","['h2o', 'glm']",James Hirschorn,https://stackoverflow.com/users/1349673/james-hirschorn,"7,886"
54888889,54888889,2019-02-26T15:26:05,2019-02-27 14:28:10Z,0,"I have a data set called 
Data
, with 30 scaled and centered features and 1 outcome with column name 
OUTCOME
, referred to 700k records, stored in 
data.table
 format. I computed its PCA, and observed that its first 8 components account for the 95% of the variance. I want to train a random forest in 
h2o
, so this is what I do:


Data.pca=prcomp(Data,retx=TRUE) # compute the PCA of Data
Data.rotated=as.data.table(Data.pca$x)[,c(1:8)] # keep only first 8 components
Data.dump=cbind(Data.rotated,subset(Data,select=c(OUTCOME))) # PCA dataset plus outcomes for training



This way I have a dataset 
Data.dump
 where I have 8 features that are rotated on the PCA components, and at each record I associated its outcome.


First question: is this rational? or do I have to permute somehow the outcomes vector? or the two things are unrelated?


Then I split 
Data.dump
 in two sets, 
Data.train
 for training and 
Data.test
 for testing, all 
as.h2o
. The I feed them to a random forest:


rf=h2o.randomForest(training_frame=Data.train,x=1:8,y=9,stopping_rounds=2,
                    ntrees=200,score_each_iteration=T,seed=1000000)
rf.pred=as.data.table(h2o.predict(rf,Data.test))



What happens is that 
rf.pred
 seems not so similar to the original outcomes 
Data.test$OUTCOME
. I tried to train a neural network as well, and did not even converge, crashing R.


Second question: is it because I am carrying on some mistake from the PCA treatment? or because I badly set up the random forest? Or I am just dealing with annoying data?


I do not know where to start, as I am new to data science, but the workflow seems correct to me.


Thanks a lot in advance.","['r', 'random-forest', 'pca', 'h2o']",marco,https://stackoverflow.com/users/3025376/marco,589
54887717,54887717,2019-02-26T14:25:49,2019-03-04 08:36:00Z,389,"I use Python's H2O (version 3.22.1.3), and I was wondering if it is possible to observe each tree's predictions in the Random Forest, like we do in the case of scikit-learn's RandomForestRegressor.estimators_ method. I tried to use h2o.predict_leaf_node_assignment(), but it brings either the prediction path for each tree or (supposedly) the id of the leaf node based on which the prediction was made. In the last version, H2O added the Tree class, but unfortunately, it does not have any predict() method. Although I can access any node in any of the random forest's trees, still my implementation of the tree predict function using the tree's recently implemented API (even if any correct), is extremely slow. So, my question is:


(a) Can I obtain tree predictions natively, and if yes, then how?


(b) If no, do the H2O developers plan to implement this feature in future releases?


Any response would be greatly appreciated.


UPDATE: Thank you, Joe, for your response. As for now (before the feature is directly implemented), here is the only workaround I could think of which generates tree predictions. 


# Suppose we have random forest model called drf with ntrees=70 and want to make predictions on df_valid
# After executing the code below, we get a dataframe tree_predictions with ntrees (in our case 70) columns, where i-th column corresponds to the predictions of i-th tree, and the same number of rows as df_valid.
# Extract the trees to create prediction intervals
# Number of trees
ntrees = 70

from h2o.tree import H2OTree
# Extract all the tree of drf, create the list of prediction trees
list_of_trees = [H2OTree(model = drf, tree_number = t, tree_class = None) for t in range(ntrees)]

# leaf_nodes contains the node_id's of tree leaves with predictions
leaf_nodes = drf.predict_leaf_node_assignment(df_valid, type='Node_ID').as_data_frame()

# tree_predictions is the dataframe with predictions for all the 70 trees
tree_predictions = pd.DataFrame(columns=['T'+str(t+1) for t in range(ntrees)])
for t in range(ntrees):
    tr = list_of_trees[t]
    node_ids = np.array(tr.node_ids)
    treePred = lambda n: tr.predictions[np.where(node_ids==n)[0][0]] 
    tree_predictions['T'+str(t+1)] = leaf_nodes['T'+str(t+1)].apply(treePred)enter code here","['python', 'tree', 'random-forest', 'h2o']",Unknown,,N/A
54885921,54885921,2019-02-26T12:46:51,2019-02-27 00:30:25Z,0,"I am trying to produce a prediction from new data using a XGBoost 
mojo
 object in H2O. When I predict though, various messages, giving warnings, get displayed


Feb 26, 2019 12:43:47 PM ml.dmlc.xgboost4j.java.NativeLibrary extractAndLoad
WARNING: Cannot load library from path lib/linux_64/libxgboost4j_gpu.so
Feb 26, 2019 12:43:47 PM ml.dmlc.xgboost4j.java.NativeLibrary extractAndLoad
WARNING: Cannot load library from path lib/libxgboost4j_gpu.so
Feb 26, 2019 12:43:47 PM ml.dmlc.xgboost4j.java.NativeLibrary doLoad
WARNING: Failed to load library from both native path and jar!
Feb 26, 2019 12:43:47 PM ml.dmlc.xgboost4j.java.NativeLibraryLoaderChain loadNativeLibs
INFO: Cannot load library: xgboost4j_gpu (lib/linux_64/libxgboost4j_gpu.so)
Feb 26, 2019 12:43:47 PM ml.dmlc.xgboost4j.java.NativeLibrary extractAndLoad
INFO: Loaded library from lib/linux_64/libxgboost4j_omp.so (/tmp/libxgboost4j_omp7945713229272382570.so)

       predict          setosa      versicolor       virginica
1       setosa 0.9961976408958 0.0030118888244 0.0007904054946
2       setosa 0.9963765740395 0.0026796606835 0.0009437160916
3       setosa 0.9963235855103 0.0028859297745 0.0007905053790
4       setosa 0.9963260293007 0.0028859369922 0.0007880008779
5       setosa 0.9961976408958 0.0030118888244 0.0007904054946



Here is a basic reproducible example:


library(tidyverse)
library(h2o)

h2o.init(nthreads = -1, max_mem_size = '5g') # All available cores

data(iris)
iris.hex <- as.h2o(iris, destination_frame = ""iris.hex"")

iris.gbm <- h2o.xgboost(y = 5, x = 1:4, training_frame = iris.hex, ntrees = 100,
                        max_depth = 3,
                        learn_rate = 0.2,
                        distribution= ""AUTO"")

h2o.download_mojo(iris.gbm, ""Mojo_models/"", get_genmodel_jar = T)
h2o.shutdown()



Warning messages occurs once I try and perform the predictions


h2o.mojo_predict_df(iris, mojo_zip_path = ""Mojo_models/XGBoost_model_R_1551184956713_1.zip"", genmodel_jar_path = ""Mojo_models/h2o-genmodel.jar"", 
                    java_option =  '-Xmx1g -XX:ReservedCodeCacheSize=256m')



Is this something I should be worried about? If not, why are the messages occurring and can I suppress them somehow?


Relevant session info:


R version 3.4.2 (2017-09-28)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.04.4 LTS

Matrix products: default
BLAS: /usr/lib/libblas/libblas.so.3.0
LAPACK: /usr/lib/lapack/liblapack.so.3.0

other attached packages:
 [1] h2o_3.22.1.1    forcats_0.3.0   stringr_1.3.1   dplyr_0.7.8     purrr_0.3.0     readr_1.2.1     tidyr_0.8.2     tibble_1.4.2    ggplot2_3.1.0   tidyverse_1.2.1","['r', 'h2o']",Unknown,,N/A
54882451,54882451,2019-02-26T09:39:35,2023-01-25 15:39:18Z,0,"When running H20's AutoML function I suddenly get this error, which I couldn't find anywhere else on the internet:


ERROR MESSAGE:


Object 'dummy' not found for argument: model_id


I ran the same code before without problems, the error only occurred after I added features to my dataset (all features are numeric, data is confidential so can't post a working example).


aml <- h2o.automl(x = features, 
                  y = response,
                  training_frame = train_hf,
                  validation_frame = valid_hf,
                  balance_classes = TRUE,
                  max_runtime_secs = 60) # tried 3600 as well



Has anybody encountered this error before and resolved it?




Full error message:


ERROR: Unexpected HTTP Status code: 404 Not Found (url = http://localhost:54321/99/Models.bin/dummy?dir=%2FUsers%2Fshiringlander%2FDocuments%2FGit%2Ftfs%2Fmodels%2Fdummy&force=FALSE)

water.exceptions.H2OKeyNotFoundArgumentException
 [1] ""water.exceptions.H2OKeyNotFoundArgumentException: Object 'dummy' not found for argument: model_id""                  
 [2] ""    water.api.ModelsHandler.getFromDKV(ModelsHandler.java:95)""                                                      
 [3] ""    water.api.ModelsHandler.exportModel(ModelsHandler.java:219)""                                                    
 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                    
 [5] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                                  
 [6] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                          
 [7] ""    java.lang.reflect.Method.invoke(Method.java:498)""                                                               
 [8] ""    water.api.Handler.handle(Handler.java:63)""                                                                      
 [9] ""    water.api.RequestServer.serve(RequestServer.java:482)""                                                          
[10] ""    water.api.RequestServer.doGeneric(RequestServer.java:327)""                                                      
[11] ""    water.api.RequestServer.doGet(RequestServer.java:251)""                                                          
[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:735)""                                                   
[13] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                   
[14] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                         
[15] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)""                                     
[16] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                             
[17] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)""                                      
[18] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                              
[19] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                  
[20] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                          
[21] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                
[22] ""    water.webserver.jetty8.Jetty8ServerAdapter$LoginHandler.handle(Jetty8ServerAdapter.java:119)""                   
[23] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                          
[24] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                
[25] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                        
[26] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                 
[27] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                  
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:973)""                
[29] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1035)""
[30] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:641)""                                               
[31] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:231)""                                          
[32] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                         
[33] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                   
[34] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                               
[35] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                
[36] ""    java.lang.Thread.run(Thread.java:748)"" 

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Object 'dummy' not found for argument: model_id



R session info:


R version 3.5.2 (2018-12-20)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Mojave 10.14.3

Matrix products: default
BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] h2o_3.23.0.4566

loaded via a namespace (and not attached):
[1] compiler_3.5.2  tools_3.5.2     RCurl_1.95-4.11 yaml_2.2.0      knitr_1.21      jsonlite_1.6    xfun_0.4        bitops_1.0-6","['r', 'h2o', 'automl']",Shirin Elsinghorst,https://stackoverflow.com/users/6623620/shirin-elsinghorst,310
54871817,54871817,2019-02-25T17:41:10,2019-02-27 01:03:39Z,0,"I would like to check result of leave-one-out cross-validation for my quite small df in h2o. This is my input df: 
https://drive.google.com/file/d/1UiIkxlHCq1tJZNOH6hQD30gEMaPdmhgh/view?usp=sharing


Is it possible to set nfolds (i.e. nfolds=nrow(df)) parameter in h2o to get such cross-validation?
I can't set nfolds > 25 for nrow(df)=69.


u$dc=as.factor(u$dc)
train <- as.h2o(u)
model <- h2o.gbm(x= colnames(train)[1:15],
                y=""dc"", training_frame=train,
                nfolds = 25,
                learn_rate = 0.06,
                ntrees = 90, max_depth = 3,   
                min_rows = 2,
                distribution = ""bernoulli"")



I get exception in the above code:


Error: water.exceptions.H2OIllegalArgumentException:
     Not enough data to create 25 random cross-validation splits. Either reduce nfolds, specify a larger dataset



It is thrown in ModelBuilder.java:


    at hex.ModelBuilder.cv_makeWeights(ModelBuilder.java:357)
    at hex.ModelBuilder.computeCrossValidation(ModelBuilder.java:276)
    at hex.ModelBuilder$1.compute2(ModelBuilder.java:207)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1263)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)","['r', 'statistics', 'cross-validation', 'h2o']",Unknown,,N/A
54869578,54869578,2019-02-25T15:29:56,2019-02-25 17:10:15Z,59,"I would like to test drive H2O run from R. I can install it locally using install.packages no problem. There are several options to scale H2O up. For example, 
H2O4GPU
 and 
H2O Sparkling Water
. For security reasons, we would like to use these options on premise. If we pay for the hardware, would H2O still be free or do certain components/functionalities cost money? Sorry, this may be obvious to you but personally I am a bit overwhelmed by the possibilities at this point in time. Any clarification, would be very much appreciated. Thanks!",['h2o'],cs0815,https://stackoverflow.com/users/283538/cs0815,17.4k
54862099,54862099,2019-02-25T08:24:13,2019-05-10 11:08:24Z,0,"I have a CSV that has close to 2 months of data with data captured on an hourly basis.


The data set is as follows


Date_Time                    Freq
2018-06-01 01:00:00           232
2018-06-01 02:00:00           99
2018-06-01 03:00:00           90



Most time series examples mention this on yearly data. I am a bit lost with how to do forecasting on an hourly basis. What would be the best way to do it in R. There are many examples online that uses h2o, knn, ARIMA etc.","['r', 'time-series', 'h2o', 'facebook-prophet']",NinjaR,https://stackoverflow.com/users/7207987/ninjar,623
54853550,54853550,2019-02-24T15:39:22,2019-02-27 21:13:40Z,262,"I want run AutoML in h2o by using rest api? I know the url is /99/AutoMLBuilder. But I have no idea that how can I send the parameters. There is no sample code on the official web site. I can access model import/export by using curl because the parameters are flat. But it seems that maybe the parameters of AutoML are nested, and I cannot find any sample code or answers about the format of the parameters.","['rest', 'h2o', 'automl']",ppzhupapa,https://stackoverflow.com/users/5620671/ppzhupapa,25
54852453,54852453,2019-02-24T13:37:44,2019-02-25 03:24:20Z,0,"I am using 
automl()
 function of 
H2o
 package in R for regression.


Consider I am using the name ""aml"" for building models.


aml <- h2o.automl(x=x, y=y, training_frame = train_set,
              max_models = 20, seed = 1,
              keep_cross_validation_predictions = TRUE)



The leaderboard of 
automl()
 shows the top performed models. I am able to print the importance of the predictors through h2o.varimp() function and plot a graph for the same using 
h2o.varimp_plot()
 function for only the leader model (the best model given by automl function). 


h2o.varimp(aml@leader)
h2o.varimp_plot(aml@leader)



Is there any way to print the variable importance of the predictors for all the models in the leaderboard and plot a graph using the above two functions?","['r', 'h2o', 'leaderboard', 'automl']",user213544,https://stackoverflow.com/users/10229584/user213544,"2,126"
54844476,54844476,2019-02-23T18:01:56,2019-02-24 14:47:49Z,0,"I am trying to launch an H2O cluster on localhost but is taking too long to start a connection. This is the code I am using: 


    pkgs <- c(""methods"",""statmod"",""stats"",""graphics"",""RCurl"",""jsonlite"",""tools"",""utils"")
    for (pkg in pkgs) {
      if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
    }
    install.packages(""h2o"", type=""source"", repos=(c(""http://h2o-release.s3.amazonaws.com/h2o/rel-turchin/8/R"")))
library(h2o)
h2o.init(nthreads = -1)



What I have in the console is: 


H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    /var/folders/0c/2vddb1fs58q9tms_p7qzp2v00000gn/T//RtmpqOJp73/h2o_name_started_from_r.out
    /var/folders/0c/2vddb1fs58q9tms_p7qzp2v00000gn/T//RtmpqOJp73/h2o_name_started_from_r.err

    java version ""9.0.1""
    Java(TM) SE Runtime Environment (build 9.0.1+11)
    Java HotSpot(TM) 64-Bit Server VM (build 9.0.1+11, mixed mode)

    Starting H2O JVM and connecting: ...



No errors are produced but it seems to be not working. what can I do?


I update this question because now an error has been shown:


Error: Unable to establish connection with R session","['r', 'h2o']",Unknown,,N/A
54820194,54820194,2019-02-22T04:42:17,2019-02-25 17:55:03Z,0,"In GBM model, following parameters are used - 




col_sample_rate


col_sample_rate_per_tree


col_sample_rate_change_per_level




I understand how the sampling works and how many variables get considered for splitting at each level for every tree. I am trying to understand how many times each feature gets considered for making a decision. Is there a way to easily extract all sample of features used for making a splitting decision from the model object?


Referring to the explanation provided by H2O, 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/col_sample_rate.html
, is there a way to know 60 randomly chosen features for each split?


Thank you for your help!","['r', 'h2o']",Unknown,,N/A
54812865,54812865,2019-02-21T17:23:35,2019-02-21 18:32:02Z,139,"I'd like to create a self-contained Jupyter notebook that uses h2o to import and model data that resides in a relational database. 
The docs
 show an example where h2o is launched with the JDBC driver in the classpath, e.g.


java -cp <path_to_h2o_jar>:<path_to_jdbc_driver_jar> water.H2OApp



I'd prefer to start h2o from a notebook that's a standalone, reproducible artifact rather than have special steps to prep the environment prior to running the notebook. If I run the following snippet:


import h2o
h2o.init()

connection_url = ""jdbc:mysql://mysql.woolford.io/mydb""
select_query = ""SELECT description, price FROM mytable""
username = ""myuser""
password = ""b@dp@ss""
mytable_data = h2o.import_sql_select(connection_url, select_query, username, password)



... the 
import_sql_select
 method fails because the driver isn't loaded:


Server error java.lang.RuntimeException:
  Error: SQLException: No suitable driver found for jdbc:mysql://mysql.woolford.io/mydb



Is there a way to load the driver when the 
h2o.init()
 call is made? Or a best practice for this?","['jupyter-notebook', 'rdbms', 'h2o']",Alex Woolford,https://stackoverflow.com/users/2626491/alex-woolford,"4,553"
54755451,54755451,2019-02-18T21:14:11,2019-02-20 21:41:36Z,222,"I am wondering to know if h2o is capable of implementing Temporal Difference (reinforcement learning)? 


I know TensorFlow has the capability.",['h2o'],msol,https://stackoverflow.com/users/10244589/msol,13
54754644,54754644,2019-02-18T20:10:04,2019-02-20 20:39:32Z,0,"I am trying to (substantially) accelerate some R code by moving to R+h2o.ai.


I am grouping by a single factor variable but I get error when I try to compute windowed quantiles, skewness, or kurtosis.


Is there a list of summary functions in h2o that are incompatible with the split-apply-combine approach?  Does it only apply to sql-analog functions like sum, count, or stdev?


This code fails:


for(i in col_idx_list){
  proc_cols_list <- names(df.hex)[i]

  group_cols_list <- c(""group_variable_factor"")

  h2o.quantile(x=df.hex[,proc_cols_list])

  temp <- h2o.group_by(data=df.hex,
                       by=group_cols_list,
                       mean(proc_cols_list),
                       var(proc_cols_list),
                       skewness(proc_cols_list),
                       gb.control=list(na.methods=""ignore"")  )

  if(i ==first_index){
    df_summs <- temp
  } else {
    df_summs <- h2o.cbind(df_summs , temp[,2:ncol(temp)])
  }
}



This code runs fine:


for(i in col_idx_list){
  proc_cols_list <- names(df.hex)[i]

  group_cols_list <- c(""group_variable_factor"")

  h2o.quantile(x=df.hex[,proc_cols_list])

  temp <- h2o.group_by(data=df.hex,
                       by=group_cols_list,
                       mean(proc_cols_list),
                       var(proc_cols_list),
                       gb.control=list(na.methods=""ignore"")  )

  if(i ==first_index){
    df_summs <- temp
  } else {
    df_summs <- h2o.cbind(df_summs , temp[,2:ncol(temp)])
  }
}



Error text (truncated for brevity):


ERROR: Unexpected HTTP Status code: 400 Bad Request (url = http://localhost:54321/99/Rapids)
Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
ERROR MESSAGE:
No enum constant water.rapids.ast.prims.mungers.AstGroup.FCN.skewness
ERROR: Unexpected HTTP Status code: 404 Not Found (url = http://localhost:54321/3/Frames/RTMP_sid_8712_17?row_count=10)

ERROR MESSAGE:

Object 'RTMP_sid_8712_17' not found for argument: key","['r', 'group-by', 'h2o', 'split-apply-combine']",EngrStudent,https://stackoverflow.com/users/2259468/engrstudent,"1,972"
54753113,54753113,2019-02-18T18:12:12,2019-02-21 19:20:36Z,230,"I have build a StackedEnsembleModel by using autoML in my java code. But I got ""water.exceptions.H2ONotFoundArgumentException: Failed to find schema for version: 3 and type: StackedEnsembleModel"" exception when I try to export the model to a mojo zip file.


initH2OCloud();
try {
    registerH2OAlgorithm();
    AutoMLBuildSpec autoMLBuildSpec = new AutoMLBuildSpec();
    Frame trainFrame = importAndParseFeatureFileToH2O(""/home/data/MODEL.csv"");

    autoMLBuildSpec.input_spec.training_frame = trainFrame._key;
    autoMLBuildSpec.input_spec.response_column = ""level"";

    autoMLBuildSpec.build_models.exclude_algos = new Algo[2];
    autoMLBuildSpec.build_models.exclude_algos[0] = Algo.DeepLearning;
    autoMLBuildSpec.build_models.exclude_algos[1] = Algo.XGBoost;
    autoMLBuildSpec.build_control.keep_cross_validation_models = true;
    autoMLBuildSpec.build_control.keep_cross_validation_predictions = true;
    autoMLBuildSpec.build_control.keep_cross_validation_fold_assignment = true;
    autoMLBuildSpec.build_control.nfolds = 5;
    autoMLBuildSpec.build_control.project_name = ""test.prj"";
    autoMLBuildSpec.build_control.stopping_criteria.set_max_runtime_secs(300);
    AutoML aml = AutoML.makeAutoML(Key.make(), new Date(), autoMLBuildSpec);
    logger.info(""Begin AutoML..."");
    long beginTick = Calendar.getInstance().getTime().getTime();
    AutoML.startAutoML(autoMLBuildSpec).get();
    long usedTick = Calendar.getInstance().getTime().getTime() - beginTick;
    //save the leader model
    String modelSavePath = ""/home/data/tmp"";
    logger.info(""best model: {}, RMSE: {}, MAE: {}, RSLE: {}。"",
    aml.leader()._key.toString(),
    bigDecimalToString(new BigDecimal(Math.sqrt(aml.leader().mse())), 12),
    bigDecimalToString(new BigDecimal(Math.sqrt(aml.leader().mae())), 12),
    bigDecimalToString(new BigDecimal(Math.sqrt(aml.leader().rmsle())), 12));
    logger.info(""used {} seconds"", usedTick/1000L);
    logger.info(""export to mojo..."");
    try {
        aml.leader().getMojo().writeTo(new FileOutputStream(new File(modelSavePath + File.separator + aml.leader()._key + "".zip"")));
        logger.info(""export finished."");
    }
    catch (Throwable t) {
        logger.error(""got exception"", t);
    }
    logger.info(""aml.leaderboard: "");
    logger.info(aml.leaderboard());
}
finally {
    stopH2OCloud();
}



The exception thrown when I call aml.leader().getMojo().writeTo() method. I'm using h2o 3.22.0.1.
Thanks in advance.","['java', 'h2o', 'mojo']",ppzhupapa,https://stackoverflow.com/users/5620671/ppzhupapa,25
54746272,54746272,2019-02-18T11:28:52,2019-02-18 15:33:12Z,687,"I am trying to run different classifiers using H2O estimator. While running the GLM classifier, however, I get an error message. Pasting the relevant code below. 


CLASSIFIERS = {
'RandomForest': H2ORandomForestEstimator(ntrees=200, keep_cross_validation_predictions=True, stopping_rounds=2, score_each_iteration=True, model_id=""rf_cv_all_folds_""+CLASSIFIER_DATE_STR, seed=1000000),
'RandomForest_depth6': H2ORandomForestEstimator(ntrees=200, max_depth=6,keep_cross_validation_predictions=True, stopping_rounds=2, score_each_iteration=True, model_id=""rf_cv_all_folds_""+CLASSIFIER_DATE_STR, seed=1000000),
'GLM': H2OGeneralizedLinearEstimator(family= ""binomial"", lambda_ = 0, compute_p_values = True, remove_collinear_columns=True, keep_cross_validation_predictions=True, model_id=""glm_cv_all_folds_""+CLASSIFIER_DATE_STR, seed=1000000), # todo: regularization? http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html#regularization-parameters-in-glm
'GLM': H2OGeneralizedLinearEstimator(family= ""binomial"", lambda_ = 0, compute_p_values = True, remove_collinear_columns=True, keep_cross_validation_predictions=True, model_id=""glm_cv_all_folds_""+CLASSIFIER_DATE_STR, seed=1000000, max_iterations=10000000), # todo: regularization? http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html#regularization-parameters-in-glm
'GBM': H2OGradientBoostingEstimator(ntrees=200, learn_rate=0.2, max_depth=20, stopping_tolerance=0.01, stopping_rounds=2, score_each_iteration=True, keep_cross_validation_predictions=True, model_id=""gbm_cv_all_folds_""+CLASSIFIER_DATE_STR, seed=1000000),
'NaiveBayes': H2ONaiveBayesEstimator(keep_cross_validation_predictions=True,model_id=""naive_bayes_cv_all_folds_""+CLASSIFIER_DATE_STR, seed=1000000)
} 



When running the code on a 
sample dataset
, I started getting the following 'UserWarning' for GLM classifier.


C:\Program Files\Anaconda2\lib\site-packages\h2o\job.py:69: UserWarning: Reached maximum number of iterations 50!
  warnings.warn(w)



However, when I tried running the code on the entire dataset (3gb), I get the following error message for GLM classifier. 


   Job with key $03017f00000132d4ffffffff$_94bc493aa6606867c224fe00dac44410 failed with an exception: hex.gram.Gram$NonSPDMatrixException
    stacktrace: 
    hex.gram.Gram$NonSPDMatrixException
            at hex.gram.Gram$Cholesky.solve(Gram.java:664)
            at hex.gram.Gram$Cholesky$1.compute(Gram.java:607)
            at jsr166y.RecursiveAction.exec(RecursiveAction.java:160)
            at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
            at jsr166y.ForkJoinTask.doInvoke(ForkJoinTask.java:360)
            at jsr166y.ForkJoinTask.invokeAll(ForkJoinTask.java:741)
            at hex.gram.Gram$Cholesky.solve(Gram.java:611)
            at hex.gram.Gram$Cholesky.getInv(Gram.java:617)
            at hex.glm.GLM$GLMDriver.fitModel(GLM.java:1119)
            at hex.glm.GLM$GLMDriver.computeSubmodel(GLM.java:1169)
            at hex.glm.GLM$GLMDriver.computeImpl(GLM.java:1254)
            at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:218)
            at hex.glm.GLM$GLMDriver.compute2(GLM.java:571)
            at water.H2O$H2OCountedCompleter.compute(H2O.java:1395)
            at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
            at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
            at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
            at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
            at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



Even when I increase 
max_iternations = 1000000
 for full dataset, I get the same error. 


Any help in this regard would be helpful. Thank you.","['machine-learning', 'h2o', 'glm']",Unknown,,N/A
54731322,54731322,2019-02-17T08:10:11,2019-02-21 16:42:42Z,120,I am just learning H2O Flow as per its tutorials on Youtube. But my titanic.hex file does not show up into Split Frame section of the notebook. Any help please.,['h2o'],Athar Kharal,https://stackoverflow.com/users/5719603/athar-kharal,11
54726489,54726489,2019-02-16T18:49:34,2019-02-21 17:34:12Z,0,"I tried to use the H2o predict_json in R, 


h2o.predict_json(modelpath, jsondata)



and got the error message:


Error: Could not find or load main class water.util.H2OPredictor



I am using h2o_3.20.0.8. 


I searched the documentation from H2o but didn't help.


> h2o.predict_json(modelpath, jsondata)
$error
[1] ""Error: Could not find or load main class water.util.H2OPredictor""

Warning message:
In system2(java, args, stdout = TRUE, stderr = TRUE) :
  running command ''java'  -Xmx4g -cp .:/Library/Frameworks/R.framework/Versions/3.5/Resources/library/mylib/Models/h2o-genmodel.jar:/Library/Frameworks/R.framework/Versions/3.5/Resources/library/mylib/Models:genmodel.jar:/ water.util.H2OPredictor  /Library/Frameworks/R.framework/Versions/3.5/Resources/library/mylib/Models/mymodel.zip '[{""da1"":252,""da2"":22,""da3"":62,""da4"":63,""da5"":84.83}]' 2>&1' had status 1","['r', 'h2o']",Bhargav Rao,https://stackoverflow.com/users/4099593/bhargav-rao,51.9k
54711909,54711909,2019-02-15T14:55:04,2019-02-20 16:50:45Z,0,"I have noticed that for some runs of:


train=as.h2o(u)
mod = h2o.glm(family= ""binomial"", x= c(1:15), y=""dc"",  
       training_frame=train, missing_values_handling = ""Skip"",
       lambda = 0, compute_p_values = TRUE, nfolds = 10,
       keep_cross_validation_predictions= TRUE)



there are NaNs in cross-validation metrics summary of AUC for some cv iterations of the model.


For example:


print(mod@model$cross_validation_metrics_summary[""auc"",])



Cross-Validation Metrics Summary: 


          mean         sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid cv_5_valid cv_6_valid cv_7_valid cv_8_valid cv_9_valid cv_10_valid

auc 0.63244045 0.24962118       0.25  0.6666667  0.8095238        1.0  0.6666667 0.46666667        NaN        NaN        1.0         0.2



NaN in CV seems to appear less frequently when I set smaller nfolds=7.


How these NaN values should be interpreted and when h2o cross-validation outputs them?


I suppose it happens when AUC can't be assessed correctly in an iteration. My training set has 70 complete rows.


Can such AUC cross-validation results (containing NaNs) be considered as reliable?","['r', 'cross-validation', 'h2o', 'glm', 'auc']",Unknown,,N/A
54671158,54671158,2019-02-13T13:17:43,2019-02-14 09:12:29Z,0,"While loading my dataset using python code on the AWS server using Spyder, I get the following error:


  File ""<ipython-input-19-7b2e7b5812b3>"", line 1, in <module>
    ffemq12 = load_h2odataframe_returns(femq12) #; ffemq12 = add_fold_column(ffemq12)

  File ""D:\Ashwin\do\init_sm.py"", line 106, in load_h2odataframe_returns
    fr=h2o.H2OFrame(python_obj=returns)

  File ""C:\Program Files\Anaconda2\lib\site-packages\h2o\frame.py"", line 106, in __init__
    column_names, column_types, na_strings, skipped_columns)

  File ""C:\Program Files\Anaconda2\lib\site-packages\h2o\frame.py"", line 147, in _upload_python_object
    self._upload_parse(tmp_path, destination_frame, 1, separator, column_names, column_types, na_strings, skipped_columns)

  File ""C:\Program Files\Anaconda2\lib\site-packages\h2o\frame.py"", line 321, in _upload_parse
    ret = h2o.api(""POST /3/PostFile"", filename=path)

  File ""C:\Program Files\Anaconda2\lib\site-packages\h2o\h2o.py"", line 104, in api
    return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)

  File ""C:\Program Files\Anaconda2\lib\site-packages\h2o\backend\connection.py"", line 415, in request
    raise H2OConnectionError(""Unexpected HTTP error: %s"" % e)



I am running this python code on Spyder on the AWS server. The code works fine up to half the dataset (1.5gb/3gb) but throws an error if I increase the data size. I tried increasing the RAM from 61gb to 122 GB but it is still giving me the same error. 


Loading the data file   


femq12 = pd.read_csv(r""H:\Ashwin\dta\datafile.csv"")    
ffemq12 = load_h2odataframe_returns(femq12)



Initializing h2o


h2o.init(nthreads = -1,max_mem_size=""150G"")



Loading h2o




Connecting to H2O server at 
http://127.0.0.1:54321
... successful.
  --------------------------  ------------------------------------  H2O cluster uptime:         01 secs  H2O cluster timezone:       UTC  H2O
  data parsing timezone:  UTC  H2O cluster version:        3.22.1.3  H2O
  cluster version age:    18 days  H2O cluster total nodes:    1  H2O
  cluster free memory:    133.3 Gb  H2O cluster total cores:    16  H2O
  cluster allowed cores:  16  H2O cluster status:         accepting new
  members, healthy H2O connection proxy:  H2O internal security:

  False  H2O API Extensions:         Algos, AutoML, Core V3, Core V4 
  Python version:             2.7.15 final






I suspect it is a memory issue. But even after increasing RAM and max_mem_size, the dataset is not loading. 


Any ideas to fix the error would be appreciated. Thank you.","['python', 'h2o']",user2905919,https://stackoverflow.com/users/2905919/user2905919,43
54670342,54670342,2019-02-13T12:35:36,2019-02-13 22:23:56Z,179,"I want to save a model (export) from H2O Web UI and load in python. Can it be done? Can the opposite be done?
If so how? For automl?","['python', 'h2o']",Georgios Kourogiorgas,https://stackoverflow.com/users/8676754/georgios-kourogiorgas,351
54665401,54665401,2019-02-13T08:15:40,2019-02-13 08:15:40Z,85,"How to disable TRACE method in H2O server?


I start up h2o server via ""java -jar h2o.jar"" command, however the TRACE http-method is enabled. For security issue, where do I disable TRACE method. 
I check the web engine of H2O is jetty, does h2o can set jetty configuration file on start up command? 


Sincerely Yours","['h2o', 'http-method']",Stanley Fan,https://stackoverflow.com/users/1436755/stanley-fan,43
54578226,54578226,2019-02-07T16:44:15,2020-08-20 08:39:23Z,0,"I am using h2o.automl to build a model for the 
Kaggle House Prices dataset
.


When trying to predict results using the model trained on the test data, I get the following error:


Error: DistributedException from localhost/127.0.0.1:54321:
'Categorical value out of bounds, got 15, next cat starts at 40',
caused by java.lang.AssertionError: Categorical value out of bounds,
got 15, next cat starts at 40



How can I find out what variable this relates to?","['r', 'h2o']",matt_jay,https://stackoverflow.com/users/1162278/matt-jay,"1,261"
54564735,54564735,2019-02-07T00:40:49,2019-02-07 19:55:48Z,350,"I'm trying to use the h2o package. I'm running macOS Mojave with Anaconda installed. 


I've done a 
pip install h2o
 and it was successful. It installed at location 
/anaconda/lib/python3.6/site-packages
. 


I try to import it and get this error message: ""H2O requires colorama module of version 0.3.8 or newer. You have version 0.3.7.""


But when I do 
pip show colorama
 it says I have version 0.4.1, so the error message doesn't make sense. It's installed at the same location as h2o at  
/anaconda/lib/python3.6/site-packages


Any thoughts?","['python', 'installation', 'version', 'h2o', 'colorama']",Tom Roth,https://stackoverflow.com/users/5381490/tom-roth,"2,054"
54546706,54546706,2019-02-06T04:39:55,2019-02-07 07:56:13Z,114,"I found H2O has the function h2o.deepfeatures in R to pull the hidden layer features

https://www.rdocumentation.org/packages/h2o/versions/3.20.0.8/topics/h2o.deepfeatures


train_features <- h2o.deepfeatures(model_nn, train, layer=3)



But I didn't find any example in Python? Can anyone provide some sample code?","['python-3.x', 'pyspark', 'h2o']",Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
54541358,54541358,2019-02-05T19:03:48,2019-02-07 07:44:13Z,0,"I have a Pandas data frame and I need to convert it to H2O frame. I use the following code-


Code:


# Convert pandas dataframe to H2O frame
start_time = time.time()
input_data_matrix = h2o.H2OFrame(input_df)
logger.debug(""3. Time taken to convert H2O Frame- "" + str(time.time() - start_time))



Output:




2019-02-05 04:38:55,238 logger DEBUG 3. Time taken to convert H2O
  Frame- 9320.119945764542




The data frame (i.e. 
input_df
) size 183K x 435 with no null or NaN values. 


It is taking around 2 hours. Is there any better way to perform this operation?","['python', 'pandas', 'performance', 'dataframe', 'h2o']",Anwar Shaikh,https://stackoverflow.com/users/1293563/anwar-shaikh,"1,651"
54540948,54540948,2019-02-05T18:36:57,2019-02-06 14:29:28Z,394,"I am running h2o in R (using RMarkdown) and it is working fine when I run the chunks one at a time. However, when I try to knit the HTML file, it gives me an error as follows:




Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix, : Unexpected CURL error: Could not resolve host: localhost




Anyone know how to fix this? My code chunk is as follows:


if (try.h2o == TRUE){
  h2o.init()
  h2o.removeAll()

  train.data.h2o <- as.h2o(train.data)
  h2o.describe(train.data.h2o)

  tic(""\n\n\nTotal Time taken by H20 Auto ML to train: "")
  aml <- h2o.automl(y = label.names, 
                    training_frame = train.data.h2o,
                    max_models = 50,
                    seed = 1,
                    project_name = ""temp"")
  toc()
}","['r-markdown', 'h2o']",Nikhil Gupta,https://stackoverflow.com/users/8925915/nikhil-gupta,"1,486"
54511042,54511042,2019-02-04T06:21:36,2019-05-07 14:21:00Z,173,I have to change numeric columns to Enum type of h2o frame in sparkling water using Scala and how to print schema of h2o frame.,"['scala', 'h2o', 'sparkling-water']",Amandeep singh saini,https://stackoverflow.com/users/11011025/amandeep-singh-saini,11
54508507,54508507,2019-02-03T23:08:24,2019-02-06 23:55:04Z,0,"I'm trying to create an ensemble model in H2O from a number of GLM, GBM, and deep learning models.


Here's what I did so far.


Import relevant libraries:


import h2o
from h2o.estimators.glm import H2OGeneralizedLinearEstimator
from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o.estimators.deeplearning import H2ODeepLearningEstimator
from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator
from h2o.grid.grid_search import H2OGridSearch



The data can be downloaded from 
here
. Import:


airlines = h2o.import_file(path = ""/Users/alexwoolford/h2o/allyears2k.csv"", destination_frame = ""airlines.hex"")



Split into training/test sets:


airlines_80,airlines_20 = airlines.split_frame(ratios=[.8], destination_frames=[""airlines_80.hex"", ""airlines_20.hex""])



Define variables (predict y as a function of all the columns in x):


x= airlines.columns
y= ""ArrDelay""
x.remove(y)



Set common properties:


folds=5
assignment_type=""Modulo""
search_criteria={'strategy': 'RandomDiscrete', 'max_models': 5, 'seed': 1}



Use H2O's grid search to create a variety of models:


# GLM
glm_params = {""alpha"": [0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.],
              ""lambda"": [0, 1e-7, 1e-5, 1e-3, 1e-1]}

glm_grid = H2OGridSearch(model=H2OGeneralizedLinearEstimator(fold_assignment=assignment_type, nfolds=folds),
                         grid_id='glm_grid',
                         hyper_params=glm_params,
                         search_criteria=search_criteria)
glm_grid.train(x=x,
               y=y,
               training_frame=airlines_80,
               validation_frame=airlines_20)

# GBM
gbm_params = {'learn_rate': [0.01, 0.03],
              'max_depth': [3, 4, 5, 6, 9],
              'sample_rate': [0.7, 0.8, 0.9, 1],
              'col_sample_rate': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]}

gbm_grid = H2OGridSearch(model=H2OGradientBoostingEstimator(fold_assignment=assignment_type, nfolds=folds),
                         grid_id='gbm_grid',
                         hyper_params=gbm_params,
                         search_criteria=search_criteria)
gbm_grid.train(x=x,
               y=y,
               training_frame=airlines_80,
               validation_frame=airlines_20)

# Deep learning
dl_params = {'activation': ['rectifier', 'rectifier_with_dropout'],
             'hidden': [[10,10], [20,15], [50,50,50]],
             'l1': [0, 1e-3, 1e-5],
             'l2': [0, 1e-3, 1e-5]}

dl_grid = H2OGridSearch(model=H2ODeepLearningEstimator(fold_assignment=assignment_type, nfolds=folds),
                        grid_id='dl_grid',
                        hyper_params=dl_params,
                        search_criteria=search_criteria)

dl_grid.train(x=x,
              y=y,
              training_frame=airlines_80,
              validation_frame=airlines_20)



Get list of all the model_id's:


all_model_ids = glm_grid.model_ids + gbm_grid.model_ids + dl_grid.model_ids



Where I try to create an ensemble:


ensemble = H2OStackedEnsembleEstimator(base_models=all_model_ids)
ensemble.train(x=x, y=y, training_frame=airlines_80, validation_frame=airlines_20)



... the following error is thrown:


stackedensemble Model Build progress: | (failed)
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-26-bc7b6094816f> in <module>()
      1 ensemble = H2OStackedEnsembleEstimator(base_models=all_model_ids)
----> 2 ensemble.train(x=x, y=y, training_frame=airlines_80, validation_frame=airlines_20)

/anaconda3/lib/python3.6/site-packages/h2o/estimators/estimator_base.py in train(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, max_runtime_secs, ignored_columns, model_id, verbose)
    235             return
    236 
--> 237         model.poll(verbose_model_scoring_history=verbose)
    238         model_json = h2o.api(""GET /%d/Models/%s"" % (rest_ver, model.dest_key))[""models""][0]
    239         self._resolve_model(model.dest_key, model_json)

/anaconda3/lib/python3.6/site-packages/h2o/job.py in poll(self, verbose_model_scoring_history)
     75             if (isinstance(self.job, dict)) and (""stacktrace"" in list(self.job)):
     76                 raise EnvironmentError(""Job with key {} failed with an exception: {}\nstacktrace: ""
---> 77                                        ""\n{}"".format(self.job_key, self.exception, self.job[""stacktrace""]))
     78             else:
     79                 raise EnvironmentError(""Job with key %s failed with an exception: %s"" % (self.job_key, self.exception))

OSError: Job with key $03017f00000132d4ffffffff$_a2359a38ec8d31316aee91398f0249f8 failed with an exception: water.exceptions.H2OIllegalArgumentException: Base model does not keep cross-validation predictions: 5
stacktrace: 
water.exceptions.H2OIllegalArgumentException: Base model does not keep cross-validation predictions: 5
    at hex.StackedEnsembleModel.checkAndInheritModelProperties(StackedEnsembleModel.java:382)
    at hex.ensemble.StackedEnsemble$StackedEnsembleDriver.computeImpl(StackedEnsemble.java:234)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:218)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1395)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



Can you seem what I'm doing wrong?","['python', 'h2o', 'ensemble-learning']",Alex Woolford,https://stackoverflow.com/users/2626491/alex-woolford,"4,553"
54438185,54438185,2019-01-30T10:18:13,2019-01-31 18:57:21Z,0,"I found out that it is now possible to use 
stopping_metric = custom
 in 
h2o v3.22.1.1
 (wasn't available in 
v3.10.0.9
), however I didn't find anywhere how to implement it in R. 


this is a toy version of the problem.


library(h2o)
h2o.init()
x <- data.frame(
   x = rnorm(1000),
   z = rnorm(1000), 
   y = factor(sample(0:1, 1000, replace = T))
)
train <- as.h2o(x)
h2o.gbm(x = c('x','z'), y = 'y', training_frame = train, stopping_metric = 'custom', stopping_rounds = 3)



the error I get is the following:


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Illegal argument(s) for GBM model: GBM_model_R_1548836369139_123.  Details: ERRR on field: _stopping_metric: Custom metric function needs to be defined in order to use it for early stopping.



how can I define the 
custom
 
stopping_metric
 for the GBM?","['r', 'machine-learning', 'h2o']",davide,https://stackoverflow.com/users/7757484/davide,325
54417507,54417507,2019-01-29T09:17:32,2019-01-31 11:27:15Z,0,"I am working with 10GB training data frame. I use H2o library for faster computation. Each time I load the dataset, I should convert the data frame into H2o object which is taking so much time. Is there a way to store the converted H2o object ? (so that i can skip the as.H2o(trainingset) step each time I make trails on building models )","['r', 'h2o']",tomerpacific,https://stackoverflow.com/users/10632369/tomerpacific,"6,109"
54410097,54410097,2019-01-28T20:50:26,2019-02-03 01:55:43Z,352,"I am trying to connect H2O to a postgres db. I have both the postgres JDBC driver and the H2O jar in the same directory, however I get the following error when I try to start H2O with the driver.


java -cp postgresql-42.2.5.jar:h2o.jar water.H2OApp
Error: Could not find or load main class water.H2OApp



I've done a clean install of H2O but I still get the following error.","['jdbc', 'h2o']",MrT,https://stackoverflow.com/users/1026403/mrt,764
54380323,54380323,2019-01-26T16:29:48,2019-01-28 18:21:13Z,0,"I'm trying to use H2O on a RHEL 6.7 server with R 3.5.0.
This is what happens when I issue the 
h2o.init()
 command after loading the package with 
library(h2o)
:


H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    /tmp/Rtmp0k66FQ/h2o_enrico_started_from_r.out
    /tmp/Rtmp0k66FQ/h2o_enrico_started_from_r.err

java version ""1.8.0_20""
Java(TM) SE Runtime Environment (build 1.8.0_20-b26)
Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode)

Starting H2O JVM and connecting: ............................................................Cannot load library from path lib/linux_64/libxgboost4j_gpu.so
Cannot load library from path lib/libxgboost4j_gpu.so
Failed to load library from both native path and jar!
Cannot load library from path lib/linux_64/libxgboost4j_omp.so
Cannot load library from path lib/libxgboost4j_omp.so
Failed to load library from both native path and jar!
01-26 17:06:39.858 127.0.0.1:54321       83682  main      INFO: Found XGBoost backend with library: xgboost4j_minimal
01-26 17:06:39.958 127.0.0.1:54321       83682  main      INFO: Your system supports only minimal version of XGBoost (no GPUs, no multithreading)!
01-26 17:06:39.959 127.0.0.1:54321       83682  main      INFO: ----- H2O started  -----
01-26 17:06:39.959 127.0.0.1:54321       83682  main      INFO: Build git branch: rel-xu
01-26 17:06:39.959 127.0.0.1:54321       83682  main      INFO: Build git hash: 3ad2470fdea521c2d6fb3aa31d8c766042bdb992
01-26 17:06:39.959 127.0.0.1:54321       83682  main      INFO: Build git describe: jenkins-master-4525-7-g3ad2470
01-26 17:06:39.959 127.0.0.1:54321       83682  main      INFO: Build project version: 3.22.1.1
01-26 17:06:39.960 127.0.0.1:54321       83682  main      INFO: Build age: 29 days
01-26 17:06:39.960 127.0.0.1:54321       83682  main      INFO: Built by: 'jenkins'
01-26 17:06:39.960 127.0.0.1:54321       83682  main      INFO: Built on: '2018-12-28 14:04:50'
01-26 17:06:39.969 127.0.0.1:54321       83682  main      INFO: Watchdog Build git branch: (unknown)
01-26 17:06:39.969 127.0.0.1:54321       83682  main      INFO: Watchdog Build git hash: (unknown)
01-26 17:06:39.973 127.0.0.1:54321       83682  main      INFO: Watchdog Build git describe: (unknown)
01-26 17:06:39.973 127.0.0.1:54321       83682  main      INFO: Watchdog Build project version: (unknown)
01-26 17:06:39.973 127.0.0.1:54321       83682  main      INFO: Watchdog Built by: (unknown)
01-26 17:06:39.973 127.0.0.1:54321       83682  main      INFO: Watchdog Built on: (unknown)
01-26 17:06:39.973 127.0.0.1:54321       83682  main      INFO: XGBoost Build git branch: (unknown)
01-26 17:06:39.973 127.0.0.1:54321       83682  main      INFO: XGBoost Build git hash: (unknown)
01-26 17:06:39.974 127.0.0.1:54321       83682  main      INFO: XGBoost Build git describe: (unknown)
01-26 17:06:39.974 127.0.0.1:54321       83682  main      INFO: XGBoost Build project version: (unknown)
01-26 17:06:39.974 127.0.0.1:54321       83682  main      INFO: XGBoost Built by: (unknown)
01-26 17:06:39.975 127.0.0.1:54321       83682  main      INFO: XGBoost Built on: (unknown)
01-26 17:06:39.975 127.0.0.1:54321       83682  main      INFO: KrbStandalone Build git branch: (unknown)
01-26 17:06:39.976 127.0.0.1:54321       83682  main      INFO: KrbStandalone Build git hash: (unknown)
01-26 17:06:39.976 127.0.0.1:54321       83682  main      INFO: KrbStandalone Build git describe: (unknown)
01-26 17:06:39.976 127.0.0.1:54321       83682  main      INFO: KrbStandalone Build project version: (unknown)
01-26 17:06:39.976 127.0.0.1:54321       83682  main      INFO: KrbStandalone Built by: (unknown)
01-26 17:06:39.976 127.0.0.1:54321       83682  main      INFO: KrbStandalone Built on: (unknown)
01-26 17:06:39.976 127.0.0.1:54321       83682  main      INFO: Processed H2O arguments: [-name, H2O_started_from_R_enrico_kcp980, -ip, localhost, -web_ip, localhost, -port, 54321, -ice_r
oot, /tmp/Rtmp0k66FQ]
01-26 17:06:39.977 127.0.0.1:54321       83682  main      INFO: Java availableProcessors: 32
01-26 17:06:39.977 127.0.0.1:54321       83682  main      INFO: Java heap totalMemory: 1.92 GB
01-26 17:06:39.977 127.0.0.1:54321       83682  main      INFO: Java heap maxMemory: 26.67 GB
01-26 17:06:39.977 127.0.0.1:54321       83682  main      INFO: Java version: Java 1.8.0_20 (from Oracle Corporation)
01-26 17:06:39.981 127.0.0.1:54321       83682  main      INFO: JVM launch parameters: [-ea]
01-26 17:06:39.981 127.0.0.1:54321       83682  main      INFO: OS version: Linux 2.6.32-554.el6.x86_64 (amd64)
01-26 17:06:39.981 127.0.0.1:54321       83682  main      INFO: Machine physical memory: 757.37 GB
01-26 17:06:39.981 127.0.0.1:54321       83682  main      INFO: X-h2o-cluster-id: 1548518797511
01-26 17:06:39.982 127.0.0.1:54321       83682  main      INFO: User name: 'enrico'
01-26 17:06:39.982 127.0.0.1:54321       83682  main      INFO: IPv6 stack selected: false
01-26 17:06:39.982 127.0.0.1:54321       83682  main      INFO: Possible IP Address: ib0 (ib0), fe80:0:0:0:e61d:2d03:e1:4371%ib0
01-26 17:06:39.982 127.0.0.1:54321       83682  main      INFO: Possible IP Address: ib0 (ib0), 10.168.22.9
01-26 17:06:39.983 127.0.0.1:54321       83682  main      INFO: Possible IP Address: eth4 (eth4), fe80:0:0:0:a236:9fff:fe13:9594%eth4
01-26 17:06:39.983 127.0.0.1:54321       83682  main      INFO: Possible IP Address: eth4 (eth4), 10.168.16.15
01-26 17:06:39.984 127.0.0.1:54321       83682  main      INFO: Possible IP Address: eth0 (eth0), fe80:0:0:0:be30:5bff:fef6:9c14%eth0
01-26 17:06:39.984 127.0.0.1:54321       83682  main      INFO: Possible IP Address: eth0 (eth0), 10.168.8.9
01-26 17:06:39.984 127.0.0.1:54321       83682  main      INFO: Possible IP Address: lo (lo), 0:0:0:0:0:0:0:1%lo
01-26 17:06:39.984 127.0.0.1:54321       83682  main      INFO: Possible IP Address: lo (lo), 127.0.0.1
01-26 17:06:39.984 127.0.0.1:54321       83682  main      INFO: Selected H2O.CLOUD_MULTICAST_IF: name:lo (lo) doesn't support multicast
01-26 17:06:40.027 127.0.0.1:54321       83682  main      INFO: H2O node running in unencrypted mode.
01-26 17:06:40.038 127.0.0.1:54321       83682  main      INFO: Internal communication uses port: 54322
01-26 17:06:40.038 127.0.0.1:54321       83682  main      INFO: Listening for HTTP and REST traffic on http://127.0.0.1:54321/
01-26 17:06:40.040 127.0.0.1:54321       83682  main      INFO: H2O cloud name: 'H2O_started_from_R_enrico_kcp980' on localhost/127.0.0.1:54321, static configuration based on -flatfile null
01-26 17:06:40.041 127.0.0.1:54321       83682  main      INFO: If you have trouble connecting, try SSH tunneling from your local machine (e.g., via port 55555):
01-26 17:06:40.041 127.0.0.1:54321       83682  main      INFO:   1. Open a terminal and run 'ssh -L 55555:localhost:54321 
[email protected]
'
01-26 17:06:40.041 127.0.0.1:54321       83682  main      INFO:   2. Point your browser to http://localhost:55555
01-26 17:06:41.838 127.0.0.1:54321       83682  main      INFO: Log dir: '/tmp/Rtmp0k66FQ/h2ologs'
01-26 17:06:41.838 127.0.0.1:54321       83682  main      INFO: Cur dir: '/home/enrico'
01-26 17:06:41.869 127.0.0.1:54321       83682  main      INFO: Subsystem for distributed import from HTTP/HTTPS successfully initialized
01-26 17:06:41.881 127.0.0.1:54321       83682  main      INFO: HDFS subsystem successfully initialized
01-26 17:06:41.887 127.0.0.1:54321       83682  main      INFO: S3 subsystem successfully initialized
01-26 17:06:41.946 127.0.0.1:54321       83682  main      INFO: GCS subsystem successfully initialized
01-26 17:06:41.947 127.0.0.1:54321       83682  main      INFO: Flow dir: '/home/enrico/h2oflows'
01-26 17:06:42.026 127.0.0.1:54321       83682  main      INFO: Cloud of size 1 formed [localhost/127.0.0.1:54321]
01-26 17:06:42.053 127.0.0.1:54321       83682  main      INFO: Registered parsers: [GUESS, ARFF, XLS, SVMLight, AVRO, PARQUET, CSV]
01-26 17:06:42.123 127.0.0.1:54321       83682  main      INFO: Watchdog extension initialized
01-26 17:06:42.123 127.0.0.1:54321       83682  main      INFO: XGBoost extension initialized
01-26 17:06:42.126 127.0.0.1:54321       83682  main      INFO: KrbStandalone extension initialized
01-26 17:06:42.126 127.0.0.1:54321       83682  main      INFO: Registered 3 core extensions in: 1099ms
01-26 17:06:42.126 127.0.0.1:54321       83682  main      INFO: Registered H2O core extensions: [Watchdog, XGBoost, KrbStandalone]
01-26 17:06:43.971 127.0.0.1:54321       83682  main      INFO: Registered: 169 REST APIs in: 1844ms
01-26 17:06:43.987 127.0.0.1:54321       83682  main      INFO: Registered REST API extensions: [XGBoost, Algos, AutoML, Core V3, Core V4]
01-26 17:06:46.023 127.0.0.1:54321       83682  main      INFO: Registered: 242 schemas in 2035ms
01-26 17:06:46.024 127.0.0.1:54321       83682  main      INFO: H2O started in 8483ms
01-26 17:06:46.024 127.0.0.1:54321       83682  main      INFO:
01-26 17:06:46.024 127.0.0.1:54321       83682  main      INFO: Open H2O Flow in your web browser: http://127.0.0.1:54321
01-26 17:06:46.024 127.0.0.1:54321       83682  main      INFO:
[1] ""localhost""
[1] 54321
[1] FALSE
[1] 504
[1] """"
Error in h2o.init() : H2O failed to start, stopping execution.



The standard output logfile shows the same information up to the last 
INFO
 statement (i.e.: no errors) and the error log file is completely empty.


Any help in getting this to work would be much appreciated! I have used H2O in the past and I'm a big fan.


Thank you!","['r', 'h2o']",enricoferrero,https://stackoverflow.com/users/1540663/enricoferrero,"2,319"
54373156,54373156,2019-01-25T21:41:46,2019-01-26 07:27:36Z,0,"Closed.
 This question does not meet 
Stack Overflow guidelines
. It is not currently accepting answers.
                                
                            
























 This question does not appear to be about 
a specific programming problem, a software algorithm, or software tools primarily used by programmers
. If you believe the question would be on-topic on 
another Stack Exchange site
, you can leave a comment to explain where the question may be able to be answered.






Closed 
5 years ago
.















                        Improve this question
                    








I've been trying to get GPU support to work for xgboost via h2o in a 
rocker
 docker container with little success. Progress so far: 
GitHub
, 
Docker Hub


I have installed CUDA + nvidia-docker on the host machine and CUDA (9.0 - 9.2) in the container. I'm running the container with the following,


nvidia-docker run -d -p 8787:8787 -e USER=tidyverse-gpu -e PASSWORD=tidyverse-gpu --name tidyverse-gpu seabbs/tidyverse-gpu



Base Xgboost works with GPU support in both R and Python (and 
nvidia-smi
 returns usage stats etc when run inside the container). When the GPU backend is enabled in 
h2o.xgboost
 the following error is returned. 


Illegal argument(s) for XGBoost model: XGBoost_model_R_1548450637489_3.  Details: ERRR on field: _backend: GPU backend (gpu_id: 0) is not functional. Check CUDA_PATH and/or GPU installation.



Initially I had not added the 
CUDA_PATH
 in the Dockerfile but testing adding this has had no effect. 


Sys.getenv(""CUDA_PATH"")
[1] ""/usr/local/cuda""



The 
h2o
 startup logs show no issue with the 
xgboost
 module (that I can see). I've tried rolling back to CUDA 8.0 but this errors in the latest rocker containers as the gcc version being used is not supported by 
xgboost
. 


Any help would be much appreciated as I don't have a clue :)","['r', 'docker', 'h2o', 'xgboost']",talonmies,https://stackoverflow.com/users/681865/talonmies,72.2k
54345247,54345247,2019-01-24T11:09:45,2019-01-24 15:30:59Z,662,"When I am running AUTO ML through H20 GUI getting below in user feedback and no GBM/Deep learning  model got created in leaderboard. pls suggest why GBM and deeplearning failed.


24  16:18:30.768    Info    ModelTraining   GBM 2 failed: java.lang.IllegalArgumentException: class_sampling_factors must have 2 elements
25  16:18:30.768    Info    ModelTraining   GBM 3 started
26  16:18:31.768    Info    ModelTraining   GBM 3 failed: java.lang.IllegalArgumentException: class_sampling_factors must have 2 elements
32  16:18:34.795    Info    ModelTraining   Default Deep Learning build failed: java.lang.IllegalArgumentException: class_sampling_factors must have 2 elements


Also when i am trying to run automl code in python then its randomly giving below error when I am trying to convert some int fields into factor.
What does this error suggests.


H2OServerError: HTTP 500 Server Error:
Server error water.util.DistributedException:
  Error: DistributedException from /127.0.0.1:54321: '-1'
  Request: None","['python', 'machine-learning', 'h2o', 'h2o4gpu']",Sarvendra Singh,https://stackoverflow.com/users/10882376/sarvendra-singh,13
54333537,54333537,2019-01-23T18:30:57,2019-01-23 19:49:43Z,0,"I am currently using 
H2O's AutoML
 for a data science project. However, nowhere in the documentation or on the internet or in the code I can find how AutoML treats factor variables - does it do one-hot encoding? Label encoding? Something more advanced? Does it consider how many levels there are? Does it depend on the algorithm?


Currently, AutoML performs really badly (marginally above the baseline), and I suspect it's because it doesn't treat categoricals right, which make up about 90% of my predictors.","['machine-learning', 'neural-network', 'h2o', 'xgboost', 'automl']",Thomas,https://stackoverflow.com/users/4629950/thomas,"5,064"
54311898,54311898,2019-01-22T15:49:48,2019-01-30 06:53:07Z,0,"I am getting the following error by running this code:


h2o.init()





H2O is not running yet, starting it now...
  Note:  In case of errors look at the following log files:
  C:\Users\312406\AppData\Local\Temp\RtmpEbcklD/h2o_312406_started_from_r.out
  C:\Users\312406\AppData\Local\Temp\RtmpEbcklD/h2o_312406_started_from_r.err


java version ""1.8.0_161""
  Java(TM) SE Runtime Environment (build 1.8.0_161-b12)
  Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)


Error in .h2o.startJar(ip = ip, port = port, nthreads = nthreads, max_memory = max_mem_size,  :

    Failed to exec ""C:/R/R-3.4.2/library/h2o/java/h2o.jar"" with return code=1

  In addition: Warning message:

  running command '""C:\Program Files\Java\jdk1.8.0_161\bin\java.exe"" -Dsys.ai.h2o.debug.allowJavaVersions=11 -ea -cp ""C:/R/R-3.4.2/library/h2o/java/h2o.jar"" water.H2OApp -name H2O_started_from_R_312406_dgc000 -ip localhost -web_ip localhost -port 54321 -ice_root C:/Users/312406/AppData/Local/Temp/RtmpEbcklD' had status 1 


H2o Version : 3.20.0.8

  R : R-3.4.2

  R Studio : Version 1.1.419

  OS : WINDOWS 10 Enterprise

  Version : 1083

  OS Build : 17143.1

  arch : x86_64","['r', 'h2o']",OTStats,https://stackoverflow.com/users/9855745/otstats,"1,868"
54299526,54299526,2019-01-22T00:11:08,2019-01-23 21:38:16Z,98,"(note: this is related to a question I posted before 

H2O (open source) for K-mean clustering
)


I am using K-Means for our data set of about 100 features (some of them are timestamps)


(1) I checked the “OUTPUT - CLUSTER MEANS” section and the timestamp filed is with the value like “1.4144556086883196e+22”. Our timestamp file is about data in year 2018 and the year 2018 Unix time is like “1541092918000”. Hence, it cannot be that big number “1.4144556086883196e+22”. My understand of the numbers in “OUTPUT - CLUSTER MEANS” section should be close to the raw data (before standarization). Right ?


(2) About standardization, can you use this example 
https://github.com/h2oai/h2o-3/blob/master/h2o-genmodel/src/test/resources/hex/genmodel/algos/kmeans/model.ini#L21-L27
 and tell me how the input data is converted to standardized value? Say, I have a raw vector of value ( a,b,c,d, 1.8 ) , I only keep last element and omit others. How can I know if it’s close to center 2 below in this example. Can you show me how H2O convert the raw data using standardize_means, standardize_mults and standardize_modes. I am sure H2O has a way to compute standardized value from the model output, but I cannot find the place and the formula.
center_2 = [2.0, 0.0, -0.5466317772145349, 0.04096506994984166, 2.1628815416218337]


Thanks.",['h2o'],Sunny Chen,https://stackoverflow.com/users/3188698/sunny-chen,3
54298102,54298102,2019-01-21T21:38:04,2019-01-23 22:02:21Z,365,"[Python 3.5.2, h2o 3.22.1.1, JRE 1.8.0_201]


I am running a 
glm
 
lambda_search
 and using the regularization path to select a lambda.


glm_h2o = H2OGeneralizedLinearEstimator(family='binomial', alpha=1., lambda_search=True, seed=param['GLM_SEED'])
glm_h2o.train(y='label', training_frame=train_h2o, fold_column='fold')

regpath_h2o = H2OGeneralizedLinearEstimator.getGLMRegularizationPath(glm_h2o)
regpath_pd = pd.DataFrame(index=np.arange(len(regpath_h2o['lambdas'])), columns=['lambda','ncoef','auc'])
for n,(lamb,coefs) in enumerate(zip(regpath_h2o['lambdas'],regpath_h2o['coefficients'])):
    mod = H2OGeneralizedLinearEstimator.makeGLMModel(model=glm_h2o, coefs=coefs)
    regpath_pd.loc[n] = [lamb, sum(1 for x in coefs.values() if abs(x)>1E-3), mod.model_performance(train_h2o).auc()]



The values in 
regpath_pd
 are as shown below:


  lambda  ncoef   auc |    lambda ncoef    auc |    lambda ncoef    auc
0  0.103    1   0.5   | 10  0.041   14   0.742 | 20  0.016   54   0.794
1  0.094    3   0.632 | 11  0.037   15   0.743 | 21  0.015   62   0.799
2  0.085    3   0.632 | 12  0.034   18   0.749 | 22  0.013   72   0.804
3  0.078    5   0.696 | 13  0.031   19   0.752 | 23  0.012   83   0.849
4  0.071    5   0.696 | 14  0.028   20   0.754 | 24  0.011   90   0.813
5  0.065    6   0.697 | 15  0.026   26   0.766 | 25  0.010  110   0.816
6  0.059    7   0.702 | 16  0.023   31   0.770 | 26  0.009  123   0.819
7  0.054    8   0.707 | 17  0.021   34   0.774 | 27  0.008  147   0.822
8  0.049   10   0.729 | 18  0.019   41   0.777 | 28  0.008  165   0.825
9  0.045   13   0.740 | 19  0.018   50   0.791 | 29  0.007  190   0.828



I was expecting that as lambda penalty decreases, ncoef and auc will increase (non-decreasing).  That is true for most of the time with one exception.  See 
index 23
 - the auc increases a fair bit and then reduces again. Is there an explanation for that?  Do I need to set some tolerance parameter or ...?  In this run 
nlambdas = 100
 (default).  When I set it to 50, lambda, ncoef and auc values are monotonic.


FYI - for the purposes of this post I have truncated lambda and auc values to 3 decimal places.  None of those values are truncated in the actual run.




UPDATE


Following the code 
here
 I re-wrote the loop so that the model is re-trained for every lambda.  That works correctly and the monotonicity is maintained.  Obviously that takes much longer to run.  Here's the approach I ended up with: 
identify the index which has an issue and train the full model for that index only.  FWIW here's that part of the code


auc_diff = regpath_pd['auc'][1:].values - regpath_pd['auc'][:-1].values
arg_bad = np.argwhere(auc_diff<-1E-3).ravel())

for n in arg_bad.tolist():
    lamb = regpath_h2o['lambdas'][n]
    mod = H2OGeneralizedLinearEstimator(family='binomial', alpha=1., lambda_search=False, Lambda=lamb, seed=param['GLM_SEED'])
    mod.train(y='label', training_frame=train_h2o, fold_column='fold')
    regpath_pd.loc[n] = [lamb, sum(1 for x in mod.coef().values() if abs(x)>1E-3), mod.model_performance().auc()]



The resulting graph is shown below (on a different scale).  Looks like the problem is with 
getGLMRegularizationPath
.","['python', 'h2o', 'glm', 'regularized']",Unknown,,N/A
54255814,54255814,2019-01-18T14:15:52,2019-01-19 11:47:27Z,0,"I am using the 
h2o
 package to train a GBM for a churn prediction problem.


all I wanted to know is what influences the size of the fitted model saved on disk (via 
h2o.saveModel()
), but unfortunately I wasn't able to find an answer anywhere.


more specifically, when I tune the GBM to find the optimal hyperparameters (via 
h2o.grid()
) on 3 non-overlapping rolling windows of the same length, I obtain models whose sizes are not comparable (i.e. 11mb, 19mb and 67mb). the hyperparameters grid is the same, and also the train set sizes are comparable.


naturally the resulting optimized hyperparameters are different across the 3 intervals, but I cannot see how this can produces such a difference in the model sizes.


moreover, when I train the actual models based on those hyperparameters sets, I end up with models with different sizes as well.


any help is appreciated!
thank you


ps. I'm sorry but I cannot share any dataset to make it reproducible (due to privacy restrictions)","['r', 'h2o']",davide,https://stackoverflow.com/users/7757484/davide,325
54223663,54223663,2019-01-16T19:03:16,2019-01-16 23:40:55Z,0,"I'm familiar with how to 
constrain the Betas
 (regression parameters) in an 
h2o.glm()
, but struggling to understand how this can be extended to constrain the intercept.


(I do understand that 
intercept=FALSE
 constrains it to zero, but I'm interested in a non-zero constraint.)


Notional example dataset:


n <- 100

set.seed(1)

getPoints <- function(n){
    rbind(
        data.frame(col= factor('red', levels=c('red','blue')), 
                   x1 = rnorm(n=n,mean=11,sd = 2), 
                   x2 = rnorm(n=n,mean=5,sd=1)),
        data.frame(col='blue', 
                   x1 = rnorm(n=n,mean=13,sd = 2), 
                   x2 = rnorm(n=n,mean=7,sd=1))
        )
}

df1     <- getPoints(n)



Example constraints:


param_names <- c('Intercept', 'x1', 'x2')
param_vals  <- c(       27.5, -1.1, -2.7)

beta_const_df <- data.frame(names = c('Intercept','x1','x2'),
                            lower_bounds = param_vals-0.1,
                            upper_bounds = param_vals+0.1,
                            beta_start   = param_vals)



The constraints will work if I 
omit
 the ""Intercept"" constraint:


glm1 <- h2o.glm(x=c('x1','x2'),
                y='col',
                family='binomial',
                lambda=0,
                alpha=0,
                training_frame = 'df1',
                beta_constraints=beta_const_df[-1,] 
                )
glm1@model$coefficients
# Intercept        x1        x2 
#  27.68408  -1.00000  -2.60000 



But if I include an ""Intercept"" constraint, the other constraints fail too.


glm2 <- h2o.glm(x=c('x1','x2'),
                y='col',
                family='binomial',
                lambda=0,
                alpha=0,
                training_frame = 'df1',
                beta_constraints=beta_const_df)   
glm2@model$coefficients
#  Intercept          x1          x2 
# 0.67783085 -0.01185921 -0.03083395 



What's the proper syntax to constrain the intercept?","['r', 'regression', 'h2o']",Unknown,,N/A
54191038,54191038,2019-01-15T00:12:21,2019-01-15 19:19:27Z,0,"When downloading H2o.ai from R on my linux, I receive several warnings that a file is missing, For example:     
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.abs.Rd:32: missing file link 'abs'


I'm using the standard install code for H2o: 
install.packages(""h2o"", type=""source"", repos=""http://h2o-release.s3.amazonaws.com/h2o/rel-xu/1/R"")


Here is the output with the warnings: 
    `    h2o.abs                                 html

    Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.abs.Rd:32: missing file link ‘abs’
        h2o.acos                                html

    Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.acos.Rd:24: missing file link ‘acos’


    h2o.ascharacter                         html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.ascharacter.Rd:16: missing file link ‘as.character’
    h2o.asfactor                            html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.asfactor.Rd:16: missing file link ‘as.factor’
    h2o.asnumeric                           html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.asnumeric.Rd:16: missing file link ‘as.numeric’

    h2o.ceiling                             html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.ceiling.Rd:18: missing file link ‘ceiling’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.cos.Rd:16: missing file link ‘cos’
    h2o.cosh                                html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.cosh.Rd:16: missing file link ‘cosh’
    h2o.coxph                               html  
    h2o.createFrame                         html  
    h2o.cross_validation_fold_assignment    html  
    h2o.cross_validation_holdout_predictions
                                            html  
    h2o.cross_validation_models             html  
    h2o.cross_validation_predictions        html  
    h2o.cummax                              html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.cummax.Rd:18: missing file link ‘cummax’
    h2o.cummin                              html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.cummin.Rd:18: missing file link ‘cummin’
    h2o.cumprod                             html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.cumprod.Rd:18: missing file link ‘cumprod’
    h2o.exp                                 html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.exp.Rd:16: missing file link ‘exp’

    h2o.floor                               html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.floor.Rd:18: missing file link ‘floor’

    h2o.ischaracter                         html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.ischaracter.Rd:16: missing file link ‘is.character’
    h2o.isfactor                            html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.isfactor.Rd:16: missing file link ‘is.factor’
    h2o.isnumeric                           html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.isnumeric.Rd:16: missing file link ‘is.numeric’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.log.Rd:16: missing file link ‘log’
    h2o.log10                               html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.log10.Rd:16: missing file link ‘log10’
    h2o.log1p                               html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.log1p.Rd:16: missing file link ‘log1p’
    h2o.log2                                html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.log2.Rd:16: missing file link ‘log2’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.max.Rd:18: missing file link ‘max’
    h2o.mean                                html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.mean.Rd:46: missing file link ‘rowMeans’
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.mean.Rd:46: missing file link ‘colMeans’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.min.Rd:18: missing file link ‘min’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.ncol.Rd:16: missing file link ‘ncol’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.rbind.Rd:32: missing file link ‘rbind’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.round.Rd:21: missing file link ‘round’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.signif.Rd:21: missing file link ‘signif’
    h2o.sin                                 html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.sin.Rd:16: missing file link ‘sin’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.sqrt.Rd:16: missing file link ‘sqrt’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.tan.Rd:16: missing file link ‘tan’
    h2o.tanh                                html  
Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.tanh.Rd:16: missing file link ‘tanh’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.trunc.Rd:17: missing file link ‘trunc’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.var.Rd:36: missing file link ‘var’

Rd warning: /tmp/RtmpdpaSKI/R.INSTALL1d3a6ccc7d9b/h2o/man/h2o.which_max.Rd:29: missing file link ‘which.max’

 building package indices
 testing if installed package can be loaded
 DONE (h2o)



I believe this issue is causing a downstream error when using automl, 
Error: DistributedException from localhost/127.0.0.1:54321: 'Categorical value out of bounds","['r', 'h2o', 'automl']",Michael Jules,https://stackoverflow.com/users/10914483/michael-jules,21
54190081,54190081,2019-01-14T22:23:38,2019-01-14 22:44:50Z,0,"I am trying to calculate Gini for my regression models and since there is no Gini index for regression models, I am getting all the scores and calculate it using Gini functions in R using this code:


preds <-h2o.predict(model, test)
pred_vs_actual <- as.data.frame(h2o.cbind(test$target,preds)



Does this code return the correct pair values for actual and predictions? I know that there is no order in a spark table but I am not sure if this is also the case for H2O object.","['r', 'h2o', 'predict']",Rio,https://stackoverflow.com/users/4614379/rio,398
54148173,54148173,2019-01-11T14:09:47,2019-01-14 10:18:04Z,0,"How do I build a H2O word2vec training_frame that distinguishes between different document/sentences etc.?


As far as I can read from the very limited documentation I have found, you simply supply one long list of words? Such as


'This' 'is' 'the' 'first' 'This' 'is' 'number' 'two'



However it would make sense to be able to distinguish – ideally something like this:


Name   | ID
This   | 1
is     | 1
the    | 1
first  | 1
This   | 2
is     | 2
number | 2
two    | 2



Is that possible?","['word2vec', 'h2o']",Emil Lykke Jensen,https://stackoverflow.com/users/4053890/emil-lykke-jensen,409
54131854,54131854,2019-01-10T15:24:35,2019-03-15 09:51:00Z,316,"I am running xgboost using H2o package in python. I set to use all 32 cores of my machine. The classifier is inside a for loop to run the classification for different parameters. I am initiating the h2o and closing it in the loop. It runs for 2-3 round in the loop and returns back error ""Cannot perform booster operation: updater is inactive on node /127.0.0.1:54321"" for some runs. 
anyone has idea why I am getting such an error?


Thanks,
Elnaz


`for dates in start_end_dates:
     for window_size in window_sizes:
          print dates[0], dates[1], dates[2], window_size
          model_string = str(dates[0])+ '_'+ str(dates[1]) + ':'+ str(dates[2])+ ':'+ str(window_size)
    ## load daily transaction types 
         ## this function runs in parallel on all cpus 
         daily_transactions_type_df = transform_transactions_types.transform(dates[0], dates[1], window_size)
         ##load daily transactions
         ## this function runs in parallel on all cpus 
         daily_transactions_df = transfrom_daily_transactions.transform(dates[0], dates[1], window_size, max_number_of_instrument)

         snapshot_date = dates[1]
         ## user status list
         user_status_list = Classification_helpers.load_user_status_data_from_gbq(snapshot_date)
    user_status_list

         ## Normalize the data
         numeric_columns = daily_transactions_type_df.iloc[:,1:].columns.tolist()  
        other_columns = []
        daily_transactions_type_df_norm = Classification_helpers.normalize_data_without_outliers(daily_transactions_type_df, numeric_columns, other_columns)

        ## Normalize the data
        numeric_columns = daily_transactions_df.iloc[:,1:-6].columns.tolist()  
        other_columns = daily_transactions_df.iloc[:,-5:].columns.tolist()
        daily_transactions_df_norm =   Classification_helpers.normalize_data_without_outliers(daily_transactions_df,numeric_columns,other_columns)

        data_frames = [daily_transactions_type_df_norm,   daily_transactions_df_norm, user_status_list[['USER_ID', 'label']]]

        df = Classification_helpers.create_labelled_data(data_frames)
        numeric_columns = df.iloc[:,1:-6].columns.tolist()  
        other_columns = df.iloc[:,-6:-1].columns.tolist()  

        nthreads = -1
        Classification_helpers.init_h2o(nthreads)

         model, performance, predictions = Classification_helpers.train_XGboost(df, numeric_columns, other_columns, model_string)
    print performance.auc()`","['python', 'h2o', 'xgboost']",Unknown,,N/A
54123267,54123267,2019-01-10T06:49:57,2019-01-14 20:02:32Z,200,"I'm trying to setup h2o4gpu library to be used in Kaggle competition, but I haven't found any useful resource to install/setup the environment. How do I set it up step by step?  


I have tried using the R package installation guide from this source : 
https://github.com/h2oai/h2o4gpu


But I still cannot make it work. I'm using below code, it installs the h2o4gpu package for R but I still receive error.


if (!require(devtools)) install.packages(""devtools"")
devtools::install_github(""h2oai/h2o4gpu"", subdir = ""src/interface_r"")



I'm hoping a simple step by step guide. Thank you.","['h2o', 'kaggle', 'h2o4gpu']",Ralph Deint,https://stackoverflow.com/users/4959970/ralph-deint,380
54102766,54102766,2019-01-09T03:14:47,2019-01-09 20:09:48Z,701,"I noticed H2O has released the target mean encoding


http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/target-encoding.html


It only comes with an R code example. Does anyone have a Python example?","['python', 'h2o']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
54091323,54091323,2019-01-08T11:55:49,2021-02-09 05:12:59Z,0,"What are the possible reasons that an h2o service might stop responding to an http REST API request? 


We are using the R CRAN package, and after a fair period of time, the h2o server stops responding. We have captured a variety of logs, but there is no obvious error or reason for this. There is also a matter of consistency, which I will detail after the logs. The h2o.logging() log shows:


Time:     2019-01-07 11:13:19.262

GET       http://localhost:54321/3/Jobs/$03017f00000132d4ffffffff$_936500deb000be7364a7e2ce61d5451e
postBody:

curlError:         FALSE
curlErrorMessage:
httpStatusCode:    200
httpStatusMessage: OK
millis:            3513

{""__meta"":{""data"":""removed as it is not relevant""}}

------------------------------------------------------------

Time:     2019-01-07 11:13:25.013

GET       http://localhost:54321/3/Jobs/$03017f00000132d4ffffffff$_936500deb000be7364a7e2ce61d5451e
postBody:

curlError:         TRUE
curlErrorMessage:  Failed to connect to localhost port 54321: Connection refused
httpStatusCode:    -1
httpStatusMessage:
millis:            88616



The main log shows it was working just before:


Connection successful!

R is connected to the H2O cluster:
    H2O cluster uptime:         2 minutes 9 seconds
    H2O cluster timezone:       Etc/UTC
    H2O data parsing timezone:  UTC
    H2O cluster version:        3.20.0.8
    H2O cluster version age:    3 months and 17 days !!!
    H2O cluster name:           H2O_started_from_R_root_ttz747
    H2O cluster total nodes:    1
    H2O cluster total memory:   255.99 GB
    H2O cluster total cores:    2
    H2O cluster allowed cores:  2
    H2O cluster healthy:        TRUE
    H2O Connection ip:          localhost
    H2O Connection port:        54321
    H2O Connection proxy:       NA
    H2O Internal Security:      FALSE
    H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4
    R Version:                  R version 3.5.1 (2018-07-02)

  |======================================================================| 100%
  |======================================================================| 100%
Error in .h2o.__checkConnectionHealth() :
  H2O connection has been severed. Cannot connect to instance at http://localhost:54321/
Failed to connect to localhost port 54321: Connection refused
Calls: <Anonymous> -> .h2o.__remoteSend -> .h2o.__checkConnectionHealth



The interesting thing about this is that it is 100% repeatable on machines that have this problem (the majority), but we also have two machines that don't show this problem (at least don't seem to, we can't rule out they never have, but they usually run it okay).


I've seen that h2o never shuts itself down on its own, so that is unlikely. We stopped any parallelism in the R script, and that didn't change anything.


We therefore need suggestions on how to identify the problem here, and of course fix it.","['r', 'h2o']",mj2008,https://stackoverflow.com/users/5544/mj2008,"6,747"
54090209,54090209,2019-01-08T10:49:56,2019-01-08 23:35:37Z,0,"How to use the .hex file provided by 
https://s3.amazonaws.com/tomk/h2o-world/megan/w2v.hex
 and build a word2vec model using R and h2o.","['r', 'h2o']",Manideep Karthik,https://stackoverflow.com/users/10538794/manideep-karthik,111
54087676,54087676,2019-01-08T08:16:01,2019-01-09 22:20:32Z,239,"I am using H2O (H2O flow, in particular) to do K-means clustering. I selected ""standardize"" checkbox which makes sure ""It standardize columns before computing distances"". It trained fine and I investigated the results. It depicts ""within_cluster_sum_of_squares"" in the result for review. My question is ""within_cluster_sum_of_squares"" the distance BEFORE or AFTER standardization ? It looks displaying distance after standardization, but the distance I see is big and it seems before standardization (I am not sure though). Any idea ? Thanks.",['h2o'],Ching-Chien Chen,https://stackoverflow.com/users/10881951/ching-chien-chen,1
54046112,54046112,2019-01-04T21:11:48,2019-01-04 22:38:20Z,0,"I am doing hyperparameter tuning for gbm model in H2o and since my loss function is Tweedie I don't want to look at mse as my model selection criteria.


In H2o documentation, it says that Gini index can be calculated for both regression and classification models, however when I try to get it for my Tweedie regression model, it returns null. Below is how I get the best model and score it on the test set.


gbm_sorted_grid <- h2o.getGrid(grid_id = ""grid_hp4"", sort_by = 
""residual_deviance"")

best_model <- h2o.getModel(gbm_sorted_grid@model_ids[[1]])  

perf <- h2o.performance(best_model, newdata = lrs_test)
h2o.giniCoef(perf)
Null



And when I try the code below, I get below error:


h2o.giniCoef(best_model) : No Gini for H2ORegressionModel



Does this only work on Bernoulli distribution?","['r', 'h2o', 'gini']",Unknown,,N/A
54045139,54045139,2019-01-04T19:46:47,2019-12-23 06:23:21Z,564,"I'm trying to use the new monotone_constraint feature in H2o GBM for R.  There doesn't seem to be any examples, and I don't understand what the documentation provided means when it says 


""A mapping representing monotonic constraints. Use +1 to enforce an increasing constraint and -1 to specify a decreasing constraint.""


gbm_1 <- h2o.gbm(
model_id = ""gbm_1""
,x = xvars
,y = yvar
,training_frame = train
,distribution = ""bernoulli""
,monotone_constraints = list(""var1"",1)
)



The error i get is 
For input string: ""list(""var1""""","['parameters', 'h2o', 'gbm']",Will.I.am,https://stackoverflow.com/users/8695616/will-i-am,25
54042259,54042259,2019-01-04T15:56:29,2019-02-07 19:52:40Z,0,"I have followed a tutorial for a first time go around with h2o in R from 
here
. What I would like to do is forecast the model on data I don't have, meaning beyond the test set, future dates.


The data is time series, and the predictions on the test set look like so:


print(automl.error.tbl)
# A time tibble: 10 x 5
# Index: Time
   Time       actual  pred   error error.pct
   <date>      <dbl> <dbl>   <dbl>     <dbl>
 1 2018-01-31  11.4  11.4   0.0342   0.00300
 2 2018-02-28  14.6  10.4   4.24     0.290  
 3 2018-03-31  12.2  11.4   0.762    0.0625 
 4 2018-04-30  15.0  10.8   4.20     0.281  
 5 2018-05-31  12.8  11.1   1.75     0.137  
 6 2018-06-30   8.67 10.8  -2.15    -0.248  
 7 2018-07-31  12.3  10.3   2.03     0.165  
 8 2018-08-31  13.5  10.4   3.17     0.234  
 9 2018-09-30  10.8   9.72  1.05     0.0976 
10 2018-10-31  10.5  10.7  -0.165   -0.0156 



What I do not know how to do and am having difficulty finding is how to predict future data. For example with 
fpp
 I can do something like:


monthly.hw.fcast <- hw(
  monthly.rr.sub.xts
  , h = 12
  , alpha = monthly.fit.hw$alpha
)



And get what I am looking for, future predictions. Is there a simple way of doing that with an h20 model?


My code is as follows:


# h2o ####
library(h2o)
tk.monthly %>% glimpse()
tk.monthly.aug <- tk.monthly %>%
  tk_augment_timeseries_signature()
tk.monthly.aug %>% glimpse()

tk.monthly.tbl.clean <- tk.monthly.aug %>%
  select_if(~ !is.Date(.)) %>%
  select_if(~ !any(is.na(.))) %>%
  mutate_if(is.ordered, ~ as.character(.) %>% as.factor)

tk.monthly.tbl.clean %>% glimpse()

train.tbl <- tk.monthly.tbl.clean %>% filter(year < 2017)
valid.tbl <- tk.monthly.tbl.clean %>% filter(year == 2017)
test.tbl  <- tk.monthly.tbl.clean %>% filter(year == 2018)

h2o.init()

train.h2o <- as.h2o(train.tbl)
valid.h2o <- as.h2o(valid.tbl)
test.h2o <- as.h2o(test.tbl)

y <- ""readmit.rate""
x <- setdiff(names(train.h2o), y)

automl.models.h2o <- h2o.automl(
  x = x
  , y = y
  , training_frame = train.h2o
  , validation_frame = valid.h2o
  , leaderboard_frame = test.h2o
  , max_runtime_secs = 60
  , stopping_metric = ""deviance""
)

automl.leader <- automl.models.h2o@leader

pred.h2o <- h2o.predict(
  automl.leader
  , newdata = test.h2o
)

h2o.performance(
  automl.leader
  , newdata = test.h2o
)

# get mape
automl.error.tbl <- tk.monthly %>%
  filter(lubridate::year(Time) == 2018) %>%
  add_column(
    pred = pred.h2o %>%
      as.tibble() %>%
      pull(predict)
    ) %>%
  rename(actual = readmit.rate) %>%
  mutate(
    error = actual - pred
    , error.pct = error / actual
  )
print(automl.error.tbl)

automl.error.tbl %>%
  summarize(
    me = mean(error)
    , rmse = mean(error^2)^0.5
    , mae = mean(abs(error))
    , mape = mean(abs(error))
    , mpe = mean(error.pct)
  ) %>%
  glimpse()","['r', 'time-series', 'h2o']",MCP_infiltrator,https://stackoverflow.com/users/1510267/mcp-infiltrator,"4,159"
54041696,54041696,2019-01-04T15:19:50,2019-01-04 15:19:50Z,294,"We use python to talk to single instance h2o (latest version 3.22.1.1).


Sometimes we get this error:


DistributedException from /10.192.21.17:54321: 'class water.fvec.Frame s3a://BUCKET_NAME/part-00001-0cd59acc-d03f-4af6-8227-e58bf7ad9562-c000.snappy.parquet is already in use.  Unable to use it now.  Consider using a different destination name.', caused by java.lang.IllegalArgumentException: class water.fvec.Frame s3a://BUCKET_NAME/part-00001-0cd59acc-d03f-4af6-8227-e58bf7ad9562-c000.snappy.parquet is already in use.  Unable to use it now.  Consider using a different destination name.
    at water.MRTask.getResult(MRTask.java:478)
    at water.MRTask.getResult(MRTask.java:486)
    at water.MRTask.doAll(MRTask.java:402)



We tried to pass our random destination_frame like this:


h2o.import_file(
                path=data_path,
                destination_frame='frame_{}'.format(str(uuid.uuid4())))



but it looks like destination_frame parameters is not used by H2O even though we see it present in the logs:


POST /3/Parse, parms: {number_columns=94, source_frames=[""s3a://BUCKET_NAME/part-00000-0cd59acc-d03f-4af6-8227-e58bf7ad9562-c000.snappy.parquet""], column_types=[""UUID"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Enum"",""Enum"",""Time"",""Numeric"",""Enum"",""Enum"",""Time"",""Time"",""Numeric"",""Enum"",""Enum"",""Numeric"",""Enum"",""Numeric"",""Numeric"",""Numeric"",""Enum"",""Enum"",""Enum"",""Enum"",""Enum"",""Numeric"",""Enum"",""Enum"",""Numeric"",""Enum"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Time"",""Numeric"",""Enum"",""Enum"",""Time"",""Numeric"",""Numeric"",""Enum"",""Enum"",""Enum"",""Enum"",""Enum"",""Numeric"",""Enum"",""Numeric"",""Enum"",""Numeric"",""Enum"",""Numeric"",""Enum"",""Numeric"",""Enum"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""UUID"",""Time"",""Numeric"",""Numeric"",""Enum"",""Numeric"",""Numeric"",""Numeric"",""Enum"",""Numeric"",""Numeric"",""Enum"",""Enum"",""Numeric"",""UUID"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Numeric"",""Enum"",""Numeric"",""Numeric"",""Numeric""], single_quotes=True, parse_type=PARQUET, destination_frame=frame_19d32a0b-812f-4179-ba83-c3e1afe1d84f, column_names=[
""ALL_COLUMN_NAMES_HERE""], delete_on_done=True, check_header=1, separator=124, blocking=False, chunk_size=77450}","['python', 'parquet', 'h2o']",anthony,https://stackoverflow.com/users/217224/anthony,293
54039594,54039594,2019-01-04T13:10:10,2019-01-04 13:39:26Z,0,"We are using H2O (latest version 3.22.1.1) to read parquet data from s3. We use python to talk to H2O. This is single H2O instance - not cluster.


Sometimes we get this error:


Server error water.exceptions.H2OIllegalArgumentException:
  Error: Cannot determine file type. for s3a://BUCKET_NAME/5c2e3fdc0c9c1800019c73f9/part-00001-c33635a2-76dc-4e49-948b-465726b7e3d9-c000.snappy.parquet


File exists and is valid parquet file. Subsequent imports work fine.


This is our python code to import file into H2O


h2o.import_file(path='s3a://BUCKET_NAME/5c2e3fdc0c9c1800019c73f9/part-00001-c33635a2-76dc-4e49-948b-465726b7e3d9-c000.snappy.parquet')


Is there any way to force h2o to use parquet type?","['python', 'amazon-s3', 'parquet', 'h2o']",anthony,https://stackoverflow.com/users/217224/anthony,293
54023053,54023053,2019-01-03T13:15:36,2019-01-03 14:32:34Z,0,"I work on Rstudio running on a dedicated linux server. I get an error initialising h2o:


> h2o.init()
H2O is not running yet, starting it now...
<simpleError in system2(command, ""-version"", stdout = TRUE, stderr = TRUE): error in running command>
Error in value[[3L]](cond) : 
  You have a 32-bit version of Java. H2O works best with 64-bit Java.
Please download the latest Java SE JDK 8 from the following URL:
http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html



I checked the java config and got:


system(""java -version"")
openjdk version ""1.8.0_77""
OpenJDK Runtime Environment (build 1.8.0_77-b03)
OpenJDK 64-Bit Server VM (build 25.77-b03, mixed mode)



Apparently I have the good version of java. I have found people with similar problems on Windows. They were able to solve the problem by finding and indicating the good java path. However in the environnement I am working in, i am not sure how to do that. 


Any idea on how to solve the problem ? to bypass the error ?","['java', 'r', 'h2o']",Lucas Morin,https://stackoverflow.com/users/1980929/lucas-morin,398
54020376,54020376,2019-01-03T10:24:58,2019-06-12 21:41:57Z,0,"After training my XGBoost model, using 5 fold cross-validation, I would like to get an idea of the model performance on new data. As far as I understand, the performance of the model on each cross-validation run in an acceptable measure of this performance.


Using h2o.performance(best_XGBoost, xval = T) I can get the confusion matrix of the cross-validation. However, the threshold was selected based on F1 and I would like to see the performance using absolute_mcc to select the threshold.


Is there a way to do it?","['r', 'h2o', 'confusion-matrix']",Diogo Santos,https://stackoverflow.com/users/2501571/diogo-santos,854
54005784,54005784,2019-01-02T11:47:03,2019-01-04 01:14:16Z,0,"I'm using H2O to analyse a dataset but I'm not sure how to correctly perform cross-validation on my dataset. I have an unbalanced dataset, so I would like to performed stratified cross-validation ( were the output variable is used to balance the groups on each partition). 


However, on top of that, I also have an issue that many of my rows are repeats (a way of implementing weights without actually having weights). Independently of the source of this problem, I have seen before that, in some cases, you can do cross-validation were some rows have to be kept together. This seams to be the usage of fold_column. However, it is not possible to do both at the same time?


If there is no H2O solution, how can I compute the fold a priori and use it on H2O?","['r', 'cross-validation', 'h2o']",topchef,https://stackoverflow.com/users/59470/topchef,19.8k
53979651,53979651,2018-12-30T17:00:27,2018-12-31 08:32:56Z,147,"i have created a model in rapid miner. it is a classification model and save the model in pmml. i want to use this model in H2O.ai to predict further. is there any way i can import this pmml model to H2O.ai an used this for further prediction.
I appreciate your suggestions.


Thanks","['h2o', 'rapidminer']",om pal,https://stackoverflow.com/users/9590576/om-pal,41
53961836,53961836,2018-12-28T16:57:37,2019-01-03 17:59:11Z,991,"I'm trying to run multiple H2O models on different response variables in a for loop.


H2O cluster uptime:         53 mins 11 secs
H2O cluster timezone:       Etc/UTC
H2O data parsing timezone:  UTC
H2O cluster version:        3.22.1.1
H2O cluster version age:    2 hours and 15 minutes
H2O cluster name:           H2O_from_python_root_np3l2m
H2O cluster total nodes:    1
H2O cluster free memory:    13.01 Gb
H2O cluster total cores:    8
H2O cluster allowed cores:  8
H2O cluster status:         locked, healthy
H2O connection url:         http://localhost:54321
H2O connection proxy:
H2O internal security:      False
H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4
Python version:             2.7.12 final



I have set a seed for selecting the train/validation sets and the model itself. I have early stopping active but according to the documentation results should be reproducible as long as score_tree_interval is active.


### This is the code that's defining the model

def append_probs(hframe, response_col, model):
  pd_df = h2o.as_list(hframe).copy()
  pd_df.loc[:,'pred'] = h2o.as_list(model.predict(hframe)).values
  pd_df.loc[:,'error'] = pd_df['pred'] - pd_df[response_col]
  return pd_df

def run_model(response_col, model_typ, hframe_train, hframe_pred):
  h2o_dtypes = [hframe_train.type(e) for e in hframe_train.columns]
  data = h2o.deep_copy(hframe_train,'data')
  mapping = {'new_email_ldsub':'live_pp',
             'new_call_ldsub':'live_pp',
             'used_email_ldsub':'live_usedplus',
             'used_call_ldsub':'live_usedplus',
             'myapp_edm_ldsub':'live_myapp',
             'cc_edm_ldsub':'live_cc',
             'fbm_call_ldsub':'live_fbm',
             'fbm_email_ldsub':'live_fbm'}
  data = data[data[mapping[response_col]]==1]

  train, valid = data.split_frame([0.8], seed=1234)

  X = hframe_train.col_names[:-14]
  print X
  y = response_col
  print y

  if model_typ == 'gbm':
    model = H2OGradientBoostingEstimator(
      ntrees=512,
      learn_rate=0.08,
      max_depth=7,
      col_sample_rate = 0.7,
      sample_rate = 0.9,
      stopping_tolerance=1e-05,
      stopping_rounds=2,
      score_tree_interval=5,
      #nfolds=5,
      #fold_assignment = ""Random"",
      distribution = 'poisson',
      seed=20000,
      stopping_metric='mae',
      min_rows = 10,
      nbins = 30

  model.train(X, y, training_frame=train, validation_frame=valid)

  pred_df = append_probs(hframe_pred,response_col,model)

  return model, pred_df

### This is the code that runs the model

gbm_results = pd.DataFrame()

gbm_mapping = {'live_pp':['new_call_ldsub','new_email_ldsub'],
           'live_usedplus':['used_call_ldsub','used_email_ldsub'],
           'live_myapp':['myapp_edm_ldsub'],
           'live_cc':['cc_edm_ldsub'],
           'live_fbm':['fbm_call_ldsub','fbm_email_ldsub']}

gbm_train_err = {}
gbm_valid_err = {}
gbm_xval_err = {}


for k,v in gbm_mapping.iteritems():
  for e in v:
    gbm_mod, gbm_pred_df = run_model(e,'gbm',hframe,hframe_forecast_pred)
    gbm_pred_df = gbm_pred_df[['id','month','pred']]
    gbm_pred_df = gbm_pred_df.groupby(['id','month'])['pred'].sum().reset_index()
    gbm_pred_df.loc[:,'product'] = str(e)
    gbm_train_err[str(e)] = [gbm_mod.mae(train=True),gbm_mod.rmse(train=True)]
    gbm_valid_err[str(e)] = [gbm_mod.mae(valid=True),gbm_mod.rmse(valid=True)]
    gbm_xval_err[str(e)] = [gbm_mod.mae(xval=True),gbm_mod.rmse(xval=True)]
    gbm_results = pd.concat([gbm_results, gbm_pred_df])

gbm_results['process_month'] = pd.to_datetime(gbm_results['process_month'],unit='ms')



Based on the documentation I'm expecting the results for each model to be reproducible/close.","['python', 'pandas', 'h2o', 'gbm']",atline,https://stackoverflow.com/users/6394722/atline,31.3k
53908238,53908238,2018-12-24T00:40:04,2019-01-02 15:51:00Z,0,"I'm trying to learn h2o and R from some book. When i try to execute the author's code i get the error i mentioned.


Here is the code :


I am using this dataset ;

https://github.com/DarrenCook/h2o/blob/bk/datasets/ENB2012_data.csv


seed = 999

library(h2o)
h2o.init(nthreads = -1)

data <- h2o.importFile(""../datasets/ENB2012_data.csv"")

factorsList <- c(""X6"", ""X8"")
data[,factorsList] <- as.factor(data[,factorsList])

splits <- h2o.splitFrame(data, 0.8, seed = seed)
train <- splits[[1]]
test <- splits[[2]]

x <- c(""X1"", ""X2"", ""X3"", ""X4"", ""X5"", ""X6"", ""X7"", ""X8"")
y <- ""Y2""  #Or ""Y1""

######


numericColumns <- setdiff(colnames(train),c(""X6"",""X8""))
d <- round( h2o.cor(train[,numericColumns]) ,2)
rownames(d) <- colnames(d)
d


#####

#breaks defaults to sturges
# ""rice"" gives more, thinner bars
# ""doane"" crashes it
# ""fd"" gives a square for X1, X7 is chunky, X4 are peaks.
# ""scott"": the bars always touch, but are of different widths.
# 
# I've added a bit more formatting than described in the book,
# just to get the plot shown in the book.

par(mar=c(5.1, 6.0, 4.1, 2.1))  #Changed 4.1 to 6.0
par(oma=c(1,0,0,0))  #Def was all zeroes
par(mfrow = c(2,5))
#ylim <- c(0,350)
ylim <- NULL
dummy <- lapply(colnames(train), function(col){
  h <- h2o.hist(train[,col], breaks=30, plot = FALSE)
  plot(h, main = col, xlab = """", ylim = ylim,
    ylab = ifelse(col %in% c(""X1"",""X6""), ""Frequency"", """"),
    cex.lab=2.0, cex.axis=2.0, cex.main=2.5, cex.sub=2.0, cex=2.0
    )
  })


# If curious, here is how it looks on all data
dummy <- lapply(colnames(data), function(col){
  h <- h2o.hist(data[,col], plot = FALSE)
  plot(h, main = col, xlab = """", ylim = ylim)
  })","['r', 'h2o']",vignerd,https://stackoverflow.com/users/8389293/vignerd,29
53883749,53883749,2018-12-21T11:09:06,2019-01-09 23:50:08Z,606,"I have a utf-8 encoded .csv-file that I load to H2O.ai in Python 3.7 using


h2o.load_dataset(""my.csv"")



The Scandinavian characters do not display correctly. The same problem persists if I save my H2OFrame to disk and open in an editor using utf-8. How can I make H2O.ai understand utf-8?


Many thanks.","['text', 'utf-8', 'h2o']",Unknown,,N/A
53879528,53879528,2018-12-21T05:15:20,2018-12-31 21:29:47Z,114,"I'm a new user of H2o flow. Not much of a coder so I like the point and click interface. I was wondering if after building a model, and using it to score a data set, there is any way to generate output that would tell me for each record, the reasons the score is what it is.


As an example, I've created a model that predicts the risk of hospital re-admissions using the autoML feature within flow.


It actually works really well, but when I use it, I'll be sending highly ranked patients to clinical people, and they want to know ""why was this person highly ranked"". 


Is there any way to show what variables in the model led to the prediction for each person as an output that I could export into a database to use in a reporting tool?


Thanks!","['machine-learning', 'h2o']",mrk,https://stackoverflow.com/users/5763590/mrk,10.3k
53863717,53863717,2018-12-20T06:50:06,2019-01-02 20:03:16Z,467,"I have a utf-8 encoded csv file with Chinese text. When I tried to import as an h2o dataframe, the data is improperly displayed as gibberish.


 dataframe = h2o.import_file('test.csv')



In the resulting dataframe, the column names are correct, but instead of Chinese text, it displays text like this:


 åœ¨ç�¡è¦ºäº†ä½ çŸ¥é�



I looked into h2o documentation and there doesn't seem to be any way to set an encoding option like in pandas when using import_file. Further, when running the following:


testing = ['你','好','嗎']
h2o.H2OFrame(testing)



it gives this error:


--------------------------------------------------------------------------
 UnicodeEncodeError                        Traceback (most recent call last)
<ipython-input-2-5f4b3eb49a84> in <module>
      1 testing = ['你','好','嗎']
----> 2 h2o.H2OFrame(testing)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\h2o\frame.py in __init__(self, python_obj, destination_frame, header, separator, column_names, column_types, na_strings, skipped_columns)
    104         if python_obj is not None:
    105             self._upload_python_object(python_obj, 
destination_frame, header, separator,
--> 106                                        column_names, 
column_types, na_strings, skipped_columns)
    107 
    108     @staticmethod

~\AppData\Local\Continuum\anaconda3\lib\site-packages\h2o\frame.py in _upload_python_object(self, python_obj, destination_frame, header, separator, column_names, column_types, na_strings, skipped_columns)
    143             csv_writer.writerow([row.get(k, None) for k in col_header])
    144         else:
--> 145             csv_writer.writerows(data_to_write)
    146         tmp_file.close()  # close the streams
    147         self._upload_parse(tmp_path, destination_frame, 1, 
separator, column_names, column_types, na_strings, skipped_columns)

~\AppData\Local\Continuum\anaconda3\lib\encodings\cp1252.py in encode(self, input, final)
     17 class IncrementalEncoder(codecs.IncrementalEncoder):
     18     def encode(self, input, final=False):
---> 19         return codecs.charmap_encode(input,self.errors,encoding_table)[0]
     20 
     21 class IncrementalDecoder(codecs.IncrementalDecoder):

UnicodeEncodeError: 'charmap' codec can't encode character '\u4f60' in position 1: character maps to <undefined>



Based on this error, it seems that cp1252 encoding is being used by h2o. Can someone offer help to have h2o import the csv file with Chinese to be in utf-8 encoding? Thank you.","['python', 'dataframe', 'h2o']",Unknown,,N/A
53810177,53810177,2018-12-17T06:51:29,2018-12-17 06:55:22Z,371,"I am trying out DAI 1.4.2 on ppc64le Redhat with V100 GPUs, but I find some weird error with dai-h2o daemon.   


It seems like it cannot initialize its GPU backend, and I find libxgboost4j_gpu.so for x86 architecture (not ppc64le) inside h2o.jar. 


Is this some kind of bug or doesn't this matter at all ?


[root@localhost home]# systemctl status dai-h2o
● dai-h2o.service - Driverless AI (H2O Process)
   Loaded: loaded (/usr/lib/systemd/system/dai-h2o.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/dai-h2o.service.d
       └─Group.conf, User.conf
   Active: active (running) since Mon 2018-12-17 14:51:23 KST; 1s ago
 Main PID: 80685 (java)
    Tasks: 93
   Memory: 155.8M
   CGroup: /system.slice/dai-h2o.service
           └─80685 
/opt/h2oai/dai/jre/bin/java -Xmx65536m -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Ddai.tmp=./tmp -jar /opt/h2oai/dai/h...

Dec 17 14:51:24 localhost.localdomain dai-env.sh[80685]: ======================================================================
 Dec 17 14:51:25 localhost.localdomain dai-env.sh[80685]: Cannot load library from path lib/linux_64/libxgboost4j_gpu.so
Dec 17 14:51:25 localhost.localdomain dai-env.sh[80685]: Cannot load library from path lib/libxgboost4j_gpu.so
Dec 17 14:51:25 localhost.localdomain dai-env.sh[80685]: Failed to load library from both native path and jar!
Dec 17 14:51:25 localhost.localdomain dai-env.sh[80685]: Cannot load library from path lib/linux_64/libxgboost4j_omp.so
Dec 17 14:51:25 localhost.localdomain dai-env.sh[80685]: Cannot load library from path lib/libxgboost4j_omp.so
Dec 17 14:51:25 localhost.localdomain dai-env.sh[80685]: Failed to load library from both native path and jar!
Dec 17 14:51:25 localhost.localdomain dai-env.sh[80685]: Cannot load library from path lib/linux_64/libxgboost4j_minimal.so
Dec 17 14:51:25 localhost.localdomain dai-env.sh[80685]: Cannot load library from path lib/libxgboost4j_minimal.so
Dec 17 14:51:25 localhost.localdomain dai-env.sh[80685]: Failed to load library from both native path and jar!

[root@localhost home]# netstat -an | grep 12345

[root@localhost home]# ls -l 
/opt/h2oai/dai/h2o.jar
-rw-r--r-- 1 root root 109623422 Dec  4 07:45 /opt/h2oai/dai/h2o.jar

[root@localhost home]# jar -xvf /opt/h2oai/dai/h2o.jar lib/linux_64/libxgboost4j_gpu.so
 inflated: lib/linux_64/libxgboost4j_gpu.so

[root@localhost home]# ls -l lib/linux_64/libxgboost4j_gpu.so
-rw-r--r-- 1 root root 34754400 Jul  8 12:56 lib/linux_64/libxgboost4j_gpu.so

[root@localhost home]# file lib/linux_64/libxgboost4j_gpu.so
lib/linux_64/libxgboost4j_gpu.so: ELF 64-bit LSB shared object, x86-64, version 1 (GNU/Linux), dynamically linked, BuildID[sha1]=c7c3682ccf33d3d0395772e924be1e416a60a2c4, not stripped","['h2o', 'driverless-ai', 'ppc64le']",nasica88,https://stackoverflow.com/users/3404962/nasica88,"1,195"
53782766,53782766,2018-12-14T15:38:52,2018-12-14 18:39:27Z,633,"I want to increase the h2o cluster memory up to 64gb. Can I do that yes or no? If no then it should be equal or less to my system memory? or if yes then how much I can allocate? 


import h2o
h2o.init(nthreads=-1,max_mem_size='16g')



Thanks","['python-3.x', 'memory', 'h2o']",Unknown,,N/A
53777268,53777268,2018-12-14T09:52:24,2018-12-14 10:28:12Z,0,"Is it possible to perform a stacked ensemble with H2O (under R) using previously ran caret models? How could we load caret models to the H2O server?


(I am aware of the existence of the 'caretEnsemble' package, but it does not handle multiclass data).


Thanks for your advices.","['r', 'r-caret', 'h2o']",user31888,https://stackoverflow.com/users/7121090/user31888,421
53762743,53762743,2018-12-13T13:16:45,2018-12-13 17:30:22Z,691,"When I go for dl_model.show(), it shows me all the information but not the accuracy of the model and as well on the performance of Validation data it also not show the AUC. when I was running this command, I am getting this error


print('AUC', dl_model.auc(valid = False))


   KeyError                    Traceback (most recent call last)

<ipython-input-655-a4a2f0946c88> in <module>()
----> 1 print('AUC', dl_model.auc())

~\Anaconda3\lib\site-packages\h2o\model\model_base.py in auc(self, train, valid, xval)
    682         tm = ModelBase._get_metrics(self, train, valid, xval)
    683         m = {}
--> 684         for k, v in viewitems(tm): m[k] = None if v is None else v.auc()
    685         return list(m.values())[0] if len(m) == 1 else m
    686 

~\Anaconda3\lib\site-packages\h2o\model\metrics_base.py in auc(self)
    165     def auc(self):
    166         """"""The AUC for this set of metrics.""""""
--> 167         return self._metric_json['AUC']
    168 
    169     def pr_auc(self):

KeyError: 'auc'



Thanks","['python-3.x', 'deep-learning', 'h2o', 'auc']",Unknown,,N/A
53718511,53718511,2018-12-11T06:24:00,2023-04-30 13:03:45Z,321,"When trying to connect H2O to SQL Server 2016 to import a table, H2O returns a SQL Exception relating to FETCH Statement.


Environment:




Ubuntu VM running H2O Version 3.22.0.2




Windows instance with SQL Server 2016






JDBC Driver/s tested: MSSQL-jdbc-7 , MSSQL-jdbc-6.2.1 and sqljdbc4


ERROR MESSAGE: SQLException: Invalid usage of the option NEXT in the FETCH 
statement. Failed to connect and read from SQL database with connection_url: 
jdbc:sqlserver://<IP_ADDRESS>:1433;databaseName=DataStage;



Any ideas on what can be done to fix this?


Edit: Query used:


importSqlTable {""connection_url"":""jdbc:sqlserver://<IP_ADDRESS>:1433;databaseName=DataStage;"",""table"":""dbo.TestTable"",""columns"":"""",""username"":""xxxx"",""password"":""XXXX""}



Edit 2: this is a clean install of H2O, so if there is any extra configuration that is meant to be done other than adding the driver/s to the class path, pointing me to the relevant documentation would be appreciated.


Edit 3: I've used a db tool (DBVisualiser) on the Ubuntu instance with the MSSQL-7 driver to confirm that the VM can connect to the SQL server, whilst H20 can't connect.","['sql-server', 'h2o']",Unknown,,N/A
53712777,53712777,2018-12-10T20:00:11,2020-11-06 16:38:18Z,0,"I am attempting to use package 'iml' in R to create plots of SHAP values from a GBM model created in H2O. 


When I try to create the R6 Predictor object using the 
Predictor.new()
 function I get an error that states 
Error : all(feature.class %in% names(feature.types)) is not TRUE
. 


From this I am guessing that there is something about one of the feature classes that is incorrect, but this is just an educated guess based upon what the error message is literally saying. 


Here is a sample of anonymized data (I can't share the real data because it is confidential):


structure(list(dlr_id_cur = c(1, 2), date_eff = structure(c(16014, 
15416), class = ""Date""), new_vec_ind = structure(c(1L, 1L), .Label = c(""NNA"", 
""UNA""), class = ""factor""), cntrct_term = c(9587879614862828, 
19), amt_financed = c(9455359, 65561175), reg_payment = c(885288, 
389371), acct_stat_cd = structure(c(3L, 3L), .Label = c(""11"", 
""22"", ""33""), class = ""factor""), base_rental = c(1, 626266), down_pymt = c(2, 
6654661), car_count = c(5, 1), dur_lease = c(3974, 6466), returned = structure(1:2, .Label = c(""00"", 
""11""), class = ""factor""), state = structure(c(10L, 1L), .Label = c(""ANA"", 
""BNA"", ""CNA"", ""DNA"", ""FNA"", ""GNA"", ""HNA"", ""INA"", ""KNA"", ""LNA"", 
""MNA"", ""NNA"", ""ONA"", ""PNA"", ""QNA"", ""RNA"", ""SNA"", ""TNA"", ""UNA"", 
""VNA"", ""WNA""), class = ""factor""), zip = c(34633, 45222), zip_two_digits = structure(c(71L, 
36L), .Label = c(""00"", ""01"", ""02"", ""03"", ""04"", ""05"", ""06"", ""07"", 
""08"", ""09"", ""110"", ""111"", ""112"", ""113"", ""114"", ""115"", ""116"", 
""117"", ""118"", ""119"", ""220"", ""221"", ""222"", ""223"", ""224"", ""225"", 
""226"", ""227"", ""228"", ""229"", ""330"", ""331"", ""332"", ""333"", ""334"", 
""335"", ""336"", ""337"", ""338"", ""339"", ""440"", ""441"", ""442"", ""443"", 
""444"", ""445"", ""446"", ""447"", ""448"", ""449"", ""550"", ""551"", ""552"", 
""553"", ""554"", ""555"", ""556"", ""557"", ""558"", ""559"", ""660"", ""661"", 
""662"", ""663"", ""664"", ""665"", ""666"", ""667"", ""668"", ""669"", ""770"", 
""771"", ""772"", ""773"", ""774"", ""775"", ""776"", ""777"", ""778"", ""779"", 
""880"", ""881"", ""882"", ""883"", ""884"", ""885"", ""886"", ""887"", ""888"", 
""889"", ""990"", ""991"", ""992"", ""993"", ""994"", ""995"", ""996"", ""997"", 
""998"", ""999"", ""ANA"", ""BNA"", ""CNA"", ""ENA"", ""GNA"", ""HNA"", ""JNA"", 
""KNA"", ""LNA"", ""MNA"", ""NNA"", ""PNA"", ""RNA"", ""SNA"", ""TNA"", ""VNA""
), class = ""factor"")
, mod_year_date = c(8156, 6278), vehic_mod_fam_code = structure(c(2L, 
2L), .Label = c(""BNA"", ""CNA"", ""ENA"", ""MNA"", ""SNA"", ""TNA"", ""VNA"", 
""XNA""), class = ""factor""), mod_class_code = structure(c(4L, 2L
), .Label = c(""BNA"", ""CNA"", ""ENA"", ""GNA"", ""MNA"", ""RNA"", ""SNA""
), class = ""factor""), count_dl_DL_CDE_CSPS_A_NP = c(945, 337), 
    DL_CDE_CSPS_A_NP_avg_dl = c(3355188283749626, 8835582388327814
    ), count_sv_DL_CDE_CSPS_A_NP = c(6532, 8475), DL_CDE_CSPS_A_NP_avg_sv = c(4471193398278526, 
    6934672627789796), count_dl_NUM_CSPS_INIT_SCR = c(774, 773
    ), NUM_CSPS_INIT_SCR_avg_dl = c(9468453388562312, 5847816458727333
    ), count_sv_NUM_CSPS_INIT_SCR = c(2467, 3882), NUM_CSPS_INIT_SCR_avg_sv = c(5857936629789154, 
    8963457353776469), count_FFV = c(8563, 2566), average_FFV = c(25697792913881564, 
    13693335921646120), csps_NUM_SV = c(8, 6), avg_SV_rating = c(9817541424596360, 
    6218928542331853), csps_FFV_ratio = c(23125612473476952, 
    2), avg_DL_rating = c(2182256921592387, 7668957586431513), 
    has_DL_rating = c(1, 8), has_bad_DL_rating = c(2, 4), serv_has_MNT = c(7, 
    3), serv_has_SCP = c(5, 4), serv_has_ELW = c(9, 4), serv_has_LCP = c(7, 
    1), ro_count = c(6, 1), ro_tot_cust_pay = c(2, 188759), ro_tot_pay = c(3, 
    764372), date_eff_weekday = structure(c(4L, 3L), .Label = c(""FNA"", 
    ""MNA"", ""SNA"", ""TNA"", ""WNA""), class = ""factor""), date_eff_month_int = c(83, 
    7), date_eff_day = c(2, 24)), .Names = c(""dlr_id_cur"", ""date_eff"", 
""new_vec_ind"", ""cntrct_term"", ""amt_financed"", ""reg_payment"", 
""acct_stat_cd"", ""base_rental"", ""down_pymt"", ""car_count"", ""dur_lease"", 
""returned"", ""state"", ""zip"", ""zip_two_digits"", ""mod_year_date"", 
""vehic_mod_fam_code"", ""mod_class_code"", ""count_dl_DL_CDE_CSPS_A_NP"", 
""DL_CDE_CSPS_A_NP_avg_dl"", ""count_sv_DL_CDE_CSPS_A_NP"", ""DL_CDE_CSPS_A_NP_avg_sv"", 
""count_dl_NUM_CSPS_INIT_SCR"", ""NUM_CSPS_INIT_SCR_avg_dl"", ""count_sv_NUM_CSPS_INIT_SCR"", 
""NUM_CSPS_INIT_SCR_avg_sv"", ""count_FFV"", ""average_FFV"", ""csps_NUM_SV"", 
""avg_SV_rating"", ""csps_FFV_ratio"", ""avg_DL_rating"", ""has_DL_rating"", 
""has_bad_DL_rating"", ""serv_has_MNT"", ""serv_has_SCP"", ""serv_has_ELW"", 
""serv_has_LCP"", ""ro_count"", ""ro_tot_cust_pay"", ""ro_tot_pay"", 
""date_eff_weekday"", ""date_eff_month_int"", ""date_eff_day""), row.names = 1:2, class = ""data.frame"")


# 1. create a data frame with just the features
features_iml <- as.data.frame(df_testR) %>% dplyr::select(-returned)

# 2. Create a vector with the actual responses
response_iml <- as.numeric(as.vector(df_testR$returned))

# 3. Create custom predict function that returns the predicted values as a
#    vector (probability of customer churn in my example)
pred <- function(model, newdata)  {
  results <- as.data.frame(h2o.predict(model, as.h2o(newdata)))
  return(results[[3L]])
}

# 4. example of prediction output
pred(GBM5, features_iml) %>% head()

# 5. create Predictor object
predictor = Predictor$new(model = GBM5, data = features_iml, y =
response_iml,  predict.fun = pred,  class = ""classification"")

Error : all(feature.class %in% names(feature.types)) is not TRUE



Here are also so basic descriptions of the dataset and model object I'm
using in the code above:


class(GBM5)

[1] ""H2OBinomialModel""

attr(,""package"")

[1] ""h2o""


class(df_testR)

[1] ""tbl_df""     ""tbl""        ""data.frame""


dim(df_testR)

[1] 47006    44



If there is anything else I can provide or if I have been unclear please let me know.","['r', 'databricks', 'h2o', 'gbm', 'iml']",pat-s,https://stackoverflow.com/users/4185785/pat-s,"6,272"
53686728,53686728,2018-12-08T20:35:08,2018-12-09 20:47:14Z,0,"Is deep learning model supports multi-label classification problem or any other algorithms in H2O?


Orginal Response Variable -Tags:

apps, email, mail
finance,freelancers,contractors,zen99
genomes
gogovan
brazil,china,cloudflare
hauling,service,moving
ferguson,crowdfunding,beacon
cms,naytev
y,combinator
in,store,
conversion,logic,ad,attribution



After mapping them on the keys of the dictionary:
Then


Response variable look like this: 


[74]
[156, 89]
[153, 13, 133, 40]
[150]
[474, 277, 113]
[181, 117]
[15, 87, 8, 11]



Thanks","['deep-learning', 'h2o', 'multilabel-classification']",Unknown,,N/A
53684039,53684039,2018-12-08T15:34:13,2018-12-09 17:46:39Z,728,"I want to predict the response variable, and it has 700 classes.


Deep learning model parameters


from h2o.estimators import deeplearning

dl_model = deeplearning.H2ODeepLearningEstimator(
                                    hidden=[200,200],
                                    epochs  = 10,
                                missing_values_handling='MeanImputation',
                                max_categorical_features=4,
                                distribution='multinomial'
                            )

# Train the model
dl_model.train(x = Content_vecs.names,
                y='tags',
               training_frame   = data_split[0],
               validation_frame = data_split[1]
               )

Orginal Response Variable -Tags: 
apps, email, mail
finance,freelancers,contractors,zen99
genomes
gogovan
brazil,china,cloudflare
hauling,service,moving
ferguson,crowdfunding,beacon
cms,naytev
y,combinator
in,store,
conversion,logic,ad,attribution

Response variable tags: 
[74]
[156, 89]
[153, 13, 133, 40]
[150]
[474, 277, 113]
[181, 117]
[15, 87, 8, 11]



Error:


OSError: Job with key $03017f00000132d4ffffffff$_8355bcac0e9e98a86257f45c180e4898 failed with an exception: java.lang.UnsupportedOperationException: error cannot be computed: too many classes


stacktrace: 
java.lang.UnsupportedOperationException: error cannot be computed: too many classes
    at hex.ConfusionMatrix.err(ConfusionMatrix.java:92)


But in h2o-core/src/main/java/hex/ConfusionMatrix.javaConfusionMatrix.java
  is written that it can compute 1000 classes.","['python-3.x', 'deep-learning', 'h2o', 'multilabel-classification']",Unknown,,N/A
53677325,53677325,2018-12-07T21:59:51,2018-12-07 23:56:42Z,0,"I've been working on validating models developed using h2o.


Specificially I've been testing a neural net implemented using h2o.deeplearning. I've been attempting to generate consistent results by setting a seed in the H2O function, but even doing this I see correlation coefficients of between 0.6 and 0.85 between different versions of the same model, even ones with identical seeds.


I did some reading, and saw that I could force reproducibility by setting the reproducible flag to TRUE, but at a significant performance cost. The input to this model is too large for that to be a feasible method.


Has anyone else ever had to solve a similar problem/found a way to force H2O neural nets to be reproducible with less performance impact?","['r', 'neural-network', 'deep-learning', 'h2o', 'reproducible-research']",Matthieu Brucher,https://stackoverflow.com/users/2266772/matthieu-brucher,22k
53677262,53677262,2018-12-07T21:53:30,2018-12-13 21:16:03Z,0,"My model is producing a negative prediction classifier when my target feature is binary. Does this mean the neural net in h2o.deeplearning just thinks negative values are over 100% likely to be a zero?


my code is as follows: 


modeldataset <- h2o.importFile(path = modeldata)

train<- as.h2o(modeldataset)
model<- h2o.deeplearning(x = colnames(train[1:45]), y = ""Target"", training_frame=train, 'exact_quantiles= False', score_training_samples = 0)

as.h2o(testdata)
as.h2o(modeldataset)

testdata$predClass = h2o.predict(model, newdata=testdata)    # obtain the class (0/1)
testdata$predProb = h2o.predict(model, newdata=testdata)

 h2o.exportFile(testdata, 'file/path', parts = 1)



Why would some of my predictions be negative? apologies in advance for any formatting errors, I am a new user of Stack Overflow.","['r', 'h2o']",bobby,https://stackoverflow.com/users/10677142/bobby,3
53673953,53673953,2018-12-07T17:03:18,2018-12-07 17:38:46Z,0,"I am using tree-based models and have noticed a big difference between 
GBM
 and 
randomForest
 in terms of how the size of training dataset impacts the size of the produced POJO.


I would not expect the size of the training dataset to affect the size of the model object very much at all,* and this holds almost true for 
GBM
.


However, for 
randomForest
, there seems to be a linear relationship between number of training-set rows and size of the exported POJO.  This result:


Size of GBM with m =  1000 and p = 10: 0.15 MB (3281 lines)
Size of GBM with m =  1000 and p = 20: 0.16 MB (3501 lines)
Size of GBM with m = 10000 and p = 10: 0.18 MB (3833 lines)
Size of GBM with m = 10000 and p = 20: 0.19 MB (3899 lines)

Size of RF  with m =  1000 and p = 10:  4.38 MB ( 63908 lines)
Size of RF  with m =  1000 and p = 20:  4.40 MB ( 63606 lines)
Size of RF  with m = 10000 and p = 10: 45.84 MB (637168 lines) <- note 10x increase
Size of RF  with m = 10000 and p = 20: 46.08 MB (635059 lines)    on 10x training rows



is obtained from my benchmark script:


library(data.table)
library(h2o)

pojo_path <- getwd() # your folder here

options(""h2o.use.data.table""=TRUE)
h2o.init(max_mem_size = '8G')
h2o.no_progress()

m_range <- c(1e3,1e4)  # of rows
p_range <- c(10,20)    # of columns

for(p in p_range){
  for(m in m_range){
    # bunch of random data
    set.seed(1)
    mtrx <- matrix(runif(n=m*p), nrow=m, ncol=p)

    # some really random outcome
    set.seed(2)
    y = rowSums(t( t(mtrx)*runif(n=p) )) + rnorm(n=m,sd=0.1)

    dt   <- data.table( mtrx) 
    dt[, `:=`(y = y, id = .I)]
    setkey(dt,id)

    gbm_nm <- paste0('gbm_m_',m,'_p_',p)
    rf_nm <- paste0('rf_m_',m,'_p_',p)
    dt_h2o <- as.h2o(dt)


    gbm <- h2o.gbm(
             x = paste0('V',1:p),
             y = 'y',
             training_frame = dt_h2o,
             nfolds=10,
             model_id = gbm_nm
           )
    rf <- h2o.randomForest(
             x = paste0('V',1:p),
             y = 'y',
             training_frame = dt_h2o,
             nfolds=10,
             model_id = rf_nm
           )

    pojo_gbm_path <- file.path(pojo_path,h2o.download_pojo(gbm, path=pojo_path ) )
    writeLines(paste0('Size of GBM with m = ', m,
                      ' and p = ',p,': ',
                      round(file.info(pojo_gbm_path
                                      )$size/(2^20),2),
                      ' MB (',length(readLines(pojo_gbm_path)),
                      ' lines)'
                      )
    )
    pojo_rf_path <- file.path(pojo_path,h2o.download_pojo(rf, path=pojo_path))
    writeLines(paste0('Size of RF  with m = ', m,
                      ' and p = ',p,': ',
                      round(file.info(pojo_rf_path
                                      )$size/(2^20),2),
                      ' MB (',length(readLines(pojo_rf_path)),
                      ' lines)'
    )
    )
  }
}



The size of the 
randomForest
 objects is becoming prohibitively large on datasets I'm working with.


Why is this happening?  Is this behavior inherent to 
randomForest
 (so decide whether to downsample or use something else) or is there something I can do about it?


[*] I understand that more training data will allow more splits if parameters like min_leaf_size are binding.  But after a certain point, we would expect those to be sufficed and object size to stop growing.


[**] I'm on H2O v 3.20.0.8, R version 3.5.1","['r', 'random-forest', 'h2o']",Unknown,,N/A
53670600,53670600,2018-12-07T13:28:53,2018-12-07 13:47:45Z,180,"I'm currently training an anomaly detection model (in Python) using H2o's 
H2OAutoEncoderEstimator
 and I've had good results at finding anomalous records in my test dataset. 


However, I'd like to take this a step further and try and pinpoint the reason for the anomaly (if possible). 


I've tried making sense of the 
.predict()
 output for my model which looks as follows:




How do I interpret this output? I've tried the docs but found it of very little help.


Also, for those of you that have tried similar approaches, could you recommend a technique for extracting the reasons for the anomaly once detected using the tools provided in the H2o library? Thanks.","['machine-learning', 'h2o']",Unknown,,N/A
53670396,53670396,2018-12-07T13:18:57,2018-12-14 00:40:10Z,191,"H2O.ai have implemented the ""histogram and binning"" technique for efficient and accurate tree-building using categorical variables of high cardinality (>100): 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm-faq/histograms_and_binning.html


Somewhere in their documentation, they have a reference to a publication that details the method but I can't seem to find it anymore - can anyone link to that publication?


Given that the method, which seems to be state-of-the-art for tree-building using categorical variables, is published - are there really no other implementations than H2O.ai?


In sklearn, this feature has been brewing for years on github but apparently still hasn't come out.


I asked the question previously on Data Science: 
https://datascience.stackexchange.com/questions/40241/histogram-and-binning-technique-for-categorical-variables-publication-and-impl","['random-forest', 'decision-tree', 'h2o', 'categorical-data']",rize,https://stackoverflow.com/users/191684/rize,859
53657146,53657146,2018-12-06T17:54:38,2018-12-06 18:21:42Z,311,"As a follow-up to 
How to suppress ""Build Progress"" bar when training an h2o model?
, I would like to 
keep
 the progress bar, but only while it is not 100% yet, after which I want it to disappear.


Note that clearing the output 
completely
 is 
not
 an answer: there may be valuable output there already that I want to keep.","['python', 'jupyter-notebook', 'h2o']",sds,https://stackoverflow.com/users/850781/sds,59.8k
53656780,53656780,2018-12-06T17:29:28,2021-08-17 18:24:30Z,0,"It appears that the difference between cross-validation and training AUC ROC with 
H2OGradientBoostingEstimator
 remains high despite my best attempts using 
min_split_improvement
.


Using the same data with 
GradientBoostingClassifier(min_samples_split=10)
 results in no overfitting, but 
I can find no analogue of 
min_samples_split
.


Prepare Data


from sklearn.datasets import make_classification
X, y = make_classification(n_samples=10000, n_features=40,
                           n_clusters_per_class=10,
                           n_informative=25,
                           random_state=12, shuffle=False)

features = [""x%02d"" % (i) for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=features)
df[""y""] = y
nfolds = 5

import h2o
h2o.init()

h2of = h2o.H2OFrame(df)
h2of[""y""] = h2of[""y""].asfactor()



Run modeling


def print_h2o_auc(m):
    print(""{m} train: {a:.2%} xv: {x:.2%}"".format(
        m=m.model_id, a=m.auc(), x=float(m.cross_validation_metrics_summary().as_data_frame().set_index("""").loc[""auc"",""mean""])))

from h2o.estimators.gbm import H2OGradientBoostingEstimator

for msi in [0.00001, 0.0001, 0.001, 0.01, 0.1]:
    m = H2OGradientBoostingEstimator(
        model_id=""gbm %g"" % (msi),
        ntrees=100, max_depth=3, min_rows=100, min_split_improvement=msi,
        nfolds=5, fold_assignment=""stratified"",
        keep_cross_validation_predictions=True, seed=1)
    m.train(x=features, y=""y"", training_frame=h2of)
    print_h2o_auc(m)



Prints


gbm 1e-05 train: 84.35% xv: 77.12%
gbm 0.0001 train: 84.35% xv: 77.12%
gbm 0.001 train: 82.71% xv: 76.53%
gbm 0.01 train: 68.06% xv: 65.49%
gbm 0.1 train: 50.00% xv: 50.00%



IOW, the difference in performance remains significant (even though it does decline).


What else can I try to reduce overfitting?","['cross-validation', 'h2o']",sds,https://stackoverflow.com/users/850781/sds,59.8k
53654605,53654605,2018-12-06T15:26:41,2018-12-30 08:23:11Z,0,"I am trying to predict the Costa Rican Household Poverty Level Prediction. There are 4 levels in the ""Target"" column which I already converted to factor. However,  I could not look up my AUC or do grid search. I keep encountering this error 




Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  :

  ERROR MESSAGE:

  Invalid argument for sort_by specified. Must be one of: [r2, mean_per_class_accuracy, max_per_class_error, err, total_rows, rmse, accuracy, err_count, logloss, mse, mean_per_class_error]




This somehow my model was set up as a regression model, not a classification model
The entire code:


class(train3.na$Target)    
gradientboost=as.h2o(train3.na)
          split=h2o.splitFrame(gradientboost,c(.6,.2),seed=1234)
          train_gb1=h2o.assign(split[[1]],""valid.hex"")
          valid_gb1=h2o.assign(split[[2]],""valid.hex"")
          test_gb1=h2o.assign(split[[3]],""valid.hex"")
          gbm_params <- list(learn_rate = c(0.01, 0.1),
                              max_depth = c(3, 5, 9),
                              sample_rate = c(0.8, 1.0),
                              col_sample_rate = c(0.2, 0.5, 1.0))

          gbm_grid1=h2o.grid(""gbm"",training_frame = train_gb1,validation_frame = valid_gb1,x=1:51,y=52,
                             grid_id =""gbm_grid1"",hyper_parameters=gbm_params, ntrees=30,seed=2000000)

          gbm_gridperf1 <- h2o.getGrid(grid_id = ""gbm_grid1"",sort_by = ""auc"",
                                       decreasing = TRUE)","['r', 'gradient', 'random-forest', 'h2o']",marc_s,https://stackoverflow.com/users/13302/marc-s,752k
53650846,53650846,2018-12-06T11:51:05,2020-06-19 06:13:18Z,0,"As H2o models are only reusable with the same major version of h2o they were saved with, an alternative is to save the model as MOJO/POJO format. Is there a way these saved models can be reused/loaded from python code. Or is there any way to keep the model for further development when upgrading the H2O version??",['h2o'],Amina,https://stackoverflow.com/users/9440879/amina,21
53649801,53649801,2018-12-06T10:53:23,2018-12-06 19:03:23Z,0,"I'm currently trying to figure out how I can load a saved H2O MOJO model and use it on a Spark DataFrame without needing Sparkling Water. The approach I am trying to use is to load up a 
h2o-genmodel.jar
 file when Spark starts up, and then use then use PySpark's 
Py4J
 interface to access it. My concrete question will be about how access the values generated by the 
py4j.java_gateway
 objects.


Below is a minimal example:


Train model


import h2o
from h2o.estimators.random_forest import H2ORandomForestEstimator
import pandas as pd
import numpy as np

h2o.init()

features = pd.DataFrame(np.random.randn(6,3),columns=list('ABC'))
target = pd.DataFrame(pd.Series([""cat"",""dog"",""cat"",""dog"",""cat"",""dog""]), columns=[""target""])
df = pd.concat([features, target], axis=1)
df_h2o = h2o.H2OFrame(df)

rf = H2ORandomForestEstimator()
rf.train([""A"",""B"",""C""],""target"",training_frame=df_h2o, validation_frame=df_h2o)



Save MOJO


model_path = rf.download_mojo(path=""./mojo/"", get_genmodel_jar=True)
print(model_path)



Load MOJO


from pyspark.sql import SparkSession

spark = SparkSession.builder.config(""spark.jars"", ""/home/ec2-user/Notebooks/mojo/h2o-genmodel.jar"").getOrCreate()

MojoModel = spark._jvm.hex.genmodel.MojoModel
EasyPredictModelWrapper = spark._jvm.hex.genmodel.easy.EasyPredictModelWrapper
RowData = spark._jvm.hex.genmodel.easy.RowData

mojo = MojoModel.load(model_path)
easy_model = EasyPredictModelWrapper(mojo)



Predict on a single row of data


r = RowData()
r.put(""A"", -0.631123)
r.put(""B"", 0.711463)
r.put(""C"", -1.332257)

score = easy_model.predictBinomial(r).classProbabilities



So, that far I have been able to get. Where I am having trouble is that I find it difficult to inpect what 
score
 is giving back to me. 
print(score)
 yields the following: 
<py4j.java_gateway.JavaMember at 0x7fb2e09b4e80>
. Presumably there must be a way to the actual generated values from this object, but how would I do that?","['python', 'pyspark', 'h2o', 'py4j']",Karl,https://stackoverflow.com/users/141789/karl,"5,782"
53642304,53642304,2018-12-05T23:21:10,2018-12-06 18:16:20Z,142,"What is the analogue of 

min_samples_split

for 
H2ORandomForestEstimator
 and 
H2OGradientBoostingEstimator
?


(h2o 
min_rows
 == sklearn 
min_samples_leaf
)","['random-forest', 'h2o']",Unknown,,N/A
53638457,53638457,2017-08-20T12:16:02,2017-08-29 16:06:00Z,0,"I am using H2O with R to calculate the euclidean distance between 2 data.frames:


set.seed(121)

#create the data
df1<-data.frame(matrix(rnorm(1000),ncol=10))
df2<-data.frame(matrix(rnorm(300),ncol=10))
#init h2o
h2o.init()

#transform to h2o
df1.h<-as.h2o(df1)
df2.h<-as.h2o(df2)



if I use normal calculations, i.e. the first row:


distance1<-sqrt(sum((df1[1,]-df2[1,])^2))



And If I use the H2O library:


distance.h2o<-h2o.distance(df1.h[1,],df2.h[1,],""l2"")

print(distance1)
print(distance.h2o)



The distance1 and distance.h2o are not the same. Does anybody knows why? Thanks!!","['r', 'distance', 'h2o', 'euclidean-distance']",Jesus,https://stackoverflow.com/users/8490853/jesus,462
53620950,53620950,2018-12-04T20:32:02,2018-12-04 21:23:06Z,129,"H2O docs claim
 that ""for all algos that support the nfolds parameter"" cross-validation is done by the 
train
 method.


However, 
H2OStackedEnsembleEstimator
 does not:




H2OValueError: Unknown parameter nfolds = 5




So, how do I cross-validate such a model?","['cross-validation', 'h2o']",sds,https://stackoverflow.com/users/850781/sds,59.8k
53605264,53605264,2018-12-04T03:27:32,2018-12-06 22:13:36Z,0,"I used to pass the following format of file path to import datafiles from s3 buckets to H2O flow (version 3.18.0.10):


importFiles [""s3a://ACCESS KEY:SECRET KEY@parvin-us-west1-data/Prod/154351418084_train/""]



After updating to version 
3.22.0.2
, i get following error with the same file path.


Error calling GET /3/ImportFiles?path=s3a%3A%2F%2ACCESS KEY%3SECRET KEY%40parvin-us-west1-data%2FProd%2F154351418084_train%2F
--------------------
HTTP connection failure: status=error, code=500, error=Server Error
--------------------



It seems that now it expects different s3 filepath format, is there any documentation about how to pass s3 filepaths with credentials to latest version of h2o?


Update:


After changing the configuration, i'm able to importfiles. After running the importfile cell, the following shows up. 


1 / 1 files imported.


Files   s3a://parvin-us-west1-data/Prod/154351418084_train/data.csv


However when i press ""parse these files"", it shows new cell with the following content but does not make any progress:


setupParse source_frames: [ ""s3a://parvin-us-west1-data/Prod/154351418084_train/data.csv""]


Also at the bottom of the page it says ""Requesting /3/ParseSetup"", but nothing happens. Even it does not give timeout error after 20 minutes.


The last line of terminal log is:
1283   #71051-12 INFO: POST /3/ParseSetup, parms: {source_frames=[""s3a://parvin-us-west1-data/Prod/154351418084_train/data.csv""]}


Note about version (3.18.0.10) without this problem:


after pressing the ""parse these files"":
It prompts the following information into a cell:


setupParse source_frames: [""s3a://ACCESS KEY:SECRET KEY@parvin-us-west1-data/Prod/154351418084_train/data.csv""]


The difference is that it also includes credential part of url.


Update:
 


I also have tried to start h2o in standalone mode (using core-site.xml to pass credentials). 
Even in this case, it is not capable of parse the files, after importing them.","['amazon-s3', 'h2o']",Unknown,,N/A
53598898,53598898,2018-12-03T17:32:54,2019-11-15 23:08:42Z,0,"I got this issue when I run H2o for xgboost. May I ask how can I solve this issue? Thank you.


I run this code


h2o.hit_ratio_table(gbm2,valid =T) 



And I encounter this error


"" Error in names(v) <- v_names : 
'names' attribute [1] must be the same length as the vector [0]""



Then I proceed run 


mean(finalRF_prediction$predict==test_gb$Cover_Type)



and I got the error:


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
ERROR MESSAGE:
Name lookup of 'NULL' failed



My model is:


gbm2=h2o.gbm(training_frame = train_gb,validation_frame = valid_gb,x=1:51,y=52,
         model_id=""gbm2_covType_v2"",
         ntrees=200,
         max_depth = 30,
         sample_rate = .7,
         col_sample_rate = .7,
         learn_rate=.3,
         stopping_round=2,
         stopping_tolerance = .01,
         score_each_iteration = T,seed=2000000)
finalRF_prediction=h2o.predict(object=gbm2,newdata = test_gb)
summary(gbm2)
h2o.hit_ratio_table(gbm2,valid=T)[1,2]
mean(finalRF_prediction$predict==test_gb$Cover_Type)","['r', 'h2o', 'xgboost']",Thai Phi,https://stackoverflow.com/users/10740194/thai-phi,13
53594777,53594777,2018-12-03T13:21:57,2018-12-04 00:23:14Z,0,"I want to predict tags using an H2o deep learning model, and I can't  interpret my H2o deep learning output. 


That's my model parameters of the H2o deep learning model.


dl_model = deeplearning.H2ODeepLearningEstimator(hidden =[200,200],
                                    epochs  = 10,
                                    missing_values_handling= 'MeanImputation',
                                    activation = ""Tanh"", 
                            )



I pass the word2vec vectors of Blog Content which names as Content.vecs and Y is also word2vec of Tags.


Train the model


dl_model.train(x= Content_vecs.names,
               y= 'Y',
               training_frame   = data_split[0],
               validation_frame = data_split[1]
               )



and the output is 


**predict
-0.700515
-0.700515
-0.700515
-0.700515
-0.700515
-0.700515
-0.700515
-0.700515
-0.700515
-0.700515**



In 
Original Data
 Predictor variable is Content and response variable is tags.
I passing Word2vec vectors of Contents as x and Tags as y of in deep learning 
Figure
.
I want to predict single or multiple tag using H2o deep learning and word2vec","['python-3.x', 'deep-learning', 'word2vec', 'h2o']",Unknown,,N/A
53594136,53594136,2018-12-03T12:44:53,2021-08-25 17:31:23Z,0,"I would like to calculate some hash value of an 
h2o.frame.H2OFrame
. Ideally, in both 
R
 and 
python
. My understanding of 
h2o.frame.H2OFrame
 is that these objects basically ""live"" on the 
h2o
 server (i.e., are represented by some 
Java
 objects) and not within 
R
 or 
python
 from where they might have been uploaded.


I want to calculate the hash value ""as close as possible"" to the actual training algorithm. That rules out calculation of the hash value on (serializations of) the underlying 
R
 or 
python
 objects, as well as on any underlying files from where the data was loaded.
The reason for this is that I want to capture all (possible) changes that 
h2o
's upload functions perform on the underlying data.


Inferring from the 
h2o docs
, there is no hash-like functionality exposed through 
h2o.frame.H2OFrame
.
One possibility to achieve a hash-like summary of the 
h2o
 data is through summing over all numerical columns and doing something similar for categorical columns. However, I would really like to have some avalanche effect in my hash function so that small changes in the function input result in large differences of the output. This requirement rules out simple sums and the like.


Is there already some interface which I might have overlooked?
If not, how could I achieve the task described above?


import h2o
h2o.init()
iris_df=h2o.upload_file(path=""~/iris.csv"")

# what I would like to achieve
iris_df.hash()
# >>> ab2132nfqf3rf37 

# ab2132nfqf3rf37 is the (made up) hash value of iris_df



Thank you for your help.","['python', 'r', 'h2o']",cryo111,https://stackoverflow.com/users/983028/cryo111,"4,474"
53588101,53588101,2018-12-03T05:44:02,2018-12-03 18:16:25Z,368,"I need to classify the text docs from elasticsearch using naive bayes classifier.
I experimented on nltk but it doent have support for incremental or stream data handling.
I referred to the below doc


H2O naive bayes


Is it possible to do incremental training with H2O if yes, how? i am also open to use some other classifier which supports incremental mining.","['python', 'h2o']",Katiyman,https://stackoverflow.com/users/3390851/katiyman,847
53587308,53587308,2018-12-03T04:03:15,2018-12-14 23:40:03Z,0,"I have read several threads on here in regards to h2o.predict() and h2o.performance() differences (as seen from link below).


How to interpret the probabilities (p0, p1) of the result of h2o.predict()


Can someone tell me which threshold does h2o.predict() use? Is it 
max f1
? If so, is it the threshold from training data, validation data, or cross validation?


I tried to use the validation threshold using 
max f1
 and 
max f0point5
 on the testing set (completely separate from training and validation data) but the predicted class from h2o.predict() and the class from using the threshold doesn't match completely.


The closest one I got is to use 
max f0point5
 threshold from training and apply it to testing set.


There is not much documentation on h2o.predict. Also, is there a best practice for threshold, i.e. mean threshold of validation and training, etc?


Thanks in advance!","['r', 'h2o']",Unknown,,N/A
53583235,53583235,2018-12-02T18:20:27,2018-12-03 11:03:25Z,0,"I have built a Random Forest model (H2O library) and then checked its accuracy on some test data. I would like to use the F1 score as a measure of the success of the model. However, I cannot find in the documentation a way to retrieve it.


I know that it is possible as this appears 
here
 


performance = best_nn.model_performance(test_data = test)
F1        = performance.F1()



However, in my case, for some reason,  performance does not have F1 as a method. 
What is wrong, and how is it possible to retreive it?


Environment:


H2O cluster uptime: 7 mins 29 secs
H2O cluster timezone:   Asia/Jerusalem
H2O data parsing timezone:  UTC
H2O cluster version:    3.22.0.2
H2O cluster version age:    10 days
H2O cluster name:   H2O_from_python_user_24aghd
H2O cluster total nodes:    1
H2O cluster free memory:    894 Mb
H2O cluster total cores:    4
H2O cluster allowed cores:  4
H2O cluster status: locked, healthy
H2O connection url: http://localhost:54321
H2O connection proxy:   None
H2O internal security:  False
H2O API Extensions: Algos, AutoML, Core V3, Core V4
Python version: 2.7.15 final","['random-forest', 'h2o']",Unknown,,N/A
53552294,53552294,2018-11-30T06:19:57,2018-11-30 08:37:43Z,0,"In the summary output the MSE for cross-validation data is 0.1641124, however, it is 0.14977892 in the  detailed Cross-Validation Metrics Summary. Are they not the same metrics? 


library(h2o)

h <- h2o.init()
data <- as.h2o(iris)
part <- h2o.splitFrame(data, 0.7, seed = 123)
train <- part[[1]]
test <- part[[2]]

m <- h2o.glm(x=2:5,y=1,train, nfolds = 10, seed = 123)
summary(m)


#...
#H2ORegressionMetrics: glm

#** Reported on cross-validation data. **
#** 10-fold cross-validation on training data (Metrics computed for combined 
#holdout predictions) **

#MSE:  ***0.1641124***
#RMSE:  0.4051079
#... 

#Cross-Validation Metrics Summary: 
#  mean  sd  cv_1_valid cv_2_valid cv_3_valid  cv_4_valid  cv_5_valid cv_6_valid  cv_7_valid cv_8_valid cv_9_valid


#...

#  mse  ***0.14977892*** 0.053578787  0.14102486 0.14244498 0.05266633  0.19028585 0.043878503 0.12635022  0.13820939 0.15831167 0.33359975","['r', 'h2o']",Unknown,,N/A
53552137,53552137,2018-11-30T06:05:55,2018-11-30 07:48:45Z,0,"I would like to get R2 between the predicted  and actual data in test dataset, why the result from h2o.performance(m,  test) is different from caret::R2() or a 'lm' model?


'h2o.performance(m,  test)' is  0.733401, and 'caret::R2(p,  a)' is 0.7577784
 summary(lmm)$r.squared is the same as 'caret::R2(p,  a)'


Example code:


library(h2o)

h <- h2o.init()
data <- as.h2o(iris)
part <- h2o.splitFrame(data, 0.7, seed = 123)
train <- part[[1]]
test <- part[[2]]

m <- h2o.glm(x=2:5,y=1,train, nfolds = 10, seed = 123)

summary(m)
predictions <- h2o.predict(m, test)

p <- as.data.frame(predictions)
a <- as.data.frame(test[1])
caret::R2(p,  a)
# 0.7577784
h2o.performance(m,  test)
# the R^2 is 0.733401
df <- data.frame(p=p, a=a)
lmm <- lm(predict ~ Sepal.Length, data =df)
summary(lmm)$r.squared
# the r.squared is 0.7577784","['r', 'h2o']",Unknown,,N/A
53545179,53545179,2018-11-29T18:12:28,2018-12-04 17:34:05Z,0,"My question is about H2O Gain/Lift table. I understand that the response rate is the proportion of all the events that fall into the group/bin. HOW to get that pieces of data that fall into bin 1, bin 2, etc.? 
I want to see how the key variables look in each group/bin in respect to the Response Rate.
 


It would be great to have a full description of how the measures in Gain/Lift table are calculated (formulas)","['response', 'h2o']",Unknown,,N/A
53511976,53511976,2018-11-28T04:07:46,2018-12-03 09:37:52Z,0,"Trying to plot ROC curve for H2O Model Object in R, however, I keep receiving the following error message: 


""Error in as.double(y) : 
  cannot coerce type 'S4' to vector of type 'double'""


My code is as follows: 


drf1 <- h2o.randomForest(x=x,y=y,training_frame = train,validation_frame = valid, nfolds = nfolds, fold_assignment = ""Modulo"",keep_cross_validation_predictions = TRUE,seed = 1)


plot((h2o.performance(drf1,valid = T)), type = ""roc"")


I followed suggestions found here: 
How to directly plot ROC of h2o model object in R
 


Any help would be greatly appreciated!","['r', 'h2o', 'roc']",Vanessa F,https://stackoverflow.com/users/10714868/vanessa-f,11
53508887,53508887,2018-11-27T22:01:49,2018-11-28 04:42:32Z,472,"Does h2o.splitFrame account for class proportion for multinomial classification? For example, if my original dataset has three classes with proportion of 20%, 70%, and 10%, when I create train, valid and test datasets, would they have similar class proportion?


Thank you for your input!",['h2o'],shan,https://stackoverflow.com/users/10713975/shan,13
53485434,53485434,2018-11-26T16:36:38,2019-01-09 22:51:10Z,0,"I'm trying to find out the exact formula used in H2O for the Mean Residual Deviance loss function for a Tweedie distribution.


Or even, in general, what would be the mean residual deviance for a Tweedie distributed dependent variable?


So far, I've found this page (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html#tweedie-models
) where the deviance formula for a tweedie distribution is given as:




However, inside the H2O code, found on github on this page line 103 (
https://github.com/h2oai/h2o-3/blob/master/h2o-core/src/main/java/hex/Distribution.java#L103
) the formula is specified differently (ignoring the omega, which is just the weight, and the lack of summation):


2 * w * (Math.pow(y, 2 - tweediePower) / ((1 - tweediePower) * (2 - tweediePower)) - y * exp(f * (1 - tweediePower)) / (1 - tweediePower) + exp(f * (2 - tweediePower)) / (2 - tweediePower))



which in equation form is:




So, is the documentation wrong or the implementation? I would appreciate any help!


Thank you!","['statistics', 'h2o', 'glm', 'gbm', 'tweedie']",Unknown,,N/A
53477412,53477412,2018-11-26T08:46:10,2018-12-06 21:14:20Z,0,"I am using the H2O-DeepLearning Model for a Regression Problem. What i observe is that Training RMSE is higher than Validation RMSE. I am using the model with default parameter which is two hidden layers with 200 Neurons each and no l1/l2 Regularization. Activation is Rectifier. No Dropout added.


I am wondering how can i tune the hyperparameters two get Training RMSE below Validation RMSE?


Any hints for parameter?


I am using separate Train, Validation and Test-Set. Training Data has 1958826 Samples, Validation and Test set have 599380 Samples each.


R-squared Value is around 0.65 - 0.7


Edit: While I am experiencing lower Validation RMSE than Training RMSE, it seems that the Values for Residual-Deviance in Training are lower than for Validation. So this seems fine.


Edit:
Training:
RMSE: 0.3592
Deviance: 0.0071


Validation:
RMSE: 0.3403
Deviance: 0.0082


I am doing quantile regression (if that is any help) and i have separate train/test data sets, while i splitted the test-set into validation and test with the h2o-split_frame-method.","['validation', 'deep-learning', 'regression', 'h2o']",Unknown,,N/A
53474151,53474151,2018-11-26T02:45:25,2018-11-28 21:17:52Z,44,"We are working with H2O version 3.22.0.1. We created a process in java 10 that communicates with the REST API utilizing jersey version 2.27 with gson 2.3.1. The process invokes ImportFiles, followed by ParseSetup and Parse. Everything works well up until that point. Then the process invokes 3/ModelBuilders/gbm/parameters. From examining the log, it appears that the H2O server responds as expected. However, gson throws a JsonSyntaxException caused by the following:


java.lang.IllegalStateException: Expected BEGIN_OBJECT but was BEGIN_ARRAY at line 1 column 4115 path $.parameters


Upon further analysis,  it appears that the H2O server is providing a GBMV3 object with an array of ModelParameterSchemaV3 objects, while the GBMV3 class, as defined in the library that our client uses, extends SharedTreeV3, which extends ModelBuilderSchema, which has a single instance of ModelParametersSchemaV3. There is an apparent discrepancy between the way the GBMV3 object provided by the H2O server is composed, and the way the class is defined in the H2O library. One has an array of ModelParameterSchemaV3 objects, while the other has a single instance of ModelParametersSchemaV3. Is that the case? If so, could you please help us understand what we may be doing wrong, and how to correct it?


See the files located at: 
https://1drv.ms/f/s!AsSlPHvlhJI1hIpB2M5X49J5L-h1qw


Run the H2O server. Import the CSV file in H2O Flow. SetupParse and Parse the data. Run the test procedure. Thank you for your kind assistance.",['h2o'],Unknown,,N/A
53451856,53451856,2018-11-23T19:07:32,2018-11-25 10:38:18Z,0,"I have been stumped on this problem for a very long time and cannot figure it out. I believe the issue stems from subsets of data.frame objects retaining information of the parent but I also feel it's causing issues when training h2o.deeplearning models on what I think is just my training set (though this may not be true). See below for sample code. I included comments to clarify what I'm doing but it's fairly short code:


dataset = read.csv(""dataset.csv"")[,-1] # Read dataset in but omit the first column (it's just an index from the original data)
y = dataset[,1] # Create response
X = dataset[,-1] # Create regressors

X = model.matrix(y~.,data=dataset) # Automatically create dummy variables
y=as.factor(y) # Ensure y has factor data type
dataset = data.frame(y,X) # Create final data.frame dataset

train = sample(length(y),length(y)/1.66) # Create training indices -- A boolean
test = (-train) # Create testing indices

h2o.init(nthreads=2) # Initiate h2o

# BELOW: Create h2o.deeplearning model with subset of dataset.
mlModel = h2o.deeplearning(y='y',training_frame=as.h2o(dataset[train,,drop=TRUE]),activation=""Rectifier"",
                           hidden=c(6,6),epochs=10,train_samples_per_iteration = -2)


predictions = h2o.predict(mlModel,newdata=as.h2o(dataset[test,-1])) # Predict using mlModel
predictions = as.data.frame(predictions) # Convert predictions to dataframe object. as.vector() caused issues for me
predictions = predictions[,1] # Extract predictions

mean(predictions!=y[test]) 



The problem is that if I evaluate this against my test subset I get almost 0% error:


[1] 0.0007531255



Has anyone encountered this issue? Have an idea of how to alleviate this problem?","['r', 'h2o']",Nicklovn,https://stackoverflow.com/users/6810989/nicklovn,279
53441181,53441181,2018-11-23T05:42:18,2018-11-29 20:39:34Z,0,"I'm a beginner with R, and just had some trouble with the prediction function.
I built a Random Forest model with 
h2o
, where the 
y
 is 0/1(buy/no buy).
 Then, I tried to use the 
predict()
 function to apply the model to a new dataset
eg: 
pre=predict(rf,test_data)


I can see the summary of my prediction result like below:


> summary(pre)
   predict p0               p1                 
   0:998   Min.   :0.0000   Min.   :5.601e-05  
   1: 97   1st Qu.:0.9989   1st Qu.:5.601e-05  
           Median :0.9989   Median :5.601e-05  
           Mean   :0.9150   Mean   :8.498e-02  
           3rd Qu.:0.9989   3rd Qu.:5.601e-05  
           Max.   :0.9999   Max.   :1.000e+00 



But, I couldn't 
View()
 my prediction result as a list. Here is what I got when using 
View(pre)
:




Ultimately, I'd like to know which row has been predicted as 1(buy), which row has been predicted as 0(no buy). Does anyone know how to resolve this issue?
thanks a lot!","['r', 'h2o']",Marcus Campbell,https://stackoverflow.com/users/9598813/marcus-campbell,"2,796"
53437936,53437936,2018-11-22T21:01:13,2018-11-22 21:44:32Z,118,"How do I integrate Apache Knox or Keycloak with H2O Steam to authenticate users?

In the H2O Security Model (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/security.html#
), I could not find anything related to custom authentication.","['openid-connect', 'keycloak', 'h2o', 'apache-knox']",Kevin,https://stackoverflow.com/users/10692909/kevin,1
53428004,53428004,2018-11-22T09:45:08,2018-11-29 13:44:15Z,0,"I m using h2o package from R on a server with CentOs 7.


I installed correctly library. Java version


java version ""1.8.0_181""
Java(TM) SE Runtime Environment (build 1.8.0_181-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)



Settled correctly as JAVA_HOME.
When I run 
h2o.init()
 I got this long message with error


rary: xgboost4j_gpu
11-22 10:42:12.375 127.0.0.1:54321       31865  main      INFO: XGBoost supported backends: [WITH_GPU, WITH_OMP]
11-22 10:42:12.375 127.0.0.1:54321       31865  main      INFO: ----- H2O started  -----
11-22 10:42:12.376 127.0.0.1:54321       31865  main      INFO: Build git branch: rel-wright
11-22 10:42:12.376 127.0.0.1:54321       31865  main      INFO: Build git hash: 0457fda98594a72aca24d06e8c3622d45bd545d2
11-22 10:42:12.376 127.0.0.1:54321       31865  main      INFO: Build git describe: jenkins-rel-latest-stable-1-g0457fda
11-22 10:42:12.376 127.0.0.1:54321       31865  main      INFO: Build project version: 3.20.0.8
11-22 10:42:12.376 127.0.0.1:54321       31865  main      INFO: Build age: 2 months
11-22 10:42:12.376 127.0.0.1:54321       31865  main      INFO: Built by: 'jenkins'
11-22 10:42:12.376 127.0.0.1:54321       31865  main      INFO: Built on: '2018-09-21 16:54:12'
11-22 10:42:12.376 127.0.0.1:54321       31865  main      INFO: Watchdog Build git branch: (unknown)
11-22 10:42:12.376 127.0.0.1:54321       31865  main      INFO: Watchdog Build git hash: (unknown)
11-22 10:42:12.377 127.0.0.1:54321       31865  main      INFO: Watchdog Build git describe: (unknown)
11-22 10:42:12.377 127.0.0.1:54321       31865  main      INFO: Watchdog Build project version: (unknown)
11-22 10:42:12.377 127.0.0.1:54321       31865  main      INFO: Watchdog Built by: (unknown)
11-22 10:42:12.377 127.0.0.1:54321       31865  main      INFO: Watchdog Built on: (unknown)
11-22 10:42:12.377 127.0.0.1:54321       31865  main      INFO: XGBoost Build git branch: (unknown)
11-22 10:42:12.377 127.0.0.1:54321       31865  main      INFO: XGBoost Build git hash: (unknown)
11-22 10:42:12.377 127.0.0.1:54321       31865  main      INFO: XGBoost Build git describe: (unknown)
11-22 10:42:12.377 127.0.0.1:54321       31865  main      INFO: XGBoost Build project version: (unknown)
11-22 10:42:12.378 127.0.0.1:54321       31865  main      INFO: XGBoost Built by: (unknown)
11-22 10:42:12.378 127.0.0.1:54321       31865  main      INFO: XGBoost Built on: (unknown)
11-22 10:42:12.378 127.0.0.1:54321       31865  main      INFO: KrbStandalone Build git branch: (unknown)
11-22 10:42:12.378 127.0.0.1:54321       31865  main      INFO: KrbStandalone Build git hash: (unknown)
11-22 10:42:12.378 127.0.0.1:54321       31865  main      INFO: KrbStandalone Build git describe: (unknown)
11-22 10:42:12.378 127.0.0.1:54321       31865  main      INFO: KrbStandalone Build project version: (unknown)
11-22 10:42:12.378 127.0.0.1:54321       31865  main      INFO: KrbStandalone Built by: (unknown)
11-22 10:42:12.378 127.0.0.1:54321       31865  main      INFO: KrbStandalone Built on: (unknown)
11-22 10:42:12.378 127.0.0.1:54321       31865  main      INFO: Processed H2O arguments: [-name, H2O_started_from_R_datascience_msi886, -ip, localhost, -web_ip, localhost, -port, 54321, -ice_root, /tmp/RtmpooR6rF]
11-22 10:42:12.378 127.0.0.1:54321       31865  main      INFO: Java availableProcessors: 4
11-22 10:42:12.379 127.0.0.1:54321       31865  main      INFO: Java heap totalMemory: 121.0 MB
11-22 10:42:12.379 127.0.0.1:54321       31865  main      INFO: Java heap maxMemory: 1.73 GB
11-22 10:42:12.379 127.0.0.1:54321       31865  main      INFO: Java version: Java 1.8.0_181 (from Oracle Corporation)
11-22 10:42:12.379 127.0.0.1:54321       31865  main      INFO: JVM launch parameters: [-Dsys.ai.h2o.debug.allowJavaVersions=11, -ea]
11-22 10:42:12.379 127.0.0.1:54321       31865  main      INFO: OS version: Linux 3.10.0-862.14.4.el7.x86_64 (amd64)
11-22 10:42:12.379 127.0.0.1:54321       31865  main      INFO: Machine physical memory: 7.80 GB
11-22 10:42:12.379 127.0.0.1:54321       31865  main      INFO: X-h2o-cluster-id: 1542879731843
11-22 10:42:12.379 127.0.0.1:54321       31865  main      INFO: User name: 'datascience'
11-22 10:42:12.379 127.0.0.1:54321       31865  main      INFO: IPv6 stack selected: false
11-22 10:42:12.379 127.0.0.1:54321       31865  main      INFO: Possible IP Address: ens160 (ens160), fe80:0:0:0:250:56ff:feb4:1f23%ens160
11-22 10:42:12.380 127.0.0.1:54321       31865  main      INFO: Possible IP Address: ens160 (ens160), 10.0.1.110
11-22 10:42:12.380 127.0.0.1:54321       31865  main      INFO: Possible IP Address: lo (lo), 0:0:0:0:0:0:0:1%lo
11-22 10:42:12.380 127.0.0.1:54321       31865  main      INFO: Possible IP Address: lo (lo), 127.0.0.1
11-22 10:42:12.380 127.0.0.1:54321       31865  main      FATAL: On localhost/127.0.0.1 some of the required ports 54321, 54322 are not available, change -port PORT and try again.
[1] ""localhost""
[1] 54321
[1] FALSE
[1] 503
[1] """"



I looked at the log and this is what's displayed


11-22 10:42:12.380 127.0.0.1:54321       31865  main      FATAL: On localhost/12                                                                                                                                                             7.0.0.1 some of the required ports 54321, 54322 are not available, change -port                                                                                                                                                              PORT and try again.



But it's quite weird since before running all this I verified that the specified port were available. I'm quite desperate, hope you can help.


Thanks


update


So I managed to run. I went under


/home/datascience/.checkpoint/2018-11-20/lib/x86_64-redhat-linux-gnu/3.5.1/h2o/java



and run 
java -jar h2o.jar
 and I found instance running under port 54323.
Then i come back to R and run


 h2o.init(ip='machine_ip',port=54321,max_mem_size='2g',startH2O=FALSE)



and it worked. But why?? I shutdown everything , reopen it and the only to make things work is to run 
java -jar h2o.jar
 and then go to R session and run 
h2o.init()","['r', 'h2o']",Unknown,,N/A
53421976,53421976,2018-11-21T23:40:41,2018-11-22 21:58:34Z,0,"I'm trying to fit an ANN model to a dataset having 7 predictor variables and the response variable is a binary.


I have converted all the required factor variables to numeric (If I am correct, this is a requirement) and the following error pops up?


In .h2o.startModelJob(algo, params, h2oRestApiVersion) :
  Dropping bad and constant columns: [Month, Day of Month, Day Of Week].


Please suggest a way out.


Thanks,
SK","['r', 'neural-network', 'artificial-intelligence', 'cluster-computing', 'h2o']",srkale,https://stackoverflow.com/users/8560902/srkale,21
53383306,53383306,2018-11-19T22:05:18,2018-11-20 01:14:21Z,751,"Training a binary classifier using h2o.ai and would like to know which label is being considered to be the 'positive' class. This makes a difference since if have labels say, 'give cookie' and 'don't give cookie', and are trying to optimize to maximize 
recall
, depending on which label is the 'positive' class we will be giving out more ('give cookie' is positive class) or less ('don't give cookie' as positive class) cookies.


Another post on SO (
How do I specify the positive class in an H2O random forest or other binary classifier?
) seems to imply that level values are assigned by alpha order by default ('a' being the lowest level and 'z' being the highest) and just trying to confirm here as it's own explicit question.


Also, is there a way to see which class is currently the 'positive' class for a model (ie. based on the ordering of the confusion matrix labels when using the 
some_h20_model.confusion_matrix(...)
 output command)?",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
53382158,53382158,2018-11-19T20:28:00,2018-12-29 02:56:11Z,0,"This question draws heavily from the solution to 
this question
 as a jumping off point.
Given that I can use R to produce a mojo model object: 


library(h2o)
h2o.init()
airlinedf <- h2o.importFile(""http://s3.amazonaws.com/h2o-public-test-data/smalldata/airlines/allyears2k_headers.zip"")
airlinemodel <- h2o.gbm(model_id = ""airlinemodel"",
                training_frame = airlinedf,
                x = c(""Year"", ""Month"", ""DayofMonth"", ""DayOfWeek"", ""UniqueCarrier""),
                y = ""IsDepDelayed"",
                max_depth = 3,
                ntrees = 5)
h2o.download_mojo(airlinemodel, getwd(), FALSE)



And bash/graphviz to produce a tree diagram of that model:


java -cp h2o.jar hex.genmodel.tools.PrintMojo --tree 0 -i airlinemodel.zip -o airlinemodel.gv
dot -Tpng airlinemodel.gv -o airlinemodel.png




My question is three fold:




How do I explain the values and decisions in this visualization and the values at the terminal nodes? What are the NAs in the second tier? If the values in the terminal nodes are ""class probabilities"", how can they be negative?




Is there a way to visualize or conceptualize a ""summary tree"" of all the trees in the model? 


How can I produce a diagram to use color or shape to indicate the binary classification assignments of items in the end node?","['r', 'graphviz', 'h2o', 'gbm']",zero323,https://stackoverflow.com/users/1560062/zero323,329k
53343397,53343397,2018-11-16T18:21:07,2018-11-16 22:35:42Z,0,"I have data for various institutes such that certain institutes provide us more fields than others. These additional data fields seem to have a high correlation to the binary outcome we are trying to predict, so ignoring them is not an option. Also, we don't want build institute specific models. 


One of the options we are considering is including institute value as a feature with that idea that a single model will consider it to be the feature used for splitting primarily. Thus, if we imagine a tree based model, each institute gets it's own tree in a single model. 


How could we force a feature to be the primary split feature?","['classification', 'h2o']",add787,https://stackoverflow.com/users/2662206/add787,393
53343283,53343283,2018-11-16T18:11:51,2018-11-16 19:24:21Z,0,"ddply
 is present on 
H2OFrame documentation
. However I can't find it.


I have the version 
3.22.0.1
 which I downloaded at 
here
. The in the source code of this 
.whl
 I can't find 
ddply
. However, in the documentation page, we see a 
link
 for the source code that contains 
ddply
.


I wonder if 
ddply
 was removed, or if it is just present for 
h2o
 in R, or if it is just present in the enterprise version.


Why I can't find it?","['python', 'r', 'h2o']",Eduardo Reis,https://stackoverflow.com/users/2313889/eduardo-reis,"1,951"
53328131,53328131,2018-11-15T21:26:58,2018-11-27 12:31:22Z,0,"I am following the H2O install manual. I can run the jar and get web access, but every time I install python packages via the instruction on the website I am getting the issue:


Version mismatch. H2O is version 3.22.0.1, but the h2o-python package is version 0.0.local


When running the h2o.init()


I am installing the h2o python on macOS using python 3.6 inside a fresh virtual environment I created.


Any Ideas",['h2o'],Lior Amar,https://stackoverflow.com/users/5625091/lior-amar,11
53294034,53294034,2018-11-14T06:03:14,2018-11-14 06:25:11Z,0,"I'm working on the Titanic. However I've run into a problem where the model no longer runs but instead throws back an error that seems to have something to do with memory allocation. 


Error: water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for DRF model: DRF_model_R_1542172359909_24373_cv_1.  Details: ERRR on field: _ntrees: The tree model will not fit in the driver node's memory (2.4 KB per tree x 500 > Zero  ) - try decreasing ntrees and/or max_depth or increasing min_rows!



Here is my code sample:


 y<-""Survived""
x<-setdiff(names(newtrain_imp),y)
rf_mod<-h2o.randomForest(x,y,train_set,
                         nfolds = 10,
                         keep_cross_validation_predictions = T,
                         seed=233,
                         mtries = 8,#sampling default-1 set to al
                         ntrees = 500,
                         max_depth = 12,
                         validation_frame = validate_set,
                         binomial_double_trees = T)","['r', 'h2o']",NelsonGon,https://stackoverflow.com/users/10323798/nelsongon,13.3k
53290612,53290612,2018-11-13T22:42:24,2018-11-13 23:33:56Z,204,"I've begun evaluating a random forest classifier using precision and recall. However, despite the train and test sets being identical for the CPU and GPU implementations of the classifier, I'm seeing differences in the returned evaluation scores. Is this a known bug within the library by chance?


Both code samples are below for reference.


Scikit-Learn (CPU)


from sklearn.metrics import recall_score, precision_score
from sklearn.ensemble import RandomForestClassifier

rf_cpu = RandomForestClassifier(n_estimators=5000, n_jobs=-1)
rf_cpu.fit(X_train, y_train)
rf_cpu_pred = clf.predict(X_test)

recall_score(rf_cpu_pred, y_test)
precision_score(rf_cpu_pred, y_test)

CPU Recall: 0.807186
CPU Precision: 0.82095



H2O4GPU (GPU)


from h2o4gpu.metrics import recall_score, precision_score
from h2o4gpu import RandomForestClassifier

rf_gpu = RandomForestClassifier(n_estimators=5000, n_gpus=1)
rf_gpu.fit(X_train, y_train)
rf_gpu_pred = clf.predict(X_test)

recall_score(rf_gpu_pred, y_test)
precision_score(rf_gpu_pred, y_test)

GPU Recall: 0.714286
GPU Precision: 0.809988","['python', 'scikit-learn', 'random-forest', 'h2o', 'h2o4gpu']",Greg,https://stackoverflow.com/users/4317871/greg,29
53275612,53275612,2018-11-13T07:08:26,2018-11-28 15:28:40Z,0,"I am trying to import an excel file with h2o, but apparently it only works with .csv files, there is some other function besides this for .xls files. Is it possible to load files with this format ?.


library(xlsx)
write.xlsx(x = iris, file = ""C:/Users/USER/Desktop/iris.xls"", row.names = FALSE)
write.csv(x = iris, file = ""C:/Users/USER/Desktop/iris.csv"", row.names = FALSE)

library(h2o)
h2o.init()

h2o.iris <- h2o.importFile(path = ""C:/Users/USER/Desktop/iris.xls"")
#h2o.iris <- h2o.importFile(path = ""C:/Users/USER/Desktop/iris.csv"")","['r', 'h2o']",Rafael Díaz,https://stackoverflow.com/users/8133525/rafael-d%c3%adaz,"2,257"
53267763,53267763,2018-11-12T18:07:10,2018-11-13 06:24:37Z,0,"I have provisioned an Azure HDInsight cluster type ML Services (R Server), operating system Linux, version ML Services 9.3 on Spark 2.2 with Java 8 HDI 3.6. 


I am able to login to Rstudio on the head node via SSH access and I ran the script 


from this tutorial - 
https://blogs.msdn.microsoft.com/azuredatalake/2017/06/26/run-h2o-ai-in-r-on-azure-hdinsight/


located here:


https://bostoncaqs.blob.core.windows.net/scriptaction/install-h2opackages.sh


to install H2o related packages unto the head and worker nodes. 


When I run the library(sparklyr) and library(dplyr) it works fine, however Rstudio does not find the h2o package and when I try to install the h2o package it fails because RCurl is not installed. Then when I try to install RCurl I get the following error ""Error : package 'bitops' required by 'RCurl' could not be found"". When I install bitops it successfully installs but RCurl does not seem to be finding the bitops package within the default install directory temp folder on the HDInsight head node VM's harddrive. 


My question is, how do I get the Rstudio Server to recognize where packages are installed on my HDInsight head node? I am using the default install directory when installing each package but the subsequent packages do not recognize dependent packages are installed.


Thanks!","['azure', 'rstudio', 'h2o', 'sparklyr', 'rstudio-server']",Kreitz Gigs,https://stackoverflow.com/users/9275146/kreitz-gigs,379
53265386,53265386,2018-11-12T15:34:40,2018-11-13 20:56:56Z,191,"h2o.cor function is very powerful because it deals with categorical data, however, on h2o's kmeans function, that also only traditionally deal with numerical data, allows you to specify the categorical_encoding used - how does the h2o.cor function deal with categorical data?",['h2o'],Carmen,https://stackoverflow.com/users/2771326/carmen,117
53217878,53217878,2018-11-08T23:52:26,2018-11-10 01:13:31Z,0,"I'm trying to understand the pros/cons and when to use the various encoding options that are available to me in h2o with the parameter 'categorical_encoding'.   


It would be helpful if people could point out general rules of thumb on how to use this.   


Typically I use the 'Enum' value because I like how all categorical values are grouped together when looking at feature importance.  On the other hand, xgboost's default value is 'label-encoder' I believe, which breaks things up by categorical level/value.  


Unfortunately, I don't really know where to begin or questions to ask around these other values available:




one hot internal


one hot explicit


sort_by_response


enum_limited 


enum
-label-encoder




Again, I primarily stick with enum, sometimes label-encoder, but honestly I don't know practical implications of these various options.  Would love a generalized understanding of when one might be better than other from someone knowledgeable !","['r', 'h2o']",runningbirds,https://stackoverflow.com/users/3788557/runningbirds,"6,565"
53213360,53213360,2018-11-08T17:42:36,2018-11-08 17:58:46Z,77,"I am looking for doing something similar to 
filter
, but with 
H2O
 data frame. I haven't find how.


H2O Group By
 in python only has specific functions for processing groups, such as 
max
, 
min
, etc.","['python', 'pandas', 'group-by', 'h2o']",Eduardo Reis,https://stackoverflow.com/users/2313889/eduardo-reis,"1,951"
53212160,53212160,2018-11-08T16:35:04,2018-11-08 16:54:19Z,0,"I am using H2O in python to make a Generalized Linear Model, binary classification problem, I made the model using


glm_fit_lambda_search = H2OGeneralizedLinearEstimator( family='binomial', 
                                      model_id='glm_fit_lambda_search', 
                                      lambda_search=True )

glm_fit_lambda_search.train( x = x, 
                         y = y, 
                         training_frame = trainH2O, 
                         validation_frame = testH2O )



Now I want to plot the ROC curve of the model, how can I do that?


Also I want to plot multiple ROC curves for comparison


Here is the question in R, 
How to directly plot ROC of h2o model object in R
, How can I do this in python?","['python', 'python-3.x', 'h2o', 'glm', 'roc']",Joe,https://stackoverflow.com/users/8345236/joe,601
53202796,53202796,2018-11-08T06:58:21,2018-11-16 11:40:56Z,0,"Creating a new H2O-3 Cluster deployment in google cloud gives only 2 options for reserving an IP address: Ephemeral and None. Is it possible to create a h2o cluster with a static IP address. Using the ""addresses"" command in gcloud like:


gcloud compute addresses create h2oflow --addresses /* ephemeral external IP assigned to h2o cluster */



assigns the h2oflow address to only 1 of the nodes in the cluster. 
Is it possible to assign a static IP to an entire cluster of h2o nodes?","['r', 'gcloud', 'h2o']",bespectacled,https://stackoverflow.com/users/552127/bespectacled,"2,881"
53202776,53202776,2018-11-08T06:56:39,2018-11-08 10:41:33Z,168,"I made a call to H2ORandomForestEstimator.train(). Is there any way to get progress when training? I haven't found that.


I saw the source code and thought about to add a ""callback"" parameter to train(). How can I contribute code? There is no ""issue"" column for this project in github. Directly pull request?",['h2o'],Qingjin Yang,https://stackoverflow.com/users/10622153/qingjin-yang,1
53202391,53202391,2018-11-08T06:16:43,2023-11-13 15:57:41Z,0,"I'm trying to implement a gradient boosting machine model using R's h2o package. However, the model keeps dropping a certain column that I know from other model build ups that this column is important.


 Warning message:
    In .h2o.startModelJob(algo, params, h2oRestApiVersion) :
      Dropping bad and constant columns:['mycolumn']



How do I stop h2o from dropping this column? Here is what I tried:


gbm_fit<-h2o.gbm(x,y,train_set,nfolds = 10,
                 ntrees = 250,
                 learn_rate = 0.15,
                 max_depth = 7,
                             validation_frame = validate_set,seed = 233,
                ignore_const_cols = F
                )","['r', 'machine-learning', 'h2o', 'gbm']",NelsonGon,https://stackoverflow.com/users/10323798/nelsongon,13.3k
53197487,53197487,2018-11-07T20:40:38,2018-11-08 10:38:18Z,40,"I have created my model POJO, I have to keep my columns in same order with same datatype when generating predictions using Hive UDF? what is the cleanest way to ignore extra columns and add the columns which are present in train data set but not in test data set, my all columns are either double or long.","['deployment', 'pojo', 'h2o']",Shubham,https://stackoverflow.com/users/8397176/shubham,1
53191182,53191182,2018-11-07T14:12:52,2022-09-29 16:43:36Z,123,"I have 
H2O
 version 
3.22.0.1
 where I have created several models using Flow.


I now want to import them into R (v 3.5.1) for further analysis.


However, the version of the package for R is 
h2o_3.20.0.8
 and I receive this error when running 
h2o.loadModel()


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
ERROR MESSAGE:
Found version 3.22.0.1, but running version 3.20.0.8



I saw somewhere in the documentation that the versions must correspond, so is there a (development?) version of the R package for 
H2O
 which corresponds to H2O version 
3.22.0.1
 ? If not, is there any other work-around to import and use models built using Flow 
3.22.0.1
 in R (other than to revert to version 
3.20.0.8
 of H2O)",['h2o'],Robert Long,https://stackoverflow.com/users/1009823/robert-long,"6,706"
53183466,53183466,2018-11-07T04:10:54,2018-11-07 05:51:02Z,0,"I'm looking at Darren Cook's book and was trying to load the data set as follows.


library(h2o)
library(tidyverse)
h2o.init()
mydata<-""https:/raw.githubusercontent.com/DarrenCook/h2o//bk/data sets/""
mydata<-h2o.importFile(paste0(mydata,""iris_wheader.csv""))



To which I got the following error: 


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Illegal character in path at index 60: https://raw.githubusercontent.com//DarrenCook//h2o//bk//data sets//iris_wheader.csv



What am I doing wrong or how can I solve this?","['r', 'h2o']",NelsonGon,https://stackoverflow.com/users/10323798/nelsongon,13.3k
53177343,53177343,2018-11-06T17:53:23,2022-04-21 15:19:06Z,496,"It gives a regression prediction as continuous score with negative values, like -1.27544 < x < 6.68112. How I interpret the negatives?","['xgboost', 'h2o']",General Grievance,https://stackoverflow.com/users/4294399/general-grievance,"4,967"
53174325,53174325,2018-11-06T14:51:52,2018-11-06 15:00:07Z,0,"I first import a dataset from csv to Spark, do some transformation in Spark, and then try to convert it into H2O Frame. Here's my code:


library(rsparkling)
library(h2o)
library(dplyr)
library(sparklyr)

sc <- spark_connect(master = ""local"")

data <- spark_read_csv(sc,""some_data"", paste(path, file_name, sep = """"), memory = TRUE,
                       infer_schema = TRUE)
data_h2o <- as_h2o_frame(sc,data)



The size of the csv file is about 750MB. The last line takes very long time to run, and it fails with the following message:


Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task
3 in stage 10.0 failed 1 times, most recent failure: Lost task 3.0 in stage 10.0
(TID 44, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space



I have 16GB of memory and the dataset can be read into H2O directly with no issues. 


Here is part of the log file:


18/11/06 09:46:45 WARN MemoryStore: Not enough space to cache rdd_16_2 in memory! (computed 32.7 MB so far)
18/11/06 09:46:45 INFO MemoryStore: Memory use = 272.0 MB (blocks) + 57.9 MB (scratch space shared across 4 tasks(s)) = 329.8 MB. Storage limit = 366.3 MB.
18/11/06 09:46:45 INFO CodeGenerator: Code generated in 92.700007 ms
18/11/06 09:46:45 INFO MemoryStore: Will not store rdd_16_0
18/11/06 09:46:45 INFO BlockManager: Found block rdd_16_2 locally
18/11/06 09:46:45 WARN MemoryStore: Not enough space to cache rdd_16_3 in memory! (computed 32.8 MB so far)
18/11/06 09:46:45 INFO MemoryStore: Memory use = 272.0 MB (blocks) + 57.9 MB (scratch space shared across 4 tasks(s)) = 329.8 MB. Storage limit = 366.3 MB.
18/11/06 09:46:45 INFO BlockManager: Found block rdd_16_3 locally
18/11/06 09:46:45 WARN MemoryStore: Not enough space to cache rdd_16_0 in memory! (computed 32.6 MB so far)
18/11/06 09:46:45 INFO MemoryStore: Memory use = 272.0 MB (blocks) + 57.9 MB (scratch space shared across 4 tasks(s)) = 329.8 MB. Storage limit = 366.3 MB.
18/11/06 09:46:45 INFO BlockManager: Found block rdd_16_0 locally
18/11/06 09:46:45 INFO CodeGenerator: Code generated in 21.519354 ms
18/11/06 09:46:45 INFO MemoryStore: Will not store rdd_16_5
18/11/06 09:46:45 WARN MemoryStore: Not enough space to cache rdd_16_5 in memory! (computed 63.6 MB so far)
18/11/06 09:46:45 INFO MemoryStore: Memory use = 272.0 MB (blocks) + 57.9 MB (scratch space shared across 4 tasks(s)) = 329.8 MB. Storage limit = 366.3 MB.","['r', 'apache-spark', 'h2o', 'sparklyr', 'sparkling-water']",Catiger3331,https://stackoverflow.com/users/5665986/catiger3331,641
53173411,53173411,2018-11-06T13:59:36,2018-11-06 22:25:59Z,0,"I have fitted various H2O models, including XGBoost, in R and also within Flow, predicting count data (non-negative integers).


I can fit XGBoost models in Flow from the ""Model"" menu. However, I would like to include XGBoost when using AutoML - however XGBoost is not listed. The available algorithms are:


GLM
DRF
GBM
DeepLearning
StackedEnsemble



The response column is coded as INT, and the version details are:


H2O Build git branch    rel-wright
H2O Build git hash  0457fda98594a72aca24d06e8c3622d45bd545d2
H2O Build git describe  jenkins-rel-latest-stable-1-g0457fda
H2O Build project version   3.20.0.8
H2O Build age   1 month and 15 days
H2O Built by    jenkins
H2O Built on    2018-09-21 16:54:12
H2O Internal Security   Disabled
Flow version    0.7.36



How can I include XGBoost when running AutoML in Flow ?",['h2o'],Unknown,,N/A
53148280,53148280,2018-11-05T04:17:51,2018-11-05 05:31:55Z,527,"In H2O when I run 
h2o.save_model
, it is easy to save model. But the file do not have extension. So what is the format of the file?


I have read the 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html#about-pojo-mojo
 but it seem not belong to either POJO or MOJO",['h2o'],Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
53142643,53142643,2018-11-04T16:00:31,2018-11-05 12:51:29Z,0,"I am using H2O from via an Amazon Ubuntu EC2 AMI I created half a year ago. It works well: When needed I fire up an instance, start H2O in rstudio, go to the flow interface, do my thing and close t down again


But when I try to update H2O to the latest build I cannot access flow. Everything apparently works in rstudio but not flow. I suspect Java, a restart of rstudio and/or the H2O build that is the bleeding edge build number even if I request the latest stable version. t could have


I follow the instructions here:


http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html#install-in-r


and this is the rstudio console


h2o.init()


H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
/tmp/RtmpKNp0jt/h2o_rstudio_started_from_r.out
/tmp/RtmpKNp0jt/h2o_rstudio_started_from_r.err

openjdk version ""1.8.0_171""
OpenJDK Runtime Environment (build 1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11)
OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode)

Starting H2O JVM and connecting: .......... Connection successful!

R is connected to the H2O cluster: 
H2O cluster uptime:         22 seconds 380 milliseconds 
H2O cluster timezone:       Etc/UTC 
H2O data parsing timezone:  UTC 
H2O cluster version:        3.21.0.4364 
H2O cluster version age:    3 months and 13 days !!! 
H2O cluster name:           H2O_started_from_R_rstudio_urm169 
H2O cluster total nodes:    1 
H2O cluster total memory:   0.86 GB 
H2O cluster total cores:    2 
H2O cluster allowed cores:  2 
H2O cluster healthy:        TRUE 
H2O Connection ip:          localhost 
H2O Connection port:        54321 
H2O Connection proxy:       NA 
H2O Internal Security:      FALSE 
H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
R Version:                  R version 3.4.2 (2017-09-28) 

Warning message:
In h2o.clusterInfo() : 
Your H2O cluster version is too old (3 months and 13 days)!
Please download and install the latest version from http://h2o.ai/download/



if (""package:h2o"" %in% search()) { detach(""package:h2o"", unload=TRUE) }


[1] ""A shutdown has been triggered. ""



if (""h2o"" %in% rownames(installed.packages())) { remove.packages(""h2o"") }


Removing package from ‘/home/rstudio/R/x86_64-pc-linux-gnu-library/3.4’
(as ‘lib’ is unspecified)



pkgs <- c(""RCurl"",""jsonlite"")
for (pkg in pkgs) {
+     if (! (pkg %in% rownames(installed.packages()))) { 
install.packages(pkg) }
+ }


install.packages(""h2o"", type=""source"", repos=(c(""
http://h2o-
 
release.s3.amazonaws.com/h2o/latest_stable_R"")))


Installing package into ‘/home/rstudio/R/x86_64-pc-linux-gnu-library/3.4’
(as ‘lib’ is unspecified)
trying URL 'http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R 
/src/contrib/h2o_3.23.0.4471.tar.gz'
Content type 'application/x-tar' length 120706169 bytes (115.1 MB)
==================================================
downloaded 115.1 MB

* installing *source* package ‘h2o’ ...
** R
** demo
** inst
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
* DONE (h2o)

The downloaded source packages are in
‘/tmp/RtmpKNp0jt/downloaded_packages’



library(h2o)


Error: package or namespace load failed for ‘h2o’ in get(method, envir = 
home):
lazy-load database '/home/rstudio/R/x86_64-pc-linux-gnu- 
library/3.4/h2o/R/h2o.rdb' is corrupt
In addition: Warning message:
In get(method, envir = home) : internal error -3 in R_decompress1



Because of the error message I restart R via the menu in rstudio


Restarting R session...



library(h2o)


----------------------------------------------------------------------

Your next step is to start H2O:
> h2o.init()

For H2O package documentation, ask for help:
> ??h2o

After starting H2O, you can use the Web UI at http://localhost:54321
For more information visit http://docs.h2o.ai

----------------------------------------------------------------------


Attaching package: ‘h2o’

The following objects are masked from ‘package:stats’:

cor, sd, var

The following objects are masked from ‘package:base’:

||, &&, %*%, apply, as.factor, as.numeric, colnames, colnames<-, ifelse, 
%in%,
is.character, is.factor, is.numeric, log, log10, log1p, log2, round, signif, 
trunc



h2o.init()


H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
/tmp/RtmpMdVz9z/h2o_rstudio_started_from_r.out
/tmp/RtmpMdVz9z/h2o_rstudio_started_from_r.err

openjdk version ""1.8.0_181""
OpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-1ubuntu0.16.04.1-b13)
OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)

Starting H2O JVM and connecting: . Connection successful!

R is connected to the H2O cluster: 
H2O cluster uptime:         1 seconds 744 milliseconds 
H2O cluster timezone:       Etc/UTC 
H2O data parsing timezone:  UTC 
H2O cluster version:        3.23.0.4471 
H2O cluster version age:    9 hours and 21 minutes  
H2O cluster name:           H2O_started_from_R_rstudio_rrc849 
H2O cluster total nodes:    1 
H2O cluster total memory:   0.86 GB 
H2O cluster total cores:    2 
H2O cluster allowed cores:  2 
H2O cluster healthy:        TRUE 
H2O Connection ip:          localhost 
H2O Connection port:        54321 
H2O Connection proxy:       NA 
H2O Internal Security:      FALSE 
H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
R Version:                  R version 3.4.2 (2017-09-28) 



From here on H2O works in rstudio but flow won´t start.


Any suggestions?","['rstudio', 'h2o']",user2523167,https://stackoverflow.com/users/2523167/user2523167,567
53115696,53115696,2018-11-02T09:16:43,2018-11-05 20:43:19Z,0,"A h2o cluster runs on google cloud. Trying to connect to it from R Studio on Ubuntu using:


conn=h2o.connect(ip = ""external ip"", port=443, strict_version_check = FALSE, username = ""username"", password = ""password"", https = TRUE, insecure = TRUE)



throws the error: Can only start H2O launcher if IP address is localhost. 
Plus, the h2o command(s) also works extremely slow. 
We have tried reinstalling R so that some of the memory profile changes done earlier would not affect the h2o program.","['r', 'h2o']",Unknown,,N/A
53111770,53111770,2018-11-02T02:03:36,2018-11-02 07:47:05Z,310,I am new to H2O and Python. I have designed a deep learning model in H2O flow. Can I convert same model in Python directly? I am looking for the python code behind my H2O flow model.,"['python', 'h2o']",user7833156,https://stackoverflow.com/users/7833156/user7833156,1
53104998,53104998,2018-11-01T16:04:24,2018-11-01 17:02:51Z,171,"I am using H2O sparking water to built GBM model. I know we can view the N folder cross validation results using code below:


gbm_model.model_performance(xval = True)



But is there a way to save each folder's model performance into a data frame? For example, save each folder's AUC into a data frame.","['python', 'h2o', 'sparkling-water']",Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
53097823,53097823,2018-11-01T08:48:11,2018-11-01 10:53:21Z,326,"We are running h2o as a single node cluster inside in AWS:


R is connected to the H2O cluster: 
    H2O cluster uptime:         5 seconds 217 milliseconds 
    H2O cluster timezone:       Etc/UTC 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.17.0.4153 
    H2O cluster version age:    10 months and 4 days !!! 
    H2O cluster name:           h2o-8ba55ebb-7d49-41bd-b4e2-d7be45b5f53e 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   22.20 GB 
    H2O cluster total cores:    8 
    H2O cluster allowed cores:  8 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
    R Version:                  R version 3.4.3 (2017-11-30) 



And starting h2o from java with nthreads -1:


java -ea -Xmx25g -jar /path/to/h2o.jar -name unique-cloud-name 
     -ip localhost -ice_root /tmp/h2o-tmp -nthreads -1



We're wondering if with a single node cluster that h2o is doing parallel processing / using all available and allowed cores.
When we do top -H in the commandline we do see coincidentally 8 active java processes and wondering if those are from h2o and are helping generate our model.","['java', 'parallel-processing', 'h2o']",chrism,https://stackoverflow.com/users/844623/chrism,95
53081964,53081964,2018-10-31T11:08:52,2018-11-01 07:51:52Z,0,"We are running h2o on AWS and training a model with a 14gb CSV dataset on a GBM algo and it gets to 36% and then fails with error:




'Java heap space', caused by java.lang.OutOfMemoryError: Java heap space




Here are the specs for our environment:


h2o Cluster Version: 3.17.0.4153
R Version: 3.4.3
java version ""1.8.0_77""
Java(TM) SE Runtime Environment (build 1.8.0_77-b03)
Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)
Algorithm: GBM
Training DataSet: CSV with 1000 cols, 2,695,297 lines, that is 14gb in size
AWS:
Model       vCPU    Mem (GiB)    Storage
t2.2xlarge  8       32           EBS-Only



Questions:


How much memory is required to train GBM on a dataset of this size?


Is there some configuration we can do to get h20 to manage memory better?  Eg; use available memory to a limit and then stream without crashing or something... 


If it takes a long time and finishes at least you have a model. 
Really we just want something that won't fail... At the moment the only answer is try a smaller dataset... and every training attempt wastes time and money.


We are relatively new to this so any help is much appreciated.  If you need more details just let me know what you need.


Thanks for your time.


Update:


So we had our java option -Xmx set to 14g and we increased that to 25g and that allowed it to continue.","['java', 'amazon-web-services', 'out-of-memory', 'h2o']",Unknown,,N/A
53070222,53070222,2018-10-30T17:57:48,2018-10-30 20:45:36Z,398,"I keep encountering errors when I'm trying to plot trees from h2o.download_mojo(). I followed the official instructions:






# Now download the latest stable h2o release from http://www.h2o.ai/download/
# and run the PrintMojo tool from the command line.
#
# (For MacOS: brew install graphviz)
java -cp h2o.jar hex.genmodel.tools.PrintMojo --tree 0 -i model.zip -o model.gv -f 20 -d 3
dot -Tpng model.gv -o model.png
open model.png








I've made sure I use the latest stable h2o and graphviz installed, while I keep getting this error:






java -cp h2o.jar hex.genmodel.tools.PrintMojo --tree 0 -i model.zip -o model.gv -f 20 -d 3
Error: Could not find or load main class hex.genmodel.tools.PrintMojo












dot -Tpng model.gv -o model.png
Error: dot: can't open model.gv








I'm not familiar with java. It will be great if somebody can help me with this. Thanks very much!","['tree', 'h2o', 'gbm', 'mojo']",Unknown,,N/A
53046533,53046533,2018-10-29T13:28:13,2020-10-09 21:51:36Z,0,"I have used 
H2O
 for about a year now to build and score models but have never used 
MOJO
 to do this. This is something I am currently wanting to do and I came across the function 
h2o.mojo_predict_df
 to score out my models which will drastically increase runtime as well as allowing me to update my 
h2o
 without the worry of my models not scoring in a later version of 
h2o
.


So I have downloaded my 
.zip
 from MOJO and corresponding 
.jar
 file, let's say they sit in the folder




C:\Folder\Test\Model.zip


C:\Folder\Test\h2o-genmodel.jar




Now am I trying to run it through, lets say my R data frame is called Data then I am using:


h2o.mojo_predict_df(frame = Data, 
                    mojo_zip_path = ""C:/Folder/Test/Model.zip"",
                    genmodel_jar_path = ""C:/Folder/Test/h2o-genmodel.jar"")



However, this produces the error:




Error in safeSystem(cmd_str): SYSTEM COMMAND FAILED (exit status 127).




I am not quite sure what this is. My experience is java is limited. I tried on a different machine and got a completely different error: 




""Error: Could not create a Java Virtual Machine"".
  Error: A fatal exception has occurred. The program will exit.
  Unrecognised option : - 1. 
  Error in safeSystem(cmd_str) : SYSTEM COMMAND FAILED (exit status 1).




I'm not sure why they got different errors. The code ran was exactly the same. 


Any help would be appreciated!","['java', 'r', 'h2o']",Unknown,,N/A
53039110,53039110,2018-10-29T05:15:21,2018-10-29 05:15:21Z,0,"I am trying to convert a column in H2O data frame to numeric.


input$col1 = as.numeric(input$col1)



col1 has 3 unique values in my dataset: 1, 0, NULL
During the above statement I get:


""DistributedException from server: 'For input string: \""\\N\""', caused by java.lang.NumberFormatException: For input string: \""\\N\""""



Can someone please help?
Thank you.","['r', 'h2o']",Mathak,https://stackoverflow.com/users/6697300/mathak,45
53038253,53038253,2018-10-29T03:12:06,2018-10-29 17:52:00Z,0,"I have two H2O models (which are saved through h2o.savemodel in R-3.x.x). How do I find out which version of H2O was the model built on? 


I am unable to load one of them with the latest version of H2O. 


>model3 <- h2o.loadModel(pcaModelFileName)

ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/99/Models.bin/)

water.exceptions.H2OIllegalArgumentException
 [1] ""water.exceptions.H2OIllegalArgumentException: Illegal argument: dir of function: importModel: PCA_model_R_1538682208857_7""
 [2] ""    water.api.ModelsHandler.importModel(ModelsHandler.java:212)""                                                          
 [3] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                          
 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                                        
 [5] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                                
 [6] ""    java.lang.reflect.Method.invoke(Method.java:498)""                                                                     
 [7] ""    water.api.Handler.handle(Handler.java:63)""                                                                            
 [8] ""    water.api.RequestServer.serve(RequestServer.java:451)""                                                                
 [9] ""    water.api.RequestServer.doGeneric(RequestServer.java:296)""                                                            
[10] ""    water.api.RequestServer.doPost(RequestServer.java:222)""                                                               
[11] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                                         
[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                         
[13] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                               
[14] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                                           
[15] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                                   
[16] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                                            
[17] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                                    
[18] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                        
[19] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                
[20] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                      
[21] ""    water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:197)""                                                            
[22] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                
[23] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                      
[24] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                              
[25] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                       
[26] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                        
[27] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)""                             
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)""             
[29] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)""                                                     
[30] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)""                                                
[31] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                               
[32] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                         
[33] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                                     
[34] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                      
[35] ""    java.lang.Thread.run(Thread.java:748)""                                                                                

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Illegal argument: dir of function: importModel: PCA_model_R_1538682208857_7



A similar error has been reported and it was said it was a problem with the version but my question is
1. Is this related to version
2. Is there a way to find out which H2o version the model was built on
3. Is there a way, within R, to port models from one version to another. (I potentially have two H2O models built on two different versions.


Using Mojo or Pojo may not be an option since I don't have the data/script needed to rebuild the model.","['r', 'h2o']",Akshay Kadidal,https://stackoverflow.com/users/5263377/akshay-kadidal,535
53007673,53007673,2018-10-26T11:23:56,2018-10-26 12:22:39Z,0,"I am trying to use an H2O library in both Python and R to produce a GLM without an intercept included. Unfortunately, it does not appear to be working. The results are completely off, the intercept coefficient is non-zero (only standardized coefficient for intercept is zero), however, this does not give me a correct prediction.


With intercept excluded from the model, I expect the prediction for a case when all other inputs equal to 0, to be 0 as well. This is not the case. The coefficient is offsetting the prediction quite significantly and actually, if I set intercept=True with simulated data that I know should have no intercept, my intercept coefficient is much closer to 0 than when I run the same data with intercept=False.


The same occurs in both R and Python, and I am unsure if I am doing something incorrectly in setting up the model.


The example of the code I have written just to test the problem in R:


library(h2o)
h2o.init()

x1 = runif(500)
x2 = runif(500)
x3 = runif(500)
y = 2.67*x1 + 1.23*x2 -7.2*x3
h2odata<-data.frame(x1,x2,x3,y)
head(h2odata)

h2odata <- as.h2o(h2odata)

predictors <- c('x1','x2','x3')
response <- 'y'

h2o.splits = h2o.splitFrame(data=h2odata,ratios=.8)
train <- h2o.splits[[1]]
valid <- h2o.splits[[2]]

glm <- h2o.glm(x=predictors,y=response,family='gaussian',link='identity',
               intercept = FALSE,training_frame = train,
               validation_frame = valid)
glm

x1=0
x2=0
x3=0
newdata = data.frame(x1,x2,x3)
colnames(newdata)<-c('x1','x2','x3')

newdata<-as.h2o(newdata)
h2o::h2o.predict(glm,newdata)



Am I missing something obvious here?","['python', 'r', 'h2o', 'sparkling-water']",Alex,https://stackoverflow.com/users/7933627/alex,53
52995416,52995416,2018-10-25T17:57:11,2018-10-26 13:26:32Z,273,"I am having trouble to import a sql table to 
H2O.ai
 using Postgresql JDBC Driver in Ubuntu. I'm getting the follow error:


ERROR MESSAGE:




SQLException: ERROR: relation ""XXX"" does not exist

    Position: 22

  Failed to connect and read from SQL database with connection_url: jdbc:postgresql://localhost:5432/...**




I am executing H2O with the follow command:


java -cp h2o.jar:/usr/share/java/postgresql-9.4.1212.jar water.H2OApp



The JDBC driver is installed and already try to construct the Connection URL in several ways.


I'm using this one right now: 


jdbc:postgresql://localhost:5432/XXX?&useSSL=false","['postgresql', 'ubuntu', 'jdbc', 'h2o']",Unknown,,N/A
52994441,52994441,2018-10-25T16:53:08,2018-10-25 18:44:14Z,0,"Just trying to understand the target encoding map and apply features in R html doc, mapping <- h2o.target_encode_create(data = train, x = list(c(""job""), c(""job"", ""marital"")),
y = ""age"") 


In the above mapping, why is job given separately as part of the list? Is it some sort of interaction variables, why did we miss marital as a separate feature? can we give n number of categorical variables as part of the list or creating separate mapping for each categorical variable is recommended?","['r', 'encoding', 'target', 'h2o']",rknimmakayala,https://stackoverflow.com/users/8366375/rknimmakayala,21
52984157,52984157,2018-10-25T07:56:10,2019-05-30 21:03:50Z,188,"Now we try to use H2o to construct training cluster. It is easy to use by running 
java -jar ./h2o.jar
 and we can setup the cluster with simple 
flatfile.txt
 which contain multiple ip and ports.


But we found that it is impossible to setup the h2o cluster within docker containers. Although we can start multiple containers to run 
java -jar ./h2o.jar
 and add the prepared 
flatfile.txt
, the h2o process will try to bind local(container's eth0) ip which is different from the one in 
flatfile.txt
. We can 
java -jar ./h2o.jar -ip $ip
 to set the one which is in 
flatfile.txt
 but h2o instance is not able to run without this ""external"" ip.","['java', 'docker', 'cluster-computing', 'h2o']",tobe,https://stackoverflow.com/users/2502090/tobe,"1,701"
52965384,52965384,2018-10-24T09:22:06,2018-10-24 18:48:56Z,841,"The 
documentation for DRF
 states




What happens when you try to predict on a categorical level not seen
  during training?

  DRF converts a new categorical level to a NA value in
  the test set, and then splits left on the NA value during scoring. The
  algorithm splits left on NA values because, during training, NA values
  are grouped with the outliers in the left-most bin.




Questions:




So h2o converts unseen levels to NAs and then treats them the same way as NAs in the training data. But what if there are also no NAs in the training data?


Assume my categorical predictor is of 
enum
 type and to be understood as non-ordinal. What does ""
grouped with the outliers in the left-most bin
"" then mean? If the predictor is non-ordinal there is no ""
left-most
"" and there are no ""
outliers
"".


Let's put questions 1 and 2 aside and focus on the part ""
The
algorithm splits left on NA values because, during training, NA values
are grouped with the outliers in the left-most bin
"". This is in contradiction to this 
SO answer
 showing a single DRF tree derived from a MOJO. One can clearly see that NAs go left and right. It also contradicts the answer to another question in the documentation that says ""
missing values as a separate category [...] can go either left or right
"", see






How does the algorithm handle missing values during training?
 Missing
  values are interpreted as containing information (i.e., missing for a
  reason), rather than missing at random. During tree building, split
  decisions for every node are found by minimizing the loss function and
  treating missing values as a separate category that can go either left
  or right.




The last point is more of a suggestion than a question. The 
documentation on missing values for GBM
 says




What happens when you try to predict on a categorical level not seen
  during training?
 Unseen categorical levels are turned into NAs, and
  thus follow the same behavior as an NA. If there are no NAs in the
  training data, then unseen categorical levels in the test data follow
  the majority direction (the direction with the most observations). If
  there are NAs in the training data, then unseen categorical levels in
  the test data follow the direction that is optimal for the NAs of the
  training data.




In contrast to the description of how DRF handles missing values, this seems to be completely consistent. Plus: using the majority path rather than always going left at split points appears to be more natural.","['random-forest', 'h2o']",cryo111,https://stackoverflow.com/users/983028/cryo111,"4,474"
52959724,52959724,2018-10-24T01:00:53,2018-10-24 18:11:19Z,0,"Using h2o python API to train some models and am a bit confused on how to correctly implement some parts of the API. Specifically, what columns should be ignored in a training dataset and how models look for the actual predictor features in a data set when actually using the model's 
predict()
 method. Also how weight columns should be handled (when the actual prediction datasets don't really have weights)


The details of the code here (I think) are not majorly important, but the basic training logic looks something like 


drf_dx = h2o.h2o.H2ORandomForestEstimator(
    # denoting update version name by epoch timestamp
    model_id='drf_dx_v'+str(version)+'t'+str(int(time.time())), 
    response_column='dx_outcome',
    ignored_columns=[
        'ucl_id', 'patient_id', 'account_id', 'tar_id', 'charge_line', 'ML_data_begin',
        'procedure_outcome', 'provider_outcome',
        'weight'
    ],
    weights_column='weight',
    ntrees=64,
    nbins=32,
    balance_classes=True,
    binomial_double_trees=True)
.
.
.
drf_dx.train(x=X_train, y=Y_train, 
          training_frame=train_u, validation_frame=val_u, 
          max_runtime_secs=max_train_time_hrs*60*60)



(note the ignored columns) and the prediction logic just looks like


preds = model.predict(X)



where X is some (h2o)dataframe with 
more (or less) columns than in X_train
 used to train the model (includes some columns for post-processing exploration (in a Jupyter notebook)). Eg. X_train columns may look like


<columns to ignore (as seen in the code)> <columns to use a features for training> <outcome label>



and X columns may look like


<columns to ignore (as seen in the code)> <EVEN MORE COLUMNS TO IGNORE> <columns to use a features for training>



My question is: Is this going to confuse the model when making predictions? Ie. is the model getting the columns to use as features by 
column name
 (in which case, I don't think the different dataframe width would be a problem) or is it going by 
column position
 (in which case adding more data columns to each sample would shift the positions and become a problem) or something else? What happens since these columns were not explicated in the 
ignored_columns
 arg in the model constructor?




** Slight aside: should the 
weights_column
 name be in the 
ignored_columns
 list or not? The example in the docs (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/weights_column.html#weights-column
) seems to use it as a predictor feature as well as seems to recommend it 




For scoring, all computed metrics will take the observation weights into account (for Gains/Lift, AUC, confusion matrices, logloss, etc.), so it’s important to also provide the weights column for validation or test sets if you want to up/down-weight certain observations (ideally consistently between training and testing).




but these weight values are not something that comes with the data used in actual predictions.",['h2o'],Unknown,,N/A
52953545,52953545,2018-10-23T16:11:19,2018-10-24 09:39:45Z,0,"Is it possible to download a JAR into the directory 
inst/java
 of a R package on the installation stage?


I want to submit a package to CRAN, but the jar is too big and they are not going to accept it. I thought one possible solution would be to automatically download the jar and place it inside 
inst/java


I think H2O do it in its 
build.gradle
.


Would it be possible? if so, do I need to use gradle?


Update


It seems in 
make-dist.sh
 they also download the jar.","['java', 'r', 'h2o', 'r-package']",Unknown,,N/A
52937340,52937340,2018-10-22T20:30:42,2018-10-24 08:45:45Z,63,"I am developing a Spring Boot Application with H2O 3.20.0.10 and need a summary / metadata of all columns in a frame. While invoking frameSummary() of H2oApi, I get an MalformedJsonException:


Caused by: com.google.gson.stream.MalformedJsonException: JSON forbids NaN and infinities: NaN at line 1 column 23937 path $.frames[0].columns[6].mean
    at com.google.gson.stream.JsonReader.nextDouble(JsonReader.java:912)
    at com.google.gson.Gson$1.read(Gson.java:319)
    at com.google.gson.Gson$1.read(Gson.java:313)
    at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:131)
    at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:222)
    at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.read(TypeAdapterRuntimeTypeWrapper.java:41)
    at com.google.gson.internal.bind.ArrayTypeAdapter.read(ArrayTypeAdapter.java:72)
    at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:131)
    at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:222)
    at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.read(TypeAdapterRuntimeTypeWrapper.java:41)
    at com.google.gson.internal.bind.ArrayTypeAdapter.read(ArrayTypeAdapter.java:72)
    at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:131)
    at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:222)
    at retrofit2.converter.gson.GsonResponseBodyConverter.convert(GsonResponseBodyConverter.java:37)
    at retrofit2.converter.gson.GsonResponseBodyConverter.convert(GsonResponseBodyConverter.java:25)
    at retrofit2.ServiceMethod.toResponse(ServiceMethod.java:116)
    at retrofit2.OkHttpCall.parseResponse(OkHttpCall.java:211)
    at retrofit2.OkHttpCall.execute(OkHttpCall.java:174)
    at water.bindings.H2oApi.frameSummary(H2oApi.java:3718)



Column at position 23937 is of type enum and has therefore no mean value.


Update MWE:


new H2oApi(""http://localhost:54321"").frameSummary(key);



the response from H2O is something like:


__meta  
        schema_version  3
        schema_name ""FramesV3""
        schema_type ""Frames""
    _exclude_fields """"
    row_offset  0
    row_count   -1
    column_offset   0
    full_column_count   -1
    column_count    -1
    job null
frames  
    0   
        __meta  
            schema_version  3
            schema_name ""FrameV3""
            schema_type ""Frame""
        _exclude_fields """"
        frame_id    
            __meta  
                schema_version  3
                schema_name ""FrameKeyV3""
                schema_type ""Key<Frame>""
            name    ""TrainR""
            type    ""Key<Frame>""
            URL ""/3/Frames/TrainR""
        byte_size   48018
        is_text false
        row_offset  0
        row_count   100
        column_offset   0
        column_count    10
        full_column_count   10
        total_column_count  10
        checksum    5296931134826174000
        rows    1233
        num_columns 10
        default_percentiles […]
        columns 
            0   {…}
            1   {…}
            2   {…}
            3   {…}
            4   {…}
            5   {…}
            6   
                __meta  
                    schema_version  3
                    schema_name ""ColV3""
                    schema_type ""Vec""
                label   ""weekday""
                missing_count   0
                zero_count  176
                positive_infinity_count 0
                negative_infinity_count 0
                mins    […]
                maxs    […]
                mean    ""NaN""
                sigma   ""NaN""
                type    ""enum""
                domain  […]
                domain_cardinality  7
                data    […]
                string_data null
                precision   -1
                histogram_bins  […]
                histogram_base  0
                histogram_stride    1
                percentiles […]
            7   {…}
            8   {…}
            9   {…}
compatible_models   null



Is there a workaround for that?","['java', 'h2o']",Unknown,,N/A
52937291,52937291,2018-10-22T20:27:16,2019-01-20 03:33:07Z,0,"Used

R version : 3.4.4 (2018-03-15)

jdk version ""1.8.0_181""

H2O cluster version: 3.20.0.10 
Installed all the dependent package as mentioned in H2O documentation   


cpu1$Average = as.numeric(as.character(cpu1$Average))
network1$Average = as.numeric(as.character(network1$Average))

cpu = cpu1$Average
network  = network1$Average

cpu = as.h2o(as.data.frame(cpu))
network = as.h2o(as.data.frame(network))



Input data:

         cpu

1 28.7341993

2  0.7342352

3  0.3874931

4  0.3959669

5  0.3862666

6  0.5541219  


network

1 444796.4000

2  29544.5167

3    193.9333

4    359.2667

5    881.7833

6 648985.3667  


data  = h2o.cbind(cpu,network)

# class(data) ""H2OFrame""

h2o.parseSetup(data) 



Getting this error please advice.


ERROR: Unexpected HTTP Status code: 400 Bad Request (url = 
http://localhost:54321/3/ParseSetup
)
java.lang.IllegalArgumentException
 [1] ""java.lang.IllegalArgumentException: Key not loaded: Key /tmp/Rtmpq3doob/file3ff4261bfad.csv_sid_82a4_1""
 [2] ""  water.api.ParseSetupHandler.guessSetup(ParseSetupHandler.java:31)""

 [3] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""

 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""

 [5] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""

 [6] ""    java.lang.reflect.Method.invoke(Method.java:498)""

 [7] ""    water.api.Handler.handle(Handler.java:63)""

 [8] ""    water.api.RequestServer.serve(RequestServer.java:451)""

 [9] ""    water.api.RequestServer.doGeneric(RequestServer.java:296)""

[10] ""    water.api.RequestServer.doPost(RequestServer.java:222)""

[11] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""

[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""

[13] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""

[14] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""

[15] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""

[16] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""

[17] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""

[18] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""

[19] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""

[20] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""

[21] ""    water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:197)""

[22] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""

[23] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""

[24] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""

[25] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""

[26] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""

[27] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)""

[28] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)"" 
[29] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)""

[30] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)""

[31] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""

[32] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""

[33] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""

[34] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""

[35] ""    java.lang.Thread.run(Thread.java:748)""

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:
Key not loaded: Key /tmp/Rtmpq3doob/file3ff4261bfad.csv_sid_82a4_1
Calls: as.h2o ... h2o.parseSetup -> .h2o.__remoteSend -> .h2o.doSafeREST
Execution halted","['r', 'dataframe', 'h2o']",Rob,https://stackoverflow.com/users/162698/rob,15.1k
52926610,52926610,2018-10-22T09:55:27,2018-11-20 22:13:11Z,0,"I want a specific older version of a package (h2o) to be installed when I load a conda env .yml file. However, the older versions for this package only seem to work if I install them using pip directly from a the url hosting the .whl file. For example if I want to install version 3.18.0.8 I need to do this:


pip install http://h2o-release.s3.amazonaws.com/h2o/rel-wolpert/8/Python/h2o-3.18.0.8-py2.py3-none-any.whl



Is there a way for me to include that url (i.e. 
http://h2o-release.s3.amazonaws.com/h2o/rel-wolpert/8/index.html
) in the .yml file so that if I create a virtual environment using the .yml file it downloads and installs the correct version of h2o?","['pip', 'virtualenv', 'conda', 'h2o']",Unknown,,N/A
52840437,52840437,2018-10-16T16:52:30,2022-06-18 00:06:24Z,0,"I am trying to create partial dependent plot using the following code


rf_pdp = rf_model .partial_plot(data = htest, cols = ['var1', 'var2', 'var3'], plot=True)
rf_pdp 



Is there a way I can save the output such as mean_resp into a data frame?","['python', 'h2o']",Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
52815447,52815447,2018-10-15T11:11:46,2018-10-15 18:55:58Z,0,"I am trying to upload csv into h20 server from a client running n R from the RStudio. This is how it looks like:


library(dplyr)
library(ggplot2)
library(h2o)

localH2O = h2o.init(ip = ""127.0.0.1"", port = 54323)
market_data_file = system.file(""extdata"", ""bank_customer_data.csv"", package = ""h2o"")

market_data  = h2o.importFile(localH2O, path = market_data_file, key = ""market_data"")

class(market_data)

summary(market_data)



The output on the console shows the following read out:




market_data_file = system.file(""extdata"", ""bank_customer_data.csv"", package = ""h2o"")


market_data  = h2o.importFile(localH2O, path = market_data_file, key = ""market_data"")
  Error in h2o.importFile(localH2O, path = market_data_file, key = ""market_data"") : 
    unused argument (key = ""market_data"")


class(market_data)
  Error: object 'market_data' not found


summary(market_data)
  Error in summary(market_data) : object 'market_data' not found




Is there anything am doing wrong?","['r', 'bigdata', 'analysis', 'h2o']",njeru,https://stackoverflow.com/users/10330809/njeru,79
52767224,52767224,2018-10-11T19:05:10,2020-05-23 19:24:25Z,0,"Has anyone had success getting the h2o python 
datatable
 package to install on Windows?  It requires clang/llvm (
https://github.com/h2oai/datatable/wiki/Build-instructions
), and the pre-built binaries for Windows are apparently insufficient when I tried. Thus it appears to need a full clang/llvm/llvmlite build from source to get working.  I'd love to see a detailed built process if someone has had luck.","['python', 'windows', 'h2o', 'py-datatable']",Pasha,https://stackoverflow.com/users/958624/pasha,"6,540"
52747370,52747370,2018-10-10T19:20:08,2018-10-12 21:22:38Z,28,"I wrote the author of a book on H2O and he suggested I post here.


Yes, I know I can use the CSV data before loading into H2O but I don't want to keep duplicate data around.  Yes I know I could use Spark/Sparkling Water, let's pretend I can't for the moment.  I attempted to contact H2O but all I get is salespeople wanting to engage me in a cocktail conversation.


I need to run custom algorithms of all kinds.  There's just no way that H2O can anticipate everything a Scientist might need to do and I don't think H2O is wise in attempting to build (what appears to be) a walled garden.


If I knew the internal format, I could write my own record iterator.


Any thoughts appreciated.


Thank you.","['iterator', 'h2o']",dreamer,https://stackoverflow.com/users/2442017/dreamer,69
52743699,52743699,2018-10-10T15:25:38,2018-10-10 17:54:00Z,0,"How to compute conditional permutation importance from 
h2o.gbm
?


I have a data set with many highly correlated variables(>0.9). And fed this data set to 
h2o.gbm
. As it turned out, RMSE increases (on CV) when I drop down correlated variables.


Now I'm trying to get variable importance and found just this function: 
h2o.varimp()
. Which is (I guess) differs from classic 
party::varimp(model, conditional = T)
.","['r', 'h2o']",Bear,https://stackoverflow.com/users/10043204/bear,682
52732359,52732359,2018-10-10T03:51:32,2018-10-16 16:49:31Z,0,"I am trying to create partial dependent plot using the following code


rf_pdp = rf_model .partial_plot(data = htest, cols = ['var1', 'var2', 'var3'], plot=True)
rf_pdp 



it runs without error and generate a table with mean_response, stddev_response, std_error_mean_response for each variables. BUT there is no plot. Is that because I run the code in Spark environment? 


I am running H2O cluster version: 3.20.0.7 using Sparkling Water under Qubole


%pyspark
# start h2o
from pysparkling import *
import h2o
hc = H2OContext.getOrCreate(spark)

# clean up the cluster just in case
h2o.remove_all()

# import data
iris = h2o.import_file(""http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris_wheader.csv"")

# convert response column to a factor
iris['class'] = iris['class'].asfactor()

# set the predictor names
predictors = iris.columns[:-1]

# split into train and validation sets
train, valid = iris.split_frame(ratios = [.8], seed = 1234)

# random forest
from h2o.estimators.random_forest import H2ORandomForestEstimator

rf_model = H2ORandomForestEstimator(
                score_each_iteration=True,
                score_tree_interval = 5,
                max_runtime_secs = 1800,
                stopping_metric = 'logloss', 
                stopping_tolerance=0.001,
                stopping_rounds= 3,
                sample_rate = 0.7, 
                col_sample_rate_per_tree = 0.7,                
                ntrees=1000,
                balance_classes=False,
                seed=456,
                nfolds=5
                )

rf_model.train(x=predictors, y ='class', training_frame=train)

# plot
rf_model.plot()","['h2o', 'sparkling-water']",Unknown,,N/A
52723180,52723180,2018-10-09T14:13:50,2018-10-10 21:12:54Z,0,"I am learning Machine Learning using H2O. I have a small Table with names, address and age, and I tried to create a histogram by age. 
I am using iPython with Python 3.


Age column is: 


24
23
22
23
23
22
25



Output with plot=Fase is:


<table>
<thead>
<tr><th style=""text-align: right;"">  breaks</th><th style=""text-align: right;"">  counts</th><th style=""text-align: right;"">  mids_true</th><th style=""text-align: right;"">   mids</th><th style=""text-align: right;"">   density</th></tr>
</thead>
<tbody>
<tr><td style=""text-align: right;"">   22.75</td><td style=""text-align: right;"">     nan</td><td style=""text-align: right;"">      nan  </td><td style=""text-align: right;"">nan    </td><td style=""text-align: right;"">nan       </td></tr>
<tr><td style=""text-align: right;"">   23.5 </td><td style=""text-align: right;"">       2</td><td style=""text-align: right;"">       11  </td><td style=""text-align: right;""> 23.125</td><td style=""text-align: right;"">  0.380952</td></tr>
<tr><td style=""text-align: right;"">   24.25</td><td style=""text-align: right;"">       3</td><td style=""text-align: right;"">       11.5</td><td style=""text-align: right;""> 23.875</td><td style=""text-align: right;"">  0.571429</td></tr>
<tr><td style=""text-align: right;"">   25   </td><td style=""text-align: right;"">       2</td><td style=""text-align: right;"">       12  </td><td style=""text-align: right;""> 24.625</td><td style=""text-align: right;"">  0.380952</td></tr>
</tbody>
</table>","['python', 'python-3.x', 'matplotlib', 'machine-learning', 'h2o']",ImportanceOfBeingErnest,https://stackoverflow.com/users/4124317/importanceofbeingernest,338k
52713094,52713094,2018-10-09T03:48:47,2018-10-11 01:09:16Z,489,"I have the following questions that still confused me after I read the h2o document. Can someone provide some explanation for me




For the stopping_tolerance = 0.001, let's use AUC for example, current AUC is 0.8. Does that mean the AUC need to increase 0.8 + 0.001 or need to increase 0.8*(1+0.1%)?


score_each_iteration, in H2O document
(
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/score_each_iteration.html
) it just say ""iteration"". But what exactly is the definition for each
""iteration"", is that each tree or each grid search or each K folder
cross validation or something else?


Can I define score_tree_interval and set score_each_iteration = True
at the same time or I can only use one of them to make the grid
search repeatable?


Is there any difference to put 'stopping_metric',
'stopping_tolerance', 'stopping_rounds' in
H2OGradientBoostingEstimator vs in search_criteria of H2OGridSearch?
I found put in H2OGradientBoostingEstimator will make the code run
much faster when I test it in Spark environment",['h2o'],Unknown,,N/A
52617898,52617898,2018-10-02T23:50:35,2018-10-05 17:45:33Z,362,"I am using the following code to run GBM in Sparkling Water. I have set up the seed and score_each_iteration, but every time, it still generates different results when I check the AUC even though I have set the seed and score_each_iteration=True.


from h2o.grid.grid_search import H2OGridSearch
from h2o.estimators.gbm import H2OGradientBoostingEstimator

# initialize the estimator
gbm_cov = H2OGradientBoostingEstimator(sample_rate = 0.7, col_sample_rate = 0.7, ntrees = 1000, balance_classes=True , score_each_iteration=True, nfolds=5, seed = 1234)

# set up hyper parameter search space
gbm_hyper_params = {'learn_rate': [0.01, 0.015, 0.025, 0.05, 0.1],
                     'max_depth': [3, 5, 7, 9, 12],
                     #'sample_rate': [i * 0.1 for i in range(6, 11)],
                     #'col_sample_rate': [i * 0.1 for i in range(6, 11)],
                     #'ntrees': [i * 100 for i in range(1, 11)]
                }

# define Search criteria
gbm_search_criteria = {'strategy': ""RandomDiscrete"", 
                        'max_models': 10, 
                        'max_runtime_secs': 1800,
                        'stopping_metric': eval_metric, 
                        'stopping_tolerance': 0.001, 
                        'stopping_rounds': 3,
                        'seed': 1
                       }

# build grid search 
gbm_grid = H2OGridSearch(model = gbm_cov,
                     hyper_params = gbm_hyper_params,
                     search_criteria = gbm_search_criteria # we can use ""Cartesian"" if search space is small
                    )

# train using the grid
gbm_grid.train(x = top_feature, y = y, training_frame =htrain)","['h2o', 'sparkling-water']",Joel,https://stackoverflow.com/users/10000141/joel,"1,574"
52606920,52606920,2018-10-02T10:52:53,2018-10-04 21:16:58Z,0,"I am working with 
h2o
 
glrm
 function. When I am trying to pass 
loss_by_col
 argument in order to specify different loss function for each column in my DataFrame (I have normal, poisson and binomial variables, so I am passing ""Quadratic"", ""Poisson"" and ""Logistic"" loss), the objective is not getting computed. The 
testmodel@model$objective
 returns 
NaN
. But at the same time summary shows that there was few iterations made and objective was NA for all of them. The quality of model is very bad, but the archetypes are somehow computed. So I am confused. How should pass different loss for every variable in my dataset? Here is a (i hope) reproducible example:


df <- data.frame(p1 = rpois(100, 5), n1 = rnorm(100), b1 = rbinom(100, 1, 0.5))
df$b1 <- factor(df$b1)
h2df <- as.h2o(df)

testmodel <- h2o.glrm(h2df,
         k=3,
         loss_by_col=c(""Poisson"", ""Quadratic"", ""Logistic""),
         transform=""STANDARDIZE"")
testmodel@model$objective
summary(testmodel)
plot(testmodel)","['r', 'h2o']",Unknown,,N/A
52599403,52599403,2018-10-01T21:56:05,2018-10-01 22:37:32Z,298,"I am using H2O to build GBM and DRF model now, I found both score_each_iteration the same as score_tree_interval are described to make the early stop repeatable. But is score_each_iteration the same as score_tree_interval = 1? If yes, why we need score_each_iteration, we can just use score_tree_interval = 1 instead",['h2o'],Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
52572541,52572541,2018-09-29T21:05:07,2018-10-09 03:46:55Z,299,"I am reading the following two examples




https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.ipynb


https://h2o-release.s3.amazonaws.com/h2o/rel-turing/10/docs-website/h2o-docs/grid-search.html




Both of them when set up grid search, it fix the ntree instead of feed a list of ntree for example 


[i * 100 for i in range(1, 11)]. 



Here is my question




I am wondering is that because early stop is set up against the
ntree? For example, we can set up ntree = 1000 and
score_tree_interval = 100, then it can evaluate the model
performance from 100, 200, ... till 1000. Do I understand correctly?


But if my grid search also include learn_rate and max_depth. Will
the early stop also evaluate against learn_rate and max_depth? I
mean within the same number of tree for example ntree = 500, when it
evaluate different learning rate [0.01, 0.015, 0.025, 0.05, 0.1],
will it stop somewhere in the list of learn rate?


In the
document of ""stopping_tolerance"" (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/stopping_tolerance.html
)
it describes ""the model will stop training after reaching three
scoring events in a row in which a model’s missclassication value
does not improve by 1e-3"". So what are the three scoring events? are
they 3 different number of tree or they could be the same number of
tree but different learning rate?","['pyspark', 'h2o']",Unknown,,N/A
52568040,52568040,2018-09-29T11:44:39,2018-10-05 16:38:57Z,881,"The above image is the H2O GBM classification model lift chart for training and validation data sets. I am confused it with the other lift charts I have seen. Normally the baseline will be 45 degrees and the lift curve used to be somewhat convex shape from the baseline curve. In the above figure if the green line shows the lift curve, why is it constant and coming down and touches the baseline? Also why the baseline is not 45 degree? Can anyone help me to interpret the model using the above graph? Is my model perform well?","['h2o', 'gbm', 'interpretation', 'lifting']",Kitooos,https://stackoverflow.com/users/10361077/kitooos,37
52559784,52559784,2018-09-28T16:57:09,2018-09-28 18:37:03Z,288,"Is it possible to predict probabilities in a binary classification task a in H2O flow? In particular I am finding difficulties in camputing the probability instead a crisp prediction because I can't see option in the UI of H2O when predicting.


If it's not possible doing it in H2O Flow, is there a way to do it in R (or Python) using the same model built in H2O flow?


Thanks in advance for any help.",['h2o'],Luca Pedretti,https://stackoverflow.com/users/10430544/luca-pedretti,41
52553363,52553363,2018-09-28T10:27:38,2018-09-28 22:14:38Z,0,"I tried tuning parameters for GBM H2O in R using,


https://github.com/h2oai/h2o-3/blob/3.10.0.7/h2o-docs/src/product/tutorials/gbm/gbmTuning.Rmd


I then tried applying the tuned hyper-parameters in Rapidminer for the same data set. In R I got accuracy 97%, whereas in RapidMiner with the same parameters I am getting only 91% accuracy. Both R and RapidMiner use the same version of H2O package. But why is this difference in the accuracy?


My RapidMiner process is given below:






<?xml version=""1.0"" encoding=""UTF-8""?><process version=""9.0.002"">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated=""true"" class=""process"" compatibility=""9.0.002"" expanded=""true"" name=""Process"">
    <parameter key=""logverbosity"" value=""init""/>
    <parameter key=""random_seed"" value=""2001""/>
    <parameter key=""send_mail"" value=""never""/>
    <parameter key=""notification_email"" value=""""/>
    <parameter key=""process_duration_for_mail"" value=""30""/>
    <parameter key=""encoding"" value=""SYSTEM""/>
    <process expanded=""true"">
      <operator activated=""true"" class=""retrieve"" compatibility=""9.0.002"" expanded=""true"" height=""68"" name=""Retrieve Mode_of_Labor_Data"" width=""90"" x=""45"" y=""34"">
        <parameter key=""repository_entry"" value=""../Data/Mode_of_Labor_Data""/>
      </operator>
      <operator activated=""true"" class=""set_role"" compatibility=""9.0.002"" expanded=""true"" height=""82"" name=""Set Role"" width=""90"" x=""313"" y=""34"">
        <parameter key=""attribute_name"" value=""Mode of Delivery""/>
        <parameter key=""target_role"" value=""label""/>
        <list key=""set_additional_roles""/>
      </operator>
      <operator activated=""true"" class=""split_data"" compatibility=""9.0.002"" expanded=""true"" height=""103"" name=""Split Data"" width=""90"" x=""447"" y=""238"">
        <enumeration key=""partitions"">
          <parameter key=""ratio"" value=""0.8""/>
          <parameter key=""ratio"" value=""0.2""/>
        </enumeration>
        <parameter key=""sampling_type"" value=""automatic""/>
        <parameter key=""use_local_random_seed"" value=""true""/>
        <parameter key=""local_random_seed"" value=""1234""/>
      </operator>
      <operator activated=""true"" class=""h2o:gradient_boosted_trees"" compatibility=""9.0.000"" expanded=""true"" height=""103"" name=""Gradient Boosted Trees"" width=""90"" x=""581"" y=""136"">
        <parameter key=""number_of_trees"" value=""10000""/>
        <parameter key=""reproducible"" value=""false""/>
        <parameter key=""maximum_number_of_threads"" value=""4""/>
        <parameter key=""use_local_random_seed"" value=""true""/>
        <parameter key=""local_random_seed"" value=""1992""/>
        <parameter key=""maximal_depth"" value=""15""/>
        <parameter key=""min_rows"" value=""16.0""/>
        <parameter key=""min_split_improvement"" value=""1.0E-4""/>
        <parameter key=""number_of_bins"" value=""1024""/>
        <parameter key=""learning_rate"" value=""0.05""/>
        <parameter key=""sample_rate"" value=""1.0""/>
        <parameter key=""distribution"" value=""bernoulli""/>
        <parameter key=""early_stopping"" value=""true""/>
        <parameter key=""stopping_rounds"" value=""5""/>
        <parameter key=""stopping_metric"" value=""AUC""/>
        <parameter key=""stopping_tolerance"" value=""1.0E-4""/>
        <parameter key=""max_runtime_seconds"" value=""0""/>
        <list key=""expert_parameters"">
          <parameter key=""nbins_cats"" value=""2048""/>
          <parameter key=""learn_rate_annealing"" value=""0.99""/>
          <parameter key=""col_sample_rate"" value=""0.76""/>
          <parameter key=""col_sample_rate_per_tree"" value=""0.91""/>
          <parameter key=""col_sample_rate_change_per_level"" value=""0.97""/>
        </list>
      </operator>
      <operator activated=""true"" class=""apply_model"" compatibility=""9.0.002"" expanded=""true"" height=""82"" name=""Apply Model"" width=""90"" x=""715"" y=""340"">
        <list key=""application_parameters""/>
        <parameter key=""create_view"" value=""false""/>
      </operator>
      <operator activated=""true"" class=""performance_classification"" compatibility=""9.0.002"" expanded=""true"" height=""82"" name=""Performance"" width=""90"" x=""782"" y=""34"">
        <parameter key=""main_criterion"" value=""accuracy""/>
        <parameter key=""accuracy"" value=""true""/>
        <parameter key=""classification_error"" value=""false""/>
        <parameter key=""kappa"" value=""false""/>
        <parameter key=""weighted_mean_recall"" value=""false""/>
        <parameter key=""weighted_mean_precision"" value=""false""/>
        <parameter key=""spearman_rho"" value=""false""/>
        <parameter key=""kendall_tau"" value=""false""/>
        <parameter key=""absolute_error"" value=""false""/>
        <parameter key=""relative_error"" value=""false""/>
        <parameter key=""relative_error_lenient"" value=""false""/>
        <parameter key=""relative_error_strict"" value=""false""/>
        <parameter key=""normalized_absolute_error"" value=""false""/>
        <parameter key=""root_mean_squared_error"" value=""false""/>
        <parameter key=""root_relative_squared_error"" value=""false""/>
        <parameter key=""squared_error"" value=""false""/>
        <parameter key=""correlation"" value=""false""/>
        <parameter key=""squared_correlation"" value=""false""/>
        <parameter key=""cross-entropy"" value=""false""/>
        <parameter key=""margin"" value=""false""/>
        <parameter key=""soft_margin_loss"" value=""false""/>
        <parameter key=""logistic_loss"" value=""false""/>
        <parameter key=""skip_undefined_labels"" value=""true""/>
        <parameter key=""use_example_weights"" value=""true""/>
        <list key=""class_weights""/>
      </operator>
      <connect from_op=""Retrieve Mode_of_Labor_Data"" from_port=""output"" to_op=""Set Role"" to_port=""example set input""/>
      <connect from_op=""Set Role"" from_port=""example set output"" to_op=""Split Data"" to_port=""example set""/>
      <connect from_op=""Split Data"" from_port=""partition 1"" to_op=""Gradient Boosted Trees"" to_port=""training set""/>
      <connect from_op=""Split Data"" from_port=""partition 2"" to_op=""Apply Model"" to_port=""unlabelled data""/>
      <connect from_op=""Gradient Boosted Trees"" from_port=""model"" to_op=""Apply Model"" to_port=""model""/>
      <connect from_op=""Apply Model"" from_port=""labelled data"" to_op=""Performance"" to_port=""labelled data""/>
      <connect from_op=""Performance"" from_port=""performance"" to_port=""result 1""/>
      <portSpacing port=""source_input 1"" spacing=""0""/>
      <portSpacing port=""sink_result 1"" spacing=""0""/>
      <portSpacing port=""sink_result 2"" spacing=""0""/>
    </process>
  </operator>
</process>","['r', 'h2o', 'rapidminer', 'gbm']",Unknown,,N/A
52549026,52549026,2018-09-28T05:44:14,2018-09-28 18:58:09Z,106,"I see H2O DAI picks up the optimized algorithm for the dataset automatically.  I hear that the contents of MLI (machine learning interpretation) from other platforms (like SAS Viya) is dependent on the algorithm it uses. For example, LOCO is not available for GBM, etc. (Of course, this is a purely hypothetical example.)


Is it the same with H2O DriverlessAI ?  Or does it always show the same MLI menus regardless of the algorithms it used?","['h2o', 'driverless-ai']",mbuechmann,https://stackoverflow.com/users/1360318/mbuechmann,"5,680"
52548947,52548947,2018-09-28T05:35:59,2018-09-28 17:27:07Z,151,"I am a newby in H2O DAI, and I think it's wonderful.  I've run several experiments with small sample CSV data, and most of the time I see GLM and GBM are used. 


Can we see the full list of all algorithms provided with H2O DAI ?


I see the algorithms provided with H2O open source 
here
, but is it the same with H2O DAI ?


One more question : Is there any way I can choose which algorithm to use manually ?","['h2o', 'driverless-ai']",Unknown,,N/A
52542640,52542640,2018-09-27T17:54:59,2019-04-19 12:29:06Z,0,"I was looking at this answer to visualize the gradient boosting tree model in H2O, it says the method on GBM can be applied to XGBoost as well:


Finding contribution by each feature into making particular prediction by h2o ensemble model
 


http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html
 


But when I try to use the method it mentioned on H2O XGBoost MOJO, it fails. 


I check the source code of hex.genmodel.tools.PrintMojo:
https://github.com/h2oai/h2o-3/blob/master/h2o-genmodel/src/main/java/hex/genmodel/tools/PrintMojo.java
 


it seems like it can only work on randomforest and GBM models, but not XGBoost model.  


Is there anyone who knows how to visualize trees in H2O XGBoost model? Thanks!","['machine-learning', 'h2o', 'xgboost']",Zhirui Wang,https://stackoverflow.com/users/7981257/zhirui-wang,184
52538201,52538201,2018-09-27T13:29:15,2018-10-04 21:25:41Z,137,"I'm currently running 
Spark 2.3.0
 with 
sparkling-water 2.3.1
. I found the documentation of the underlying H2O library by looking at the 
changelog
 that links to 
this
. So apparently it uses 
H2O 3.18
.


By looking at the DNN I noticed the lack of a 
batch_size
 parameter, but instead it offers a 
mini_batch_size
 parameter which is not actually documented. The only documentation regarding this parameter that I found is 
here
, which refers to 
H2O 2.4
, and I assumed that it still applies to the version I'm using (I don't know if this assumption is correct).




mini batch


The number of training data rows to be processed per iteration. Note that independent of this parameter, each row is used immediately to update the model with (online) stochastic gradient descent. The mini batch size controls the synchronization period between nodes in a distributed environment and the frequency at which scoring and model cancellation can happen. For example, if mini-batch is set to 10,000 on H2O running on 4 nodes, then each node will process 2,500 rows per iteration, sampling randomly from their local data. Then, model averaging between the nodes takes place, and scoring can happen (dependent on scoring interval and duty factor). Special values are 0 for one epoch per iteration and -1 for processing the maximum amount of data per iteration. If “replicate training data” is enabled, N epochs will be trained per iteration on N nodes, otherwise one epoch.




From this I interpret that the batch size is actually fixed to 1 as it always performs an Online Gradient Descent.


I also started digging into the source code of H2O in order to see what is its default value, and AFAIU the default parameters are contained in 
this class
.


From the 
line 1694
:


// stochastic gradient descent: mini-batch size = 1
// batch gradient descent: mini-batch size = # training rows
public int _mini_batch_size = 1;



So from the comment it seems that it doesn't actually perform Online Gradient Descent, but it seems to actually behave as the batch size. And a value of 1 is non-sense if we assume that the documentation of 
H2O 2.4
 still applies.


Furthermore from 
line 2173
 where it sets the user given parameters:


if (fromParms._mini_batch_size > 1) {
    Log.warn(""_mini_batch_size"", ""Only mini-batch size = 1 is supported right now."");
    toParms._mini_batch_size = 1;



Actually I just had a quick lock at the source code and I may be missing something, but I really cannot understand how the 
mini_batch_size
 parameter works and how it relates with the batch size. Can someone shed some light on this?","['apache-spark', 'neural-network', 'deep-learning', 'h2o', 'sparkling-water']",Community,https://stackoverflow.com/users/-1/community,1
52528611,52528611,2018-09-27T02:40:09,2021-01-04 15:57:00Z,786,Is it possible to extract native xgboost model pickle file from H2OXGBoostEstimator model in Python and read in by raw XGBoost Python API? Thanks!,"['python', 'machine-learning', 'h2o', 'xgboost']",Unknown,,N/A
52522640,52522640,2018-09-26T16:49:00,2021-01-13 16:31:39Z,555,"Let me first start by saying that this post is related to many on this site and that I am simply posting this for posterity and helping to add evidence as to why this particular error occurs.


So I have a script that I run that pulls in data to a data frame from sql, then it does some operations and writes the output back to the sql server. I first began experiencing the random ""Kernel Died"" error when I added the 
executemany()
 to my 
sqlalchemy
 script for writing to the server using pandas 
to_sql
 command. I could not figure this out for the life of me. 


Continue down the road a bit and I began writing large amounts of information back to the server without using 
executemany()
 and was receiving the error that I had duplicate primary keys when writing to the server. I have known historically that there is a known issue in converting from pandas to h2o frames that the h2o frame will sometimes create a duplicate record in the frame. When this converts back to pandas you will still have the duplicate and when you write that back to the server you will still have the duplicate. One of the values being written back to my server is a primary key and so cannot have a duplicate entry and causes an error. 


All of that being said. When I turn off 
executemany()
, I get the error that there is a duplicate in the primary key and the operation stops. When I turn on the 
executemany()
, I get the error that the ""Kernel has Died"".


So with that evidence I am proposing that the Kernel has Died error is some error in another package/entity that is not being translated back to the kernel and is causing a kernel has died error.


The question is: Where are we on these two issues? The posts I have seen are ~11 months old


Also if this post isn't allowed here where can I post information like this in order to help the developers understand the common problem?


EDIT: Follow on...


Just ran a test in jupyter with the same script. I received the appropriate error for duplicate primary key with executemany(). Just to be clear: in spyder I would have received ""Kernel Died"". 


However i also received this error. 


C:\Anaconda3\lib\site-packages\sqlalchemy\engine\base.py in 
_execute_context(self, dialect, constructor, statement, parameters, *args)
   1169                         parameters,
-> 1170                         context)
   1171             elif not parameters and context.no_parameters:

C:\Anaconda3\lib\site-packages\sqlalchemy\engine\default.py in 
do_executemany(self, cursor, statement, parameters, context)
    503     def do_executemany(self, cursor, statement, parameters, 
context=None):
--> 504         cursor.executemany(statement, parameters)
    505","['sql-server', 'pandas', 'sqlalchemy', 'spyder', 'h2o']",Unknown,,N/A
52521844,52521844,2018-09-26T15:55:19,2019-09-03 18:29:30Z,420,"H2O's 
documentation
 doesn't provide clear definitions for each column in the gains/lift table output. I'm not sure how the capture rate is being calculated, and there is a 
score
 column that is not mentioned in the documentation.


Here's what the output looks like.


The raw java file is 
here
 -- I tried finding the answer to my question in there but had difficulty making sense of it. Thanks.","['model', 'h2o']",coys,https://stackoverflow.com/users/5386571/coys,13
52498406,52498406,2018-09-25T12:37:00,2018-09-26 02:01:59Z,213,"I have 2 python virtual environments. One is a POC env and has minimal code.
Second one is with the application code base. 


The POC works fine. While I try to run with the application code and environment, the 
h2o.init()
 command fails.


Following is the traceback - 


Attempting to start a local H2O server...
Java Version: openjdk version ""1.8.0_181""; OpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13); OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)
Starting server from /home/roopak/.local/share/virtualenvs/xxxxxxxxxxxxx-b0M6BPI4/lib/python2.7/site-packages/h2o/backend/bin/h2o.jar
Ice root: /tmp/tmp9leaDh
JVM stdout: /tmp/tmp9leaDh/h2o_roopak_started_from_python.out
JVM stderr: /tmp/tmp9leaDh/h2o_roopak_started_from_python.err
Traceback (most recent call last):
File ""app.py"", line 98, in <module>
    application = setup(app, **os.environ)
File ""app.py"", line 76, in setup
    h2o.init()
File ""/home/roopak/.local/share/virtualenvs/xxxxxxxxxxxxx-b0M6BPI4/local/lib/python2.7/site-packages/h2o/h2o.py"", line 261, in init
    min_mem_size=mmin, ice_root=ice_root, port=port, extra_classpath=extra_classpath)
File ""/home/roopak/.local/share/virtualenvs/xxxxxxxxxxxxx-b0M6BPI4/local/lib/python2.7/site-packages/h2o/backend/server.py"", line 121, in start
    mmax=max_mem_size, mmin=min_mem_size)
File ""/home/roopak/.local/share/virtualenvs/xxxxxxxxxxxxx-b0M6BPI4/local/lib/python2.7/site-packages/h2o/backend/server.py"", line 309, in _launch_server
    raise H2OServerError(""Server process terminated with error code %d"" % proc.returncode)
h2o.exceptions.H2OServerError: Server process terminated with error code -6





Edit: Adding logs

There was 2 log files created - out and err. err was empty. 
out contained the following lines.


# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f51f4a75311, pid=436, tid=0x00007f51f4c66700
#
# JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build     1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13)
# Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [ld-linux-x86-64.so.2+0xc311]","['python', 'h2o']",Unknown,,N/A
52496836,52496836,2018-09-25T11:12:59,2018-10-22 11:41:03Z,0,"I am trying to import my 3000 observation & 77 features .csv file as H2O dataframe (while I am on a Spark session):


(1st way)


# Convert pandas dataframe to H2O dataframe
import h2o
h2o.init()
data_train = h2o.import_file('/u/users/vn505f6/data.csv')



However, I am getting the following error:


   Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/frame.py"", line 102, in __init__
    column_names, column_types, na_strings)
  File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/frame.py"", line 143, in _upload_python_object
    self._upload_parse(tmp_path, destination_frame, 1, separator, column_names, column_types, na_strings)
  File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/frame.py"", line 319, in _upload_parse
    self._parse(rawkey, destination_frame, header, sep, column_names, column_types, na_strings)
  File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/frame.py"", line 326, in _parse
    return self._parse_raw(setup)
  File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/frame.py"", line 355, in _parse_raw
    self._ex._cache.fill()
  File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/expr.py"", line 346, in fill
    res = h2o.api(""GET "" + endpoint % self._id, data=req_params)[""frames""][0]
  File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/h2o.py"", line 103, in api
    return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
  File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/backend/connection.py"", line 402, in request
    return self._process_response(resp, save_to)
  File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/backend/connection.py"", line 725, in _process_response
    raise H2OResponseError(data)
h2o.exceptions.H2OResponseError: Server error water.exceptions.H2OIllegalArgumentException:
  Error: Unknown parameter: full_column_count
  Request: GET /3/Frames/Key_Frame__upload_84df978b98892632a7ce19303c4440f3.hex
    params: {u'row_offset': '0', u'row_count': '10', u'full_column_count': '-1', u'column_count': '-1', u'column_offset': '0'}



Let me notice that when I am doing this on my local machine then I am getting no error. I am getting the error above when I am doing the same thing on a Spark/Hadoop cluster.


Alternatively , I tried to do the following in the Spark cluster:


(2nd way)


from pysparkling import H2OContext
from ssat_utils.spark import SparkUtilities
import h2o

h2o_context = H2OContext.getOrCreate(SparkUtilities.spark)
data_train = h2o.import_file('/u/users/vn505f6/data.csv')



and then I got the following error:


Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
 File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/h2o.py"", line 414, in import_file
   return H2OFrame()._import_parse(path, pattern, destination_frame, header, sep, col_names, col_types, na_strings)
 File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/frame.py"", line 311, in _import_parse
   rawkey = h2o.lazy_import(path, pattern)
 File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/h2o.py"", line 282, in lazy_import
   return _import(path, pattern)
 File ""/u/users/svcssae/pyenv/prod_python_libs/lib/python2.7/site-packages/h2o/h2o.py"", line 291, in _import
   if j[""fails""]: raise ValueError(""ImportFiles of "" + path + "" failed on "" + str(j[""fails""]))
ValueError: ImportFiles of /u/users/vn505f6/data.csv failed on [u'/u/users/vn505f6/data.csv']



The column names of the pandas dataframe are strings like the following: 
u_cnt_days_with_sale_14day
.


What is this error about and how can I fix this?


P.S.


These are the command line commands which create the Spark cluster/context:


SPARK_HOME=/u/users/******/spark-2.3.0 \
Q_CORE_LOC=/u/users/******/q-core \
ENV=local \
HIVE_HOME=/usr/hdp/current/hive-client \
SPARK2_HOME=/u/users/******/spark-2.3.0 \
HADOOP_CONF_DIR=/etc/hadoop/conf \
HIVE_CONF_DIR=/etc/hive/conf \
HDFS_PREFIX=hdfs:// \
PYTHONPATH=/u/users/******/q-core/python-lib:/u/users/******/three-queues/python-lib:/u/users/******/pyenv/prod_python_libs/lib/python2.7/site-packages/:$PYTHON_PATH \
YARN_HOME=/usr/hdp/current/hadoop-yarn-client \
SPARK_DIST_CLASSPATH=$(hadoop classpath):$(yarn classpath):/etc/hive/conf/hive-site.xml \
PYSPARK_PYTHON=/usr/bin/python2.7 \
QQQ_LOC=/u/users/******/three-queues \
/u/users/******/spark-2.3.0/bin/pyspark \
--master yarn \
--executor-memory 10g \
--num-executors 128 \
--executor-cores 10 \
--conf spark.port.maxRetries=80 \
--conf spark.dynamicAllocation.enabled=False \
--conf spark.default.parallelism=6000 \
--conf spark.sql.shuffle.partitions=6000 \
--principal ************************ \
--queue default \
--name interactive_H2O_MT \
--keytab /u/users/******/.******.keytab \
--driver-memory 10g","['python', 'pandas', 'h2o']",Unknown,,N/A
52496397,52496397,2018-09-25T10:50:32,2018-09-25 19:09:48Z,0,"I'm trying to run h2o.svd in spark cluster via sparkling water & h2o. The process went well and I could get the SVD object from h2o command but I could only see the result below.


#Exclude ID column in h2o data frame
my_svd <- h2o.svd(my_h2o_df[,2:138], nv = 10)

my_svd

Model Details:
==============

H2ODimReductionModel: svd
Model ID:  SVD_model_R_1537868492645_2
Singular values:

      sval1     sval2     sval3     sval4     sval5     sval6     sval7
1 80.821459 53.024006 40.153390 38.508806 36.984611 35.530345 33.960273
      sval8     sval9    sval10
1 33.189426 27.904307 27.607862

NULL



Basically, in base R I can use 
svd
 and it'll provide the result of 
$d
, 
$u
, 
$v
 simultaneously in the model object.


Since I'm new to h2o workflow, I assume that the the result from above h2o object return only 
$d
 only compare to base R. How could I get the matrices 
$u
 and 
$v
?


My Environment




Azure Databricks cloud cluster, Latest stable (Scala 2.11)


Spark 2.3.1


SparklyR 0.2.8


sparkling-water-assembly_2.11-2.3.13-all


R-h2o 3.20.0.7


rsparkling 0.2.8","['r', 'h2o', 'svd', 'sparklyr']",silencepill,https://stackoverflow.com/users/10412518/silencepill,23
52495778,52495778,2018-09-25T10:17:20,2020-08-03 04:20:08Z,0,"Hi i'm using 
h2o
 in R. 


Just a couple of weeks ago i update 
h2o
 package to the latest version


 h2o.getVersion()
[1] ""3.20.0.2""



But when i Initialize a new h2o session with 
h2o.init
 i recieve a warning message like that


In h2o.clusterInfo() : 
Your H2O cluster version is too old (3 months and 9 days)!
Please download and install the latest version from http://h2o.ai/download/



What should I do? When I installed h2o for the first time I don't recall having any file downloaded from h2o website.


Other info: OS Windows 10


 R.version
               _                           
platform       x86_64-w64-mingw32          
arch           x86_64                      
os             mingw32                     
system         x86_64, mingw32             
status                                     
major          3                           
minor          5.0                         
year           2018                        
month          04                          
day            23                          
svn rev        74626                       
language       R                           
version.string R version 3.5.0 (2018-04-23)
nickname       Joy in Playing","['r', 'h2o']",Unknown,,N/A
52488482,52488482,2018-09-24T23:07:58,2018-09-26 22:39:10Z,0,"I am trying to find the best max_depth value using the following code






   library(h2o)
h2o.init()
# import the titanic dataset
df <- h2o.importFile(path = ""http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv"")
dim(df)
head(df)
tail(df)
summary(df,exact_quantiles=TRUE)

# pick a response for the supervised problem
response <- ""survived""

# the response variable is an integer.
# we will turn it into a categorical/factor for binary classification
df[[response]] <- as.factor(df[[response]])

# use all other columns (except for the name) as predictors
predictors <- setdiff(names(df), c(response, ""name""))

# split the data for machine learning
splits <- h2o.splitFrame(data = df,
                         ratios = c(0.6,0.2),
                         destination_frames = c(""train.hex"", ""valid.hex"", ""test.hex""),
                         seed = 1234)
train <- splits[[1]]
valid <- splits[[2]]
test  <- splits[[3]]

# Establish a baseline performance using a default GBM model trained on the 60% training split
# We only provide the required parameters, everything else is default
gbm <- h2o.gbm(x = predictors, y = response, training_frame = train)

# Get the AUC on the validation set
h2o.auc(h2o.performance(gbm, newdata = valid))
# The AUC is over 94%, so this model is highly predictive!
[1] 0.9480135

# Determine the best max_depth value to use during a hyper-parameter search.
# Depth 10 is usually plenty of depth for most datasets, but you never know
hyper_params = list( max_depth = seq(1,29,2) )
# or hyper_params = list( max_depth = c(4,6,8,12,16,20) ), which is faster for larger datasets

grid <- h2o.grid(
  hyper_params = hyper_params,

  # full Cartesian hyper-parameter search
  search_criteria = list(strategy = ""Cartesian""),

  # which algorithm to run
  algorithm=""gbm"",

  # identifier for the grid, to later retrieve it
  grid_id=""depth_grid"",

  # standard model parameters
  x = predictors,
  y = response,
  training_frame = train,
  validation_frame = valid,

  # more trees is better if the learning rate is small enough
  # here, use ""more than enough"" trees - we have early stopping
  ntrees = 10000,

  # smaller learning rate is better, but because we have learning_rate_annealing,
  # we can afford to start with a bigger learning rate
  learn_rate = 0.05,

  # learning rate annealing: learning_rate shrinks by 1% after every tree
  # (use 1.00 to disable, but then lower the learning_rate)
  learn_rate_annealing = 0.99,

  # sample 80% of rows per tree
  sample_rate = 0.8,

  # sample 80% of columns per split
  col_sample_rate = 0.8,

  # fix a random number generator seed for reproducibility
  seed = 1234,

  # early stopping once the validation AUC doesn't improve by at least
  # 0.01% for 5 consecutive scoring events
  stopping_rounds = 5,
  stopping_tolerance = 1e-4,
  stopping_metric = ""AUC"",

  # score every 10 trees to make early stopping reproducible
  # (it depends on the scoring interval)
  score_tree_interval = 10)

# by default, display the grid search results sorted by increasing logloss
# (because this is a classification task)
grid

# sort the grid models by decreasing AUC
sortedGrid <- h2o.getGrid(""depth_grid"", sort_by=""auc"", decreasing = TRUE)
sortedGrid

# find the range of max_depth for the top 5 models
topDepths = sortedGrid@summary_table$max_depth[1:5]
minDepth = min(as.numeric(topDepths))
maxDepth = max(as.numeric(topDepths))

> sortedGrid








I am getting the following errors:




'NULL'  for the line  'h2o.auc(h2o.performance(gbm, newdata = valid))'


'ERRR on field: _stopping_metric: Stopping metric cannot be AUC for regression. '  when trying to execute the fnction 'h2o.grid'




How to resolve the issues? 


The issue is resolved using the above sample code. The issue was mainly because I was using data which was encoded. After using data without encoding and also importing the data using "" h2o.importFile"" command instead of ""read.csv"", the issues were resolved!","['r', 'classification', 'h2o', 'auc', 'gbm']",Unknown,,N/A
52457218,52457218,2018-09-22T13:41:19,2018-09-22 13:41:19Z,104,"I am having trouble in deploying h2o.ai in a cluster in EMR. 
I am trying to installing with flatfile but it seems to be probing some issues in communicating with each other
Require help in this.",['h2o'],Twamanish Das,https://stackoverflow.com/users/5686407/twamanish-das,1
52440401,52440401,2018-09-21T09:12:29,2018-09-22 07:41:25Z,0,"For each category I need to fit h2o model.


  Category            a             b
       <chr>         <dbl>         <dbl>
     1   aa           36.6          1.4
     2   aa           5.30          0   
     3   bb           4.62          1.2
     4   bb           3.71          1.5
     5   cc           3.41          12
    ... ...            ...         ...   



Is it possible to do it without splitting dataset on categories and running training on each category in the loop. Is it possible to do with dplyr?","['r', 'group-by', 'dplyr', 'h2o']",Dmytro Fedoriuk,https://stackoverflow.com/users/8966751/dmytro-fedoriuk,351
52433970,52433970,2018-09-20T22:05:00,2018-09-24 20:27:31Z,0,"I saw there are a few places that we can set up seed when we do grid search for tuning hyper parameters, for example, we can set up seed in the following 3 places 




when we initialize the estimator using H2OGradientBoostingEstimator, 


when we define the search_criteria, we can also put seed


when we start to use the defined grid to train, we can also put seed in the train function 




Are these 3 redundant, we only need set up in one of them or each of them play different role?


thanks!","['python', 'h2o']",Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
52431210,52431210,2018-09-20T18:31:16,2018-09-20 19:41:39Z,0,"I know that I can access the predictor 
names
 of an 
H2OModel
 via the 
@parameters
 slot, but can I access the predictor 
data types
?


I'm trying to generate an input schema for my 
h2OModel
, and right now I have to cross-reference the 
training_frame
 and get data types from there.  Obviously, this would be a problem if my 
training_frame
 was no longer in memory.


Here's my current approach:


getInputSchema <- function(model){
  require(jsonlite)
  require(h2o)
  training_frame <- h2o.getFrame(model@parameters$training_frame)
  toJSON(
      setNames( h2o.getTypes(training_frame), 
                names(training_frame)        
               )[model@parameters$x], 
      auto_unbox = T 
      )
}



and an example of how it could be used:


#--- Example dataset ----
library(h2o)
library(data.table)
options('h2o.use.data.table'=TRUE)
library(rpart.plot) # for 'ptitanic' dataset
h2o.init()
data(ptitanic, package='rpart.plot')
survival <- as.h2o(
              setDT( ptitanic)[, `:=`( age   = as.numeric(age),
                                       sibsp = as.integer(sibsp),
                                       parch = as.integer(parch)
                                     ) ] 
            )
#--- Example model -----
fit <- 
  h2o.gbm( x = c('pclass','sex','age','sibsp','parch'),
           y = 'survived',
           training_frame = survival
          )
#--- Example use ----
getInputSchema(fit)
# {""pclass"":""enum"",""sex"":""enum"",""age"":""real"",""sibsp"":""int"",""parch"":""int""}



I'm looking for a solution that I could apply to an existing model where the dataset referenced in 
training_frame
 is missing.","['r', 'types', 'h2o', 'r-s4']",Unknown,,N/A
52424067,52424067,2018-09-20T11:25:00,2018-09-20 16:19:57Z,0,"I am running the R package 
h2o
 version 3.20.0.2 on an azure cluster.


After fitting many h2o models, the h2o cluster seems to have become unresponsive with this error message:




Warning in .h2o.__checkConnectionHealth() :
  H2O cluster node 127.0.0.1:54321 is behaving slowly and should be inspected manually.




I have tried to reset the cluster with 
h2o.shutdown()
 but the problem persists and 
h2o.init()
 fails.
Without admin rights, how can I truly restart the h2o server and how would I avoid this problem in the future ?","['r', 'h2o']",sm925,https://stackoverflow.com/users/5269047/sm925,"2,678"
52414987,52414987,2018-09-19T22:26:26,2020-06-11 14:42:10Z,0,"To Whom It May Concern,


The code below is being run in a Docker container based on jupyter's data science notebook;
however, I've install Java 8 and h2o (version 3.20.0.7), as well as exposed the necessary ports. The docker container is being run on a system using Ubuntu 16.04 and has 32 threads and over 300G of RAM.

h2o is using all the threads and 26.67 Gb of memory. I'm attempted to classify text as either a 0 or a 1 using the code below.

However, despite setting max_runtime_secs to 900 or 15 minutes, the code hadn't finished executing and was still tying up most of the machine resources ~15 hours later. As a side note, it took df_train about 20 minutes to parse. Any thoughts on what's going wrong?


    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

    df = pd.read_csv('Data.csv')[['Text', 'Classification']]

    vectorizer = CountVectorizer(analyzer='word',token_pattern=r'\w{1,}',
                                 ngram_range=(1, 3), stop_words = 'english')

    x_train_vec = vectorizer.fit_transform(df['Text'])
    y_train = df['Classification']

    import h2o
    from h2o.automl import H2OAutoML
    h2o.init()

    df_train = h2o.H2OFrame(x_train_vec.A, header=-1, column_names=vectorizer.get_feature_names())
    df_labels = h2o.H2OFrame(y_train.reset_index()[['Classification']])
    df_train = df_train.concat(df_labels)

    x_train_cn = df_train.columns
    y_train_cn = 'Classification'
    x_train_cn.remove(y_train_cn)

    df_train[y_train_cn] = df_train[y_train_cn].asfactor()

    h2o_aml = H2OAutoML(max_runtime_secs = 900, exclude_algos = [""DeepLearning""])

    h2o_aml.train(x = x_train_cn , y = y_train_cn, training_frame = df_train)

    lb = h2o_aml.leaderboard

    y_predict = h2o_aml.leader.predict(df_train.drop('Classification'))

    print('accuracy: {}'.format(accuracy_score(y_pred=y_predict, y_true=y_train)))
    print('precision: {}'.format(precision_score(y_pred=y_predict, y_true=y_train)))
    print('recall: {}'.format(recall_score(y_pred=y_predict, y_true=y_train)))
    print('f1: {}\n'.format(f1_score(y_pred=y_predict, y_true=y_train)))","['h2o', 'text-classification', 'automl']",Isac Moura,https://stackoverflow.com/users/8266386/isac-moura,"6,778"
52410238,52410238,2018-09-19T16:16:41,2020-07-08 11:34:33Z,694,"I am running H2O in Python and build GBM model for binary target variable (1 vs 0). The model perform well and I can see the threshold in the output. But I want to save the threshold to a variable (we can call it cut_point). So when I score a new data set, I can use the threshold to define either 1 or 0. Has anyone has done this before?","['python', 'h2o']",Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
52395198,52395198,2018-09-18T22:03:26,2018-09-19 23:23:36Z,81,"Having apparent compatibility issues running H2O (via the 3.18.0.2 MapR 5.2 
driver
 (trying with the latest driver (3.20.0.7) as recommended in another SO 
post
 did not help the problem)) on MapR 6.0. 


While able to 
start
 an H2O cluster on MapR 6.0 (via something like 
hadoop jar h2odriver.jar -nodes 3 -mapperXmx 6g -output hdfsOutputDirName

) and seem to be able to access h2o Flow UI, having problems accessing the cluster via 
python API
 (
pip show h2o
 confirms matching package version to driver being used). 


Is the MapR 5.2 driver (currently the latest MapR driver version offered by H2O) incompatible with MapR 6.0 (would not be asking if not for the fact that seem to be able to use the H2O Flow UI on cluster instance started on MapR 6.0)? Any workaround other than standalone driver version (would like to still be able to leverage YARN on hadoop cluster)?


The code and error being seen when trying to connect to the running H2O using the python APIis shown below.


# connect to h2o service
h2o.init(ip=h2o_cnxn_ip)



where the h2o_cnxn_ip is the IP and port generated after starting the h2o cluster on the MapR 6.0 system. Produces error


Checking whether there is an H2O instance running at http://172.18.0.123:54321...
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-5-1728877a03a2> in <module>()
      1 # connect to h2o service
----> 2 h2o.init(ip=h2o_cnxn_ip)

/home/me/projects/myproject/lib/python2.7/site-packages/h2o/h2o.pyc in init(url, ip, port, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, **kwargs)
    250                                      auth=auth, proxy=proxy,cookies=cookies, verbose=True,
    251                                      _msgs=(""Checking whether there is an H2O instance running at {url}"",
--> 252                                             ""connected."", ""not found.""))
    253     except H2OConnectionError:
    254         # Backward compatibility: in init() port parameter really meant ""baseport"" when starting a local server...

/home/me/projects/myproject/lib/python2.7/site-packages/h2o/backend/connection.pyc in open(server, url, ip, port, https, auth, verify_ssl_certificates, proxy, cookies, verbose, _msgs)
    316             conn._stage = 1
    317             conn._timeout = 3.0
--> 318             conn._cluster = conn._test_connection(retries, messages=_msgs)
    319             # If a server is unable to respond within 1s, it should be considered a bug. However we disable this
    320             # setting for now, for no good reason other than to ignore all those bugs :(

/home/me/projects/myproject/lib/python2.7/site-packages/h2o/backend/connection.pyc in _test_connection(self, max_retries, messages)
    558                 raise H2OServerError(""Local server was unable to start"")
    559             try:
--> 560                 cld = self.request(""GET /3/Cloud"")
    561                 if cld.consensus and cld.cloud_healthy:
    562                     self._print("" "" + messages[1])

/home/me/projects/myproject/lib/python2.7/site-packages/h2o/backend/connection.pyc in request(self, endpoint, data, json, filename, save_to)
    400                                     auth=self._auth, verify=self._verify_ssl_cert, proxies=self._proxies)
    401             self._log_end_transaction(start_time, resp)
--> 402             return self._process_response(resp, save_to)
    403 
    404         except (requests.exceptions.ConnectionError, requests.exceptions.HTTPError) as e:

/home/me/projects/myproject/lib/python2.7/site-packages/h2o/backend/connection.pyc in _process_response(response, save_to)
    711         if content_type == ""application/json"":
    712             try:
--> 713                 data = response.json(object_pairs_hook=H2OResponse)
    714             except (JSONDecodeError, requests.exceptions.ContentDecodingError) as e:
    715                 raise H2OServerError(""Malformed JSON from server (%s):\n%s"" % (str(e), response.text))

/home/me/projects/myproject/lib/python2.7/site-packages/requests/models.pyc in json(self, **kwargs)
    882                 try:
    883                     return complexjson.loads(
--> 884                         self.content.decode(encoding), **kwargs
    885                     )
    886                 except UnicodeDecodeError:

/usr/lib64/python2.7/json/__init__.pyc in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
    349     if parse_constant is not None:
    350         kw['parse_constant'] = parse_constant
--> 351     return cls(encoding=encoding, **kw).decode(s)

/usr/lib64/python2.7/json/decoder.pyc in decode(self, s, _w)
    364 
    365         """"""
--> 366         obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    367         end = _w(s, end).end()
    368         if end != len(s):

/usr/lib64/python2.7/json/decoder.pyc in raw_decode(self, s, idx)
    380         """"""
    381         try:
--> 382             obj, end = self.scan_once(s, idx)
    383         except StopIteration:
    384             raise ValueError(""No JSON object could be decoded"")

/home/me/projects/myproject/lib/python2.7/site-packages/h2o/backend/connection.pyc in __new__(cls, keyvals)
    823         for k, v in keyvals:
    824             if k == ""__meta"" and isinstance(v, dict):
--> 825                 schema = v[""schema_name""]
    826                 break
    827             if k == ""__schema"" and is_type(v, str):

KeyError: u'schema_name'",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
52383740,52383740,2018-09-18T09:48:11,2019-06-20 20:10:23Z,0,"I'm using 
h2o
 in R to fit a gbm model to my data. I have a couple of categorical variables and i'm using the option 
categorical_encoding
 in the 
h2o.gbm
 function to encode these variables.


When dealing with predictions in test set, how I handle categorical variables? It seems that the function 
predict
doesn't have an option that automatically encode categorical variables.","['r', 'h2o']",Mike,https://stackoverflow.com/users/9061264/mike,"4,202"
52377348,52377348,2018-09-18T00:32:55,2018-09-18 00:32:55Z,63,"In my Build path it is mssql-jdbc-7.0.0.jre10.jar driver. When I try to import sql table in H2o , getting SQL Exception error below is the image:","['sql-server', 'jdbc', 'h2o']",Promila,https://stackoverflow.com/users/7966057/promila,43
52315526,52315526,2018-09-13T14:06:54,2018-09-13 14:33:23Z,322,"I applied a binary classification using H2O. I simply divided my set into 3 which are train, calibrate and test. After training and calibration I checked the results on the test set. Here the corresponding part:


final_grid = H2OGridSearch(model=H2OGradientBoostingEstimator(model_id = 'contract_gbm2', 
                                    stopping_rounds = 5, stopping_tolerance = 1e-4, seed = 23,
                                    stopping_metric = ""AUC"",balance_classes = True,
                                    max_runtime_secs=300, calibrate_model=True, calibration_frame=valid,

                                    nfolds = 5),
                       hyper_params=hyper_params_gbm,search_criteria=search_criteria)



What I have noticed is that the predicted class and the given probabilities are not always consistent. See below:




As seen the prediction is not decided based on the highest probability? What am I missing?","['python-3.x', 'classification', 'h2o', 'gbm']",mlee_jordan,https://stackoverflow.com/users/3198674/mlee-jordan,842
52305760,52305760,2018-09-13T02:17:04,2018-09-13 14:43:59Z,42,"I am using h2o-genmodel to parse Mojo model.


But I'm somewhat confused about some of the parameters in the generated shareTreeNode CLASS. I queried the API documentation(
http://docs.h2o.ai/h2o/latest-stable/h2o-genmodel/javadoc/index.html
) and source code, there is no text description of any parameters.


I really need the explain of all parameters, because I need to change it to my parameters defined in my project as another format.


Here is the parameters in SharedTreeNode, some parameters like 
colName
 I can understand by myself. But parameters like 
inclusiveNa
 I am really don't know.


public class SharedTreeNode {
    final SharedTreeNode parent;
    final int subgraphNumber;
    int nodeNumber;
    float weight;
    final int depth;
    int colId;
    String colName;
    boolean leftward;
    boolean naVsRest;
    float splitValue = 0.0F / 0.0;
    String[] domainValues;
    GenmodelBitSet bs;
    float predValue = 0.0F / 0.0;
    float squaredError = 0.0F / 0.0;
    SharedTreeNode leftChild;
    public SharedTreeNode rightChild;
    private boolean inclusiveNa;
    private BitSet inclusiveLevels;
}



Here is my code.",['h2o'],liyuhui,https://stackoverflow.com/users/7124383/liyuhui,"1,250"
52304696,52304696,2018-09-12T23:21:39,2018-09-13 20:15:38Z,0,"I would like to understand the meaning of the value (result) of 
h2o.predict()
 function from H2o R-package. I realized that in some cases when the 
predict
 column is 
1
, the 
p1
 column has a lower value than the column 
p0
. My interpretation of 
p0
 and 
p1
 columns refer to the probabilities for each event, so I expected when 
predict=1
 the probability of 
p1
 should be higher than the probability of the opposite event (
p0
), but it doesn't occur always as I can show in the following example: using 
prostate dataset
.


Here is executable example:


library(h2o)
h2o.init(max_mem_size = ""12g"", nthreads = -1)
prostate.hex <- h2o.importFile(""https://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv"")
prostate.hex$CAPSULE  <- as.factor(prostate.hex$CAPSULE)
prostate.hex$RACE     <- as.factor(prostate.hex$RACE)
prostate.hex$DCAPS    <- as.factor(prostate.hex$DCAPS)
prostate.hex$DPROS    <- as.factor(prostate.hex$DPROS)

prostate.hex.split = h2o.splitFrame(data = prostate.hex,
  ratios = c(0.70, 0.20, 0.10), seed = 1234)
train.hex     <- prostate.hex.split[[1]]
validate.hex  <- prostate.hex.split[[2]]
test.hex      <- prostate.hex.split[[3]]

fit <- h2o.glm(y = ""CAPSULE"", x = c(""AGE"", ""RACE"", ""PSA"", ""DCAPS""),
  training_frame = train.hex,
  validation_frame = validate.hex,
  family = ""binomial"", nfolds = 0, alpha = 0.5)

prostate.predict = h2o.predict(object = fit, newdata = test.hex)
result <- as.data.frame(prostate.predict)
subset(result, predict == 1 & p1 < 0.4)



I get the following output for the result of the 
subset
 function:


   predict        p0        p1
11       1 0.6355974 0.3644026
17       1 0.6153021 0.3846979
23       1 0.6289063 0.3710937
25       1 0.6007919 0.3992081
31       1 0.6239587 0.3760413



For all the above observations from 
test.hex
 dataset the prediction is 
1
, but 
p0 > p1
.


The total observation where 
predict=1
 but 
p1 < p0
 is:


>   nrow(subset(result, predict == 1 & p1 < p0))
[1] 14



On contrary there are no 
predict=0
 where 
p0 < p1


>   nrow(subset(result, predict == 0 & p0 < p1))
[1] 0



Here is the table for 
table
 information for 
predict
:


> table(result$predict)

 0  1 
18 23 



We are using as a decision variable 
CAPSULE
 with the following values:


> levels(as.data.frame(prostate.hex)$CAPSULE)
[1] ""0"" ""1""



Any suggestion?


Note
: The question with a similar topic: 
How to interpret results of h2o.predict
 does not address this specific issue.","['r', 'machine-learning', 'deep-learning', 'h2o', 'glm']",Sandipan Dey,https://stackoverflow.com/users/4706171/sandipan-dey,22.9k
52298858,52298858,2018-09-12T15:34:32,2018-09-12 17:42:40Z,0,"I'm trying to save 
all
 the models from an 
h2o.automl
 as part of the 
h2o
 package. Currently I am able to save a single model using 
h2o.saveModel(aml@leader, path = ""/home/data/user"")
.


How can I save 
all
 the models?


Here is my attempt on a sample dataset:


 library(h2o)
 h2o.init()
 prostate.hex <- h2o.importFile(path = paste(""https://raw.github.com"",
    ""h2oai/h2o-2/master/smalldata/logreg/prostate.csv"", sep = ""/""),
    destination_frame = ""prostate.hex"")



Get data from github or import via 
readr
:


 library(readr)
 prostate <- read_csv(""/home/data/user/prostate.csv"")

 prostate.hex<- as.h2o(prostate, ""prostate.hex"")

 aml <- h2o.automl(y = ""CAPSULE"", x = c(""AGE"",""RACE"",""PSA"",""DCAPS""),
    training_frame = prostate.hex,
    max_runtime_secs = 180,
    exclude_algos = c(""StackedEnsemble"")
    )



Now I'm trying to save the models within 
aml
:


mod_ids <- as_tibble(aml@leaderboard$model_id)



Now I can't figure out how to save the models:


 for(i in 1:nrow(mod_ids)) {
   print(mod_ids[i,])
   #h2o.saveModel(object = aml@leaderboard[[i]], ""/home/data/user/"")
 }



Here is what I've tried: 


tutorial automl


H2O AUTOML: How to save reuse and build on top of existing automl models","['r', 'h2o']",Ryan John,https://stackoverflow.com/users/6497137/ryan-john,"1,430"
52296301,52296301,2018-09-12T13:23:54,2018-09-12 13:56:10Z,348,"I'm using Ubuntu v16.04.5 as a guest VMWARE in my PC. Below is the installation and status log whereby when I use the Firefox browser to open 
http://localhost:12345
 , no page is displayed (Page not found). However, if I changed the URL to 
http://localhost:54321
 I can see the H2o flow SW displayed in the browser. 


I have started the installation using the command 


$sudo dpkg -i dai_1.3.0_amd64.deb 



and 


$sudo DAI_USER=ubuntu DAI_GROUP=root dpkg -i dai_1.3.0_amd64.deb 



in a separate Ubuntu v16.04.5 fresh VMWARE (guest). After that, I run the following command to start H2o driverless:


# Start Driverless AI.
sudo systemctl start dai



H2O AI driverless UBUNTU DEB installation and status log


I have also try enabling port 12345 in /etc/dai/config.toml file below and restart but the same problem still occur.


/etc/dai/config.toml


How can I get this H2o driverless working?","['ubuntu', 'h2o', 'driverless-ai']",Unknown,,N/A
52283577,52283577,2018-09-11T20:13:09,2018-09-11 20:56:05Z,270,"I am hyperparameter tuning a random forest and I would like to tune the parameter regarding the maximum features of each tree. By sklearn's 
documentation
 it is:




The number of features to consider when looking for the best split: If
  int, then consider max_features features at each split.




If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.


If “auto”, then max_features=sqrt(n_features).


If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).


If “log2”, then max_features=log2(n_features).


If None, then max_features=n_features.






I tried looking through the h2o 
documentation
 to no avail.


Does this parameter or any of the different ways you can adjust that parameter (e.g. log of features) exist in h2o?","['python', 'random-forest', 'h2o', 'hyperparameters']",Roy Z,https://stackoverflow.com/users/8234332/roy-z,67
52281173,52281173,2018-09-11T17:23:11,2018-09-13 22:54:08Z,0,"I have a dataset with zipcode column. They have some significance in output and I want to use it as a feature. I am using random forest model. 


I need a suggestions on best way to use zipcode column as a feature. (For example should I get lat/long for that zipcode rather than directly feeding zipcodes etc.)


Thanks in advance !!","['machine-learning', 'random-forest', 'h2o', 'feature-selection', 'zipcode']",Unknown,,N/A
52252770,52252770,2008-10-20T13:18:09,2022-09-08 01:07:47Z,0,"This question's answers are a 
community effort
. Edit existing answers to improve this post. It is not currently accepting new answers or interactions.
                                
                            














What are Null Pointer Exceptions (
java.lang.NullPointerException
) and what causes them?


What methods/tools can be used to determine the cause so that you stop the exception from causing the program to terminate prematurely?","['java', 'nullpointerexception']",Unknown,,N/A
52230318,52230318,2018-09-07T22:29:55,2018-09-11 00:59:22Z,0,"I'm trying to create a function to linearly spline a variable in an h2o dataset, but can't get h2o to evaluate the function properly.


Here's my initial attempt on intermediate spline:


df <- data.frame( AGE = sample(1:100, 1e6, replace = TRUE))
df_A.hex <- as.h2o( df, 'df_A.hex' )

simple_spline <- function( x, L, U ) min( max(x-L,0), U-L)
spline_vector <- Vectorize( simple_spline, vectorize.args = 'x', USE.NAMES = FALSE )

df_A.hex[, 'AGE_12_24'] <- spline_vector( df_A.hex[, 'AGE'], 12, 24) 



And here is the result:


  AGE AGE_12_24
1   9        12
2   7        12
3  33        12
4  84        12
5  86        12
6  25        12



I tried using 
pmin
 and 
pmax
, on the assumption that maybe it wasn't vectorizing the columns, but I get the following error:


> simple_spline <- function( x, L, U ) pmin( pmax(x-L,0), U-L)
> df_A.hex[, 'AGE_12_24'] <- simple_spline( df_A.hex[, 'AGE'], 12, 24) 
Error in each[change] : invalid subscript type 'environment'



I'm guessing it's because the 
pmin
 and 
pmax
 aren't implemented in h2o?


I also tried using apply, but also hit an error:


> simple_spline <- function( x, L, U ) min( max(x-L,0), U-L)
> df_A.hex[, 'AGE_12_24'] <- apply( df_A.hex[, 'AGE'], 1, simple_spline, 12, 24) 
> [1] ""Lookup failed to find min""
Error in .process.stmnt(stmnt, formalz, envs) : 
  Don't know what to do with statement: min



I could write a function that iteratively overwrites the spline column like so:


df_A.hex[, 'AGE_12_24'] <- df_A.hex[, 'AGE'] - 12
df_A.hex[, 'AGE_12_24'] <- h2o.ifelse( df_A.hex[, 'AGE_12_24'] < 0, 0, df_A.hex[, 'AGE_12_24'] )
df_A.hex[, 'AGE_12_24'] <- h2o.ifelse( df_A.hex[, 'AGE_12_24'] > 12, 12, df_A.hex[, 'AGE_12_24'] )



This gets me my expected result:


  AGE AGE_12_24
1   9         0
2   7         0
3  33        12
4  84        12
5  86        12
6  25        12



But it's a fairly ugly way of getting there. I'd like to know what I'm doing wrong and how to have a function pass on the values to the h2o frame.","['r', 'h2o']",Unknown,,N/A
52227446,52227446,2018-09-07T17:52:29,2021-02-10 23:09:01Z,0,"I am using H2O via R. I am trying to build random forest, XGBoost, GBM models to solve multiclass problem.


The model performance insights that H2O provides are great but as one of the success criterias I have my own custom function that scores the model accuracy when model is used to score a set of users say, validation set. Generally speaking, this function rewards the prediction and punishes the mis-prediction. Given that the target classes in this problem are ordinal categories, the punishment score depends on the number of places by which prediction was missed. The inverse of this custom function can also be thought of as a loss function.


I am wondering if and how I can plug such custom loss function into H2O while training models.","['r', 'machine-learning', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
52210521,52210521,2018-09-06T18:54:20,2018-09-06 21:12:37Z,0,"I have a general question on a specific topic. 


I am using the vectors generated by Word2Vec to feed as features into my Distributed Random Forest model for classifying some records. I have millions of records and am receiving new records on a daily basis. Because of the new records coming in I want the new records to be encoded with the same vector model as the previous records. Meaning that the word ""AT"" will be the same vector now and in the future. 
I know that Word2Vec uses a random seed to generate the vectors for the words in the corpus but I want to turn this off. I need to set the seed such that if I train a model on a section of the data today and then again on the same data in the future, I want it to generate the same model with the exact same vectors for each word.
The problem with generating new models and then encoding is that it takes a great deal of time to encode these records and then on top of that my DRF model for classification isn't any good anymore because the vector for the words have changed. So I have to retrain a new DRF.
Normally this would not be an issue since I could just train one model each and then use that forever;However I know that a good practice is to update your packages on the regular. This is a problem for h2o since once you update there is no backward comparability with model generated on previous version.


Are there any sources that I could read on how to set the seed on the Word2Vec model for h2o in python? I am using Python version 3 and h2o version 3.18","['python', 'word2vec', 'h2o']",error_unknown,https://stackoverflow.com/users/10256173/error-unknown,30
52207399,52207399,2018-09-06T15:21:09,2018-09-10 15:38:08Z,0,"The 
h2o documentation
 states for the 
weights_column
 option that




This option specifies the column in a training frame to be used when determining weights. Weights are per-row observation weights and do not increase the size of the data frame. This is typically the number of times a row is repeated, but non-integer values are also supported. During training, rows with higher weights matter more, due to the larger loss function pre-factor.




I am particularly interested in the effect of the weights column on the regression trees of 
DRF
 (random forest).
I find the description ""This is typically the number of times a row is repeated"" confusing. While they say that the frame's size is actually not increased, it insinuates that rows with higher/lower weight get over/under-sampled (when the per-tree training data is selected according to 
sample_rate
). However, looking at the h2o source code on github, this does not seem to be the case.
The relevant parts of the code where the weights are used are in 
DHistogram.java
 and read


double wy = weight * y;
double wyy = wy * y;  // This is the correct implementation.
int b = bin(col_data);
_vals[3*b + 0] += weight;
_vals[3*b + 1] += wy;
_vals[3*b + 2] += wyy;



This indicates that the weights are only used for calculating the number of 
weighted
 rows (
_vals[3*b + 0]
) and for the 
weighted
 sum of squared errors (via 
_vals[3*b + 1]
 and 
_vals[3*b + 2]
, see 
DTree.java
).


Furthermore, I did some tests with different weights in 
R
. I trained different 
DRF
 models, each with homogeneous weights across all observations but with different weight magnitude across the models. My suspicion that the weights are only used for the weighted row count and weighted squared errors seems to be confirmed.


library(h2o)

h2o.init()

#different weights for each model
iris$weight0=0.5
iris$weight1=1
iris$weight2=2
irisH=as.h2o(iris)
predNames=setdiff(colnames(irisH),c(""Sepal.Length"",""weight2"",""weight1"",""weight0""))
exludeLinesRegex=""(.*DRF_model_R_.*)|(.*AUTOGENERATED.*)|(.*UUID.*)|(.*weight.*)""
pojoList=list()

#train 3 models, each with different weights magnitude
for (i in 0:2) {
    weightColName=paste0(""weight"",i)
    tmpRf=h2o.randomForest(y=""Sepal.Length"",
                           x=predNames,
                           training_frame = irisH,
                           seed = 1234,
                           ntrees = 10,
                           #min_rows has to be adjusted-it refers to weighted rows
                           min_rows= 20*irisH[1,weightColName],
                           max_depth = 3,
                           mtries = 4,
                           weights_column = weightColName)
    tmpPojo=capture.output(h2o.download_pojo(tmpRf))
    pojoList[[length(pojoList)+1]]=tmpPojo[!grepl(exludeLinesRegex,tmpPojo)]
}

h2o.shutdown(FALSE)

# all forests are the same
length(unique(pojoList))
# 1



As one can see above, all 3 forests are the same despite having different weight magnitudes. The only adjustment that had to be done is 
min_rows
 because it refers to a weighted row number.
If the rows really would get over/undersampled, I would expect to see (small) differences between the models.


My questions are therefore:




Are the weights used anywhere else than for calculating the number of weighted rows and sum of squared errors?


Are regression 
DRF
 models generally invariant under homogeneous scaling of the weights, i.e., if I multiply the weights column by a scalar 
a>0
 and adjust 
min_rows
 accordingly, do the models stay the same? (As shown in the R code example above.)


If yes, does this also hold for forests with classification trees and 
GBM
 models?




Thank you for your help!","['random-forest', 'h2o']",cryo111,https://stackoverflow.com/users/983028/cryo111,"4,474"
52205507,52205507,2018-09-06T13:44:31,2018-09-06 13:44:31Z,0,"I'm learning h2o with R and Python (using a virtual machine with Linux). In order to do that, i tried the following code, but the data import failed.


loan_csv <- ""https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv""
data <- h2o.importFile(loan_csv, destination_frame = ""loanD"")



And the error is
""Error in h2o.importFolder(path, pattern = """", destination_frame = destination_frame,  : 
  all files failed to import""


Someone can give me a hint?","['r', 'h2o', 'data-import']",Red Noise,https://stackoverflow.com/users/7187100/red-noise,131
52181839,52181839,2018-09-05T09:34:35,2018-09-05 09:34:35Z,43,"I am doing some DRF train using h2o-bindings and the code is copy from examples in h2o.


    GBMParametersV3 gbmParams = new GBMParametersV3();
    gbmParams.trainingFrame = H2oApi.stringToFrameKey(""train"");
    gbmParams.validationFrame = H2oApi.stringToFrameKey(""test"");

    ColSpecifierV3 responseColumn = new ColSpecifierV3();
    responseColumn.columnName = ATT_LABLE_IRIS;
    gbmParams.responseColumn = responseColumn;

    GBMV3 gbmBody = h2o.train_gbm(gbmParams);

    JobV3 job = h2o.waitForJobCompletion(gbmBody.job.key);

    ModelKeyV3 modelKey = (ModelKeyV3)job.dest;
    ModelsV3 models = h2o.model(modelKey);
    LogService.getRoot().log(Level.INFO,""models: "" + models);

    GBMModelV3 model = (GBMModelV3)models.models[0];



But the GBMModelV3 doesn't have POJO and MOJO in source code.
As we can see it shows this.havePojo = false;


public class GBMModelV3 extends SharedTreeModelV3<GBMParametersV3, GBMModelOutputV3> {
public GBMModelV3() {
    this.checksum = 0L;
    this.algo = """";
    this.algoFullName = """";
    this.responseColumnName = """";
    this.timestamp = 0L;
    this.havePojo = false;
    this.haveMojo = false;
}

public String toString() {
    return (new Gson()).toJson(this);
}



}


So how can I get a MOJO model using h2o-bindings api? Or how can I get the tree information in the GBModelV3? Because the GBMModelV3 I got has so many parameters other than the final tree data.


The reason I want to get the MOJO model is that someone told me I can use h2o-genmodel to compute the ""mojo_model.zip"" and get the tree. But h2o-bindings, how to do it?","['java', 'h2o']",liyuhui,https://stackoverflow.com/users/7124383/liyuhui,"1,250"
52176359,52176359,2018-09-05T02:49:23,2018-09-05 02:49:23Z,39,"I want to use H2O in my java application, but when I search in maven repository, there is several modules to be chosen.


h2o-core
h2o-genmodel
h2o-bindings
h2o-algos
h2o-automl
h2o-persist hdfs
...



What's the relationship of them?
Furthermore, I searched some examples, I found that I can train a data and get a model by using h2o-bindings or h2o-genmodel or h2o-algos.
Even that there are Classes that have same name in these modules, like DRFModel.


I'm really confused about how to use them.","['java', 'h2o']",liyuhui,https://stackoverflow.com/users/7124383/liyuhui,"1,250"
52176020,52176020,2018-09-05T01:52:39,2018-09-05 01:52:39Z,93,"I saw the examples of how to init h2o and use it in python.


import h2o;
h2o.init();
h2o.train_gbm(**);



But how can I init h2o and use it in java?
Is there any example code or document?","['java', 'h2o']",liyuhui,https://stackoverflow.com/users/7124383/liyuhui,"1,250"
52146179,52146179,2018-09-03T08:47:11,2018-09-05 09:51:15Z,190,"Why do mojo or pojo need to be downloaded first and read after it? Why can't I just use it as an intermediate variable in Java?
I searched the examples and I found that all the examples are like this:


h2o.down_load(modelName);

URL mojoURL = MojoUtil.class.getResource(modelName);
 MojoReaderBackend reader = MojoReaderBackendFactory.createReaderBackend(mojoURL, MojoReaderBackendFactory.CachingStrategy.MEMORY);
 MojoModel model = ModelMojoReader.readFrom(reader);



Can I use some method like this:


String modelName = h2o.train_drf(param).getModelName();
MojoModel model = h2o.getMojoModel(modelName);



Furthermore, in document 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html
, there are only mentioned R and python of how to download mojo model: saving an H2O binary model with h2o.saveModel (R), h2o.save_model (Python).


How can I do it in java?","['java', 'h2o']",Unknown,,N/A
52124300,52124300,2018-09-01T02:10:55,2018-09-10 01:10:29Z,379,"I'm using AutoML in my java code, and I use AUC as my sort_metric. I got below training summary after 20 mins training:


09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: 09:51:05.869 Info  ModelTraining    Built: 1 models for search: DeepLearning hyperparameter search 1
09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: 09:51:05.869 Info  ModelTraining    New leader: DeepLearning_grid_0_AutoML_20180901_092939_model_0, AUC: 0.9170303558590623
09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: 09:51:05.869 Info  ModelTraining    DeepLearning hyperparameter search 1 complete
09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: 09:51:05.869 Info  ModelTraining    AutoML: out of time; skipping DL hyperparameter search
09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: 09:51:05.870 Info  ModelTraining    AutoML: out of time; skipping DL hyperparameter search
09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: 09:51:05.870 Info  ModelTraining    StackedEnsemble builds skipped due to the exclude_algos option.
09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: 09:51:05.870 Info  Workflow         AutoML: build done; built 2 models
09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: Leaderboard for project my.proj (models sorted in order of AUC, best first):
09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: #                                            model_id       auc   logloss  mean_per_class_error      rmse       mse
09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: 0  DeepLearning_grid_0_AutoML_20180901_092939_model_0  0.917030  0.273277              0.147665  0.189239  0.035812
09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: 1  DeepLearning_0_AutoML_20180901_092939               0.937039  0.150729              0.214383  0.193596  0.037479
09-01 09:51:05.872 127.0.0.1:54321       18273  FJ-1-9    INFO: 2  DeepLearning_0_AutoML_20180901_092936               0.958391  0.127028              0.181791  0.179389  0.032180



It seems that ""DeepLearning_0_AutoML_20180901_092936"" is the best one because it has highest AUC value, but actually ""DeepLearning_grid_0_AutoML_20180901_092939_model_0"" is the first one, and 
 AtuoML.leader() also return model ""DeepLearning_grid_0_AutoML_20180901_092939_model_0"". So which one is best? 


Here are the codes to run the AutoML:


    AutoMLBuildSpec autoMLBuildSpec = new AutoMLBuildSpec();

    autoMLBuildSpec.input_spec.training_frame = frame._key;
    autoMLBuildSpec.input_spec.response_column = ""class"";
    autoMLBuildSpec.input_spec.sort_metric = ""AUC"";

    autoMLBuildSpec.build_control.balance_classes = true;
    autoMLBuildSpec.build_control.class_sampling_factors = new float[2];
    autoMLBuildSpec.build_control.class_sampling_factors[0] = 1.0f;
    autoMLBuildSpec.build_control.class_sampling_factors[1] = 1.0f;
    autoMLBuildSpec.build_control.nfolds = nfolds;
    autoMLBuildSpec.build_control.keep_cross_validation_models = true;
    autoMLBuildSpec.build_control.keep_cross_validation_predictions = true;
    autoMLBuildSpec.build_control.project_name = ""my.proj"";
    HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria randomDiscreteValueSearchCriteria = new HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria();
    randomDiscreteValueSearchCriteria.set_max_runtime_secs(Double.parseDouble(autoModelRuntimeSeconds));
    randomDiscreteValueSearchCriteria.set_stopping_metric(ScoreKeeper.StoppingMetric.AUTO);
    randomDiscreteValueSearchCriteria.set_stopping_tolerance(0.0);
    autoMLBuildSpec.build_control.stopping_criteria = randomDiscreteValueSearchCriteria;

    AutoMLBuildSpec.AutoMLBuildModels autoMLBuildModels = new AutoMLBuildSpec.AutoMLBuildModels();
    autoMLBuildModels.exclude_algos = new AutoML.algo[4];
    autoMLBuildModels.exclude_algos[0] = AutoML.algo.DRF;
    autoMLBuildModels.exclude_algos[1] = AutoML.algo.GBM;
    autoMLBuildModels.exclude_algos[2] = AutoML.algo.GLM;
    autoMLBuildModels.exclude_algos[3] = AutoML.algo.StackedEnsemble;

    autoMLBuildSpec.build_models = autoMLBuildModels;

    logger.info(""begin training ..."");
    AutoML aml = AutoML.makeAutoML(Key.make(), new Date(), autoMLBuildSpec);
    AutoML.startAutoML(aml);
    AutoML.startAutoML(autoMLBuildSpec).get();
    logger.info(""training finished."");

    for (Model model: aml.leaderboard().getModels()) {
            logger.info(""========================================================================================================"");
            logger.info(""model key: {}"", model._key);
            logger.info(""_scoring_history"");
            logger.info(model._output._scoring_history.toString(10, true));
            logger.info(""model auc: {}"", Utils.doubleToString(model.auc(), 6));
            logger.info(""========================================================================================================"");
    }
    logger.info(""leader model scoring history:"");
    logger.info(aml.leader()._output._scoring_history.toString(10, true));","['metrics', 'h2o', 'automl']",Unknown,,N/A
52120546,52120546,2018-08-31T17:51:41,2018-08-31 18:39:26Z,0,"I'm trying to import a file using h2o in Python. 


h2o.init() is successful, but when I do the following: 


df = h2o.import_file(path = ""Combined Database - Final.csv"")


I get a number of errors that I can't find any help on. Here is the last one that shows up: 




H2OConnectionError: Unexpected HTTP error:
  HTTPConnectionPool(host='127.0.0.1', port=54321): Max retries exceeded
  with url: /3/Jobs/
  $03017f00000132d4ffffffff$_a6edaa906ba7a556a417c13149c940db (Caused by
  NewConnectionError(': Failed to establish a new connection: [WinError
  10048] Only one usage of each socket address (protocol/netw ork
  address/port) is normally permitted',))




Above it, there are “OSError”, “NewConnectionError”, “MaxRetryError”. 


This is my first time using h2o, and I can't even import my data. Any help would be much appreciated!","['python', 'import', 'h2o']",pixiesweet44,https://stackoverflow.com/users/7101962/pixiesweet44,33
52120263,52120263,2018-08-31T17:28:48,2018-09-01 04:45:07Z,0,"I ran h2o.grid with the set of different hiperparameters. Below you can see code connected with it:


       library(caret)
        library(ROCR)
        library(h2o)
        h2o.init()

    #creditcard <- read.csv(""creditcard.csv"") #https://www.kaggle.com/mlg-ulb/creditcardfraud
        as_h2o <- function(df) {
          for (colname in colnames(df)) {
            if (class(df[[colname]]) == ""character"") {
              df[[colname]] <- as.factor(df[[colname]])
            }
          }
          df <- as.h2o(df)
          df
        }

        creditcard[,-31]<-scale(creditcard[,-31])

        index <- createDataPartition(creditcard$Class, p = 0.3, list = FALSE)
        train.set <- creditcard[-index, -1]
        test.set <- creditcard[index, -1]

    index.valid <- createDataPartition(creditcard$Class, p = 0.2, list = FALSE)
    train.set <- creditcard[-index.valid, -1]
    valid.set <- creditcard[index.valid, -1]

Y = ""Class""
        X = colnames(test.set[,-30])     

     hyper_params <- list(
       activation = c(""Rectifier"", ""Maxout"", ""Tanh"", ""RectifierWithDropout"", ""MaxoutWithDropout"", ""TanhWithDropout""), 
       hidden = list(c(17,16,15), c(19,15,11), c(16,14,12),c(20,15,10),c(25,17,10),c(15,10,5)),
       epochs = c(50, 100, 200),
       l1 = c(0, 0.001,0.00001, 0.0001), 
       l2 = c(0,0.001, 0.00001, 0.0001),
       rate = c(0, 0.1, 0.005, 0.001),
       rate_annealing = c(1e-8, 1e-7, 1e-6),
       rho = c(0.9, 0.95, 0.99, 0.999),
       epsilon = c(1e-10, 1e-8, 1e-6, 1e-4),
       momentum_start = c(0, 0.5),
       momentum_stable = c(0.99, 0.5, 0),
       input_dropout_ratio = c(0, 0.1, 0.2),
       max_w2 = c(10, 100, 1000, 3.4028235e+38)
     )

     search_criteria <- list(strategy = ""RandomDiscrete"", 
                             max_models = 100,
                             max_runtime_secs = 900,
                             stopping_tolerance = 0.001,
                             stopping_rounds = 15)

     dl_grid <- h2o.grid(algorithm = ""deeplearning"", 
                         x = X,
                         y = Y,
                         grid_id = ""dl_grid"",
                         training_frame = as_h2o(train.set),
                         validation_frame = as_h2o(valid.set),
                         nfolds = 25,                           
                         fold_assignment = ""Stratified"",
                         hyper_params = hyper_params,
                         search_criteria = search_criteria
     )



but in the results I received some unexpected sizes of neural networks that weren't mentioned in the hyper_params, for example: [10, 10, 10, 10], [50, 50, 50]. 


The whole result:


> dl_grid
H2O Grid Details
================

Grid ID: dl_grid 
Used hyper parameters: 
  -  activation 
  -  epochs 
  -  epsilon 
  -  hidden 
  -  input_dropout_ratio 
  -  l1 
  -  l2 
  -  max_w2 
  -  momentum_stable 
  -  momentum_start 
  -  rate 
  -  rate_annealing 
  -  rho 
Number of models: 13 
Number of failed models: 1 

Hyper-Parameter Search Summary: ordered by increasing logloss
             activation             epochs epsilon           hidden
1             Rectifier 24.666234282086002  1.0E-6     [19, 15, 11]
2             Rectifier  27.58637697029444  1.0E-6 [10, 10, 10, 10]
3             Rectifier  20.26209344328687  1.0E-6     [15, 16, 17]
4             Rectifier  18.57634281485049  1.0E-6     [17, 16, 15]
5             Rectifier 50.032621172309156  1.0E-6     [17, 16, 15]
6             Rectifier 50.032621172309156  1.0E-6     [17, 16, 15]
7                Maxout   8.38177768101728  1.0E-4     [20, 15, 10]
8     MaxoutWithDropout 1.6076279182111595  1.0E-8     [17, 16, 15]
9  RectifierWithDropout 0.5012088413637236 1.0E-10     [15, 15, 15]
10 RectifierWithDropout 0.5012088413637236 1.0E-10     [15, 15, 15]
11    MaxoutWithDropout 28.578195951798776  1.0E-4     [12, 13, 12]
12    MaxoutWithDropout 10.073383841883308  1.0E-4     [15, 16, 17]
13 RectifierWithDropout 0.5012088413637236 1.0E-10     [50, 50, 50]



Can anyone explain why it happened?","['r', 'deep-learning', 'h2o']",Unknown,,N/A
52110839,52110839,2018-08-31T07:48:58,2018-08-31 07:48:58Z,71,"H2O have so many submodules, what's the relationship of them?


I created a flow by using h2o-bindings, but I can't get a MOJO model from the result.


I searched the answer and found that I have to add dependency of h2o-genmodel.
But I'm still cannot get the mojo model from the result created by h2o-bindings.


So what's the relationship of h2o-bindings and h2o-genmodel?
What should I do if I want to train my data, get the mojo model and use it to build a tree in java?


This is my code using h2o-bindings.


@Override
ModelMetricsListSchemaV3 train(H2oApi h2o) throws IOException {

    DRFParametersV3 drfParams = new DRFParametersV3();
    drfParams.trainingFrame = H2oApi.stringToFrameKey(""train"");
    drfParams.validationFrame = H2oApi.stringToFrameKey(""test"");
    drfParams.ntrees=3;

    ColSpecifierV3 responseColumn = new ColSpecifierV3();
    responseColumn.columnName = ATT_LABEL_GOLF;
    drfParams.responseColumn = responseColumn;

    DRFV3 drfBody = h2o.train_drf(drfParams);

    JobV3 job = h2o.waitForJobCompletion(drfBody.job.key);

    ModelKeyV3 modelKey = (ModelKeyV3)job.dest;
    ModelsV3 models = h2o.model(modelKey);
    DRFModelV3 model = (DRFModelV3)models.models[0];

    //I want to get mojo model but the output isn't right.
    ModelExportV3 modelExportV3 = h2o.exportMojo(modelKey);

    ModelMetricsListSchemaV3 predictParams = new ModelMetricsListSchemaV3();
    predictParams.model = modelKey;
    predictParams.frame = drfParams.trainingFrame;
    predictParams.predictionsFrame = H2oApi.stringToFrameKey(""predictions"");
    System.out.println(predictParams);

    return predictParams;
}",['h2o'],liyuhui,https://stackoverflow.com/users/7124383/liyuhui,"1,250"
52103781,52103781,2018-08-30T19:01:24,2018-08-30 21:02:10Z,371,"I'm using H2O 3.20.0.5 to train some models.


I want build model by using AutoML in my java code. Now I can import and parse csv file. But when I try to call 
AutoML.startAutoML().get()
, I always got a j
ava.lang.ArrayIndexOutOfBoundsException in ModelBuilder.java
.


It is seemed that there is no any ALGOBASES defined in 
ModelBuilder
. But I'm using AutoML  which is supposed to create algorithm automatically. So I'm totally confused. Here are my codes:


private static void trainByAutoML(Frame frame, String projectName) throws FileNotFoundException {

    AutoMLBuildSpec autoMLBuildSpec = new AutoMLBuildSpec();

    autoMLBuildSpec.input_spec.training_frame = frame._key;
    autoMLBuildSpec.input_spec.response_column = ""level"";
    autoMLBuildSpec.input_spec.sort_metric = ""AUTO"";

    autoMLBuildSpec.build_control.balance_classes = true;
    autoMLBuildSpec.build_control.class_sampling_factors = new float[2];
    autoMLBuildSpec.build_control.class_sampling_factors[0] = 1.0f;
    autoMLBuildSpec.build_control.class_sampling_factors[1] = 1.0f;
    autoMLBuildSpec.build_control.nfolds = nfolds;
    autoMLBuildSpec.build_control.keep_cross_validation_models = false;
    autoMLBuildSpec.build_control.keep_cross_validation_predictions = true;
    autoMLBuildSpec.build_control.project_name = projectName;
    HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria randomDiscreteValueSearchCriteria = new HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria();
    randomDiscreteValueSearchCriteria.set_max_runtime_secs(Double.parseDouble(autoModelRuntimeSeconds));
    randomDiscreteValueSearchCriteria.set_stopping_metric(ScoreKeeper.StoppingMetric.AUTO);
    randomDiscreteValueSearchCriteria.set_stopping_tolerance(0.0);
    autoMLBuildSpec.build_control.stopping_criteria = randomDiscreteValueSearchCriteria;

    AutoMLBuildSpec.AutoMLBuildModels autoMLBuildModels = new AutoMLBuildSpec.AutoMLBuildModels();
    autoMLBuildModels.exclude_algos = new AutoML.algo[4];
    autoMLBuildModels.exclude_algos[0] = AutoML.algo.DRF;
    autoMLBuildModels.exclude_algos[1] = AutoML.algo.GBM;
    autoMLBuildModels.exclude_algos[2] = AutoML.algo.GLM;
    autoMLBuildModels.exclude_algos[3] = AutoML.algo.StackedEnsemble;

    autoMLBuildSpec.build_models = autoMLBuildModels;

    AutoML aml = AutoML.makeAutoML(Key.make(), new Date(), autoMLBuildSpec);
    AutoML.startAutoML(aml);
    AutoML.startAutoML(autoMLBuildSpec).get();
    aml.leader().getMojo().writeTo(new FileOutputStream(new File(""/home/aaa/model_data/1.zip"")));
}



I always got that exception when the code run into 
AutoML.startAutoML(autoMLBuildSpec).get();
. Here are the stack trace of this exception:


08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR: java.lang.ArrayIndexOutOfBoundsException: -1
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at hex.ModelBuilder.algoName(ModelBuilder.java:106)
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at ai.h2o.automl.AutoML.trainModel(AutoML.java:519)
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at ai.h2o.automl.AutoML.defaultDeepLearning(AutoML.java:808)
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at ai.h2o.automl.AutoML.learn(AutoML.java:1058)
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at ai.h2o.automl.AutoML.run(AutoML.java:369)
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at ai.h2o.automl.H2OJob$1.compute2(H2OJob.java:32)
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at water.H2O$H2OCountedCompleter.compute(H2O.java:1267)
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
08-31 02:38:53.583 127.0.0.1:54321       9246   FJ-1-15   ERRR:     at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)","['java', 'h2o', 'automl']",lealceldeiro,https://stackoverflow.com/users/5640649/lealceldeiro,14.9k
52087252,52087252,2018-08-29T23:14:01,2018-08-30 19:08:16Z,0,"I'm using sparklyr and H2O in R to implement develop some ML models. Getting an error on the initial data read. I pull data in using 
spark_read_csv
, set up partitions using 
sdf_partition
 then define an H2O data frame using 
as_h2o_frame


df <- spark_read_csv(sc,
                     ""frame_name"",                                       
                     ""aPathToData.csv"")

partitions <- df %>% sdf_partition(training = 0.6, 
                               test_validate = 0.4,
                               seed=12)

train_set    <- as_h2o_frame(sc, 
                             partitions$training,
                             name=""train_set"")



This returns the error:




Error: C stack usage  38903392 is too close to the limit




I've successfully run this exact code on a much smaller dataset: 145 mb vs my current csv which is 2.3 gb. Still, I have 32 gb of memory and it doesn't seem to be the size of the dataset, I threw away most of the rows and got it down to 32mb, still gives the error. Must be something unique to the dataset other than size.


UPDATE: the error is due to the number of columns in the dataset. When I run 
as_h2o_frame
 with a number of columns in the spark data frame over 1689, I get the error. 1689 or fewer columns, no error.","['r', 'h2o', 'sparklyr']",Unknown,,N/A
52084632,52084632,2018-08-29T19:16:39,2018-08-29 19:16:39Z,0,"Hello I am trying to get some twitter data into a h2o random forest model in R. I keep getting a cannot determine file type error. I am in R version 1.1.453 using Rstudio.


code:


library(dplyr)
library(tidyr)
library(ace)
library(h2o)
library(data.table)

hdfs.kinit()
h2oStart()

tweet.dt <- as.data.table(fread(file = ""/tech/home/js87549e/ExtractedTweets.csv"", col.names = c('Party','Handle','Tweet')))
tweet.dt$outcome <- 0
tweet.dt$outcome[tweet.dt$Party == ""Democrat""] <- 1
tweet.dt$outcome <- as.factor(tweet.dt$outcome)
tweet.dt$Tweet <- as.character(tweet.dt$Tweet)

inp.dt <- as.data.table(tweet.dt %>%
  mutate(string = strsplit(Tweet, split = "" "")) %>%
  unnest(string))

inp.dt$string <- as.factor(inp.dt$string)

dat.dt <- as.data.table(inp.dt[,c(4,5)])

inp.hex <- as.h2o(dat.dt)



Data: 
https://www.kaggle.com/kapastor/democratvsrepublicantweets/downloads/democratvsrepublicantweets.zip/4


ERROR MESSAGE:


Cannot determine file type. for /tech/appl/user/js87549e/tmp/RtmpdvNc9d/file2cef348f3bde4.csv_sid_9333_9","['r', 'hdfs', 'h2o']",Joseph Sansevero,https://stackoverflow.com/users/7549940/joseph-sansevero,1
52073624,52073624,2018-08-29T09:08:31,2018-08-30 21:34:38Z,567,"I am using random forest in h2o.
But I don't understand the meaning of the parameters in the returned result.
This is my original data.



I would have liked to see results like this:
(I set number of trees = 3 and response column = ""Play"".)


tree1:
Wind = false: yes {no=0, yes=6}
Wind = true
|   Temperature > 77.500: no {no=2, yes=0}
|   Temperature ≤ 77.500: yes {no=1, yes=5}

tree2:
Humidity > 92.500: no {no=3, yes=0}
Humidity ≤ 92.500: yes {no=2, yes=9}

tree3:
Wind = false: yes {no=0, yes=6}
Wind = true
|   Temperature > 77.500: no {no=2, yes=0}
|   Temperature ≤ 77.500: yes {no=1, yes=5}



But I got a model contains many parameters but results.
This is my code and the results I got:


    DRFParametersV3 drfParams = new DRFParametersV3();
    drfParams.trainingFrame = H2oApi.stringToFrameKey(""train"");
    drfParams.validationFrame = H2oApi.stringToFrameKey(""test"");
    drfParams.ntrees=3;
    System.out.println(""drfParams: "" + drfParams);

    ColSpecifierV3 responseColumn = new ColSpecifierV3();
    responseColumn.columnName = ATT_LABEL_GOLF;
    drfParams.responseColumn = responseColumn;
    System.out.println(""About to train DRF. . ."");

    DRFV3 drfBody = h2o.train_drf(drfParams);
    System.out.println(""drfParams: "" + drfBody);

    JobV3 job = h2o.waitForJobCompletion(drfBody.job.key);
    System.out.println(""DRF build done."");

    ModelKeyV3 modelKey = (ModelKeyV3)job.dest;
    ModelsV3 models = h2o.model(modelKey);
    System.out.println(""models: "" + models);
    System.out.println(""models'size: "" + models.models.length);

    DRFModelV3 model = (DRFModelV3)models.models[0];
    System.out.println(""new DRF model: "" + model);



And the result ""DRFModelV3"" is so confused. Where is the ""forest"" build by h2o?","['random-forest', 'h2o']",liyuhui,https://stackoverflow.com/users/7124383/liyuhui,"1,250"
52068666,52068666,2018-08-29T02:30:55,2018-12-03 18:08:53Z,541,"I am trying to train a decision tree model by using h2o. I am aware that no specific library for decision trees exist in h2o. 


This is the code when I use GBM algorithm in h2o, but I can use Decision Tree like this. Because there is no Decision Tree code in h2o.


GBMParametersV3 gbmParams = new GBMParametersV3();
gbmParams.trainingFrame = H2oApi.stringToFrameKey(""train"");
gbmParams.validationFrame = H2oApi.stringToFrameKey(""test"");

ColSpecifierV3 responseColumn = new ColSpecifierV3();
responseColumn.columnName = ATT_LABLE_IRIS;
gbmParams.responseColumn = responseColumn;

GBMV3 gbmBody = h2o.train_gbm(gbmParams);
...



So, how can I use Decision Tree algorithm in h2o?","['decision-tree', 'h2o']",liyuhui,https://stackoverflow.com/users/7124383/liyuhui,"1,250"
52053444,52053444,2018-08-28T08:32:10,2018-09-05 08:46:53Z,0,"I'm trying to set up my R environment to run h2o algorithms on a YARN cluster.
(have no access to the internet due to security reasons - running on R Server)


Here are my current environment settings:




spark version: 2.2.0.2.6.3.0-235 (2.2) 


master: YARN client 


rsparkling version: 0.2.5 


sparkling water: 2.2.16 


h2o version: 3.18.0.10


sparklyr version: 0.7.0




I checked the h2o_version table for all the version mappings, but still get this error when I run the code:


options(rsparkling.sparklingwater.version = ""2.2.16"")
options(rsparkling.sparklingwater.location = ""path to my sparkling water.jar"") 

Sys.setenv(SPARK_HOME = ""path to my spark"") 
Sys.setenv(SPARK_VERSION = ""2.2.0"") 
Sys.setenv(HADOOP_CONF_DIR = ""..."") 
Sys.setenv(MASTER = ""yarn-client"") 

library(sparklyr) 
library(h2o) 
library(rsparkling) 

sc = spark_connect(master = Sys.getenv(""SPARK_MASTER""), spark_home = Sys.getenv(""SPARK_HOME""), version = Sys.getenv(""SPARK_VERSION"")) 
h2o_context(sc) 

R Server ERROR output: 
Error: java.lang.ClassNotFoundExecption: water.fvec.Frame 
       at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
       at java.lang.ClassLoader.loadClass(ClassLoader.java:424) 
...



Things I've tried:




Follow the instructions 
here


Reinstalling the h2o package and multiple retries


Trying different versions of h2o and sparkling water (3.18.0.5 and 2.2.11 respectively)




I am sure it would not be a version error since I've been matching them according to h2o_release_table() as shown. Please help or guide me to a solution.","['r', 'scala', 'apache-spark', 'h2o', 'sparklyr']",Unknown,,N/A
52036739,52036739,2018-08-27T09:51:44,2018-08-27 16:19:43Z,71,"I have built a machine learning model in R for Preventing Application Fraud in Loans using 
ensemble of 5 submodels. I am looking to deploy it but I am clueless how to use h2o for this. can anyone explain briefly how to use it?",['h2o'],Sachin,https://stackoverflow.com/users/10279223/sachin,1
52009091,52009091,2018-08-24T17:15:22,2018-08-24 21:17:06Z,0,"I have a dataframe of 50 rows (subjects) and 572288 columns (variables)


When parsing the 
data.frame
 into an h2o object I lose variables and end up with
51 rows and 419431 variables.


It does not change if I reduce the number of rows or increase them.


library(""data.table"")
library(""h2o"")
options(""h2o.use.data.table""=T)
h2o.init()
trainset=as.data.frame(matrix(ncol=572288,nrow=50,1))
fwrite(trainset, ""train.csv"", sep="","")
train=h2o.importFile(""train.csv"", sep="","")
dim(trainset)
dim(train)



My output is:


> h2o.init()
 Connection successful!

R is connected to the H2O cluster:
H2O cluster uptime:         1 hours 2 minutes
H2O cluster timezone:       Europe/Berlin
H2O data parsing timezone:  UTC
H2O cluster version:        3.18.0.11
H2O cluster version age:    3 months
H2O cluster name:           H2O_started_from_R_chiocchetti_lub856
H2O cluster total nodes:    1
H2O cluster total memory:   9.84 GB
H2O cluster total cores:    24
H2O cluster allowed cores:  20
H2O cluster healthy:        TRUE
H2O Connection ip:          localhost
H2O Connection port:        54321
H2O Connection proxy:       NA
H2O Internal Security:      FALSE
H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4
R Version:                  R version 3.4.3 (2017-11-30)

> trainset=as.data.frame(matrix(ncol=572288,nrow=50,1))
> fwrite(trainset, ""train.csv"", sep="","")
>
> train=h2o.importFile(""train.csv"", sep="","")
|======================================================================|100%
> dim(train)
[1]     51 538177
> dim(trainset)
[1]     50 572288



It seems to me that I am running in some kind of memory issue when reading back the lines from the file. However, I have no idea how to overcome this problem.


The final aim is to do a randomForest.","['r', 'h2o']",Peter Mortensen,https://stackoverflow.com/users/63550/peter-mortensen,31.6k
52005264,52005264,2018-08-24T13:10:46,2018-08-24 18:03:40Z,0,"I'm using the package H2O in R and i'm trying to improve my score with gbm model. I tried a grid search using a training and a valid sets. 
But when it finished, the log loss curves between the two sets is very different. Indeed there's overfitting on my train set so the accuracy is higher than my valid set 


Here on H2O, my gbm's parameters :
ntrees = 100, 
max_depth = 3, 
learn_rate = 0.01, 
nfolds = 5,
seed = 1234



Could you give me some way to resolve my problems ?","['r', 'h2o']",Antoine F,https://stackoverflow.com/users/10025919/antoine-f,191
51998925,51998925,2018-08-24T06:49:25,2018-08-27 16:14:28Z,0,"has this issue been resolved by now? I encounter the same error message.


Usecase: I am doing binary classification using h2o's 
deeplearning()
 function. Below, I provide randomly generated data the same size as my actual usecase. System specs:


# R version 3.3.2 (2016-10-31)
# Platform: x86_64-w64-mingw32/x64 (64-bit)
# Running under: Windows >= 8 x64 (build 9200)
# h2o version h2o_3.20.0.2



I am currently learning how to use h2o, so I have played with that function quite a bit. Everything runs smoothly until I specify parameters for cross validation. 


The problem occurs when specifying the 
nfolds
 parameter for cross-validation. Interestingly, I can specify low values for nfolds and everything goes fine. For my use case, even nfolds > 3 produced an error message (see below). I provide an example below, here I was able to specify nfolds < 7 (not really consistent... sometimes just up to nfolds = 3). Above those values, the REST API give the above mentioned error: 
object not found for argument: key
.


# R version 3.3.2 (2016-10-31)
# Platform: x86_64-w64-mingw32/x64 (64-bit)
# Running under: Windows >= 8 x64 (build 9200)
# h2o version h2o_3.20.0.2


#does not matter whether run on debian or windows, does not matter how many threads are used
#error occurs with options for cross validation, otherwise works fine
#no error occurs with specifying a low nfold number(in my actual use case, maximum of 3 folds possible without running into that error message)

require(h2o)
h2o.init(nthreads = -1)

x = matrix(rnorm(900*96, mean=10, sd=2), nrow=900, ncol=96)
y = sample(size=900, x=c(0,1), replace=T)

sampleData = cbind(x, y)
sampleData = data.frame(sampleData)
sampleData[,97] = as.factor(sampleData[,97])

m = h2o.deeplearning(x = 1:96, y = 97,
                     training_frame = as.h2o(sampleData), reproducible = T,
                     activation = ""Tanh"", hidden = c(64,16), epochs = 10000, verbose=T,
                     nfolds = 4, balance_classes = TRUE, #Cross-validation
                     standardize = TRUE, variable_importances = TRUE, seed=123,
                     stopping_rounds=2, stopping_metric=""misclassification"", stopping_tolerance=0.01, #early stopping
)

performance = h2o.performance(model = m)
print(performance)

######### gives error message
# ERROR: Unexpected HTTP Status code: 404 Not Found (url = http://localhost:xxxxx/3/Models/DeepLearning_model_R_1535036938222_489)
# 
# water.exceptions.H2OKeyNotFoundArgumentException
# [1] ""water.exceptions.H2OKeyNotFoundArgumentException: Object 'DeepLearning_model_R_1535036938222_489' not found for argument: key""



I cannot understand why it does work only for low values of nfolds. Any suggestions? What am I missing here? I've searched most remotely related threads on Google Groups and also here on stackoverflow, but without success. If this is to do with a changed API for h2o 3.x as suggested above (though that post was 18 months ago...) I would highly appreciate some documentary on how to correctly specify the syntax to do CV with h2o.deeplearning(). Thanks in advance!","['r', 'h2o', 'sparkling-water']",KSA,https://stackoverflow.com/users/10266534/ksa,3
51986590,51986590,2018-08-23T13:09:31,2018-08-23 23:14:53Z,0,"Due to the fact that I'm currently working on a highly unbalanced multi-class classification problem, I'm considering balanced random forests (
https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf
). Do you have some experience implementing balanced random forests using H2O? If so, could you please elaborate on the following question: 


Is it even possible to change the default process of creating bootstrap samples within H2O to come up with balanced sub-samples (for each iteration in the random forest, draw a bootstrap sample from the minority class. Randomly draw the same number of cases, with replacement, from the majority classes) of the original data set for each tree to grow?","['r', 'random-forest', 'h2o', 'multiclass-classification']",Flo,https://stackoverflow.com/users/10264814/flo,1
51985198,51985198,2018-08-23T11:51:39,2018-08-24 00:14:41Z,367,"I want to do some pre-processing (scaling, feature engineering, for instance target encoding) with cross-validation. I know that the best and theoretically right way to do this is to pre-process the data separately for each train/test step of the cross-validation. 


However, I am using H2O, which, unless I'm mistaken, doesn't allow me to create a pre-processing pipeline. 


A 
h2o documentation page on target encoding
 offers a workaround to avoid leakage from training folds to the validation fold: ""The target average is calculated on the out of fold data to prevent overfitting."" So the pre-processing on a given fold excludes data from this fold.  


It looks to me that this avoids leakage from the training sample to the test sample, but the opposite seems not true. 


Can I safely use this workaround (provided the #observations >> #features)  or should I be looking for another framework allowing pre-processing pipelines (or doing cross validation by hand) ?","['cross-validation', 'h2o', 'feature-extraction']",PierreC.,https://stackoverflow.com/users/10176085/pierrec,28
51982255,51982255,2018-08-23T09:15:02,2018-08-24 21:54:20Z,72,"I have two questions.


1)
I'm testing H2O 3.10.5.1 version for xgboost modeling.
There is a known bug (PUBDEV-4585) that binary save/load of xgboost doesn't work.
Has it been fixed in the recent version? Confirmation is needed in order to make a decision with the server admin whether to upgrade the system or not.


2)
H2O.ai xgboost documentation says there is some limitation to platforms.
The ""compilation OS"" is Ubuntu 14.04, but is there a limitation to any other linux OS version like Redhat?
h2o.xgboost.available() returns TRUE but I need to make sure.


Thanks",['h2o'],user10263887,https://stackoverflow.com/users/10263887/user10263887,1
51954502,51954502,2018-08-21T18:13:48,2018-08-28 16:33:03Z,0,"I am currently using h2o.ai to perform some NLP. I have a trained model for my corpus in Word2Vec and have successfully aggregated a number of records with the method ""Average"". The problem comes in when I want to create features for my DRF model by using this w2v model to create a bag of words for each entry. When I use the aggregate method ""none"" the vectors are returned in a single column containing NaN's where The records begin and end, however the unknown words in the model are also being mapped to NaN and not the the unknown word vector. This is stopping me from reorganizing the vectors into a bag of words for each record because the record separation association is lost due to the extra and unpredictably entered NaNs. Is there a fix for this? 


I am currently going to use the original tokenized list to make an index of the original double NaN structure that is used to deliminate between records and then recombine my vectors based off of this. Just wanted to throw this out there to see if anyone else is dealing with this or if there is some type of fix in place that I cannot find on the interwebs.


DATA = pd.read_sql(sql, conn1)

steps = [
    (r'[\n\t\’\–\”\“\!~`\""@#\$%\^\&\*()_+\{\}|:<>\?\-=\[\]\\;\',./\d]', ' '), 

    (r'\s+', ' ')
    ]

steps = [ (re.compile(a), b) for (a, b) in steps ] 

def do_steps(anarr):
    for pattern,replacement in steps:
        anarr = pattern.sub(replacement,anarr)
    return anarr

DATA.NARR = DATA.NARR.apply(do_steps)

train_hdata = h2o.H2OFrame(DATA).ascharacter()
train_narr = train_hdata[""NARR""]
train_key = train_hdata[""KEY""]
train_tokens_narr = train_narr.tokenize(split=' ')

train_vecs = w2v.transform(train_tokens_narr, aggregate_method='NONE')
VECS = train_vecs.as_data_frame()
df = train_tokens_narr.as_data_frame()
B=(VECS.isnull()&df.isnull())
idx = B[B['C1'] == True].index.tolist()
X = []
X.append('')
j=0
for i in tqdm(range(len(VECS.C1)-1)):
    if i in idx:
        X[j]= X[j][:-2]
        j+=1
        X.append('')
    else:
        X[j]= X[j] + str(VECS.C1[i])[:6] + ', '

s = pd.DataFrame({""C1"":X})
print(s)



The above is the current code looking to take some records and encode them with the word2vec model for a bag of words. The bottom portion is a draft loop that I am using to put the correct vectors with the correct records. Let me know if I need to clarify.","['python', 'word2vec', 'h2o']",Unknown,,N/A
51952342,51952342,2018-08-21T15:45:12,2019-02-14 20:01:38Z,0,"I trained an 
xgboost
 using  h2o framework in R on top of a virtual machine. I saved the model in a server's folder using the 
h2o.saveModel()
 function and I can correctly load it using the 
h2o.loadModel()
 function. 


xgboost_model    <- h2o.xgboost(
                      x = predictors,
                      y = response,
                      training_frame = train_h2o,
                      model_id       = ""xgboost_v1""
                      )
path_for_model = ""/data/R/xxx/xxx/xxx/""
path_saved_model <- h2o.saveModel(xgboost_model, path = path_for_model, force = FALSE)

loaded_model     <- h2o.loadModel(path_saved_model)



When I download the file in the 
path_for_model
 directory generated by the 
h2o.saveModel()
 function 
 on the local hard disk I'm not able to load it on the local RStudio environment and I get the following error message 




Cannot find Builder for algo url name xgboost




:


ERROR: Unexpected HTTP Status code: 400 Bad Request (url = 
http://localhost:54321/99/Models.bin/)

java.lang.IllegalArgumentException
[1] ""java.lang.IllegalArgumentException: Cannot find Builder for algo url 
name xgboost""                           
[2] ""    hex.ModelBuilder.ensureBuilderIndex(ModelBuilder.java:141)""                                              
[3] ""    hex.ModelBuilder.havePojo(ModelBuilder.java:120)""                                                        
[4] ""    hex.Model.havePojo(Model.java:119)""                                                                      
[5] ""    water.api.schemas3.ModelSchemaV3.fillFromImpl(ModelSchemaV3.java:73)""                                    
[6] ""    water.api.schemas3.ModelSchemaV3.fillFromImpl(ModelSchemaV3.java:21)""                                    
[7] ""    water.api.ModelsHandler.importModel(ModelsHandler.java:210)""                                             
[8] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                             
[9] ""    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)""                                             
[10] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)""                                         
[11] ""    java.lang.reflect.Method.invoke(Unknown Source)""                                                         
[12] ""    water.api.Handler.handle(Handler.java:63)""                                                               
[13] ""    water.api.RequestServer.serve(RequestServer.java:451)""                                                   
[14] ""    water.api.RequestServer.doGeneric(RequestServer.java:296)""                                               
[15] ""    water.api.RequestServer.doPost(RequestServer.java:222)""                                                  
[16] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                            
[17] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                            
[18] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                  
[19] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                              
[20] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                      
[21] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                               
[22] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                       
[23] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                           
[24] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                   
[25] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                         
[26] ""    water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:197)""                                               
[27] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                   
[28] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                         
[29] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                 
[30] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""          
[31] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""           
[32] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)""                
[33] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)""
[34] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)""                                        
[35] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)""                                   
[36] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                  
[37] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""            
[38] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                        
[39] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                         
[40] ""    java.lang.Thread.run(Unknown Source)""                                                                    

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Cannot find Builder for algo url name xgboost



I know that there is the possibility to download the pojo and/or mojo java version of the model, but I want to get stuck to the R binary file.
I do not see any related post similar to my error anywhere. 


I'm using the H2O version 3.20.0.3 with the R version 3.4.4 .","['r', 'h2o', 'xgboost']",Unknown,,N/A
51948440,51948440,2018-08-21T12:13:53,2019-12-15 13:54:03Z,0,"I have installed h2o's latest stable release using pip install and then while running h2o.init() i get the following error.


Checking whether there is an H2O instance running at http://localhost:54321..... not found.
Attempting to start a local H2O server...
  Java Version: java version ""10.0.1"" 2018-04-17; Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10); Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode)
  Starting server from /anaconda3/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar
  Ice root: /var/folders/cm/rqcrr3rx145f85_k4z_1vkfm0000gn/T/tmpafw3fd0y
  JVM stdout: /var/folders/cm/rqcrr3rx145f85_k4z_1vkfm0000gn/T/tmpafw3fd0y/h2o_nikhil_started_from_python.out
  JVM stderr: /var/folders/cm/rqcrr3rx145f85_k4z_1vkfm0000gn/T/tmpafw3fd0y/h2o_nikhil_started_from_python.err
---------------------------------------------------------------------------
H2OConnectionError                        Traceback (most recent call last)
/anaconda3/lib/python3.6/site-packages/h2o/h2o.py in init(url, ip, port, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, **kwargs)
    251                                      _msgs=(""Checking whether there is an H2O instance running at {url}"",
--> 252                                             ""connected."", ""not found.""))
    253     except H2OConnectionError:

/anaconda3/lib/python3.6/site-packages/h2o/backend/connection.py in open(server, url, ip, port, https, auth, verify_ssl_certificates, proxy, cookies, verbose, _msgs)
    317             conn._timeout = 3.0
--> 318             conn._cluster = conn._test_connection(retries, messages=_msgs)
    319             # If a server is unable to respond within 1s, it should be considered a bug. However we disable this

/anaconda3/lib/python3.6/site-packages/h2o/backend/connection.py in _test_connection(self, max_retries, messages)
    587             raise H2OConnectionError(""Could not establish link to the H2O cloud %s after %d retries\n%s""
--> 588                                      % (self._base_url, max_retries, ""\n"".join(errors)))
    589 

H2OConnectionError: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries
[28:07.29] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x111d2b080>: Failed to establish a new connection: [Errno 61] Connection refused',))
[28:07.50] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x111d1c978>: Failed to establish a new connection: [Errno 61] Connection refused',))
[28:07.71] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x111d2ba58>: Failed to establish a new connection: [Errno 61] Connection refused',))
[28:07.93] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x111d4b160>: Failed to establish a new connection: [Errno 61] Connection refused',))
[28:08.14] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x111d4b828>: Failed to establish a new connection: [Errno 61] Connection refused',))



During handling of the above exception, another exception occurred:


H2OServerError                            Traceback (most recent call last)
<ipython-input-10-95453bf1556d> in <module>()
----> 1 h2o.init()

/anaconda3/lib/python3.6/site-packages/h2o/h2o.py in init(url, ip, port, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, **kwargs)
    259             raise H2OConnectionError('Can only start H2O launcher if IP address is localhost.')
    260         hs = H2OLocalServer.start(nthreads=nthreads, enable_assertions=enable_assertions, max_mem_size=mmax,
--> 261                                   min_mem_size=mmin, ice_root=ice_root, port=port, extra_classpath=extra_classpath)
    262         h2oconn = H2OConnection.open(server=hs, https=https, verify_ssl_certificates=not insecure,
    263                                      auth=auth, proxy=proxy,cookies=cookies, verbose=True)

/anaconda3/lib/python3.6/site-packages/h2o/backend/server.py in start(jar_path, nthreads, enable_assertions, max_mem_size, min_mem_size, ice_root, port, extra_classpath, verbose)
    119         if verbose: print(""Attempting to start a local H2O server..."")
    120         hs._launch_server(port=port, baseport=baseport, nthreads=int(nthreads), ea=enable_assertions,
--> 121                           mmax=max_mem_size, mmin=min_mem_size)
    122         if verbose: print(""  Server is running at %s://%s:%d"" % (hs.scheme, hs.ip, hs.port))
    123         atexit.register(lambda: hs.shutdown())

/anaconda3/lib/python3.6/site-packages/h2o/backend/server.py in _launch_server(self, port, baseport, mmax, mmin, ea, nthreads)
    306         while True:
    307             if proc.poll() is not None:
--> 308                 raise H2OServerError(""Server process terminated with error code %d"" % proc.returncode)
    309             ret = self._get_server_info_from_logs()
    310             if ret:

H2OServerError: Server process terminated with error code 1","['java', 'python', 'macos', 'h2o']",ldz,https://stackoverflow.com/users/6911095/ldz,"2,215"
51942913,51942913,2018-08-21T06:36:36,2019-03-01 22:29:46Z,0,"Following my answered question: 
R or Python - loop the test data - Prediction validation next 24 hours (96 values each day)


I want to predict the next day using H2o Package. 
You can find detail explanation for my dataset in the same above link
.


The data dimension in H2o is different. 


So, after making the prediction, I want to calculate the MAPE


I have to change training and testing data to H2o format 


train_h2o <- as.h2o(train_data)

test_h2o <- as.h2o(test_data)

mape_calc <- function(sub_df) {
  pred <- predict.glm(glm_model, sub_df)
  actual <- sub_df$Ptot
  mape <- 100 * mean(abs((actual - pred)/actual))

  new_df <- data.frame(date = sub_df$date[[1]], mape = mape)

  return(new_df)
}

# LIST OF ONE-ROW DATAFRAMES
df_list <- by(test_data, test_data$date, map_calc)

# FINAL DATAFRAME
final_df <- do.call(rbind, df_list)



The upper code works well for ""
Non-H2o
"" prediction validation for the day-ahead and it calculates the MAPE for every day.


I tried to convert the H2o predicted model to normal format but according to to:
https://stackoverflow.com/a/39221269/9341589
, it is not possible.


To make a prediction in H2O:


for instance, let say we want to create a Random Forest Model


y <- ""RealPtot"" #target
x <- names(train_h2o) %>% setdiff(y) #features


rforest.model <- h2o.randomForest(y=y, x=x, training_frame = train_h2o, ntrees = 2000, mtries = 3, max_depth = 4, seed = 1122)



Then we can get the prediction for complete dataset as shown below.


predict.rforest <- as.data.frame(h2o.predict(rforest.model, test_h2o)



But in my case I am trying to get one-day prediction using mape_calc 




NOTE:
 Any thoughts in R or Python will be appreciated.


UPDATE2(reproducible example
):** Following @Darren Cook steps:


I provided a simpler example - Boston housing dataset.  


library(tidyverse)
library(h2o)
h2o.init(ip=""localhost"",port=54322,max_mem_size = ""128g"")


data(Boston, package = ""MASS"")

names(Boston)
[1] ""crim""    ""zn""      ""indus""   ""chas""    ""nox""     ""rm""      ""age""     ""dis""     ""rad""     ""tax""     ""ptratio""
[12] ""black""   ""lstat""   ""medv""   


set.seed(4984)
#Added 15 minute Time and date interval 
Boston$date<- seq(as.POSIXct(""01-09-2017 03:00"", format = ""%d-%m-%Y %H:%M"",tz=""""), by = ""15 min"", length = 506)

#select first 333 values to be trained and the rest to be test data
train = Boston[1:333,]
test = Boston[334:506,]

#Dropped the date and time
train_data_finialized  <- subset(train, select=-c(date))

test_data_finialized <- test

#Converted the dataset to h2o object.
train_h2o<- as.h2o(train_data_finialized)
#test_h2o<- as.h2o(test)

#Select the target and feature variables for h2o model
y <- ""medv"" #target
x <- names(train_data_finialized) %>% setdiff(y) #feature variables

# Number of CV folds (to generate level-one data for stacking)
nfolds <- 5

#Replaced RF model by GBM because GBM run faster
# Train & Cross-validate a GBM
my_gbm <- h2o.gbm(x = x,
                  y = y,
                          training_frame = train_h2o,
                          nfolds = nfolds,
                          fold_assignment = ""Modulo"",
                          keep_cross_validation_predictions = TRUE,
                          seed = 1)

mape_calc <- function(sub_df) {
  p <- h2o.predict(my_gbm, as.h2o(sub_df))
  pred <- as.vector(p)
  actual <- sub_df$medv
  mape <- 100 * mean(abs((actual - pred)/actual))
  new_df <- data.frame(date = sub_df$date[[1]], mape = mape)
  return(new_df)
}


# LIST OF ONE-ROW DATAFRAMES
df_list <- by(test_data_finialized, test_data_finialized$date, mape_calc)

final_df <- do.call(rbind, df_list)



This is the error I am getting now:




Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion,
  urlSuffix = page,  : 


ERROR MESSAGE:


Provided column type POSIXct is unknown.  Cannot proceed with parse
  due to invalid argument.","['python', 'r', 'loops', 'prediction', 'h2o']",Unknown,,N/A
51901091,51901091,2018-08-17T18:31:40,2018-08-20 18:57:40Z,164,"For simplicity, say that I am attempting to predict the following day of a sequence of single-valued variables, therefore my datasaet would be in the form of:


input    label
   x1       x2
   x2       x3
   x3       x4
  ...      ...
   xt      xt+1



However, my data has the same sequences in time for many different users, therefore it is in the following form:


input    label
 u1x1     u1x2
 u1x2     u1x3
 u1x3     u1x4
  ...      ...
 u1xt   u1xt+1
 u2x1     u2x2
 u2x2     u2x3
 u2x3     u2x4
  ...      ...
 u2xt   u2xt+1
  ...      ...
 unx1     unx2
 unx2     unx3
 unx3     unx4
  ...      ...
 unxt   unxt+1



What is an acceptable way to structure this data and feed it into DAI such that it is not treated as one entire long sequence, but rather a bunch of not directly related sequences parallel in time?


Edit: The data has a 'UserID' column. Can DAI automatically use this to overcome the problem I am explaining?","['dataset', 'time-series', 'h2o', 'driverless-ai']",KOB,https://stackoverflow.com/users/4564080/kob,"4,495"
51872773,51872773,2018-08-16T08:29:19,2018-08-17 16:55:32Z,494,"I am trying to build stacked ensemble models using H2O Java APIs. 


For this, I trained 2 models




A GBM Model 


A DRF Model




I exported these models in both Mojo and Binary format. For exporting models, I used the following code snippet:


For Mojo Format Export:


        water.api.ModelsHandler modelsHandler = new ModelsHandler();
        water.api.StreamingSchema streamingSchema = modelsHandler.fetchMojo(3, modelsV3); //water.api.schemas3.ModelsV3



For Binary format Export:


        water.api.ModelsHandler modelsHandler = new ModelsHandler();
        modelsHandler.exportModel(3, modelExport); //water.api.schemas3.ModelExportV3



I also exported the cross validation holdout data as it is required later to train stacked ensemble models.


Lets assume my exported model names and their holdout data names are as below:


ModelName: StackGBMReg1    CVDataName: cv_holdout_prediction_StackGBMReg1


ModelName: StackDRFReg1    CVDataName: cv_holdout_prediction_StackDRFReg1


Training Stacked Ensemble Models


Then I import these models and their CV data later to H2O Server to train stacked Ensemble models. Here is the code snippet for this operation:


To import holdout data:


    ImportFilesV3 importFile = h2o.importFiles(workingDir + fileName); //fileName: cv_holdout_prediction_StackGBMReg1 or DRFReg1 one.



To import model:


    ModelsHandler modelsHandler = new ModelsHandler();
    water.api.schemas3.ModelsV3 importedModel = modelsHandler.importModel(3, modelImport); //water.api.schemas3.ModelImportV3



I get following error when I try to import Mojo Models.


    H2OException: Error while importing model : StackGBMReg1.zip
        at ImportAndScore.importModel(ImportAndScore.java:306)
        at ImportAndScore.main(ImportAndScore.java:61)
    Caused by: java.lang.IllegalArgumentException: Missing magic number 0x1CED at stream start
        at water.AutoBuffer.<init>(AutoBuffer.java:287)
        at hex.Model.importBinaryModel(Model.java:2380)
        at water.api.ModelsHandler.importModel(ModelsHandler.java:209)
        at ImportAndScore.importModel(ImportAndScore.java:302)
        ... 1 more



As per the reply I got on h2o forum, import of Mojo Models is not supported. I find this really strange.


To overcome this problem, I imported Binary Models which are successful. Then trained stacked ensemble models which worked fine for me. 


My questions are:


1. Since, mojo Model import is not working using ModelsHandler.importModel(), is there another API available or work around which can help me to import Mojo Model in H2O?
2. Can we convert, POJO or MOJO models into binary Model for import purpose?
3. As per last reply for h2o, binary models are not backward compatible. So, if I upgrade H2O later, my older trained models will not work for training new stacked ensemble models. Actually, it will fail at import step itself. 
    a. So, is there a way to use the binary models without having the backward compatibility issue?
    b. If binary models are the only way to go, then is my approach right for training stacked Ensemble models(using previously exported/saved models)?
    c. Am I likely to face any other issue with binary models in future which I dont for see now?



My main concern is get rid of backward compatibility issue. If there is any way to get around with it, that will really help my work. Since, I am using Java code, I wouldn't mind using any internal h2o api, which is not directly exposed.


Please note that I am not talking about loading MOJO models for scoring purpose. I understand that we can easily use Mojo Models for scoring as per this link:

http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html","['java', 'machine-learning', 'h2o', 'ensemble-learning', 'sparkling-water']",AdityaK01,https://stackoverflow.com/users/489703/adityak01,5
51869414,51869414,2018-08-16T03:58:53,2020-07-20 13:42:35Z,0,"Anyone able to match the sklearn confusion matrix to h2o?


They never match....


Doing something similar with Keras produces a perfect match.


But in h2o they are always off. Tried it every which way...


Borrowed some code from:

Any difference between H2O and Scikit-Learn metrics scoring?


# In[30]:
import pandas as pd
import h2o
from h2o.estimators.gbm import H2OGradientBoostingEstimator
h2o.init()

# Import a sample binary outcome train/test set into H2O
train = h2o.import_file(""https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv"")
test = h2o.import_file(""https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv"")

# Identify predictors and response
x = train.columns
y = ""response""
x.remove(y)

# For binary classification, response should be a factor
train[y] = train[y].asfactor()
test[y] = test[y].asfactor()

# Train and cross-validate a GBM
model = H2OGradientBoostingEstimator(distribution=""bernoulli"", seed=1)
model.train(x=x, y=y, training_frame=train)

# In[31]:
# Test AUC
model.model_performance(test).auc()
# 0.7817203808052897

# In[32]:

# Generate predictions on a test set
pred = model.predict(test)

# In[33]:

from sklearn.metrics import roc_auc_score, confusion_matrix

pred_df = pred.as_data_frame()
y_true = test[y].as_data_frame()

roc_auc_score(y_true, pred_df['p1'].tolist())
#pred_df.head()

# In[36]:

y_true = test[y].as_data_frame().values
cm = pd.DataFrame(confusion_matrix(y_true, pred_df['predict'].values))

# In[37]:

print(cm)
    0     1
0  1354   961
1   540  2145

# In[38]:
model.model_performance(test).confusion_matrix()

Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.353664307031828: 

    0         1     Error   Rate
0   964.0   1351.0  0.5836  (1351.0/2315.0)
1   274.0   2411.0  0.102   (274.0/2685.0)
Total   1238.0  3762.0  0.325   (1625.0/5000.0)

# In[39]:
h2o.cluster().shutdown()","['python-3.x', 'scikit-learn', 'classification', 'h2o', 'confusion-matrix']",QuanTomatic,https://stackoverflow.com/users/7700274/quantomatic,51
51863256,51863256,2018-08-15T17:05:51,2019-05-20 09:41:43Z,0,"I want to extract the individual categorical levels from a variable importance standpoint for a given model. There are several categorical predictors in the dataset supplied below, yet when I go to calculate feature importance, only the ""whole column's"" importance is shown, as opposed to the importances being broken up into something like 
C1_level0: importance
 and 
C1_level1: importance
. How can I view the importance of the columns similar to what I'd see if I manually one-hot-encoded these discrete levels?


>>> import h2o
>>> h2o.init()
Checking whether there is an H2O instance running at http://localhost:54321. connected.
--------------------------  ----------------------------------------
H2O cluster uptime:         48 mins 24 secs
H2O cluster timezone:       America/Chicago
H2O data parsing timezone:  UTC
H2O cluster version:        3.20.0.5
H2O cluster version age:    6 days
H2O cluster name:           H2O_from_python_user_9znggm
H2O cluster total nodes:    1
H2O cluster free memory:    1.464 Gb
H2O cluster total cores:    8
H2O cluster allowed cores:  8
H2O cluster status:         locked, healthy
H2O connection url:         http://localhost:54321
H2O connection proxy:
H2O internal security:      False
H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4
Python version:             3.6.5 final
--------------------------  ----------------------------------------
>>>
>>> df = h2o.create_frame(categorical_fraction=0.5)
Create Frame progress: |██████████████████████████████████████████████████████████████████████| 100%
>>>
>>> model = H2OGradientBoostingEstimator()
>>> model.train(x=[c for c in df.columns if c != 'C1'], y='C1', training_frame=df)
gbm Model Build progress: |███████████████████████████████████████████████████████████████████| 100%
>>>
>>> model.varimp(True)
  variable  relative_importance  scaled_importance  percentage
0       C3          4448.583984           1.000000    0.255125
1       C9          4424.002930           0.994474    0.253715
2       C6          4273.684082           0.960684    0.245094
3       C4          4249.320312           0.955207    0.243697
4      C10            12.800615           0.002877    0.000734
5       C7            12.022744           0.002703    0.000689
6       C8             8.271964           0.001859    0.000474
7       C2             4.649746           0.001045    0.000267
8       C5             3.567022           0.000802    0.000205","['python', 'h2o']",boot-scootin,https://stackoverflow.com/users/5015569/boot-scootin,12.5k
51848917,51848917,2018-08-14T19:57:40,2018-08-15 20:20:46Z,602,"Trying to start an 
h2o
 cluster on 
(MapR) hadoop
 via python 


# startup hadoop h2o cluster
import os
import subprocess
import h2o
import shlex
import re

from Queue import Queue, Empty
from threading import Thread

def enqueue_output(out, queue):
    """"""
    Function for communicating streaming text lines from seperate thread.
    see https://stackoverflow.com/questions/375427/non-blocking-read-on-a-subprocess-pipe-in-python
    """"""
    for line in iter(out.readline, b''):
        queue.put(line)
    out.close()

# clear legacy temp. dir.
hdfs_legacy_dir = '/mapr/clustername/user/mapr/hdfsOutputDir'
if os.path.isdir(hdfs_legacy_dir ):
    print subprocess.check_output(shlex.split('rm -r %s'%hdfs_legacy_dir ))

# start h2o service in background thread
local_h2o_start_path = '/home/mapr/h2o-3.18.0.2-mapr5.2/'
startup_p = subprocess.Popen(shlex.split('/bin/hadoop jar {}h2odriver.jar -nodes 4 -mapperXmx 6g -timeout 300 -output hdfsOutputDir'.format(local_h2o_start_path)), 
                             shell=False, 
                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)

# setup message passing queue
q = Queue()
t = Thread(target=enqueue_output, args=(startup_p.stdout, q))
t.daemon = True # thread dies with the program
t.start()

# read line without blocking
h2o_url_out = ''
while True:
    try:  line = q.get_nowait() # or q.get(timeout=.1)
    except Empty:
        continue
    else: # got line
        print line
        # check for first instance connection url output
        if re.search('Open H2O Flow in your web browser', line) is not None:
            h2o_url_out = line
            break
        if re.search('Error', line) is not None:
            print 'Error generated: %s' % line
            sys.exit()

print 'Connection url output line: %s' % h2o_url_out
h2o_cnxn_ip = re.search('(?<=Open H2O Flow in your web browser: http:\/\/)(.*?)(?=:)', h2o_url_out).group(1)
print 'H2O connection ip: %s' % h2o_cnxn_ip



frequently throws a timeout error




Waiting for H2O cluster to come up...
H2O node 172.18.4.66:54321 requested flatfile
H2O node 172.18.4.65:54321 requested flatfile
H2O node 172.18.4.67:54321 requested flatfile
ERROR: Timed out waiting for H2O cluster to come up (300 seconds)
Error generated: ERROR: Timed out waiting for H2O cluster to come up (300 seconds)
Shutting down h2o cluster





Looking at the docs (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/faq/general-troubleshooting.html
) (and just doing a wordfind for the word ""
timeout
""), was unable to find anything that helped the problem (eg. extending the timeout time via 
hadoop jar h2odriver.jar -timeout <some time>
 did nothing but extend the time until the timeout error popped up).


Have noticed that this happens often when there is another instance of an h2o cluster already up and running (which I don't understand since I would think that YARN could support multiple instances), yet also 
sometimes
 when there is no other cluster initialized.


Anyone know anything else that can be tried to solve this problem or get more debugging info beyond the error message being thrown by h2o?




UPDATE
:


Trying to recreate the problem from the commandline, getting


[me@mnode01 project]$ /bin/hadoop jar /home/me/h2o-3.20.0.5-mapr5.2/h2odriver.jar -nodes 4 -mapperXmx 6g -timeout 300 -output hdfsOutputDir
Determining driver host interface for mapper->driver callback...
    [Possible callback IP address: 172.18.4.62]
    [Possible callback IP address: 127.0.0.1]
Using mapper->driver callback IP address and port: 172.18.4.62:29388
(You can override these with -driverif and -driverport/-driverportrange.)
Memory Settings:
    mapreduce.map.java.opts:     -Xms6g -Xmx6g -XX:PermSize=256m -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Dlog4j.defaultInitOverride=true
    Extra memory percent:        10
    mapreduce.map.memory.mb:     6758
18/08/15 09:18:46 INFO client.MapRZKBasedRMFailoverProxyProvider: Updated RM address to mnode03.cluster.local/172.18.4.64:8032
18/08/15 09:18:48 INFO mapreduce.JobSubmitter: number of splits:4
18/08/15 09:18:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1523404089784_7404
18/08/15 09:18:48 INFO security.ExternalTokenManagerFactory: Initialized external token manager class - com.mapr.hadoop.yarn.security.MapRTicketManager
18/08/15 09:18:48 INFO impl.YarnClientImpl: Submitted application application_1523404089784_7404
18/08/15 09:18:48 INFO mapreduce.Job: The url to track the job: https://mnode03.cluster.local:8090/proxy/application_1523404089784_7404/
Job name 'H2O_66888' submitted
JobTracker job ID is 'job_1523404089784_7404'
For YARN users, logs command is 'yarn logs -applicationId application_1523404089784_7404'
Waiting for H2O cluster to come up...
H2O node 172.18.4.65:54321 requested flatfile
H2O node 172.18.4.67:54321 requested flatfile
H2O node 172.18.4.66:54321 requested flatfile
ERROR: Timed out waiting for H2O cluster to come up (300 seconds)
ERROR: (Try specifying the -timeout option to increase the waiting time limit)
Attempting to clean up hadoop job...
Killed.
18/08/15 09:23:54 INFO client.MapRZKBasedRMFailoverProxyProvider: Updated RM address to mnode03.cluster.local/172.18.4.64:8032

----- YARN cluster metrics -----
Number of YARN worker nodes: 6

----- Nodes -----
Node: http://mnode03.cluster.local:8044 Rack: /default-rack, RUNNING, 0 containers used,  0.0 / 7.0 GB used, 0 / 2 vcores used
Node: http://mnode05.cluster.local:8044 Rack: /default-rack, RUNNING, 0 containers used, 0.0 / 10.4 GB used, 0 / 2 vcores used
Node: http://mnode06.cluster.local:8044 Rack: /default-rack, RUNNING, 0 containers used, 0.0 / 10.4 GB used, 0 / 2 vcores used
Node: http://mnode01.cluster.local:8044 Rack: /default-rack, RUNNING, 0 containers used,  0.0 / 5.0 GB used, 0 / 2 vcores used
Node: http://mnode04.cluster.local:8044 Rack: /default-rack, RUNNING, 1 containers used, 7.0 / 10.4 GB used, 1 / 2 vcores used
Node: http://mnode02.cluster.local:8044 Rack: /default-rack, RUNNING, 1 containers used,  2.0 / 8.7 GB used, 1 / 2 vcores used

----- Queues -----
Queue name:            root.default
    Queue state:       RUNNING
    Current capacity:  0.00
    Capacity:          0.00
    Maximum capacity:  -1.00
    Application count: 0

Queue 'root.default' approximate utilization: 0.0 / 0.0 GB used, 0 / 0 vcores used

----------------------------------------------------------------------

WARNING: Job memory request (26.4 GB) exceeds queue available memory capacity (0.0 GB)
WARNING: Job virtual cores request (4) exceeds queue available virtual cores capacity (0)
ERROR:   Only 3 out of the requested 4 worker containers were started due to YARN cluster resource limitations

----------------------------------------------------------------------

For YARN users, logs command is 'yarn logs -applicationId application_1523404089784_7404'



and noticing the later outputs




WARNING: Job memory request (26.4 GB) exceeds queue available memory capacity (0.0 GB) 
WARNING: Job virtual cores request (4) exceeds queue available virtual cores capacity (0) 
ERROR:   Only 3 out of the requested 4 worker containers were started due to YARN cluster





I am confused by the reported 0GB mem. and 0 vcores becuase there are no other applications running on the cluster and looking at the cluster details in the YARN RM web UI shows




(using image, since could not find unified place in log files for this info and why the mem. availability is so uneven despite having no other running applications, I do not know). At this point, should mention that don't have much experience tinkering with / examining YARN configs, so it's difficult for me to find relevant information at this point. 


Could it be that I am starting h2o cluster with 
-mapperXmx=6g
, but (as shown in the image) one of the nodes only has 5g mem. available, so if this node is randomly selected to contribute to the initialized h2o application, it does not have enough memory to support the requested mapper mem.? Changing the startup command to 
/bin/hadoop jar /home/me/h2o-3.20.0.5-mapr5.2/h2odriver.jar -nodes 4 -mapperXmx 5g -timeout 300 -output hdfsOutputDir
 and start/stopping multiple times without error seems to support this theory (though need to check further to determine if I'm interpreting things correctly).",['h2o'],Unknown,,N/A
51847076,51847076,2018-08-14T17:44:03,2018-10-22 11:28:34Z,0,"R Version:
                  R version 3.5.1 (2018-07-02) 


H2O cluster version:
        3.20.0.2 


The dataset used here is available on Kaggle (Home credit risk). Prior to using h2o automl, the necessary treatment of missing values and selection of relevant categorical variables has already been carried out. Can you assist me in figuring out what is the underlying cause for this error? 
Thanks


Code:


h2o.init()
 h2o.no_progress()
 # y_train_processed_tbl is the target variable
 # x_train_processed_tbl is the remaining data post dealing with Missing 
 #  values
 data_h2o <- as.h2o(bind_cols(y_train_processed_tbl, x_train_processed_tbl))
 splits_h2o <- h2o.splitFrame(data_h2o, ratios = c(0.7, 0.15), seed = 1234)
 train_h2o <- splits_h2o[[1]]
 valid_h2o <- splits_h2o[[2]]
 test_h2o  <- splits_h2o[[3]]

 y <- ""TARGET""
 x <- setdiff(names(train_h2o), y)

 automl_models_h2o <- h2o.automl(x = x,y = y,
 training_frame    = train_h2o, validation_frame  = valid_h2o,
 leaderboard_frame = test_h2o,
 max_runtime_secs  = 90
 )

 automl_leader <- automl_models_h2o@leader
 # Error in performance_h2o 
 performance_h2o <- h2o.performance(automl_leader, newdata = test_h2o)


ERROR: Unexpected HTTP Status code: 404 Not Found

water.exceptions.H2OKeyNotFoundArgumentException
 [1] ""water.exceptions.H2OKeyNotFoundArgumentException: Object 'dummy' not 
 found in function: predict for argument: model""
 [2] ""    water.api.ModelMetricsHandler.score(ModelMetricsHandler.java:235)""  
 [3] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                    
 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)""                                                    
 [5] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)""                                                
 [6] ""    java.lang.reflect.Method.invoke(Unknown Source)""                                                                
 [7] ""    water.api.Handler.handle(Handler.java:63)""                                                                      
 [8] ""    water.api.RequestServer.serve(RequestServer.java:451)""                                                          
 [9] ""    water.api.RequestServer.doGeneric(RequestServer.java:296)""                                                      
[10] ""    water.api.RequestServer.doPost(RequestServer.java:222)""                                                         
[11] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                                   
[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                   
[13] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                         
[14] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                                     
[15] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                             
[16] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                                      
[17] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                              
[18] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                  
[19] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                          
[20] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                
[21] ""    water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:197)""                                                      
[22] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                          
[23] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                
[24] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                        
[25] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                 
[26] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                  
[27] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)""                       
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)""       
[29] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)""                                               
[30] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)""                                          
[31] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                         
[32] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                   
[33] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                               
[34] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                
[35] ""    java.lang.Thread.run(Unknown Source)""                                                                           



Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = 
page,  : 

ERROR MESSAGE:

Object 'dummy' not found in function: predict for argument: model","['r', 'h2o']",Unknown,,N/A
51747696,51747696,2018-08-08T13:13:58,2018-08-09 21:05:14Z,0,"Running 
h2o.automl()
 returns a single model in leaderboard; however, when trying to access the actual model via 
@leader@model
, the following error ensues:




Error in is.H2OFrame(x) : trying to get slot ""metrics"" from an object
  of a basic class (""NULL"") with no slots




As well, when calling 
h2o.predict()
 on the leader model, got the error message:




Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion,
  urlSuffix = page,  : ERROR MESSAGE: Object 'dummy' not found in
  function: predict for argument: model




Model was run in the same session using 
h2o
 
v3.20.0.2
 in 
R
.","['r', 'h2o', 'automl']",Saurabh Chauhan,https://stackoverflow.com/users/5835763/saurabh-chauhan,"3,201"
51735459,51735459,2018-08-07T21:07:58,2018-08-10 15:23:56Z,936,"When I run: 


data_h = h2o.H2OFrame(data)

### Edit: added asfactor() below to change integer target array.
data_h[""BPA""] = data_h[""BPA""].asfactor()

train, valid = data_h.split_frame(ratios=[.7], seed = 1234)
features = [""bq_packaging_consumepkg"", ""bq_packaging_microwave_v3"", ""bq_packaging_plasticbottle_v2"", 
              ""bq_packaging_hotdrink_v3"", ""bq_packaging_microwsaran_v3"",""bq_food_cannedfoods_v2""]
target = ""BPA""

# Hyperparameter tuning
params = {""ntrees"": [50, 100, 200, 300, 400, 500, 600],
          ""max_depth"": [10, 30, 50, 70, 90, 110],
          ""min_rows"": [1,5,10,15,20,25]}

criteria = {""strategy"": ""RandomDiscrete"", 
            ""stopping_rounds"": 10,
            ""stopping_tolerance"": 0.00001,
            ""stopping_metric"": ""misclassification""}

# Grid search and Training
grid_search = H2OGridSearch(model= rf_h, hyper_params= params, 
search_criteria = criteria)
grid_search.train(x = features, y = target, training_frame=train, 
validation_frame = valid)

# Sorting the grid
sorted_grid = grid_search.get_grid(sort_by='auc', decreasing = True)



Calling 
grid_search.get_grid(sort_by = 'auc', decreasing = True)
 produces the following error:


  H2OResponseError: Server error water.exceptions.H2OIllegalArgumentException:
  Error: Invalid argument for sort_by specified. Must be one of: [mae, residual_deviance, r2, mean_residual_deviance, rmsle, rmse, mse]
  Request: GET /99/Grids/Grid_DRF_py_29_sid_95b5_model_python_1533334963198_8
    params: {'sort_by': 'auc', 'decreasing': 'True'}



Looking at the example in the 
documentation for the grid search
 I believe that I am using the method correctly.


Edit: Added changing target array to be a factor array from an integer array.","['python', 'h2o', 'grid-search']",Unknown,,N/A
51734915,51734915,2018-08-07T20:28:35,2018-08-10 15:37:52Z,0,"is there any way I can display an image or diagram of my neural net using h20 in R. Also, I went through the h20 documentation but couldn't figure out to extract weights from the neural net object.","['r', 'machine-learning', 'neural-network', 'deep-learning', 'h2o']",Dishant Shetty,https://stackoverflow.com/users/9649652/dishant-shetty,75
51722798,51722798,2018-08-07T08:59:17,2018-08-07 13:55:43Z,0,"Environment




**OS platform, distribution and version : Redhat 7.5 ALT ppc64le Python version (optional): python 3.6 from DriverlessAI CUDA/cuDNN
  version: CUDA 9.2 cuDNN 7.1 GPU model (optional): V100 CPU model:
  POWER9 RAM available: 512GB R version : 3.4.1 Tensorflow version :
  1.8.0 (built from source)
  




I am trying to use 
h2o.deepwater
 included in DriverlessAI in python and R environment, instead of the web GUI of DAI. Plus, I'd like to use tensorflow as backend.


For this, I set the environment variables to use the python from DriverlessAI.


$ export PATH=/opt/h2oai/dai/python/bin:$PATH
$ export LD_LIBRARY_PATH=/opt/h2oai/dai/python/lib:/opt/h2oai/dai/lib:$LD_LIBRARY_PATH
$ export PYTHONPATH=/opt/h2oai/dai/cuda-9.2/lib/python3.6/site-packages



This works fine with 
h2o.deeplearning
.


gpu_xgb <- h2o.deeplearning(x = c(""TemperatureCelcius"",""ExhaustVacuumHg"",""AmbientPressureMillibar"",""RelativeHumidity""),
y = ""HourlyEnergyOutputMW"",
training_frame = train
)



However, 
h2o.deepwater
 produces an error or





""Unable to initialize the native Deep Learning backend: No backend
  found. Cannot build a Deep Water model.""
  




Below is the error message related to running 
h2o.deepwater
 in R with backend of tensorflow.


$ cat t4.R
'# Package Load
library(reticulate)
use_python(""/opt/h2oai/dai/python/bin/python"")
library(Metrics)
library(h2o)
h2o.init(max_mem_size = ""500g"")
'# Data Load
df <- read.csv('/data/rpjt/R_script/user/yslee/powerplant_output.csv')
'# Randomly sample 80% of the rows for the training set
set.seed(1)
train_idx <- sample(1:nrow(df), 0.8*nrow(df))
'# h2o Dataset
train <- df[train_idx,]
test <- df[-train_idx,]
train <- as.h2o(train,col.types=c(""string""))
test <- as.h2o(test,col.types=c(""string""))
'# h2o.deepwater model
gpu_dl <- h2o.deepwater(x = c(""TemperatureCelcius"",""ExhaustVacuumHg"",""AmbientPressureMillibar"",""RelativeHumidity""),
y = ""HourlyEnergyOutputMW"",
training_frame = train,
backend = ""tensorflow"",
hidden = 10,
standardize =T,
activation = ""Tanh"",
seed = 1234)
h2o.performance(gpu_dl, newdata = test)

$ Rscript t4.R
...
R is connected to the H2O cluster:
H2O cluster uptime: 16 minutes 30 seconds
H2O cluster timezone: Asia/Seoul
H2O data parsing timezone: UTC
H2O cluster version: 3.20.0.2
H2O cluster version age: 1 month and 22 days
H2O cluster name: dai
H2O cluster total nodes: 1
H2O cluster total memory: 227.37 GB
H2O cluster total cores: 128
H2O cluster allowed cores: 128
H2O cluster healthy: TRUE
H2O Connection ip: localhost
H2O Connection port: 54321
H2O Connection proxy: NA
H2O Internal Security: FALSE
H2O API Extensions: Algos, MLI, MLI-Driver, AutoML, Core V3, Core V4
R Version: R version 3.4.1 (2017-06-30)

|======================================================================| 100%
|======================================================================| 100%
| | 0%

java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: No backend found. Cannot build a Deep Water model.

java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: No backend found. Cannot build a Deep Water model.
at hex.deepwater.DeepWaterModelInfo.setupNativeBackend(DeepWaterModelInfo.java:267)
at hex.deepwater.DeepWaterModelInfo.(DeepWaterModelInfo.java:214)
at hex.deepwater.DeepWaterModel.(DeepWaterModel.java:227)
at hex.deepwater.DeepWater$DeepWaterDriver.buildModel(DeepWater.java:131)
at hex.deepwater.DeepWater$DeepWaterDriver.computeImpl(DeepWater.java:118)
at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:214)
at hex.deepwater.DeepWater$DeepWaterDriver.compute2(DeepWater.java:111)
at water.H2O$H2OCountedCompleter.compute(H2O.java:1260)
at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

Error: java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: No backend found. Cannot build a Deep Water model.
Execution halted



install.packages(""tensorflow"")
 and 
library(tensorflow)
 worked fine in R,


$ ls -l /usr/local/lib64/R/library/tensorflow
total 12
-rw-rw-r-- 1 root root 2456 Aug 7 17:45 DESCRIPTION
drwxrwxr-x 5 root root 112 Aug 7 17:45 examples
drwxrwxr-x 2 root root 125 Aug 7 17:45 help
drwxrwxr-x 2 root root 39 Aug 7 17:45 html
-rw-rw-r-- 1 root root 1095 Aug 7 17:45 INDEX
drwxrwxr-x 2 root root 113 Aug 7 17:45 Meta
-rw-rw-r-- 1 root root 2713 Aug 7 17:45 NAMESPACE
drwxrwxr-x 2 root root 84 Aug 7 17:45 R



Also, tensorflow is installed in python from DriverlessAI.


$ which python
/opt/h2oai/dai/python/bin/python

$ pip list | grep tensorflow
tensorflow 1.8.0","['r', 'tensorflow', 'backend', 'h2o']",s__,https://stackoverflow.com/users/6498650/s,"9,475"
51722344,51722344,2018-08-07T08:35:23,2018-08-07 17:15:51Z,92,"I am learning h2o model predictions. When I do:


data_frame = h2o.H2OFrame(python_obj=data[1:], column_names=data[0])
data_train, data_valid, data_test = data_frame.split_frame(ratios= config.trainer_analizer_ratios, seed=config.trainer_analizer_seed)

# H2OGeneralizedLinearEstimator
allLog += ""/n Starting H2OGeneralizedLinearEstimator""
model_gle = h2o.estimators.H2OGeneralizedLinearEstimator()
model_gle.train(x=predictors, y=response, training_frame= data_train, validation_frame= data_valid)
print(model_gle)
perf_gle = model_gle.model_performance(test_data= data_test)
print(""GBM Precision:"",perf_gle)



I get the following output


** Reported on test data. **

MSE: 494.25950875189955
RMSE: 22.23194792976764
MAE: 17.380709221249717
RMSLE: 1.217426465475652
R^2: 0.04331665117221439
Mean Residual Deviance: 494.25950875189955
Null degrees of freedom: 1177
Residual degrees of freedom: 1174
Null deviance: 608812.1064795277
Residual deviance: 582237.7013097376
AIC: 10660.224689554776



Why don't I get the 
ACU
 metric? I need that to score different models.","['python', 'h2o']",James Whiteley,https://stackoverflow.com/users/8230810/james-whiteley,"3,460"
51715175,51715175,2018-08-06T20:27:43,2018-08-13 16:39:23Z,0,"I'm trying to test a process in which I build a model in H2O through R (using the 
H2O
 package), download the MOJO, create a function to call 'h2o.mojo_predict_df', then use 
plumber
 to create a restful API. As I understand the implementation of 
plumber
, my the method in which I obtain predictions needs to be wrapped in a function.


To test I'm using the 
iris
 dataset.


The result of `sessionInfo()' is:


R version 3.4.3 (2017-11-30)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 16299)

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252    LC_MONETARY=English_United States.1252 LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] purrr_0.2.5  h2o_3.20.0.2

loaded via a namespace (and not attached):
[1] Rcpp_0.12.18     crayon_1.3.4     dplyr_0.7.6      assertthat_0.2.0 bitops_1.0-6     R6_2.2.2         jsonlite_1.5     magrittr_1.5     pillar_1.3.0    
[10] rlang_0.2.1      bindrcpp_0.2.2   tools_3.4.3      glue_1.3.0       RCurl_1.95-4.11  compiler_3.4.3   pkgconfig_2.0.1  tidyselect_0.2.4 bindr_0.1.1     
[19] tibble_1.4.2



My code to train/save/download is here:


library(RODBC)
library(caret)
library(h2o)

if(Sys.info()[""nodename""]!=""WINX-08947"")
{
    Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_161')
}

fulldata<-iris
summary(fulldata)

fulldata_trainindex<-createDataPartition(fulldata$Species, p=0.75, list=FALSE)
train<-fulldata[fulldata_trainindex,]
test<-fulldata[-fulldata_trainindex,]

## Using H2O
h2o.shutdown(prompt=FALSE)
Sys.sleep(10)
localH2Oconn = h2o.init(ip=""localhost"",port=54321,startH2O=TRUE, nthreads=-1, max_mem_size = ""4g"")
X<-train[,c(1:4)]
target<-as.data.frame(train$Species)
colnames(target)<-""Species""

train_h2o <- as.h2o(train, destination_frame=""train_h2o"")
test_h2o <- as.h2o(test, destination_frame=""test_h2o"")

##### AutoML
aml<-h2o.automl(x=colnames(X)
                , y=colnames(target)
                , training_frame=train_h2o
                , validation_frame=test_h2o
                , leaderboard_frame=test_h2o
                , nfolds=5
                , max_runtime_secs=600
                , project_name='AML_MODEL'
                )


aml_best<-aml@leader
aml_best_MODEL_ID<-aml_best@model_id

h2o.download_mojo(aml_best, path = getwd(), get_genmodel_jar = TRUE)
preds<-h2o.mojo_predict_df(test, ""C:/Users/jeng209/Documents/GBM_grid_0_AutoML_20180731_202910_model_3.zip"", genmodel_jar_path = ""C:/Users/jeng209/Documents/h2o-genmodel.jar"", java_options = ""-Xmx256m -XX:ReservedCodeCacheSize=256m"" , verbose = TRUE)

test$PREDICTION<-preds$predict



The code for a test case that returns predictions in the needed format is


library(h2o)
newdata <- data.frame(
            Sepal.Length=5
            , Sepal.Width=3
            , Petal.Length=2
            , Petal.Width=1
        )
mojo_res <- h2o.mojo_predict_df(newdata, ""C:/Users/jeng209/Documents/GBM_grid_0_AutoML_20180731_202910_model_3.zip"", genmodel_jar_path = ""C:/Users/jeng209/Documents/h2o-genmodel.jar"", java_options = ""-Xmx256m -XX:ReservedCodeCacheSize=256m"" , verbose = FALSE)
mojo_pred <- as.character(mojo_res$predict)
mojo_pred



with the output of 


> newdata <- data.frame(
+ Sepal.Length=5
+ , Sepal.Width=3
+ , Petal.Length=2
+ , Petal.Width=1
+ )
> mojo_res <- h2o.mojo_predict_df(newdata, ""C:/Users/jeng209/Documents/GBM_grid_0_AutoML_20180731_202910_model_3.zip"", genmodel_jar_path = ""C:/Users/jeng209/Documents/h2o-genmodel.jar"", java_options = ""-Xmx256m -XX:ReservedCodeCacheSize=256m"" , verbose = FALSE)
[1] ""+ CMD: java -Xmx256m -XX:ReservedCodeCacheSize=256m -cp C:/Users/jeng209/Documents/h2o-genmodel.jar hex.genmodel.tools.PredictCsv --mojo C:\\Users\\jeng209\\Documents\\GBM_grid_0_AutoML_20180731_202910_model_3.zip --input C:\\Users\\jeng209\\AppData\\Local\\Temp\\RtmpSeY5TX/input.csv --output C:\\Users\\jeng209\\AppData\\Local\\Temp\\RtmpSeY5TX/prediction.csv --decimal""
[1] 0
[1] ""data.frame""
Warning message:
In dir.create(tmp_dir) :
'C:\Users\jeng209\AppData\Local\Temp\RtmpSeY5TX' already exists
> mojo_pred <- as.character(mojo_res$predict)
> mojo_pred
[1] ""versicolor""



Note the extra output after the `mojo_res' object creation. Putting this into a function results in the following:


> iris_pred <- function(sl, sw, pl, pw){
+ newdata <- data.frame(
+ Sepal.Length=sl
+ , Sepal.Width=sw
+ , Petal.Length=pl
+ , Petal.Width=pw
+ )
+ mojo_res <- invisible(h2o.mojo_predict_df(newdata, ""C:/Users/jeng209/Documents/GBM_grid_0_AutoML_20180731_202910_model_3.zip"", genmodel_jar_path = ""C:/Users/jeng209/Documents/h2o-genmodel.jar"", java_options = ""-Xmx256m -XX:ReservedCodeCacheSize=256m"" , verbose = FALSE))
+ mojo_pred <- as.character(mojo_res$predict)
+ return(mojo_pred)
+ }
> iris_pred(5,3,2,1)
[1] ""+ CMD: java -Xmx256m -XX:ReservedCodeCacheSize=256m -cp C:/Users/jeng209/Documents/h2o-genmodel.jar hex.genmodel.tools.PredictCsv --mojo C:\\Users\\jeng209\\Documents\\GBM_grid_0_AutoML_20180731_202910_model_3.zip --input C:\\Users\\jeng209\\AppData\\Local\\Temp\\RtmpSeY5TX/input.csv --output C:\\Users\\jeng209\\AppData\\Local\\Temp\\RtmpSeY5TX/prediction.csv --decimal""
[1] 0
[1] ""data.frame""
[1] ""versicolor""
Warning message:
In dir.create(tmp_dir) :
  'C:\Users\jeng209\AppData\Local\Temp\RtmpSeY5TX' already exists



I've yet to find a way to only return the value of 
versicolor
 from the function call. So far I've tried 
sink
 and 
invisible
, but they keep the prediction from being able to be retrieved.


Is their any known way to get around this issue and only retrieve the resulting dataframe from 
h2o.mojo_predict_df
 without all of the extra output being generated and stored?","['r', 'function', 'h2o']",JRW,https://stackoverflow.com/users/2918051/jrw,43
51711737,51711737,2018-08-06T16:11:13,2018-08-06 16:35:27Z,0,"I am trying to train deep learning neural nets on my AWS server using Python and H2O and I would like to enable GPU acceleration to speed up the training. Please let me know the code snippet to use GPU instead of CPU. AWS uses OpenGL. The elastic GPU type is eg1.xlarge with 4 GB memory.


The code for my model is:


nn = H2OGridSearch(model=H2ODeepLearningEstimator,
                               hyper_params = {
        'activation' :[ ""Rectifier"",""Tanh"",""Maxout"",""RectifierWithDropout"",""TanhWithDropout"",""MaxoutWithDropout""],
        'hidden':[[20,20],[50,50],[30,30,30],[25,25,25,25]],            ## small network, runs faster
#        'rate' :[0.0005,0.001,0.0015,0.002,0.0025,0.003,0.0035,0.0040,0.0045,0.005],
         'l1':[0,1e-4,1e-6],
         'l2':[0,1e-4,1e-6]
        })

start_time = time.time()

nn.train(
        train1_x, train1_y,train1,
        score_validation_samples = 10000,      ## sample the validation dataset (faster)
        stopping_rounds = 2,
        stopping_metric =""MSE"", ## alternatives: ""MSE"",""logloss"",""r2""
        epochs=1000000,  
        stopping_tolerance = 0.01,
        max_w2 = 10
        )
end_time = time.time()","['python', 'gpu', 'h2o']",Chandra,https://stackoverflow.com/users/6188922/chandra,69
51697330,51697330,2018-08-05T18:50:56,2018-08-16 19:49:15Z,0,"Final Edit:
 this problem ended up occurring because the target array were integers that were supposed to represent categories so it was doing a regression. Once I converted them into factors using 
.asfactor()
, then the confusion matrix method detailed in the answer below worked




I am trying to run a confusion matrix on my Random Forest Model (
my_model
), but the documentation has been less than helpful. From 
here
 it says the command is 
h2o.confusionMatrix(my_model)
 but there is no such thing in 3.0. 


Here are the steps to fit the model:


from h2o.estimators.random_forest import H2ORandomForestEstimator

data_h = h2o.H2OFrame(data)
train, valid = data_h.split_frame(ratios=[.7], seed = 1234)

my_model = H2ORandomForestEstimator(model_id = ""rf_h"", ntrees = 400, 
max_depth = 30, nfolds = 8, seed = 25)
my_model.train(x = features, y = target, training_frame=train)
pred = rf_h.predict(valid)



I have tried the following:


my_model.confusion_matrix()

AttributeError: type object 'H2ORandomForestEstimator' has no attribute 
'confusion_matrix'



Gotten from 
this example
.


I have attempted to use tab completion to find out what it might be and have tried:


h2o.model.confusion_matrix(my_model)

TypeError: 'module' object is not callable



and


h2o.model.ConfusionMatrix(my_model)



which outputs simply all the model diagnostics and then the error:


H2OTypeError: Argument `cm` should be a list, got H2ORandomForestEstimator 



Finally,


h2o.model.ConfusionMatrix(pred)



Which gives the same error as above.


Not sure what to do here, how can I view the results of the confusion matrix of the model?


Edit: Added more code to the beginning of the question for Context","['python', 'random-forest', 'h2o', 'confusion-matrix']",Unknown,,N/A
51688729,51688729,2018-08-04T19:03:41,2018-08-04 22:35:06Z,131,"i had succeeded in implementing a super-learner in H2o-ai and spark
but as per the second step super-learner utilizes a meta learning algorithm


Super-learner algorithm


1Set up the ensemble.
1.a Specify a list of L base algorithms (with a specific set of model parameters).
1.b Specify a metalearning algorithm


the complete algorithm is available at 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html


So for the same meta learning algorithm i had utilized a function 


val metaLearningModel= new H2ODeepLearning()(hc, spark.sqlContext)



And it seems that it is using an inbuilt package from h2o-ai so i want to know which meta learning algorithm it is using as default","['apache-spark', 'machine-learning', 'h2o', 'ensemble-learning']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
51681591,51681591,2018-08-04T00:56:30,2018-08-04 02:16:55Z,302,"I have been training a neural net with hyperparameters but am unable get results out as I am getting the following error message.
nn


Error message: 'int' object is not iterable


Code:


      nn = H2OGridSearch(model=H2ODeepLearningEstimator,
                                   hyper_params = {
            'activation' :[ ""Rectifier"",""Tanh"",""Maxout"",""RectifierWithDropout"",""TanhWithDropout"",""MaxoutWithDropout""],
            'hidden':[[20,20],[50,50],[30,30,30],[25,25,25,25]],            ## small network, runs faster
            'epochs':1000000,                      ## hopefully converges earlier...
            'rate' :[0.0005,0.001,0.0015,0.002,0.0025,0.003,0.0035,0.0040,0.0045,0.005],
            'score_validation_samples':10000,      ## sample the validation dataset (faster)
            'stopping_rounds':2,
            'stopping_metric':""misclassification"", ## alternatives: ""MSE"",""logloss"",""r2""
            'stopping_tolerance':0.01})
nn.train(train1_x, train1_y,train1)","['python-3.x', 'h2o']",Chandra,https://stackoverflow.com/users/6188922/chandra,69
51680643,51680643,2018-08-03T22:10:36,2018-08-04 00:28:57Z,0,"I am trying to use h2o automl feature using flow using diabetes dataset that I got from example flows, I am using the version 3.20.0.4 as of now. but the feature doesnot work and throws the below message.appreciate if you can help


""Error calling POSTJSON /99/AutoMLBuilder with opts {""input_spec"":{""training_frame"":""datase...


ERROR MESSAGE: Field not found: 'keep_cross_validation_fold_assignment' on object water.automl.api.schemas3.AutoMLBuildSpecV99$AutoMLBuildControlV99@32ca26fc""","['r', 'h2o', 'automl']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
51668826,51668826,2018-08-03T08:54:03,2018-08-04 00:07:47Z,610,"My classification is a 0-1 classification.So what exactly are the numbers at the bottom telling me [ why are they negative and all] ? Also what do the NAs mean? Do they say that this particular feature is not classified on a numeric comparison hence NA like BreedName?


Here are the columns of my data and 'L' is the 0-1 column i.e my y.



Ques2- This tree 0 is the main model right since i have done cv(n=3) and i have got 4 trees [0,1,2,3] and whatever i interpret should be based on the 0th tree right?","['python', 'classification', 'h2o', 'xgboost', 'gbm']",Unknown,,N/A
51665333,51665333,2018-08-03T04:25:38,2018-08-04 19:08:15Z,0,"I am facing an error while running this command in H2O Deep Learning in R:


model <- h2o.deeplearning(x = x, y = y, seed = 1234,
                          training_frame = as.h2o(trainDF),
                          nfolds = 3, 
                          stopping_rounds = 7, 
                          epochs = 400,
                          overwrite_with_best_model = TRUE,
                          activation = ""Tanh"",
                          input_dropout_ratio = .1,
                          hidden = c(10,10),
                          l1 = 6e-4,
                          loss = ""automatic"",
                          distribution = 'AUTO',
                          stopping_metric = ""MSE"")



ERROR as below:




Error in h2o.deeplearning(x = x, y = y, seed = 1234, training_frame = as.h2o(trainDF),  : 
    unused arguments (training_frame = as.h2o(trainDF), stopping_rounds = 7, overwrite_with_best_model = TRUE, distribution = ""AUTO"", stopping_metric = ""MSE"")","['r', 'h2o']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
51657527,51657527,2018-08-02T15:39:43,2018-08-03 00:42:36Z,0,"I'm trying to implement the 
FBeta_Score()
 of the 
MLmetrics
 
R package
:


FBeta_Score <- function(y_true, y_pred, positive = NULL, beta = 1) {
   Confusion_DF <- ConfusionDF(y_pred, y_true)
   if (is.null(positive) == TRUE) 
   positive <- as.character(Confusion_DF[1,1])
   Precision <- Precision(y_true, y_pred, positive)
   Recall <- Recall(y_true, y_pred, positive)
   Fbeta_Score <- (1 + beta^2) * (Precision * Recall) / (beta^2 * Precision + 
   Recall)
   return(Fbeta_Score)
 }



in the 
H2O distributed random forest model
 and I want to optimize it during the training phase using the 
custom_metric_func
 option.
The help documentation of the 
h2o.randomForest()
 function says: 




Reference to custom evaluation function, format:
  'language:keyName=funcName'




But I don't understand how to use it directly from R and what I should specify in the 
stopping_metric
 option. 


Any help would be appreciated!","['r', 'customization', 'package', 'random-forest', 'h2o']",FR_,https://stackoverflow.com/users/7856706/fr,147
51655007,51655007,2018-08-02T13:33:04,2018-08-02 20:09:39Z,0,"The error that i am getting is this. The subset[~100k examples] of my data has exactly the same number of columns as the original dataset [400k examples].But it runs perfectly on the original dataset but not on the subset.


Traceback (most recent call last)
<ipython-input-14-35cf02055a2e> in <module>()
     15 from h2o.estimators.gbm import H2OGradientBoostingEstimator
     16 gbm_cv3 = H2OGradientBoostingEstimator(nfolds=2)
---> 17 gbm_cv3.train(x=x, y=y, training_frame=train)
     18 ## Getting all cross validated models
     19 all_models = gbm_cv3.cross_validation_models()



error_count = 2
    http_status = 412
    msg = u'Illegal argument(s) for GBM model: 
GBM_model_python_1533214798867_179.  Details: ERRR on field: 
_response: Response cannot be constant.'
    dev_msg = u'Illegal argument(s) for GBM model: 
GBM_model_python_1533214798867_179.  Details: ERRR on field: 
_response: Response cannot be constant.'","['python', 'classification', 'h2o', 'xgboost', 'gbm']",James Z,https://stackoverflow.com/users/4420967/james-z,12.3k
51640086,51640086,2018-08-01T18:31:44,2020-07-22 19:07:57Z,0,"I have a binary classification problem, and I am using ""h2o.automl"" to obtain a model.


Is it possible to obtain a plot of the importances of my dataset features from the ""h2o.automl"" model?


A pointer to some python 3 code would be much appreciated.


Thanks.
Charles","['python-3.x', 'h2o', 'automl']",user274610,https://stackoverflow.com/users/274610/user274610,509
51614241,51614241,2018-07-31T13:12:07,2018-07-31 13:23:15Z,0,"I have established a deep learning model with the h2o package of the R software. I gained a model with good presence and I wanna to save it. However, I tried all kinds of methods but failed. The code ""save()"" and ""save.image()"" are provided in the base package of R software. I used the ""save()"" function to conserve my model. But when I want to use the built model to run new data, it is said that the ""model"" object is not found in the function. I am really confused about this problem for a few days. If you have any good ideas, just tell me. Thanks for your reading~


load(""F:/R/Rstudy/myfile"") ##download the saved file
library(h2o)
h2o.init()
Te <- read.csv(""F:/Rdata/Test.csv"") ##  import testing data
Te <- as.h2o(Te)
Te[,2] <- as.factor(Te[,2])
perf <- h2o.performance(model, Te) ## test model





ERROR: Unexpected HTTP Status code: 404 Not Found (url = 
http://localhost:54321/3/ModelMetrics/models/DeepLearning_model_R_1533035975237_1/frames/RTMP_sid_8185_2
)


ERROR MESSAGE:
   Object 'DeepLearning_model_R_1533035975237_1' not found in function: predict for  argument: model","['r', 'model', 'save', 'h2o']",phiver,https://stackoverflow.com/users/4985176/phiver,23.6k
51613450,51613450,2018-07-31T12:29:59,2018-09-28 03:38:33Z,213,"I am running Ubuntu 18.04 with CUDA 9.1. I have downloaded the CUDA 9.0 Driverless AI .deb installation from the H2O website's download page: 
https://www.h2o.ai/driverless-ai-download/


Unfortunately I am getting the following:


kevin@Ubuntu-XPS:~/Downloads$ sudo dpkg -i dai_1.1.4_amd64.deb 
[sudo] password for kevin: 
Selecting previously unselected package dai.
(Reading database ... 486543 files and directories currently installed.)
Preparing to unpack dai_1.1.4_amd64.deb ...
Unpacking dai (1.1.4) ...
Setting up dai (1.1.4) ...
User configuration file /etc/dai/User.conf already exists.
Group configuration file /etc/dai/Group.conf already exists.
Configured user in /etc/dai/User.conf is 'dai'.
Configured group in /etc/dai/Group.conf is 'dai'.
Group 'dai' already exists.
User 'dai' already exists.
Creating /opt/h2oai/dai/tmp...
Creating /opt/h2oai/dai/home...
Creating /opt/h2oai/dai/log... (Note for systemd users this log dir will be unused; use journalctl instead.)
Adding systemd configuration files in /etc/systemd/system...
Failed to lookup unit file state: Invalid argument
dpkg: error processing package dai (--install):
 installed dai package post-installation script subprocess returned error exit status 62
Errors were encountered while processing:
 dai



Could it be any of the following problems:




Driverless AI is not yet supported on Ubuntu 18.04?


Driverless AI is not yet supported on CUDA 9.1?


The .deb file ends in 
amd64
 but I have an Intel i7? (Although the Ubuntu 18.04 image file from the official downloads page also ends in 
amd64
)","['ubuntu', 'h2o', 'deb', 'driverless-ai']",talonmies,https://stackoverflow.com/users/681865/talonmies,72.2k
51612472,51612472,2018-07-31T11:36:41,2019-01-20 13:23:49Z,0,"I am running the h2o package in Rstudio, I am getting an error while converting Tibble into h2o.


Below is my code


#Augment Time Series Signature
PO_Data_aug = PO_Data %>%
  tk_augment_timeseries_signature()

PO_Data_aug


# Split into training, validation and test sets
train_tbl = PO_Data_aug %>% filter(Date <= '2017-12-29')
valid_tbl = PO_Data_aug %>% filter(Date>'2017-12-29'& Date <='2018-03-31')
test_tbl  = PO_Data_aug %>% filter(Date > '2018-03-31')
str(train_tbl)
train_tbl$month.lbl<-as.character(train_tbl$month.lbl)

h2o.init()        # Fire up h2o

##hex
train_h2o = as.h2o(train_tbl)
valid_h2o = as.h2o(valid_tbl)
test_h2o  = as.h2o(test_tbl)



ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/3/Parse)



ERROR MESSAGE:

Provided column type ordered is unknown.  Cannot proceed with parse due to invalid argument.



Kindly Suggest","['machine-learning', 'rstudio', 'h2o', 'h2o4gpu']",Rob,https://stackoverflow.com/users/162698/rob,15.1k
51606637,51606637,2018-07-31T06:22:36,2018-08-07 17:37:20Z,0,"I am trying to build a stacked ensemble using H2O in R. It has five base learners - Random Forest, XGBoost, GLM, GBM and Naive Bayes. It is a classification problem with three levels. The base learners ran successfully and returned accuracy values on a test dataset. 


When the base learners are used in h2o.stackedEnsemble, below error is returned:




Error: water.exceptions.H2OIllegalArgumentException: water.exceptions.H2OIllegalArgumentException: Don't know how to determine the distribution for a multinomial classifier.




Below is the code snippet for the stacked ensemble section:


ensemble <- h2o.stackedEnsemble(x = setdiff(colnames(trainPCA), 
                                    c(depVarsMulti,""weightage"")), #Names of indep vars
                                y = depVarsMulti, #dep var
                                training_frame = trainPCA,
                                model_id = ""123"",
                                base_models = c(ModelOneRF@model_id, 
 ModelTwoXGBoost@model_id,ModelThreeGLM@model_id,ModelFourGBM@model_id,ModelFiveBayes@model_id),
                                metalearner_algorithm = ""drf"",
                                metalearner_nfolds = nfolds)



Additonal details:




I am able to build stacked ensemble models similar to the grid example given 
here


H2O Version: ""3.21.0.4359"" | R Version: ""3.4.1 (2017-06-30)""


The H2O cluster is a local one




EDIT(Aug 3, 2018):


As suggested by Darren, I am adding a script that reproduces the problem using an open dataset Cars93 (from package CARS)


#######################################################################
# Minimum reproducible example for Stackoverflow
#######################################################################

# R version: 3.4.4 (2018-03-15)
# H2O cluster version: 3.21.0.4376
#OS: Linux (Azure Data Science VM)

#Installing and loading necessary libraries
cat(""\n Installing and loading necessary libraries \n"")
libsNeeded <- c(""dplyr"", ""data.table"", ""randomForest"", ""stringr"",""doParallel"", ""parallel"", ""doSNOW"", ""rlang"", ""nlme"", ""MASS"", ""survival"", ""stringi"", ""dummies"", ""missRanger"",""cluster"", ""e1071"",""xgboost"",""ranger"", ""caret"")
if(length(setdiff(libsNeeded, rownames(installed.packages()))) > 0){
  install.packages(setdiff(libsNeeded, rownames(installed.packages())))
}
lapply(libsNeeded, require, character.only = T)

#Installing latest H2O if not done already:
# install.packages(""h2o"", type=""source"", repos=(c(""http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R"")))
library(h2o)
#Starting an H2O cluster
h2o.init(max_mem_size = ""23g"")


library(CARS)
dataFrame <- Cars93

#Removing rows where Passengers = 8 or 7 or 2, as their occurence frequency is low and for demonstration purpose, want to avoid errors coming because of this reason
dataFrame <-  dataFrame[!(dataFrame$Passengers %in% c(""2"", ""7"", ""8"")),]

#Making the dependent variable as factor
dataFrame$Passengers <-  as.factor(dataFrame$Passengers)

#Defining the variables to be used in modeling
depVars <- ""Passengers""
indepNumVars <- c(""Price"",""MPG.highway"",""EngineSize"",""Horsepower"")
indepFactVars <- c(""AirBags"",""Type"")

#Keeping only columns of interest
dataFrame <- dataFrame[,c(indepFactVars,indepNumVars,depVars)]

#Converting dependent variables into dummy variables:
dataFrame <- dummy.data.frame(dataFrame, names=colnames(dataFrame[,indepFactVars]), sep=""_"")
names(dataFrame) <- gsub("" "", ""_"", names(dataFrame))


#Creating the train and test datasets
trainIndex <- createDataPartition(dataFrame[,depVars], times = 1, p = 0.75)
trainingData <- dataFrame[trainIndex$Resample1,]
testingData <- dataFrame[-trainIndex$Resample1,]


# H2O Frames
train <- as.h2o(trainingData)
test <- as.h2o(testingData)

# Perform PCA
depData <- train[, depVars]


train <- train[, setdiff(names(train), c(depVars))]

pca_model <- h2o.prcomp(training_frame = train,
                        model_id = NULL,
                        ignore_const_cols = TRUE,
                        transform = ""STANDARDIZE"",
                        pca_method = ""GramSVD"",
                        k = 10,
                        max_iterations = 5000,
                        seed = -1,
                        score_each_iteration = TRUE,
                        use_all_factor_levels = FALSE,
                        compute_metrics = TRUE,
                        max_runtime_secs = 0,
                        impute_missing = T)

cum_prop <- pca_model@model$model_summary[""Cumulative Proportion"", ]

# print(cum_prop)

cum_prop_to_consider <- length(cum_prop[cum_prop < .95]) + 1

cat(""\n\n Number of principal components that explain 95% variance = "",cum_prop_to_consider,""\n\n"")

trainPCA <- h2o.predict(pca_model, train)
if(cum_prop_to_consider > ncol(trainPCA)){
  trainPCA <- trainPCA[, 1:(cum_prop_to_consider - 1)] 
}else{
  trainPCA <- trainPCA[, 1:cum_prop_to_consider]
}

# pca_data <- as.data.table(pca_data)
trainPCA[, depVars] <- depData[, depVars]

#Preparing the test data:
testPCA <- h2o.predict(pca_model,test)
if(cum_prop_to_consider > ncol(testPCA)){
  testPCA <- testPCA[, 1:(cum_prop_to_consider - 1)] 
}else{
  testPCA <- testPCA[, 1:cum_prop_to_consider]
}
testPCA[, depVars] <- test[, depVars]

# For binary classification, response should be a factor
trainPCA[,depVars] <- as.factor(trainPCA[,depVars])
testPCA[,depVars] <- as.factor(test[,depVars])

#Weights of the training data:
trainPCA$weightage <- ifelse(trainPCA[,depVars] == ""5"", 1, ifelse(trainPCA[,depVars] == ""4"", 2, ifelse(trainPCA[,depVars] == ""6"", 2,1)))

# Number of CV folds (to generate level-one data for stacking)
nfolds <- 5


####################################################################################################
# Stacked Ensemble modeling
####################################################################################################

modelIteration <- Sys.Date()
modelIteration <- gsub(""-"", ""_"", modelIteration)
i = ""withInsp""

# Train & Cross-validate a RF
ModelOneRF <- h2o.randomForest(x = setdiff(colnames(trainPCA),depVars),
                               y = depVars,
                               training_frame = trainPCA,
                               ntrees = 15,
                               nfolds = nfolds,
                               fold_assignment = ""Stratified"",
                               max_depth = 30,
                               min_rows = 1,
                               mtries = 3,
                               keep_cross_validation_predictions = TRUE,
                               seed = 1,
                               # verbose = T,
                               weights_column = ""weightage"",
                               model_id = paste0(i,""_ModelOneRF_"",modelIteration))
cat(""\n\n Mean accuracy of Random Forest Model (on cross validation):"",ModelOneRF@model$cross_validation_metrics_summary[1,1],""\n\n"")
perf_RF <- h2o.performance(model = ModelOneRF, newdata = testPCA)
cat(""\n\n Accuracy of Random Forest Model (on test data):"",1 - perf_RF@metrics$mean_per_class_error,""\n\n"")


# Train & Cross-validate a XGBoost
ModelTwoXGBoost <- h2o.xgboost(x = setdiff(colnames(trainPCA),depVars),
                               y = depVars,
                               training_frame = trainPCA,
                               nfolds = nfolds,
                               fold_assignment = ""Stratified"",
                               weights_column = ""weightage"",
                               ntrees = 15,
                               max_depth = 20,
                               min_rows = 1,
                               learn_rate = 0.1,
                               eta = 0.3,
                               keep_cross_validation_predictions = TRUE,
                               seed = 1,
                               # verbose = T,
                               model_id = paste0(i,""_ModelTwoXGBoost_"",modelIteration))
cat(""\n\n Mean accuracy of XGBoost Model (on cross validation):"",ModelTwoXGBoost@model$cross_validation_metrics_summary[1,1],""\n\n"")
perf_XGBoost <- h2o.performance(model = ModelTwoXGBoost, newdata = testPCA)
cat(""\n\n Accuracy of XGBoost Model (on test data):"",1 - perf_XGBoost@metrics$mean_per_class_error,""\n\n"")

#Train and cross validate a Generalized Linear Model (GLM)
ModelThreeGLM <- h2o.glm(family= ""multinomial"",
                         x = setdiff(colnames(trainPCA),depVars),
                         y = depVars,
                         training_frame = trainPCA,
                         nfolds = nfolds,
                         fold_assignment = ""Stratified"",
                         weights_column = ""weightage"",
                         alpha = 0.0,
                         lambda_search = T,
                         standardize = T,
                         seed = 1,
                         # verbose = T,
                         model_id = paste0(i,""_ModelThreeGLM_"",modelIteration),
                         keep_cross_validation_predictions = TRUE)

cat(""\n\n Mean accuracy of GLM Model (on cross validation):"",ModelThreeGLM@model$cross_validation_metrics_summary[1,1],""\n\n"")
perf_GLM <- h2o.performance(model = ModelThreeGLM, newdata = testPCA)
cat(""\n\n Accuracy of GLM Model (on test data):"",1 - perf_GLM@metrics$mean_per_class_error,""\n\n"")

#Train and cross validate a Gradient Boosting Machine (GBM)
ModelFourGBM <- h2o.gbm(x = setdiff(colnames(trainPCA),depVars),
                        y = depVars,
                        training_frame = trainPCA,
                        nfolds = nfolds,
                        fold_assignment = ""Stratified"",
                        weights_column = ""weightage"",
                        ntrees = 10,
                        max_depth = 20,
                        seed = 1,
                        learn_rate = 0.05,
                        learn_rate_annealing = 0.99,
                        # verbose = T,
                        keep_cross_validation_predictions = TRUE,
                        model_id = paste0(i,""_ModelFourGBM_"",modelIteration))
cat(""\n\n Mean accuracy of GBM Model (on cross validation):"",ModelFourGBM@model$cross_validation_metrics_summary[1,1],""\n\n"")
perf_GBM <- h2o.performance(model = ModelFourGBM, newdata = testPCA)
cat(""\n\n Accuracy of GBM Model (on test data):"",1 - perf_GBM@metrics$mean_per_class_error,""\n\n"")

#Train and cross validate a NaÃ¯ve Bayes Model
ModelFiveBayes <- h2o.naiveBayes(x = setdiff(colnames(trainPCA),c(depVars,""weightage"")),
                                 y = depVars,
                                 training_frame = trainPCA,
                                 nfolds = nfolds,
                                 fold_assignment = ""Stratified"",
                                 # weights_column = ""weightage"",
                                 seed = 1,
                                 # verbose = T,
                                 keep_cross_validation_predictions = TRUE,
                                 model_id = paste0(i,""_ModelFiveBayes_"",modelIteration))

cat(""\n\n Mean accuracy of Naive Bayes Model (on cross validation):"",ModelFiveBayes@model$cross_validation_metrics_summary[1,1],""\n\n"")
perf_Bayes <- h2o.performance(model = ModelFiveBayes, newdata = testPCA)
cat(""\n\n Accuracy of Naive Bayes Model (on test data):"",1 - perf_Bayes@metrics$mean_per_class_error,""\n\n"")



# Train a stacked ensemble using the GBM and RF above
ensemble <- h2o.stackedEnsemble(x = setdiff(colnames(trainPCA),c(depVars,""weightage"")),
                                y = depVars,
                                training_frame = trainPCA,
                                # model_id = paste0(i,""_ModelEnsemble_"",modelIteration),
                                model_id = paste0(i,""_ModelEnsemble_2_"",modelIteration),
                                base_models = c(ModelOneRF@model_id, ModelTwoXGBoost@model_id,ModelThreeGLM@model_id,ModelFourGBM@model_id,ModelFiveBayes@model_id),
                                metalearner_algorithm = ""drf"",
                                metalearner_nfolds = nfolds)","['r', 'h2o', 'multinomial']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
51605417,51605417,2018-07-31T04:21:57,2019-07-03 13:25:54Z,0,"I can't get the h2o to work in my R. It shows the following error. Have no clue what it means. Previously it gave me an error because I didn't have Java 64 bit version. I downloaded the 64bit - restarted my pc - and started the process again and now it gives me this error.


Any suggestions?


> library(h2o) 
>h2o.init()



    `H2O is not running yet, starting it now.`..

        Note:  In case of errors look at the following log files:
            C:\Users\E0475878\AppData\Local\Temp\RtmpieqnRc/h2o_E0475878_started_from_r.out


C:\Users\E0475878\AppData\Local\Temp\RtmpieqnRc/h2o_E0475878_started_from_r.err

    java version ""1.6.0_26""
    Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
    Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)

    Starting H2O JVM and connecting: ............................................................ 
    [1] ""localhost""
    [1] 54321
    [1] TRUE
    [1] -1
    **[1] ""Failed to connect to localhost port 54321: Connection refused""**
    [1] 127
    **

    Error in h2o.init(ip = ""localhost"", port = 54321) : 
      H2O failed to start, stopping execution.
    In addition: Warning message:
    running command 'curl 'http://localhost:54321'' had status 127

** 



Please advice","['machine-learning', 'rstudio', 'h2o', 'h2o4gpu']",Rob,https://stackoverflow.com/users/162698/rob,15.1k
51602029,51602029,2018-07-30T20:45:27,2018-07-30 22:37:23Z,0,"I am trying to import a pandas dataframe into a h2o frame and specify the column types that I want. The problem is am eventually trying to do an .rbind() with two datasets, but sometimes depending on the values of certain columns h2o will force them to either real or int, and then they cant perform .rbind() because the column types are different. I want to make sure I can get two different datasets with the same column types so these failures done happen. 


Reproducible example below:


import pandas as pd
import h2o

my_df1 = pd.DataFrame({'a':[1,1,0,0,1],
                       'b':[1,0,.5,.2,0]})

my_df2 = pd.DataFrame({'a':[.5,.8,0,0,1],
                       'b':[1,0,.5,.2,0]})

h2o.init()
my_h2o1 = h2o.H2OFrame(my_df1)
my_h2o2 = h2o.H2OFrame(my_df2)

my_h2o1.rbind(my_h2o2)  ### This fails

### try to manually specify the column names and types
col_names = [k for k in my_h2o1.types.keys()]
col_types = [v for v in my_h2o1.types.values()]

my_h2o3 = h2o.H2OFrame(my_df2,column_names=col_names, column_types=col_types)

my_h2o1.types.values() == my_h2o3.types.values()

my_h2o1.rbind(my_h2o3)  ### This fails still","['python', 'h2o']",Nate Thompson,https://stackoverflow.com/users/4008123/nate-thompson,635
51584970,51584970,2018-07-29T22:10:28,2018-07-30 16:46:45Z,0,"The CRAN implementation of random forests offers both variable importance measures: the Gini importance as well as the widely used 
permutation importance
 defined as 




For classification, it is the increase in percent of times a case is
  OOB and misclassified when the variable is permuted. For regression,
  it is the average increase in squared OOB residuals when the variable
  is permuted




By default 
h2o.varimp()
 computes only the former. Is there really no option in h2o to get the alternative measure out of a random forest model?


Thanks!
ML","['random-forest', 'h2o']",Markus Loecher,https://stackoverflow.com/users/1062645/markus-loecher,378
51583633,51583633,2018-07-29T19:06:45,2018-07-30 20:14:24Z,652,"Is it possible to kick off h2o's AutoML to train and evaluate models in parallel either locally or on a cluster, using something like joblib or dask?",['h2o'],Unknown,,N/A
51566988,51566988,2018-07-28T00:21:28,2018-07-29 07:07:47Z,68,How do I set up a backend for Deepwater (h2o) on Ubuntu 16.04? The GPUs I am using are AMD Radeon RX Vega. Anyone experienced in this topic? Do you need further information? Most explanations and procedures described here and elsewhere seem to refer to NVIDIA cards.,['h2o'],Unknown,,N/A
51561719,51561719,2018-07-27T16:07:28,2018-07-30 19:36:50Z,0,"I use the h2o package in R (Windows machine) and can't initialize it by h2o.init after updating the package recently. 


The error message is produced below. I guess the cause of the problem is that h2o xgboost doesn't support Windows (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html
). Is my guess correct? If so, how can I ask the h2o package to exclude xgboost in initialization to avoid the error?


Thanks a lot in advance. 


java version ""1.7.0_51""
Java(TM) SE Runtime Environment (build 1.7.0_51-b30)
Java HotSpot(TM) Client VM (build 24.51-b03, mixed mode)

Starting H2O JVM and connecting: ............................................................Cannot load library from path lib/windows_32/xgboost4j_gpu.dll
Cannot load library from path lib/xgboost4j_gpu.dll
Failed to load library from both native path and jar!
Cannot load library from path lib/windows_32/xgboost4j_omp.dll
Cannot load library from path lib/xgboost4j_omp.dll
Failed to load library from both native path and jar!
Cannot load library from path lib/windows_32/xgboost4j_minimal.dll
Cannot load library from path lib/xgboost4j_minimal.dll
Failed to load library from both native path and jar!
07-27 11:24:19.197 127.0.0.1:54321       14488  main      INFO: Cannot initialize XGBoost backend! Xgboost (enabled GPUs) needs: 
07-27 11:24:19.197 127.0.0.1:54321       14488  main      INFO:   - CUDA 8.0
07-27 11:24:19.197 127.0.0.1:54321       14488  main      INFO: XGboost (minimal version) needs: 
07-27 11:24:19.197 127.0.0.1:54321       14488  main      INFO:   - GCC 4.7+
07-27 11:24:19.197 127.0.0.1:54321       14488  main      INFO: For more details, run in debug mode: `java -Dlog4j.configuration=file:///tmp/log4j.properties -jar h2o.jar`
07-27 11:24:19.230 127.0.0.1:54321       14488  main      INFO: ----- H2O started  -----
07-27 11:24:19.230 127.0.0.1:54321       14488  main      INFO: Build git branch: rel-wright
07-27 11:24:19.230 127.0.0.1:54321       14488  main      INFO: Build git hash: 24410cf788602f231fd699037bdaff0ad20b4d1a
07-27 11:24:19.230 127.0.0.1:54321       14488  main      INFO: Build git describe: jenkins-3.20.0.1-26-g24410cf
07-27 11:24:19.230 127.0.0.1:54321       14488  main      INFO: Build age: 1 month and 11 days
07-27 11:24:19.230 127.0.0.1:54321       14488  main      INFO: Built by: 'jenkins'
07-27 11:24:19.230 127.0.0.1:54321       14488  main      INFO: Built on: '2018-06-15 18:39:02'
07-27 11:24:19.230 127.0.0.1:54321       14488  main      INFO: Watchdog Build git branch: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: Watchdog Build git hash: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: Watchdog Build git describe: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: Watchdog Build project version: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: Watchdog Built by: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: Watchdog Built on: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: KrbStandalone Build git branch: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: KrbStandalone Build git hash: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: KrbStandalone Build git describe: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: KrbStandalone Build project version: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: KrbStandalone Built by: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: KrbStandalone Built on: (unknown)
07-27 11:24:19.231 127.0.0.1:54321       14488  main      INFO: Processed H2O arguments: [-name, H2O_started_from_R_534474366_nnz606, -ip, 127.0.0.1, -port, 54321, -ice_root, C:/Users/534474~1/AppData/Local/Temp/RtmpCc9kQI]
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: Java availableProcessors: 4
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: Java heap totalMemory: 15.5 MB
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: Java heap maxMemory: 989.9 MB
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: Java version: Java 1.7.0_51 (from Oracle Corporation)
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: JVM launch parameters: [-Xmx1g, -ea]
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: OS version: Windows 7 6.1 (x86)
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: Machine physical memory: 4.00 GB
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: X-h2o-cluster-id: 1532705056352
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: User name: '534474366'
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: IPv6 stack selected: false
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: Possible IP Address: lo (Software Loopback Interface 1), 127.0.0.1
07-27 11:24:19.232 127.0.0.1:54321       14488  main      INFO: Possible IP Address: lo (Software Loopback Interface 1), 0:0:0:0:0:0:0:1
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:net0 (WAN Miniport (SSTP))
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:net1 (WAN Miniport (L2TP))
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:net2 (WAN Miniport (PPTP))
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:ppp0 (WAN Miniport (PPPOE))
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth0 (WAN Miniport (IPv6))
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth1 (WAN Miniport (Network Monitor))
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth2 (WAN Miniport (IP))
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:ppp1 (RAS Async Adapter)
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:net3 (WAN Miniport (IKEv2))
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Possible IP Address: eth3 (Intel(R) Ethernet Connection I217-LM), 10.95.108.135
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth4 (Intel(R) Ethernet Connection I217-LM-McAfee NDIS Light-Weight Filter-0000)
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth5 (Intel(R) Ethernet Connection I217-LM-WFP LightWeight Filter-0000)
07-27 11:24:19.233 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth6 (Intel(R) Ethernet Connection I217-LM-QoS Packet Scheduler-0000)
07-27 11:24:19.234 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth7 (WAN Miniport (Network Monitor)-McAfee NDIS Light-Weight Filter-0000)
07-27 11:24:19.234 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth8 (WAN Miniport (Network Monitor)-QoS Packet Scheduler-0000)
07-27 11:24:19.234 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth9 (WAN Miniport (IP)-McAfee NDIS Light-Weight Filter-0000)
07-27 11:24:19.234 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth10 (WAN Miniport (IP)-QoS Packet Scheduler-0000)
07-27 11:24:19.234 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth11 (WAN Miniport (IPv6)-McAfee NDIS Light-Weight Filter-0000)
07-27 11:24:19.234 127.0.0.1:54321       14488  main      INFO: Network interface is down: name:eth12 (WAN Miniport (IPv6)-QoS Packet Scheduler-0000)
07-27 11:24:19.234 127.0.0.1:54321       14488  main      FATAL: On /127.0.0.1 some of the required ports 54321, 54322 are not available, change -port PORT and try again. 
[1] ""127.0.0.1""
[1] 54321
[1] TRUE
[1] -1
[1] ""Failed to connect to 127.0.0.1 port 54321: Connection refused""
curl: (1) Protocol ""'http"" not supported or disabled in libcurl
[1] 1","['r', 'h2o', 'xgboost']",Zhongfang He,https://stackoverflow.com/users/10145307/zhongfang-he,1
51543937,51543937,2018-07-26T16:52:38,2018-08-01 19:07:47Z,474,"Is there a way to list the expected data type for each feature for an existing h2o model, without looking at the training data? 


Similar to the output of 
h2o_frame.types
, but without looking at the training data just looking at the model object. 


In this case I have a saved model file (the generic type that can be imported back into h2o, not mojo/pojo). The algorithm is gbm.",['h2o'],Unknown,,N/A
51543158,51543158,2018-07-26T16:02:56,2018-07-26 20:58:08Z,230,"I understand that 
sklearn
 requires categorical features to be encoded to dummy variables or one-hot encoded when running the 
sklearn.ensemble.RandomForestRegressor
 method, and that 
XGBoost
 requires the same, but 
h2o
 permitted raw categorical features to be used in its 
h2o.estimators.random_forest.H2ORandomForestEstimator
 method. Since 
h2o4gpu
's implementation of random forest is built on top of 
XGBoost
, does this mean support for raw categorical features is not included?","['python', 'scikit-learn', 'h2o', 'h2o4gpu']",S.Kumar,https://stackoverflow.com/users/7910606/s-kumar,23
51527546,51527546,2018-07-25T20:58:15,2018-08-17 17:08:50Z,135,"I created a model using a python script and after that, I run the following instructions:


print(rfmodel.model_performance(test_data=train).r2())
print(rfmodel.model_performance(test_data=test).r2())



Then, I get: 


0.8875126069766902
0.7730216155314876



If I load the same model in Flow, in Training Metrics I see r2=0.776610.


What´s the difference between python first instruction r2 and Flow r2?","['python', 'h2o']",Mauro Assis,https://stackoverflow.com/users/3310237/mauro-assis,435
51478588,51478588,2018-07-23T12:16:49,2018-07-25 21:33:34Z,0,"I am attempting to install the h2o4gpu Python module as per the instructions listed here: 
https://github.com/h2oai/h2o4gpu/issues/464


pip says that it successfully installed all packages, including h2o4gpu-0.1.0.


However I then still get 


    import h2o4gpu
ImportError: No module named h2o4gpu



and 


kevin@Ubuntu-XPS:~/Downloads$ pip show tensorflow-gpu
Name: tensorflow-gpu
Version: 1.9.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: 
[email protected]

License: Apache 2.0
Location: /usr/local/lib/python2.7/dist-packages
Requires: grpcio, mock, protobuf, enum34, gast, wheel, absl-py, backports.weakref, termcolor, six, numpy, tensorboard, setuptools, astor
kevin@Ubuntu-XPS:~/Downloads$ pip show h2o4gpu
kevin@Ubuntu-XPS:~/Downloads$ 



thus showing that pip acknowledges that 
tensorflow-gpu
 is installed, but not 
h2o4gpu
.


I am running Ubuntu 18.04 - could the cause of this be that h2o4gpu isn't yet supported on version 18? 
import h2o
 works fine.","['python', 'pip', 'h2o', 'h2o4gpu']",KOB,https://stackoverflow.com/users/4564080/kob,"4,495"
51471526,51471526,2018-07-23T04:20:07,2018-10-18 05:39:25Z,0,"I am following the installation guide for Sparkling Water but it does not work at all. It consists of 8 steps as you can see in: 
rsparkling






First problem from  Step 2 install an old version of sparklyr 
(not compatible with Spark 2.3.1)
, solved using 
install.packages(""https://github.com/rstudio/sparklyr/archive/v0.8.0.tar.gz"", repos = NULL, type=""source"")


Step 3, version 2.3.1 of Spark is not available as shown by the command 
sparklyr::spark_available_versions() #2.3.0
. Solved installing directly from the page 
Apache Spark
.


Step 6 does not work, install an unsupported version of 
rsparkling
 with h2o, 
packageVersion(""h2o"") #'3.21.0.4359'




I'm trying to do the following, download the latest version of sparkling water, unzip the file. And use the following code:


install.packages(""C:/Users/USER/Downloads/sparkling-water-2.3.259_nightly/rsparkling.tar.gz"", repos=NULL, type=""source"")
* installing *source* package 'rsparkling' ...
** package 'rsparkling' successfully unpacked and MD5 sums checked
** R
** inst
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
* DONE (rsparkling)
In R CMD INSTALL



Up to here everything seems fine.


options(rsparkling.sparklingwater.version = ""2.3.259_nightly"")
library(rsparkling)
# 7. Connect to Spark
sc <- sparklyr::spark_connect(master = ""local"")
Error: invalid version specification ‘2.3.259_nightly’





Error: invalid version specification ‘2.3.259_nightly’




Note:

Download 
Sparkling Water Nightly Bleeding Edge
 version.
The packages h2o, SparkR, sparklyr and the connections work correctly on windows 7 R version 3.4.4, I only have problems with 
rsparkling
.


system('spark-submit --version')
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.1
      /_/

Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_151
Branch 
Compiled by user vanzin on 2018-06-01T20:37:04Z



As I can solve this problem, I have installed the appropriate version of 
rsparkling
, compatible with the latest version of 
h2o
.


Edit question:
 Well Lauren thanks for the links, now I'm working with the latest stable version of h2o 3.20.0.5 and spparkling water. But apparently I think the problem will not be with the rsparkling package, but with the sparklyr package, as the last version of apache spark 2.3.1 was released (Jun 08 2018), while the latest update of sparklyr 0.8.4 was (May 25 2018) that is, it was launched a month earlier (
spark 2.3.1 did not exist
). Therefore the command:


spark_available_versions()
   spark
1  1.6.3
2  1.6.2
3  1.6.1
4  1.6.0
5  2.0.0
6  2.0.1
7  2.0.2
8  2.1.0
9  2.1.1
10 2.2.0
11 2.2.1
12 2.3.0

# Set spark connection
sc <- spark_connect(master = ""local"", version = ""2.3.1"") #It does not work
Error in spark_install_find(version, hadoop_version, latest = FALSE, hint = TRUE) : 
Spark version not installed. To install, use spark_install(version = ""2.3.1"")
spark_install(version = ""2.3.1"")
Error in spark_install_find(version, hadoop_version, installed_only = FALSE,  : 
Spark version not available. Find available versions, using spark_available_versions()
sc <- spark_connect(master = ""local"") #it works perfectly



I think the solution will be waiting for sparklyr 0.9.0","['r', 'apache-spark', 'rstudio', 'h2o']",Unknown,,N/A
51469394,51469394,2018-07-22T21:26:39,2021-05-24 22:54:10Z,593,"I am trying to write a java wrapper to use my h2o mojo model. When I load my model zip files in, I receive a null pointer exception. Below is a sample of my code:


public static void main(String[] args) throws Exception {
    EasyPredictModelWrapper predict_model = new EasyPredictModelWrapper(
      MojoModel.load(""prediction_football_model.zip""));
    EasyPredictModelWrapper class_model = new EasyPredictModelWrapper(
      MojoModel.load(""classification_football_model.zip""));   
  }



and error Message:


Exception in thread ""main"" java.lang.NullPointerException: entry
    at java.util.zip.ZipFile.getInputStream(ZipFile.java:346)
    at hex.genmodel.ZipfileMojoReaderBackend.getTextFile(ZipfileMojoReaderBackend.java:18)
    at hex.genmodel.ModelMojoReader.parseModelInfo(ModelMojoReader.java:154)
    at hex.genmodel.ModelMojoReader.readFrom(ModelMojoReader.java:27)
    at hex.genmodel.MojoModel.load(MojoModel.java:35)
    at GamePrediction.main(GamePrediction.java:52)



I have been working on this code in eclipse and I have placed both of the zip files in the main project folder. I created the models in R using the h2o.download_mojo() function. I looked into the error message and found that my ZipEntry for the given path's were Null, but I could not find a solution to that.


Any help or insight would be helpful. I couldn't find any other issues like this so if this is a duplicate, please point me to the right direction!","['java', 'nullpointerexception', 'data-science', 'h2o']",Andrew Thompson,https://stackoverflow.com/users/418556/andrew-thompson,169k
51453164,51453164,2018-07-21T05:40:09,2018-07-22 00:20:31Z,0,"When using:


""keep_cross_validation_predictions"": True
""keep_cross_validation_fold_assignment"": True



in H2O's XGBoost Estimator, I am not able to map these cross validated probabilities back to the original dataset.  There is one documentation example for 
R
 but not for Python (combining holdout predictions).


Any leads on how to do this in Python?","['python', 'h2o', 'xgboost']",Community,https://stackoverflow.com/users/-1/community,1
51432797,51432797,2018-07-19T22:57:59,2021-01-01 09:33:36Z,0,"I want to be able to use R's NeuralNetTools tools library to plot the network layout of a h2o deep neural network. Below is a sample code that plots the network layout of the model from the neural net package.


library(NeuralNetTools)
library(neuralnet)
data(neuraldat)
wts_in <- neuralnet(Y1 ~ X1 + X2 + X3, data = neuraldat, hidden = c(4), 
rep=1)
plotnet(wts_in)



I want to do the same thing but use H2o deep neural model. The code shows how to generate a layout by only knowing the number of layers and weight structure. 


library(NeuralNetTools)
# B1-H11, I1-H11, I2-H11, B1-H12, I1-H12, I2-H12, B2-H21, H11-H21, H12-H21, 
# B2-H22, H11-H22, H12-H22, B3-O1, H21-O1, H22-O1 
wts_in <- c(1.12, 1.49, 0.16, -0.11, -0.19, -0.16, 0.5, 0.2, -0.12, -0.1, 
        0.89, 0.9, 0.56, -0.52, 0.81)
struct <- c(2, 2, 2, 1) # two inputs, two (two nodes each), one output
x_names<-c(""No"",""Yes"") #Input Variable Names
y_names<-c(""maybe"") #Output Variable Names
plotnet(wts_in, struct=struct)



Below is the above neuralnet model but I have used H2o to generate it. I’m stumped on how to get the number of layers.


library(h2o)
h2o.init()
neuraldat.hex <- as.h2o(neuraldat)
h2o_neural_model<-h2o.deeplearning(x = 1:4, y = 5,
             training_frame= neuraldat.hex, 
             hidden=c(2,3),
             epochs = 10, 
             model_id = NULL)

 h2o_neural_model@model



I can use the weights #h2o.weights(object, matrix_id = 1) and bias function #h2o.biases(object, vector_id = 1) to build the structure but I need it to determine the number layers. I know I can specify the number layers in the model to start with but I sometimes write code that will determine the number of layers going into the model and so I need to a function determine the layers in network structure and weights for the plotnet() function below. 


  plotnet(wts_in, struct=struct)



As an alternative, it would be nice if I had a ggplot2 function instead of the plotnet() function. 


Any help is greatly appreciated.","['r', 'ggplot2', 'h2o']",jhwatts,https://stackoverflow.com/users/4468348/jhwatts,51
51405410,51405410,2018-07-18T15:07:08,2018-07-18 23:02:21Z,588,"I keep running out of memory building Random Forest models in H2O in Jupyter notebook. I find that with my 20 GB memory instance, I can build approximately two 50-tree models with 10-fold cross validation (22 total models) before it throws the ""tree model will not fit in driver node's memory"" exception. With a for loop, I can remove the cross validation models after the metrics have been calculated and displayed, but with GridSearch, there doesn't seem to be any way to drop the extraneous CV models while searching. Is this the case? Is there any sort of workaround? (I can always increase the amount of memory allocated, but that ultimately will be a finite value running on my local machine). Does anyone have any tips for GridSearch and limited memory? Thanks.",['h2o'],yaolipan,https://stackoverflow.com/users/10100418/yaolipan,13
51377739,51377739,2018-07-17T09:25:00,2018-07-22 00:59:33Z,628,"I am currently trying to run an H2O Server Instance on a external ssh Machine(Linux) from Python.


But I get the following Error when running 
h2o.init()
:


H2OConnectionError: Could not establish link to the H2O cloud http://127.0.0.1:54321 after 20 retries
[02:43.11] H2OServerError: HTTP 403 Forbidden:



So the Server is running and the Error fires at testing the Connection. I think this has something to do with Proxy-Settings. But I don't know how and where to configure.


Any help?","['linux', 'ssh', 'proxy', 'http-status-code-403', 'h2o']",Unknown,,N/A
51376724,51376724,2018-07-17T08:35:10,2018-07-22 01:04:23Z,81,"After some investigation, I have found out that the sparkling water H2O flow UI has a very limited set of plots - just Box plots, and distributions, for data visualization in Scala.


But if I want to use a third party library (
need recommendations on this
, I have already checked the Scala-charts library), how would I embed the generated plots in the H2O flow UI itself?","['scala', 'h2o', 'sparkling-water']",halfer,https://stackoverflow.com/users/472495/halfer,20.4k
51373908,51373908,2018-07-17T05:37:05,2018-07-18 23:18:24Z,538,"I'm trying to build a web app which lets users to upload a data set , select an h2o algorithm of their choice and train their data set to build a model.
Now, if multiple users will hit the server simultaneously, h2o starts processing the requests( starts training the models) parallelly. This increases the training time for all the users.
So, what is the best way in which I should use h2o in this scenario?
Should I keep those requests in a queue and execute them one by one? (this will add a waiting time for a new user)


I ran a GLM model for classification on a 50 MB structured and clean data set. It took almost twice the time when I executed two api calls simultaenously (model 1 and 2) than when I ran it individually (model 3)","['machine-learning', 'deep-learning', 'data-science', 'h2o']",Shubham Gupta,https://stackoverflow.com/users/8380096/shubham-gupta,17
51367755,51367755,2018-07-16T18:06:30,2021-03-26 12:45:43Z,0,"I am currently using H2O for a classification problem dataset. I am testing it out with 
H2ORandomForestEstimator
 in a python 3.6 environment. I noticed the results of the predict method was giving values between 0 to 1(I am assuming this is the probability). 


In my data set, the target attribute is numeric i.e. 
True
 values are 1 and 
False
 values are 0. I made sure I converted the type to category for the target attribute, I was still getting the same result. 


Then I modified to the code to convert the target column to factor using 
asfactor()
 method on the H2OFrame still, there wasn't any change on the result. 


But when I changed the values in the target attribute to True and False for 1 and 0 respectively, I was getting the expected result(i.e) the output was the classification rather than the probability.




What is the right way to get the classified prediction result?


If probabilities are the outcomes for numerical target values, then how do I handle it in case of a multiclass classification?","['python', 'machine-learning', 'classification', 'random-forest', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
51366109,51366109,2018-07-16T16:14:31,2018-07-16 16:54:06Z,0,"I put my predictor variables in the gridsearch below. As far as I understood, this gridsearch selects the best variables that should be used in our model and throws away the others. However, I do not know based on which algorithm/ selection metric it selects the best variables. Can somebody tell me how it selects the variables to keep and the variables to throw away?


The function:


  grid.f <-               h2o.grid(algorithm = ""glm"",                                     # Setting algorithm type
                                   grid_id = ""grid.f"",                                    # Id so retrieving information on iterations will be easier later
                                   x = predictors,                                        # Setting predictive features
                                   y = response,                                          # Setting target variable
                                   training_frame = data,                                 # Setting training set
                                   hyper_params = hyper_parameters,                       # Setting apha values for iterations
                                   remove_collinear_columns = T,                          # Parameter to remove collinear columns
                                   lambda_search = T,                                     # Setting parameter to find optimal lambda value
                                   seed = p.seed,                                         # Setting to ensure replicateable results
                                   keep_cross_validation_predictions = F,                 # Setting to save cross validation predictions
                                   compute_p_values = F,                                  # Calculating p-values of the coefficients
                                   family = family,                                       # Distribution type used
                                   standardize = T,                                       # Standardizing continuous variables
                                   nfolds = p.folds,                                      # Number of cross-validations
                                   #max_active_predictors = p.max,                         # Setting for number of features
                                   fold_assignment = ""Modulo"",                            # Specifying fold assignment type to use for cross validations
                                   link = p.link)                                         # Link function for distribution","['r', 'h2o', 'glm']",Jan Janiszewski,https://stackoverflow.com/users/5015382/jan-janiszewski,442
51328685,51328685,2018-07-13T15:43:32,2018-07-13 15:50:49Z,0,"Unfortunately I can't reproduce this problem on the open source data that I've just experimented with (I can't provide the original data on which I have a problem as it's commercially sensitive). However, ...


I have built a h2o.gbm() on a dataset with a set of parameters and seed set to 1. If I take the features for which feature importance > 0, and then build a new gbm (everything exactly the same as before, but slightly less features specified), then I obtain a very different model!


How can this be?


I will look to try and provide something re-producable, but in the meantime any insight would be appreciated. Happy to clarify the problem if I've not been articulate enough.


Thanks,
Tom","['r', 'h2o', 'gbm']",TomFromWales,https://stackoverflow.com/users/4747599/tomfromwales,85
51328091,51328091,2018-07-13T15:06:34,2018-07-15 02:59:59Z,0,"I am running h2o grid search on R. The model is a glm using a gamma distribution.
I have defined the grid using the following settings.
hyper_parameters = list(alpha = c(0, .5), missing_values_handling = c(""Skip"", ""MeanImputation""))


                                 h2o.grid(algorithm = ""glm"",                            # Setting algorithm type
                                 grid_id = ""grid.s"",                                    # Id so retrieving information on iterations will be easier later
                                 x = predictors,                                        # Setting predictive features
                                 y = response,                                          # Setting target variable
                                 training_frame = data,                                 # Setting training set
                                 validation_frame = validate,                           # Setting validation frame
                                 hyper_params = hyper_parameters,                       # Setting apha values for iterations
                                 remove_collinear_columns = T,                          # Parameter to remove collinear columns
                                 lambda_search = T,                                     # Setting parameter to find optimal lambda value
                                 seed = 1234,                                           # Setting to ensure replicateable results
                                 keep_cross_validation_predictions = F,                 # Setting to save cross validation predictions
                                 compute_p_values = F,                                  # Calculating p-values of the coefficients
                                 family = 'gamma',                                      # Distribution type used
                                 standardize = T,                                       # Standardizing continuous variables
                                 nfolds = 2,                                            # Number of cross-validations
                                 fold_assignment = ""Modulo"",                            # Specifying fold assignment type to use for cross validations
                                 link = ""log"") 



When i run the above script, i get the following error:
Error in hyper_names[[index2]] : subscript out of bounds


Please can you help me find where the error is","['r', 'h2o', 'grid-search', 'hyperparameters']",Shavani Vandeyar,https://stackoverflow.com/users/10076711/shavani-vandeyar,11
51324376,51324376,2018-07-13T11:45:26,2018-07-13 11:54:39Z,510,"I am using 
h2o
 on Python 3.6.5 and try to call 
h2o.init(port=54617)
 inside a Jupyter notebook on Ubuntu Bash (Windows 10).


I installed Open-JDK-8.


However, I have the error of 255 
https://pastebin.com/Zb621fps


Checking whether there is an H2O instance running at http://localhost:54617..... not found.
    Attempting to start a local H2O server...
      Java Version: openjdk version ""1.8.0_171""; OpenJDK Runtime Environment (build 1.8.0_171-8u171-b11-0ubuntu0.18.04.1-b11); OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode)
      Starting server from /home/user/anaconda3/envs/barco3/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar
      Ice root: /tmp/tmp43tw2ak8
      JVM stdout: /tmp/tmp43tw2ak8/h2o_user_started_from_python.out
      JVM stderr: /tmp/tmp43tw2ak8/h2o_user_started_from_python.err
    ---------------------------------------------------------------------------
    H2OConnectionError                        Traceback (most recent call last)
    ~/anaconda3/envs/barco3/lib/python3.6/site-packages/h2o/h2o.py in init(url, ip, port, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, **kwargs)
        251                                      _msgs=(""Checking whether there is an H2O instance running at {url}"",
    --> 252                                             ""connected."", ""not found.""))
        253     except H2OConnectionError:

    ~/anaconda3/envs/barco3/lib/python3.6/site-packages/h2o/backend/connection.py in open(server, url, ip, port, https, auth, verify_ssl_certificates, proxy, cookies, verbose, _msgs)
        317             conn._timeout = 3.0
    --> 318             conn._cluster = conn._test_connection(retries, messages=_msgs)
        319             # If a server is unable to respond within 1s, it should be considered a bug. However we disable this

    ~/anaconda3/envs/barco3/lib/python3.6/site-packages/h2o/backend/connection.py in _test_connection(self, max_retries, messages)
        587             raise H2OConnectionError(""Could not establish link to the H2O cloud %s after %d retries\n%s""
    --> 588                                      % (self._base_url, max_retries, ""\n"".join(errors)))
        589 

    H2OConnectionError: Could not establish link to the H2O cloud http://localhost:54617 after 5 retries
    [39:11.07] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54617): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2c4c69e198>: Failed to establish a new connection: [Errno 111] Connection refused',))
    [39:12.28] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54617): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2c519308d0>: Failed to establish a new connection: [Errno 111] Connection refused',))
    [39:13.48] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54617): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2c51bc4438>: Failed to establish a new connection: [Errno 111] Connection refused',))
    [39:14.69] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54617): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2c51bc4978>: Failed to establish a new connection: [Errno 111] Connection refused',))
    [39:15.89] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54617): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2c51930dd8>: Failed to establish a new connection: [Errno 111] Connection refused',))

    During handling of the above exception, another exception occurred:

    H2OServerError                            Traceback (most recent call last)
    <ipython-input-29-db4aa26775ed> in <module>()
    ----> 2 h2o.init(port = 54617)

    ~/anaconda3/envs/barco3/lib/python3.6/site-packages/h2o/h2o.py in init(url, ip, port, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, **kwargs)
        259             raise H2OConnectionError('Can only start H2O launcher if IP address is localhost.')
        260         hs = H2OLocalServer.start(nthreads=nthreads, enable_assertions=enable_assertions, max_mem_size=mmax,
    --> 261                                   min_mem_size=mmin, ice_root=ice_root, port=port, extra_classpath=extra_classpath)
        262         h2oconn = H2OConnection.open(server=hs, https=https, verify_ssl_certificates=not insecure,
        263                                      auth=auth, proxy=proxy,cookies=cookies, verbose=True)

    ~/anaconda3/envs/barco3/lib/python3.6/site-packages/h2o/backend/server.py in start(jar_path, nthreads, enable_assertions, max_mem_size, min_mem_size, ice_root, port, extra_classpath, verbose)
        119         if verbose: print(""Attempting to start a local H2O server..."")
        120         hs._launch_server(port=port, baseport=baseport, nthreads=int(nthreads), ea=enable_assertions,
    --> 121                           mmax=max_mem_size, mmin=min_mem_size)
        122         if verbose: print(""  Server is running at %s://%s:%d"" % (hs.scheme, hs.ip, hs.port))
        123         atexit.register(lambda: hs.shutdown())

    ~/anaconda3/envs/barco3/lib/python3.6/site-packages/h2o/backend/server.py in _launch_server(self, port, baseport, mmax, mmin, ea, nthreads)
        315         while True:
        316             if proc.poll() is not None:
    --> 317                 raise H2OServerError(""Server process terminated with error code %d"" % proc.returncode)
        318             ret = self._get_server_info_from_logs()
        319             if ret:

    H2OServerError: Server process terminated with error code 255



How to fix this problem?","['python', 'python-3.x', 'java-8', 'h2o']",Unknown,,N/A
51319990,51319990,2018-07-13T07:37:08,2018-07-13 12:51:10Z,0,"I found this information into the H2O Flow documentation :




H2O Flow supports REST API, R scripts, and CoffeeScript




H2O Flow Documentation


Into H2O Flow there are special cells for Scala code, but I didn't found any way to use R code inside the flow.","['r', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
51317385,51317385,2018-07-13T03:51:32,2018-07-13 06:17:30Z,0,"UPDATE...
so I kinda figure out my problem in other way and I will leave my code below.....


Another thing is, I'd still like to know if a dataframe(with coefficients in table) can be converted to a model object like glm ...?? 


=================================================


so I am learning h2o package in R and I have a problem in getting model from h2o object:


So, I have went through the h2o training session and got my S4 object ""fit"",
by subset this ""fit"" object I can get model and coefficients table;
the question is , how do I use this ""coefficients table"" as a model, like what we usually do in glm ? 


Here is the code:


#using dataset germancredit as sample

data(""GermanCredit"")
#ease for demo
Sub_German=GermanCredit[  ,c(""amount"",""present_residence"",""duration"",""age"")]    
target=ifelse(GermanCredit$credit_risk==""good"",0,1)

data=cbind(Sub_German,target)


library(h2o)

localH2O = h2o.init()

dth2o = as.h2o(data)  


# h2o.glm  
fit = h2o.glm(y=""target"", training_frame=dth2o,  seed=17,
              family=""binomial"", nfolds=2, alpha=1, lambda_search=TRUE) # summary(fit)


model_fit_h2o= fit@model
class(model_fit_h2o)
# [1] ""list""

model_fit_coe_table= model_fit_h2o$coefficients_table
class(model_fit_coe_table)
# [1] ""H2OTable""   ""data.frame""


# predict
dt_h2o_pred= predict(fit, type='response', dth2o)
class(dt_h2o_pred)
# [1] ""H2OFrame""

# convert to dataframe and get p1 as predicted probability for '1'
dt_h2o_pred_df=as.data.frame(dt_h2o_pred) 
dt_h2o_num=dt_h2o_pred_df$p1
class(dt_h2o_num)
# [1] ""numeric""



So as seen, how do I convert this ""model_fit_coe_table"" into a model object?
What I usually do is using glm, as shows :


# glm ------
model = glm(target ~ ., family = binomial(link='logit'),  data = data)
summary(model)

# Select a formula-based model by AIC
m_step = step(model, direction=""both"", trace=FALSE)
model_fin = eval(m_step$call)
class(model_fin)
# (""glm"" ""lm"")


#predicted proability
dt_pred = predict(model_fin, type='response', data)



In this case I can apply ""predict"" function with ""model_fin"" of type glm. 


Admittedly, I think I could manually create a logistic function like 
f(x)= ax1+bx2+cx3....+cont,  using the coef table from h2o object;


but if I'm playing with the independent variables , this means I need do this by hand every time I change input...so this is totally inefficient....


Anyone got any solutions? Or is there another way to achieve my goal?
Thank you!!","['r', 'model', 'h2o', 'glm', 'predict']",Unknown,,N/A
51302010,51302010,2018-07-12T09:30:11,2018-07-12 09:30:11Z,540,"I have a problem trying to connect to the h2o server. The first time i'm able to do it. But the second time i get this error: 


[18:56.78] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='127.0.0.1', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3041303dd8>: Failed to establish a new connection: [Errno 111] Connection refused',))



At the begining of my script i have:


h2o.init(ip=""127.0.0.1"", port=""54321"")



I down know if the first time the connection didnt close. Any body can help me?


Thank you very much","['python-3.x', 'h2o']",Pilar82,https://stackoverflow.com/users/10069627/pilar82,21
51266468,51266468,2018-07-10T13:23:28,2018-07-10 14:11:35Z,0,"I aim to create a 
partial dependence plot (PDP)
 which shows the interaction of two input variables on the response for my h2o random forest model. Here is a reproducible setup of an example situation:  


library(h2o)
data(diabetes, package = 'lars')

h2o.init()

train_data <- as.h2o(cbind(diabetes$x, target=diabetes$y))
X_names <- colnames(diabetes$x)
y_name  <- ""target""

rf <- h2o.randomForest(x=X_names, y=y_name, training_frame=train_data,
                       ntrees=10, max_depth=5)

h2o.partialPlot(rf, train_data, c(""age"", ""bmi""))  



The code above plots two PDPs separately for 
age
 and 
bmi
, but it doesn't show anything about how they affect the response when they vary together.    


Say I instead want to plot 
age
 in the x axis, 
bmi
 in the y axis, and the mean response in the z axis in a 3-dimensional plot. 


Is there a functionality (or a good-practice guideline) in h2o that helps build plots like this? I can work my way around by creating the response data myself, but that can get a bit cumbersome.  


See related:  


https://stats.stackexchange.com/questions/197455/how-to-plot-3d-partial-dependence-in-gbm","['r', 'machine-learning', 'h2o']",Cihan,https://stackoverflow.com/users/6916631/cihan,"2,307"
51262821,51262821,2018-07-10T10:23:48,2018-07-13 05:25:40Z,917,I am building a classification model in h2o DRF and GBM. I want to change probability of prediction such that if p0 <0.2 then predict= 0 else predict=1,"['python-3.x', 'random-forest', 'h2o', 'gbm']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
51249564,51249564,2018-07-09T15:54:11,2018-07-09 15:54:11Z,60,"The Notebook demo of 
NYC Citibike Demand with Weather (Large)
 can't be run locally due to errors accessing the Citibike datasets. When the following cell is run:


big_test =   [mylocate(""bigdata/laptop/citibike-nyc/2013-07.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2013-08.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2013-09.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2013-10.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2013-11.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2013-12.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2014-01.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2014-02.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2014-03.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2014-04.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2014-05.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2014-06.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2014-07.csv""),
          mylocate(""bigdata/laptop/citibike-nyc/2014-08.csv"")]

# ----------

# 1- Load data - 1 row per bicycle trip.  Has columns showing the start and end
# station, trip duration and trip start time and day.  The larger dataset
# totals about 10 million rows
print(""Import and Parse bike data"")
data = h2o.import_file(path=big_test)



The following error is output if data_source_is_s3 is set to true:


H2OResponseError: Server error java.lang.IllegalArgumentException:
  Error: AWS Access Key ID and Secret Access Key must be specified as the username or password (respectively) of a s3n URL, or by setting the fs.s3n.awsAccessKeyId or fs.s3n.awsSecretAccessKey properties (respectively).
  Request: GET /3/ImportFiles
    params: {'path': 's3n://h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-07.csv'}



and the following if data_source_is_s3 is set to False:


ValueError: File not found: bigdata/laptop/citibike-nyc/2013-10.csv","['python', 'h2o']",xhlulu,https://stackoverflow.com/users/10054695/xhlulu,11
51217452,51217452,2018-07-06T20:50:59,2018-07-08 20:24:08Z,77,"I am trying to launch H2O AutoML from command line with CURL and could not get it working.. 


The base command is something as below:


curl -X POST -H 'application/x-www-form-urlencoded; charset=UTF-8' -d 'training_frame=12cfbae9-af66-42fd-835f-13ccc5a508ab'  ""http://localhost:54321/99/AutoMLBuilder""



I tried with various parameters with but I always get error as unknown parameter.",['h2o'],AvkashChauhan,https://stackoverflow.com/users/1325423/avkashchauhan,20.6k
51216030,51216030,2018-07-06T18:34:38,2018-07-07 08:45:59Z,0,"I have a .csv that I am importing into h2o which has dates stored as ""YYYY-mm-dd"" format. When I import this into h2o through R, these columns are read in as time (milliseconds) since 1970 (as explained by the problem listed here - 
https://0xdata.atlassian.net/browse/PUBDEV-3434
).


> head(data.hex$date_used_dt)
   date_used_dt
1 1489449600000
2 1520380800000
3 1469491200000
4 1465862400000
5 1464912000000
6 1516147200000



I need to turn this column into a date format. h2o.as_date() cannot work since this is not a factor or string. Is there a function that converts the time variable from h2o to a date 
within h2o
? Something like h2o.as_date(), but that could be used on time variables? I need to keep this dataset in h2o.","['r', 'h2o']",M. Gleebs,https://stackoverflow.com/users/10043908/m-gleebs,21
51169089,51169089,2018-07-04T08:26:14,2023-05-22 21:58:28Z,0,"I have an app in Python that I want to run in a Docker container and it has a line:


h2o.connect(ip='127.0.0.1', port='54321')



The h2o server is running in a Docker container and it always has a different IP. One time it was started on 172.19.0.5, the other time 172.19.0.3, sometimes 172.17.0.3.
So it is always random, and I can't connect the Python app.
I tried to expose the port of h2o server to localhost and then connect the Python (the code above), but it is not working.","['python', 'docker', 'url', 'database-connection', 'h2o']",rgettman,https://stackoverflow.com/users/1707091/rgettman,178k
51149201,51149201,2018-07-03T07:48:33,2024-05-06 16:48:42Z,480,"I'm new to docker and I'm trying to run 
h2o
 in docker and then use python to connect to it.
I have folder with:
  model-generator folder in which I have the python script and Dockerfile to build an image
  h2o-start folder in which I have h2o.jar file and Dockerfile to start that jar
  docker-compose.yml file with:


version: ""3""
services:
   h2o-start:
      image: milanpanic2/h2o-start
      build: 
         context: ./h2o-start
      restart: always
   model-generator:
      image: milanpanic2/model-generator
      build:
         context: ./model-generator
      restart: always



My python script contains:


import h2o   

h2o.connect(ip='172.19.0.3', port='54321')



When I run docker-compose up it gives me an error that python can't connect, because there isn't anything on 172.19.0.3


Dockerfile for python


FROM python:2.7-slim
WORKDIR /app
ADD . /app
RUN pip install > --trusted-host pypi.python.org -r requirements.txt 
EXPOSE 80 
ENV NAME World 
CMD [""python"", ""passhash.py""]



Dockerfile for h2o


FROM openjdk:8
ADD h2o.jar h2o.jar
EXPOSE 54321 EXPOSE 54322
ENTRYPOINT [""java"", ""-jar"", ""h2o.jar""]","['docker', 'ip', 'h2o']",aynber,https://stackoverflow.com/users/1007220/aynber,23k
51136687,51136687,2018-07-02T12:51:49,2018-07-02 14:25:47Z,294,"In the documentation of H2O is written: 




mini_batch_size: Specify a value for the mini-batch size. (Smaller values lead to a better fit; larger values can speed up and generalize better.)




but when I run a model using the FLOW UI (with mini_batch_size > 1) in the log file is written: 




WARN: _mini_batch_size Only mini-batch size = 1 is supported right now.




so the question: is the mini_batch_size really used??","['neural-network', 'h2o', 'mini-batch']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
51135888,51135888,2018-07-02T12:08:11,2018-07-02 18:37:50Z,61,"I am using 
H2O GLRM
 model in my scala code.
Now migrating scala code to python.


However I am not able to find following equivalent methods in H2O python module


1) 
allStringVecToCategorical()
 [Belongs to H2OFrameSupport Trait]


Using the api as follow in the code:


withLockAndUpdate(h2OFrameForImputation) 
  {
      allStringVecToCategorical(_)
  }



2) public Frame 
score(Frame fr)
 [Belongs to 
hex.Model
]


Using the api as follow in the code:


glrmModel.score(h2OFrameForImputation)



Please let me know the equivalent method in 
H2O
 python module.",['h2o'],Gsk,https://stackoverflow.com/users/3032364/gsk,"2,927"
51107672,51107672,2018-06-29T18:41:03,2018-07-03 22:57:57Z,0,"The latest H2O documentation states that ""The data is divided into groups by quantile thresholds of the response probability. Note that the default number of groups is 20; if there are fewer than 20 unique probability values, then the number of groups is reduced to the number of unique quantile thresholds."" 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/flow.html#interpreting-the-gains-lift-chart


However, in reality, only 16 rows are generated, even when there are more than 20 unique probability values in the input data, and it is unclear how they should be interpreted. 


You can see this even in this example code which is taken directly from the h2o.gainsLift() help page:


library(h2o)
h2o.init()
prosPath <- system.file(""extdata"", ""prostate.csv"", package=""h2o"")
hex <- h2o.uploadFile(prosPath)
hex[,2] <- as.factor(hex[,2])
model <- h2o.gbm(x = 3:9, y = 2, distribution = ""bernoulli"",
                                 training_frame = hex, validation_frame = hex, nfolds=3)
h2o.gainsLift(model)              ## extract training metrics. Note that there are only 16 rows in the Gains/Lift Table.
h2o.gainsLift(model, valid=TRUE)  ## extract validation metrics (here: the same)
h2o.gainsLift(model, xval =TRUE)  ## extract cross-validation metrics
h2o.gainsLift(model, newdata=hex) ## score on new data (here: the same)
# Generating a ModelMetrics object
perf <- h2o.performance(model, hex)
h2o.gainsLift(perf)               ## extract from existing metrics object. Note that there are still only 16 rows in the Gains/Lift Table.

# There are 380 unique predicted probability values, which is greater than 20. 
length(unique(as.data.frame(h2o.predict(model, hex))$p1))



Furthermore, I am inclined to think that these rows do not represent 16 evenly-binned quantiles, given that the ""sanity checks"" for gains/lift displayed on this page include uneven bins: 
https://github.com/h2oai/h2o-3/blob/master/h2o-r/tests/testdir_jira/runit_pubdev_2372_gainLift.R


See line 36 on that page, in which I believe the bins are defined. They are shown as:
probs = c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.85,0.9,0.95,0.96,0.97,0.98,0.99)


How am I to understand what is being displayed on the Gains/Lift table? Can I customize the n-tile bins being displayed? I would prefer to see 10 bins, ideally. 


Thank you.","['r', 'h2o']",0012,https://stackoverflow.com/users/2411578/0012,162
51100483,51100483,2018-06-29T11:13:44,2018-06-29 11:13:44Z,309,"I am currently working on calculation of prediction intervals derived through quantile regression with H2o DeepLearning and GradientBoosting. In H2o you have to build and train separate Models for each interval, e.g.:


For the 95% Prediction Interval you would need a separate model for the lower bound 
(100-95)/2)=2.5%
 and the upper bound 
(100 - (100-95)/2)=97.5%
. 
Now, I am trying to optimise model-parameter through Grid-Search. What would be the best approach?


Running Grid-Search for one of the models and using the parameters of the best model of the grid for all models.


Or running a separate grid for each model and getting two models with different parameters in the end?


My concern is, that in option two the whole interval will be not consistent, as the two models are not comparable anymore.


Has anyone experience with this?","['python', 'regression', 'intervals', 'h2o', 'quantile']",dnks23,https://stackoverflow.com/users/5358418/dnks23,369
51084343,51084343,2018-06-28T13:36:10,2020-08-11 20:14:53Z,0,"I get this error when using the ""explain"" function from the ""
lime
"" library on a h2o random forest.


Error in elnet(x, is.sparse, ix, jx, y, weights, offset, type.gaussian,  : 
  y is constant; gaussian glmnet fails at standardization step



I can't find documentation online, or help about this question online.  Can you help me root-cause and resolve it?


Here is my code:


explainer_h2o_rf  <- lime(x=big_df, 
                          model=fit_rf.hex, 
                          bin_continuous = FALSE,
                          use_density = T, 
                          quantile_bins = F)

# for(i in 1:25){
i <- c(1,2)
explanation_rf <- explain(x = x_lime[i,],
                          explainer = explainer_h2o_rf,
                          n_features = 15,
                          feature_select = ""auto"",
                          labels = ""1"")



Notes: 




I am predicting a binomial variable within the h2o model,
'fit_rf.hex'.


I worked through 
this
 and it worked, but my current approach does not


The ""lime"" tag does not seem to apply to this lime library, but to something that is used for unit testing.


The non-gaussian shouldn't be a problem, because (I think) I have set the flags that deal with non-gaussian (nearly all my data is non-Gaussian) data using kernel methods.




Here are sites/questions that didn't contain my answer:




Lasso error in glmnet NA/NaN/Inf


glmnet training throws error on x,y dataframe arguments: am I using it wrong?


Error - Error in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs)= etc


Error resulting in using cv.glmnet","['r', 'random-forest', 'h2o']",EngrStudent,https://stackoverflow.com/users/2259468/engrstudent,"1,972"
51059737,51059737,2018-06-27T09:55:24,2018-06-27 20:57:07Z,0,"I'm trying to use 
h2o.automl
 from H2O package.
However, I keep getting the error 
ERROR: Unexpected HTTP Status code: 500 Server Error


This error occurs whenever I set 


                  max_runtime_secs = 0,      # Max time (sec.)
                  max_models = NULL, 



Here's the output:


> model_automl = h2o.automl(x = features,
+                           y = target,
+                           training_frame = h_train,
+                           validation_frame = NULL,  # Optional; Early Stopping of individual models
+                           leaderboard_frame = NULL, # If provided, then Leaderboard scored using this dataframe
+                           nfolds = 10,
+                           fold_column = NULL,       # Column with cross-validation fold index assignment per observation
+                           weights_column = NULL,    # 0 = excluding; 2 = repeating that row twice
+                           balance_classes = TRUE,   # Over/under-sampling available
+                           class_sampling_factors = NULL,
+                           max_after_balance_size = 10,
+                           max_runtime_secs = 0,      # Max time (sec.)
+                           max_models = NULL,         # Max no. of models
+                           stopping_metric = ""AUC"", # Metric to optimize   #'AUC'
+                           stopping_tolerance = NULL,
+                           stopping_rounds = 5,
+                           seed = n_seed,
+                           project_name = ""final1"",
+                           exclude_algos = NULL,    # If you want to exclude any algo 
+                           keep_cross_validation_predictions = TRUE,
+                           keep_cross_validation_models = FALSE,
+                           sort_metric = ""AUTO"")

ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://localhost:54321/99/AutoML/final1)

java.lang.ArrayIndexOutOfBoundsException
 [1] ""java.lang.ArrayIndexOutOfBoundsException: 0""                                                                        
 [2] ""    ai.h2o.automl.Leaderboard.toTwoDimTable(Leaderboard.java:735)""                                                  
 [3] ""    ai.h2o.automl.Leaderboard.toTwoDimTable(Leaderboard.java:727)""                                                  
 [4] ""    water.automl.api.schemas3.AutoMLV99.fillFromImpl(AutoMLV99.java:78)""                                            
 [5] ""    water.automl.api.AutoMLHandler.fetch(AutoMLHandler.java:17)""                                                    
 [6] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                    
 [7] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                                  
 [8] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                          
 [9] ""    java.lang.reflect.Method.invoke(Method.java:498)""                                                               
[10] ""    water.api.Handler.handle(Handler.java:63)""                                                                      
[11] ""    water.api.RequestServer.serve(RequestServer.java:451)""                                                          
[12] ""    water.api.RequestServer.doGeneric(RequestServer.java:296)""                                                      
[13] ""    water.api.RequestServer.doGet(RequestServer.java:220)""                                                          
[14] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:735)""                                                   
[15] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                   
[16] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                         
[17] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                                     
[18] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                             
[19] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                                      
[20] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                              
[21] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                  
[22] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                          
[23] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                
[24] ""    water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:197)""                                                      
[25] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                          
[26] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                
[27] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                        
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                 
[29] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                  
[30] ""    org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)""                
[31] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)""
[32] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)""                                               
[33] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)""                                          
[34] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                         
[35] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                   
[36] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                               
[37] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                
[38] ""    java.lang.Thread.run(Thread.java:748)""                                                                          

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

0","['r', 'h2o', 'automl']",LeGeniusII,https://stackoverflow.com/users/7017940/legeniusii,960
51058833,51058833,2018-06-27T09:12:29,2018-06-27 09:12:29Z,0,"I am using H2O with R. When defining nthreads, the commands are using more than the threads determined. This is my code:




library(h2o)


h2o.init(nthreads=8)




H2O is not running yet, starting it now...
Note:  In case of errors look at the following log files:
    /tmp/RtmpklPi6J/h2o_mballesta_started_from_r.out
    /tmp/RtmpklPi6J/h2o_mballesta_started_from_r.err

openjdk version ""1.8.0_171""
OpenJDK Runtime Environment (build 1.8.0_171-b10)
OpenJDK 64-Bit Server VM (build 25.171-b10, mixed mode)

Starting H2O JVM and connecting: . Connection successful!

R is connected to the H2O cluster:
    H2O cluster uptime:         23 seconds 421 milliseconds
    H2O cluster timezone:       Europe/Madrid
    H2O data parsing timezone:  UTC
    H2O cluster version:        3.20.0.2
    H2O cluster version age:    11 days
    H2O cluster name:           H2O_started_from_R_mballesta_knw859
    H2O cluster total nodes:    1
    H2O cluster total memory:   26.67 GB
    H2O cluster total cores:    32
    H2O cluster allowed cores:  8
    H2O cluster healthy:        TRUE
    H2O Connection ip:          localhost
    H2O Connection port:        54321
    H2O Connection proxy:       NA
    H2O Internal Security:      FALSE
    H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4
    R Version:                  R version 3.5.0 (2018-04-23)



The allowed cores are 8. But when sending a command, it can use up to 24 cores (
image attached
). How can I limit the maximum number of cores? I am using Linux CentOS.","['r', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
51019728,51019728,2018-06-25T09:01:49,2018-06-26 20:45:04Z,0,"I am having the following figure when training an H2O Deep Learning model with dropout 


Misaligned predictions


The code used to train the net is 


  m.nn <- h2o.deeplearning(x = 1:(nc-1),                       
                       y = nc,                             
                       training_frame = datTra,  
                       #validation_frame = datTst,
                       nfolds = 5,  
                       activation = 'RectifierWithDropout', 
                       #input_dropout_ratio = 0.2, 
                       hidden_dropout_ratios = c(dro, dro, dro),          
                       hidden = c(120,30,8),
                       #hidden = 20,
                       epochs = 999,                          
                       #mini_batch_size = 100,
                       #variable_importances = TRUE,         
                       standardize = TRUE,
                       regression_stop = 1e-3,    
                       stopping_metric = ""MSE"",
                       stopping_tolerance = 1e-6,           
                       stopping_rounds = 10)



Figure corresponds to dro=0.1


Why am I having that misalignment? Is there any option i am missing out?


You can find a piece of code to try below
(download 'SampleData.csv' from 
here
)


library(h2o) 
library(readr)
library(ggplot2)

df <- as.data.frame(read_delim(file = 'SampleData.csv', delim = "";""))

localH2O <- h2o.init(ip = ""localhost"", startH2O = TRUE, nthreads = 2, max_mem_size = '4g')

dat_h2o <- as.h2o(x = df)

model.ref <- h2o.deeplearning(x = 1:(ncol(df)-1), y = ncol(df),                             
                         training_frame = dat_h2o, 
                         hidden = c(120,30,8), 
                         activation = 'Rectifier', 
                         epochs = 199, 
                         mini_batch_size = 10,
                         regression_stop = 0.1,
                         stopping_metric = ""MSE"",
                         stopping_tolerance = 1e-6,           
                         stopping_rounds = 10)

model.dro <- h2o.deeplearning(x = 1:(ncol(df)-1), y = ncol(df),                             
                         training_frame = dat_h2o,  
                         hidden = c(120,30,8),
                         activation = 'RectifierWithDropout', 
                         hidden_dropout_ratios = c(0.2, 0.2, 0.2),          
                         epochs = 199,
                         mini_batch_size = 10,                           
                         regression_stop = 0.1,
                         stopping_metric = ""MSE"",
                         stopping_tolerance = 1e-6,           
                         stopping_rounds = 10)

pred.ref <- as.data.frame(h2o.predict(object = model.ref, newdata = dat_h2o))
pred.dro <- as.data.frame(h2o.predict(object = model.dro, newdata = dat_h2o))

dfRes <- data.frame(cbind(df$SeqF, pred.ref$predict, pred.dro$predict))
colnames(dfRes) <- c('act', 'pred', 'pred2')

ggplot(data = dfRes) + geom_point(aes(x=act, y=pred), color='blue') + 
  geom_point(aes(x=act, y=pred2), color='red') + geom_abline()","['r', 'h2o', 'dropout']",Unknown,,N/A
51007240,51007240,2018-06-24T05:41:55,2018-06-29 18:59:28Z,144,"I'm using spark-2.3.0-bin-hadoop2.7 and sparkling-water-2.3.5 on Windows 10 64 bit.


I've taken the following steps and looking for help for the Steps 4 and 5.


Step 1: Run Spark shell by executing bin/sparkling-shell. Fine


Step 2: At scala prompt (Fine)


    scala> import org.apache.spark.h2o._
    scala> val h2oContext = H2OContext.getOrCreate(spark)
    scala> import h2oContext._



Step 3: - openFlow command at scala prompt to open Flow UI in the browser. Fine. 


Step 4: At scala prompt (Not working)


    scala> openSparkUI command at scala prompt to open the Spark UI in the browser
    - error: not found: value openSparkUI



Step 5: Looking for an editor to write scala code and how to submit that code at scala prompt","['scala', 'apache-spark', 'h2o']",Deb,https://stackoverflow.com/users/3889664/deb,87
50987850,50987850,2018-06-22T12:22:52,2018-06-25 08:27:38Z,0,"in my Pandas Dataframe I have loads of boolean Features (
True/False
). Pandas correctly represents them as 
bool
 if I do 
df.dtypes
. If I pass my data frame to h2o (
h2o.H2OFrame(df)
) the boolean features are represented as 
enum
. So they are interpreted as categorical features with 2 categories. 


Is there a way to change the type of the features from 
enum
 to 
bool
? In Pandas I can use 
df.astype('bool')
, is there an equivalent in H2o?


One idea was to encode 
True/False
 to their numeric representation (
1/0
) before converting 
df
 to a H2o-Frame. But H2o now recognises this as 
int64
.


Thanks in Advance for help!","['python', 'pandas', 'enums', 'boolean', 'h2o']",dnks23,https://stackoverflow.com/users/5358418/dnks23,369
50976438,50976438,2018-06-21T19:45:02,2018-06-21 19:45:02Z,915,"I am using H2o's Auto ML package and would like to know if it is possible to get a single AUC, Confusion Matrix and ROC curve for all the methods combined. For instance I have AUC values for the individual models GLM, Stacked Ensemble, deep learning etc. Can you get these three values for all the methods combined? The goal is to be able to compare the Auto ML package to other similar packages.","['h2o', 'roc', 'confusion-matrix', 'auc', 'automl']",Kevin,https://stackoverflow.com/users/9974106/kevin,1
50968068,50968068,2018-06-21T11:59:10,2018-06-21 13:18:45Z,207,"I am looking into using H2O to create a client-facing application from which they will be able to import data and run ML models on. As H2O only offers a limited number of models at the moment, is there any way to build custom models (an LSTM in TensorFlow, for example), import them into H2O where they can then be run just like any of H2O's included models?


It seems as though H2O's Deep Water was the nearest solution to this, but they have now discontinued its development. 


In other words, is there any way to facilitate for different types of models that H2O does not support? (SVM, RNN, CNN, GAN, etc.)","['tensorflow', 'h2o']",Unknown,,N/A
50965715,50965715,2018-06-21T10:00:16,2018-06-21 10:26:54Z,0,"I am running 
h2o
 in 
R
 using 
h2o.glm()
.


For some reason I keep getting this error:


CURL ERROR: Recv failure: Connection reset by peer



I varied the size of my cluster and dataset and yet the connection seems to break.


Does anybody know how to solve this I am running out of ideas!


Thanks in advance,
J","['r', 'database-connection', 'h2o', 'lasso-regression']",Saurabh Chauhan,https://stackoverflow.com/users/5835763/saurabh-chauhan,"3,201"
50946104,50946104,2018-06-20T10:32:00,2018-06-21 07:59:10Z,0,"I have import my dataset into h2o flow, I have one column which is categorical type, I wanna convert this into numerical data type. 


If I use pandas for this task I'll do like this,


df['category_column'] = df['category_column'].astype('category')
df['category_column'] = df['category_column'].apply(lambda x: x.cat.codes)



How to do this in h2o flow,


I tried following,




while parsing data i changed Data type to numeric from enum but data shows 
·
 like this.


I tried 
convert to numeric
 option, But it didn't work as I wish.




I don't know whether I'm going in right direction or not. 
Please help me to solve this issue.


Update on question as suggested:


Why GLM forced me to use numerical column?


Error evaluating cell


My dataset looks like this:




When I use GLM to build model and, 
I
 is my response_column i'm getting following error




Error calling POST /3/ModelBuilders/glm with opts {""model_id"":""glm-e2ed0066-636c-4c71-bf8...




ERROR MESSAGE: Illegal argument(s) for GLM model: glm-e2ed0066-636c-4c71-bf8c-04525eb05002. Details: ERRR on field: _response: Regression requires numeric response, got categorical. For more information visit: 
http://jira.h2o.ai/browse/TN-2",['h2o'],Unknown,,N/A
50941901,50941901,2018-06-20T06:48:58,2018-06-20 16:26:53Z,316,"I am new to H2O, I installed H2O Driverless AI in evaluation license. I can successfully perform visualisation and classification model prediction. But I'm wondering how to start with clustering. Because I don't find any option for unsupervised learning or clustering technique? where should i perform clustering operation in driverless AI? Is Clustering operation available in Driverless AI or not? 


Thanks in Advance.",['h2o'],Mohamed Thasin ah,https://stackoverflow.com/users/4684861/mohamed-thasin-ah,11.2k
50932650,50932650,2018-06-19T16:05:27,2018-06-19 20:41:56Z,53,"I've been having issue persisting mojos with latest h2o release. The code has been working fine for older version (3.10)


here is an example code and exception that is being thrown.


   public static Schema[] getAllSchemas() {
        ServiceLoader<Schema> schemaLoader = 
        ServiceLoader.load(Schema.class);
        List<Schema> allSchemas = new ArrayList<>();
        for (Schema schema : schemaLoader) {
            allSchemas.add(schema);
        }
        return allSchemas.toArray(new Schema[allSchemas.size()]);
   }

   public static byte[] extractIce(Model model) throws IOException {
       final UUID uuid = UUID.randomUUID();
       String tempFile = ""/tmp/"" + uuid;
       model.exportMojo(tempFile, true);
       final byte[] bytes = IOUtils.toByteArray(new 
                                   FileInputStream(tempFile)); 
       return bytes; 
    }


   public static void main (String []args) {
      H2OApp.main();
      SchemaServer.registerAllSchemasIfNecessary(getAllSchemas());
      // get the model that needs to be persisted
      // Model model = getH2OModel();
      byte[] extractIce(model);
   }



and this is the exception being thrown.


Caused by: java.lang.IllegalArgumentException: Cannot find Builder for algo url name drf
   at hex.ModelBuilder.ensureBuilderIndex(ModelBuilder.java:141) ~[xyz-platform-all-7.9.0-SNAPSHOT.jar:7.9.0-SNAPSHOT]
   at hex.ModelBuilder.havePojo(ModelBuilder.java:120) ~[xyz-platform-all-7.9.0-SNAPSHOT.jar:7.9.0-SNAPSHOT]
   at hex.Model.havePojo(Model.java:118) ~[xyz-platform-all-7.9.0-SNAPSHOT.jar:7.9.0-SNAPSHOT]
   at water.api.schemas3.ModelSchemaV3.fillFromImpl(ModelSchemaV3.java:73) ~[xyz-platform-all-7.9.0-SNAPSHOT.jar:7.9.0-SNAPSHOT]
   at water.api.schemas3.ModelSchemaV3.fillFromImpl(ModelSchemaV3.java:21) ~[xyz-platform-all-7.9.0-SNAPSHOT.jar:7.9.0-SNAPSHOT]
   at hex.ModelMojoWriter.writeModelDetails(ModelMojoWriter.java:277) ~[xyz-platform-all-7.9.0-SNAPSHOT.jar:7.9.0-SNAPSHOT]
   at hex.ModelMojoWriter.writeTo(ModelMojoWriter.java:178) ~[xyz-platform-all-7.9.0-SNAPSHOT.jar:7.9.0-SNAPSHOT]
   at hex.ModelMojoWriter.writeTo(ModelMojoWriter.java:169) ~[xyz-platform-all-7.9.0-SNAPSHOT.jar:7.9.0-SNAPSHOT]
   at hex.ModelMojoWriter.writeTo(ModelMojoWriter.java:161) ~[xyz-platform-all-7.9.0-SNAPSHOT.jar:7.9.0-SNAPSHOT]",['h2o'],sanket,https://stackoverflow.com/users/921033/sanket,93
50920389,50920389,2018-06-19T03:52:07,2018-06-22 18:58:35Z,0,"I am relative new to h2o and was trying to use xgboost with grid search. I ran my stuff on edgenode with 40 cores and 26 gb memory with version 3.20.0.2 of h2o package in R and h2o. just cpu as backend.


I have run gbm and randomforest without issues (some gbm takes about 2 hours to finish with grid search and they all ran fine). However, when I was trying to run xgboost, i always get error.


If i ran a simple example without grid search, it will run. however, when i ran xgboost with grid search, i always got error as ""
Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  : 
  Unexpected CURL error: Recv failure: Connection was reset
"" . 


I did my search online and try to figure out what is going on. I found two examples both given by LeDell and one works but not the other. 


I got error in R as ""
Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  : Unexpected CURL error: Recv failure: Connection was reset
"" for code below

https://gist.github.com/ledell/71e0b8861d4fa35b59dde2af282815a5


library(h2o)
h2o.init()


# Load the HIGGS dataset
train <- h2o.importFile(""https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv"")
test <- h2o.importFile(""https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv"")
y <- ""response""
x <- setdiff(names(train), y)
family <- ""binomial""

#For binary classification, response should be a factor
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])

# Some XGboost/GBM hyperparameters
hyper_params <- list(ntrees = seq(10, 1000, 1),
                     learn_rate = seq(0.0001, 0.2, 0.0001),
                     max_depth = seq(1, 20, 1),
                     sample_rate = seq(0.5, 1.0, 0.0001),
                     col_sample_rate = seq(0.2, 1.0, 0.0001))
search_criteria <- list(strategy = ""RandomDiscrete"",
                        max_models = 10, 
                        seed = 1)

# Train the grid
xgb_grid <- h2o.grid(algorithm = ""xgboost"",
                     x = x, y = y,
                     training_frame = train,
                     nfolds = 5,
                     seed = 1,
                     hyper_params = hyper_params,
                     search_criteria = search_criteria)


# Sort the grid by CV AUC
grid <- h2o.getGrid(grid_id = xgb_grid@grid_id, sort_by = ""AUC"", decreasing = TRUE)
grid_top_model <- grid@summary_table[1, ""model_ids""]



Plus i also got error in my edgenode as 
libgomp: Thread creation failed: Resource temporarily unavailable#
[thread 140207508600576 also had an error]


A fatal error has been detected by the Java Runtime Environment:
SIGSEGV (0xb) at pc=xxxxxxxxxxx[thread 140207503337216 also had an error][thread 140207504389888 also had an error], pid=40095, tid=0x00007f849aaea700


JRE version: Java(TM) SE Runtime Environment (8.0_162-b12) (build 1.8.0_162-b12)
Java VM: Java HotSpot(TM) 64-Bit Server VM (25.162-b12 mixed mode linux-amd64 compressed oops)


Problematic frame:


C  [libc.so.6+0x358e5]  exit+0x35


but i got no issue when i ran code below ( this is also a example given by LeDell in another post)


train <- h2o.importFile(""https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv"")

y <- ""response""
x <- setdiff(names(train), y)

train[,y] <- as.factor(train[,y])

hyperparameters_xgboost <- list(ntrees = seq(10, 20, 10),
                     learn_rate = seq(0.1, 0.2, 0.1),
                    sample_rate = seq(0.9, 1.0, 0.1),
                     col_sample_rate = seq(0.5, 0.6, 0.1))

xgb <- h2o.grid(""xgboost"",
                x = x,
                y = y, 
                seed = 1,
                training_frame = train,
                max_depth = 3,
                hyper_params = hyperparameters_xgboost) 



Therefore, I cannot tell what went wrong? originally i thought the xgboost does not work, then i had successful run with xgboost only (no grid). Then I guess it must be the grid search part, and then i did get a successful run with the latter example. I am out of ideas and wonder if someone may have some insights about my error?","['r', 'h2o', 'xgboost']",Unknown,,N/A
50886609,50886609,2018-06-16T09:24:35,2018-06-19 12:08:10Z,97,"I am trying to calculate variance of multiple columns across each row.
Hence the result would have dimension no_of_rows*1.


I tried following way:


import pandas as pd    
test = pd.DataFrame({'p1':[0.8,0.7,0.3],'p10':[0.4,0.6,0.3],'p11':[0.9,0.6,0.4],'p12':[0.44,9.8,0.4],'p13':[0.8,0.4,0.5],'p14':[0.7,0.7,0.7],'p15':[0.8,0.8,0.5]})
test_h2o = h2o.H2OFrame(test)
test_h2o[['p1','p10','p11','p12','p13','p14','p15']].head().apply(lambda x: x.var(),1)



and I get the error ""Expected a Frame but found a class water.rapids.vals.ValRow"".
However with mean as function it works


test_h2o[['p1','p10','p11','p12','p13','p14','p15']].head().apply(lambda x: x.mean(),1)","['python', 'h2o']",Unknown,,N/A
50883728,50883728,2018-06-15T23:17:30,2018-06-24 03:14:24Z,241,"I think fairly simple question - I'm looking to understand how I can extract various bits of metadata out of a h2o model I built in python?


In R I can run the following to get this sort of information.


my_h2o_model@algorithm
my_h2o_model@parameters

my_performance = h2o.performance(my_h2o_model)
my_performance@metrics$thresholds_and_metric_scores # awesome see all classifier metrics at each threshold



In python if I try 


my_h2o_model['algorithm'] 



or


my_h2o_model[0]

perf=model_performance(my_h2o_model)
perf['metrics']['thresholds_and_metric_scores] # !!!!





TypeError: 'H2OGradientBoostingEstimator' object is not subscriptable




How can get this information out of the models in python??","['python', 'h2o']",Unknown,,N/A
50853542,50853542,2018-06-14T08:57:58,2018-06-14 09:46:06Z,0,"I am currently trying to preprocess a very large dataset with a lot of categorical features for Scikit-Learns' RandomForest Model (Regression). The nature of the categorical data requires to not have any ordinality added through encoding schemes. 
The H2o ML-Framework (
Link
) offers of 
enum
-encoding which would suite perfectly for my data. However I rely on Scikit-Learns RandomForest. 


Is anyone aware of some 
enum
-encoding for Scikit-Learn Models? (One-Hot-Encoding is not an option)


Thanks in Advance!","['python', 'encoding', 'enums', 'scikit-learn', 'h2o']",dnks23,https://stackoverflow.com/users/5358418/dnks23,369
50853492,50853492,2018-06-14T08:55:14,2018-08-02 07:17:28Z,0,"We have h2o cluster on linux machine (ran through command line), and we are connecting it from our local machine (Windows) which is on the same network.
When we try to call saveModel we are getting errors.


ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = 
http://10.0.0.4:54321/99/Models.bin/
)


//Code to load model from local machine,IDE- Rstudio


ModelName <- ""GLM_model_R_1522217891094_1279""
modelpath <-file.path(""file://dsvm-dev/Models"",ModelName)
Model.h2o <- h2o.loadModel(modelpath)



//to save model


h2o.saveModel(object = best_model,path = ""file://dsvm-dev/Models2/"",force = TRUE) 



Please suggest any alternate ways to save model to local machine(Windows) instead of on server. Also while saving it on server it saves to library folder, because we do not have access to machine.","['r', 'file', 'h2o']",Sunil Ajagekar,https://stackoverflow.com/users/5002012/sunil-ajagekar,51
50851937,50851937,2018-06-14T07:30:32,2018-06-21 18:04:42Z,237,"I have converted a dataframe to and h2oframe as follows:


val myH2OFrame = h2oContext.asH2OFrame(mydatframe, ""myH2OFrame"")



But I get a ""cannot resolve symbol"" error whenever I try to print rows using:


myH2OFrame.show



So how can I see the contents of an h2oframe?",['h2o'],Alper t. Turker,https://stackoverflow.com/users/3269809/alper-t-turker,35.1k
50832187,50832187,2018-06-13T08:10:54,2018-06-13 16:20:02Z,151,"I'm trying to run an h2o automl model for a multi class problem in the following manner 


h2o.automl(y = result,
              training_frame = train,
              max_runtime_secs = 30,
              sort_metric = ""logloss"")



But I end up getting an error 




Error in h2o.automl(y = result, training_frame = train_to_model, max_runtime_secs = 30,  : 
    unused argument (sort_metric = ""logloss"")




Based on the documentation in 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/sort_metric.html
 I should be able to use logloss in a multi class classification problem but I'm not able to. 


My h2o version is 3.18.0.11 and R version is 3.4.1


Can you please tell me why this might be happening? Thank you!","['h2o', 'automl']",elvikingo,https://stackoverflow.com/users/1823293/elvikingo,987
50827435,50827435,2018-06-13T00:05:09,2018-06-13 00:57:28Z,458,"I have successfully installed 
h2o
 package using pip. But I am having a weird problem when I try to import it using 
windows cmd
 it works well but it fails when I try to do it through 
anaconda prompt command
, 
jyputer
 or 
Spyder
 ? I tried to restart them and run the command but still having the same problem. 


See this screenshoot below :","['python', 'machine-learning', 'h2o']",smerllo,https://stackoverflow.com/users/7362261/smerllo,"3,325"
50819691,50819691,2018-06-12T14:31:02,2018-06-18 21:22:46Z,0,"I'm trying to initialize 
h2o
 on my windows machine using 
h2o.init()
. It was working fine till yesterday, but today it's taking too long to initialize and is giving warning messages:


h2o.init()
 Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         5 days 23 hours 
    H2O cluster timezone:       America/New_York 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.18.0.11 
    H2O cluster version age:    19 days  
    H2O cluster name:           H2O_started_from_R_tfx859 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   0.46 GB 
    H2O cluster total cores:    8 
    H2O cluster allowed cores:  8 
    H2O cluster healthy:        FALSE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         Algos, AutoML, Core V3, Core V4 
    R Version:                  R version 3.5.0 (2018-04-23) 





Warning in .h2o.__checkConnectionHealth() :
    H2O cluster node 1.0.0.1:5 is behaving slowly and should be inspected manually
  Warning in .h2o.__checkConnectionHealth() :
    Check H2O cluster status here: 
http://localhost:5/3/Cloud?skip_ticks=true




How do I solve this problem?


Also, it's throwing errors after making a connection to 
h2o
 when I'm trying to convert a data frame to 
h2o
 object using 
test.h2o <- as.h2o(test)
.","['r', 'h2o']",sm925,https://stackoverflow.com/users/5269047/sm925,"2,678"
50810039,50810039,2018-06-12T05:50:24,2018-06-12 21:04:29Z,785,"I am using h2o4gpu and the parameters which i have set are


h2o4gpu.solvers.xgboost.RandomForestClassifier model.

XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
colsample_bytree=1.0, gamma=0, learning_rate=0.1, max_delta_step=0,
max_depth=8, min_child_weight=1, missing=nan, n_estimators=100,
n_gpus=1, n_jobs=-1, nthread=None, num_parallel_tree=1, num_round=1,
objective='binary:logistic', predictor='gpu_predictor',
random_state=123, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,
seed=None, silent=False, subsample=1.0, tree_method='gpu_hist')



When i am training this model and then predicting, everything is running fine on GPU.


However, when i am saving my model in pickle and then loading back into another notebook and then running a prediction through predict_proba on it, then everything is running on CPU.


Why is my prediction not running on GPU?","['h2o', 'gpu', 'h2o4gpu']",Unknown,,N/A
50809461,50809461,2018-06-12T04:54:27,2018-06-15 11:14:03Z,0,"I would like to introduce some bias. I have 
n-risk factors
 (predictors) but based on the evidence I collected! I consider one of the 
Risk factors
 more relevant than the other ones. There is a 
weights_column
 parameter (see 
description
), but it is not clear for me how to use it and if it can be used for my purpose. 


The documentation states (version 3.20.0.1):


This option specifies the column in a training frame to be used when determining weights. Weights are per-row observation weights and do not increase the size of the data frame. This is typically the number of times a row is repeated, but non-integer values are also supported. During training, rows with higher weights matter more, due to the larger loss function pre-factor.


I don't know if it is suitable for my purpose. I have the following questions/comments, that would help me to understand how to use it, but also for improving the documentation in next version:




""The weights are per row observation""
, I was expecting by column (we are defining a weight in a specific column). It seems to me that the algorithm is adding dummy rows, but then it says that is not increasing the number of rows. 
i) What is the logic for adding such fictitious rows?
 (Is it a copy of another row or a modified row, if so how it's modified?)


""Due to the larger loss function pre-factor""
: 
ii) What does it mean in this context? ii) Does it apply only for 
loss-function
 metric? iv) What is the pre-factor?


v) Can we specify more than one column?
 The name of the parameter is in plural, but the example and the documentation, seem to refer to just one column.




Then the next paragraph says: 
""For example, a weight of 2 is identical to duplicating a row""
, but the user can only specify the column name. 
vi) Can we specify the weight factor number?


The example provided in the documentation does not clarify the purpose of using such parameter based on the problem nature and there is no comparison on how the result may be affected by the use of such parameter. 
vii) What is the rationale in this case for setting this parameter with the column 
weight
?


For example, running the script with and without setting 
weights_column
 we get:


[1] ""AUC with weights_column""
[1] 0.9645522
[1] ""AUC without weights_column""
[1] 0.9803922



The example shows how to use the argument, but it is not suitable to see the benefit of using it.","['r', 'h2o']",Unknown,,N/A
50806677,50806677,2018-06-11T22:03:13,2018-06-11 22:03:13Z,134,"Here is my code:


from pysparkling import H2OConf #commenting this line makes it work
import logging

logging.basicConfig(filename='my_log.log',level=logging.INFO)
logging.info('test')



I cannot get the log file to get created, unless I comment the first line of the code. If I do that, then everything works as normal. I'm assuming it's an import inside 
H2OConf
 that messes up the handlers. Anybody experience anything similar?


I run the script with


spark-submit --master yarn script.py


Thanks","['python', 'logging', 'pyspark', 'h2o', 'sparkling-water']",Tiberiu,https://stackoverflow.com/users/2621303/tiberiu,"1,020"
50761201,50761201,2018-06-08T12:59:44,2020-04-22 08:39:50Z,577,"I am processing my model on a very large data set (of size about 1TB) by breaking it into chunks and then run a prediction on each chunks and then append the result in a file.


After running on each chunk I have to free the memory of GPU so that the other chunk can be processed and predicted.


Instead of running ""Restart Kernel"" again and again, I can run the loop if some command to clear the GPU memory exist.","['gpu', 'gpgpu', 'h2o', 'h2o4gpu']",Unknown,,N/A
50755994,50755994,2018-06-08T07:57:09,2018-06-11 07:35:39Z,744,"I am working on the extension development of rapidminer. I downloaded the template of the extension project from github and developed my own operator. After launching rapidminer, the extension was successfully installed.




In the test code, I manually created the process and added operator. The operation was successful and the operator successfully completed the calculation.
 




But when I dragged the module from the rapidminer software to accomplish the same thing, I reported this error:




This my operator :







And this is rapidminer' method to checkPermission:","['java', 'h2o', 'rapidminer']",liyuhui,https://stackoverflow.com/users/7124383/liyuhui,"1,250"
50753130,50753130,2018-06-08T03:55:54,2018-06-08 17:48:32Z,356,"In H2O site, it says




H2O’s core code is written in Java. Inside H2O, a Distributed Key/Value store is used to access and reference data, models, objects, etc., across all nodes and machines. The algorithms are implemented on top of H2O’s distributed Map/Reduce framework and utilize the Java Fork/Join framework for multi-threading.




Does this mean H2O will not work better than other libraries if it runs on single node cluster? But will work well on multiple nodes cluster. Is that right?


Also what's the difference between h2o on multi-nodes and h2o on hadoop?","['performance', 'hadoop', 'machine-learning', 'cluster-computing', 'h2o']",howaboutthat,https://stackoverflow.com/users/9912048/howaboutthat,1
50749961,50749961,2018-06-07T20:49:10,2018-06-08 18:49:03Z,0,"I am still having the problem outlined by another user in this question: 
as.h2o produces additional row when column names contain special characters


Currently, h2o is on version 3.18.0.11, and it looks like this issue has only been resolved up to 3.18.0.08. I have tried to downgrade the installation of h2o on my computer but I got an error when trying to run this. 


Is there any way to circumvent this problem? Might it work on a different computer (if I can get access to one)?


Edit
: I am using RStudio on a laptop running Windows 7, and the version of R is 3.4.4. 


Edit
: Here is an example of what causes this bug.


df <- replicate(3, rnorm(5))
colnames(df) <- c(""–coliform"", ""‘’append"", ""dog"")
df.h2o <- as.h2o(df)



The output from this code is:


Ã¯Â¿Â½coliform Ã¯Â¿Â½Ã¯Â¿Â½append         dog
1            NaN                NaN         NaN
2      1.3680317         0.33229608 -0.82884927
3     -0.8913680         2.79798207  0.21854663
4     -0.1836785        -0.07519385  1.44215138
5      1.6093652         0.79405964  0.07038501
6     -0.5938197        -0.10297580  0.36824972","['r', 'h2o']",Unknown,,N/A
50749476,50749476,2018-06-07T20:10:17,2018-06-11 10:30:35Z,354,"When using the 
h2o-genmodel.jar
 (either from 
maven central
 or that is output when generating a mojo) SLF4j gives the error




SLF4J: Class path contains multiple SLF4J bindings

  SLF4J: Found binding in [jar:file:~/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]

  SLF4J: See 
http://www.slf4j.org/codes.html#multiple_bindings
 for an explanation.




Using maven or SBT's transitive dependency exclusion doesn't work, so right now I'm using the jar output with the mojo and removing the dependencies from inside the jar by hand.


Is there some better way to use h2o modelgen without having to manually mess with the inside of the jar (using maven instead would be preferable)?","['java', 'maven', 'machine-learning', 'dependencies', 'h2o']",kag0,https://stackoverflow.com/users/2133111/kag0,"6,014"
50747502,50747502,2018-06-07T18:00:20,2018-06-07 18:00:20Z,0,"I want to apply many distinct filters to an h2o dataframe to create unique subsets of data.  I also want to be conscious of the memory management process that h2o uses, because I will be applying this to gigabytes of data.


As far as I can tell from similar questions, there aren't many definitive answers for this topic, and most questions are more than a few years old.


library(h2o)    
h2o.init()    
h_mtcars <- as.h2o(mtcars)

### Subset of am == 1 and gear == 5 ###
index_am <- h_mtcars[[""am""]] == 1
index_gear <- h_mtcars[[""gear""]] == 5

index_combined <- index_am * index_gear 

h_mtcars[index_combined, ]



Is there any way to do this with a string, like we can with rlang and a dataframe?


library(rlang)

expressions <- ""am == '1' & gear == '5'""

index_local <- expressions %>% rlang::parse_quosure() %>% rlang::eval_tidy(mtcars)

mtcars[index_local, ]","['r', 'h2o']",kputschko,https://stackoverflow.com/users/3790973/kputschko,774
50740316,50740316,2018-06-07T11:42:45,2018-12-03 18:17:28Z,0,"I am trying to train a decision tree model using h2o. I am aware that no specific library for decision trees exist in h2o. But, h2o has an implemtation of random forest  
H2ORandomForestEstimator
 . Can we implement a decision tree in h2o by tuning certain input arguments of random forests ? Because we can do that in scikit module (a popular python library for machine learning)


Ref link :


Why is Random Forest with a single tree much better than a Decision Tree classifier?


In scikit the code looks something like this 


RandomForestClassifier(n_estimators=1, max_features=None, bootstrap=False)



Do we have a equivalant of this code in h2o ?","['python', 'machine-learning', 'scikit-learn', 'decision-tree', 'h2o']",ishaan arora,https://stackoverflow.com/users/2983144/ishaan-arora,541
50738058,50738058,2018-06-07T09:44:28,2018-06-07 11:01:00Z,0,"Closed
. This question needs to be more 
focused
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Update the question so it focuses on one problem only by 
editing this post
.






Closed 
6 years ago
.















                        Improve this question
                    








I am reading a file using
 
file=pd.read_csv('file_1.csv'
)


which is taking a long time on 
CPU
.


Is there any method to read this using 
GPU
.","['python-3.x', 'gpu', 'h2o', 'h2o4gpu']",Harkamal,https://stackoverflow.com/users/3790350/harkamal,506
50727029,50727029,2018-06-06T18:15:36,2018-06-07 11:49:34Z,115,"When setting up 
h2o.ai for production
 the documentation tells you how ""Build and extract a model"" which ultimately leads you to




Download the MOJO and the resulting h2o-genmodel.jar




What I'm wondering, is if that 
h2o-genmodel.jar
 is really tied to the mojo zip file, or if one jar can work with multiple different model zips?","['jvm', 'dependencies', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
50712929,50712929,2018-06-06T05:41:35,2018-06-07 06:45:42Z,596,"After installing 
h2o
 couldn't initialize server and receiving error while trying to start.




h2o.exceptions.h2oServererror: Server process terminated with error code 1






import h2o
      h2o.init()
      Checking whether there is an H2O instance running at 
http://localhost:54321
..... not found.
      Attempting to start a local H2O server...
      ; Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode)
        Starting server from C:\Users\Prem\AppData\Local\Programs\Python\Python36-32\lib\site-packages\h2o\backend\bin\h2o.jar
        Ice root: C:\Users\Prem\AppData\Local\Temp\tmpvyk4vufd
        JVM stdout: C:\Users\Prem\AppData\Local\Temp\tmpvyk4vufd\h2o_Prem_started_from_python.out
        JVM stderr: C:\Users\Prem\AppData\Local\Temp\tmpvyk4vufd\h2o_Prem_started_from_python.err
      Traceback (most recent call last):
        File ""C:\Users\Prem\AppData\Local\Programs\Python\Python36-32\lib\site-packages\h2o\h2o.py"", line 252, in init
          ""connected."", ""not found.""))
        File ""C:\Users\Prem\AppData\Local\Programs\Python\Python36-32\lib\site-packages\h2o\backend\connection.py"", line 318, in open
          conn._cluster = conn._test_connection(retries, messages=_msgs)
        File ""C:\Users\Prem\AppData\Local\Programs\Python\Python36-32\lib\site-packages\h2o\backend\connection.py"", line 588, in _test_connection
          % (self._base_url, max_retries, ""\n"".join(errors)))
      h2o.exceptions.H2OConnectionError: Could not establish link to the H2O cloud 
http://localhost:54321
 after 5 retries
      [08:22.10] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError(': Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))
      [08:24.37] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError(': Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))
      [08:26.59] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError(': Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))
      [08:28.84] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError(': Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))
      [08:31.15] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError(': Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))








During handling of the above exception, another exception occurred:


Traceback (most recent call last):
  File """", line 1, in 
  File ""C:\Users\Prem\AppData\Local\Programs\Python\Python36-32\lib\site-packages\h2o\h2o.py"", line 261, in init
    min_mem_size=mmin, ice_root=ice_root, port=port, extra_classpath=extra_classpath)
  File ""C:\Users\Prem\AppData\Local\Programs\Python\Python36-32\lib\site-packages\h2o\backend\server.py"", line 121, in start
    mmax=max_mem_size, mmin=min_mem_size)
  File ""C:\Users\Prem\AppData\Local\Programs\Python\Python36-32\lib\site-packages\h2o\backend\server.py"", line 328, in _launch_server
    raise H2OServerError(""Server process terminated with error code %d""'%proc.returncode)
h2o.exceptions.H2OServerError:Server process terminated with error code 1",['h2o'],Unknown,,N/A
50696160,50696160,2018-06-05T09:00:57,2022-04-23 05:35:07Z,0,"I have a csv file, and want to use H2O to do DeepLearning. But it has some Chinese and datetime that when I finish my Deeplearning need to save output to csv, it can't return to original data.


I use small data to show my problem here.


 In[1]: df = pd.DataFrame({'datetime':['2016-12-17 00:00:00'],'time':['00:00:30'],'month':['月'], 'weekend':['周六']})
        print(df.dtypes)
        df
out[1]: datetime    object
        time        object
        month       object
        weekend     object
        dtype: object
             datetime   time              month weekend
        0   2016-12-17 00:00:00 00:00:30    月   周六 

In[2]: h2o_frame = h2o.H2OFrame(df);h2o_frame ;h2o_frame.types ;h2o_frame





C:\Users\thi\Anaconda3\lib\site-packages\h2o\utils\shared_utils.py:170: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1]




out[2]: Parse progress: |█████████████████████████████████████████████████████████| 100%
                  datetime                time       month        weekend
        2016-12-17 00:00:00  1970-01-01 00:00:30    <0xA4EB>    <0xA9>P<0xA4BB>



the time I want it just only 00:00:30, any way to fix it?


month and weekends I don't find any way to let it show Chinese, but I still finish my deeplearning


But when I want to let h2oframe back to DataFrame and save to csv file, it save  
<0xA4EB>
 for me but not 
月
, and datetime change to int


 In[3]: dff = h2o_frame.as_data_frame();dff
out[3]:         datetime     time     month        weekend
        0   1481932800000   30000   <0xA4EB>    <0xA9>P<0xA4BB>





How to correctly return character from h2oframe to DataFrame


How to correctly return datetime from h2oframe to DataFrame","['python', 'pandas', 'dataframe', 'jupyter-notebook', 'h2o']",Henry Ecker,https://stackoverflow.com/users/15497888/henry-ecker,35.5k
50642358,50642358,2018-06-01T11:23:55,2018-06-03 02:05:03Z,0,"I am currently trying to serialize a h2o gb model into a pickle object and reuse it. Due to some constraints, I can't use the default method or POJO and MOJO given at - 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html
. The model gets pickled, however on unpickling(pickle.loads), the following error comes up -


__new__() missing 1 required positional argument: 'keyvals'



The code below for reference-


import h2o as h2o
import pickle as pickle
from h2o.estimators.gbm import H2OGradientBoostingEstimator
h2o.init()

csv_url = ""https://h2o-public-test-data.s3.amazonaws.com/smalldata/wisc/wisc-diag-breast-cancer-shuffled.csv""
data = h2o.import_file(csv_url)
y = 'diagnosis'
x = data.columns
del x[0:1]
train, test = data.split_frame(ratios=[0.75], seed=1)


model = H2OGradientBoostingEstimator(distribution='bernoulli',
                                ntrees=100,
                                max_depth=4,
                                learn_rate=0.1)
model.train(x=x, y=y, training_frame=train, validation_frame=test)

loaded_model = pickle.loads(saved_model)
perf = loaded_model.model_performance(test)
perf.auc()



I tried to understand the pickle module and make some changes, but it didn't work. Any workaround/help will be highly appreciated. Thanks.","['python', 'serialization', 'machine-learning', 'h2o']",Unknown,,N/A
50625442,50625442,2018-05-31T13:25:11,2018-06-01 00:51:14Z,54,"I ran H2O on yarn according to the official requirements:


http://h2o-release.s3.amazonaws.com/h2o/rel-wolpert/11/index.html



This is my command:


cd ~/opt/h2o-3.18.0.8-hdp2.6
hadoop jar h2odriver.jar -nodes 1 -mapperXmx 6g -output /user/spark/h2o-3_output



And the h2o cluster is running successfully.



But after I run the example flow in the h2o-flow, I didn't see any calculations related to the GBM algorithm, only H2O itself.



I thought I would see something like this.This was the result of a decision tree flow diagram using RapidMiner, which presents each specific task.","['apache-spark', 'hadoop', 'hadoop-yarn', 'h2o']",Unknown,,N/A
50607610,50607610,2018-05-30T14:46:17,2018-05-30 18:19:21Z,0,"I have a csv file which is more than 20gb. I can read the first few lines using readlines and then figure out which columns I want to import. Is it possible to import only these columns using 
h2o.importFile()
 or some other way in h2o so that I am not loading unnecessary columns?","['r', 'h2o']",deepAgrawal,https://stackoverflow.com/users/2796123/deepagrawal,723
50601823,50601823,2018-05-30T10:01:51,2018-05-30 15:54:40Z,0,"Whenever I transfer my data into h2o from R, the h2o server appears to convert any blanks into NA - I want to be able to differentiate the two cases.


This happens when i use 
as.h2o(mydata)
 or if i save the data to a csv, then load it into h2o through the R interface via 
data.table::fwrite()
 then 
h2o.uploadFile()
. I noticed that as.h2o replicated the fwrite, uploadFile steps so I would prefer to rely on it since it has some other checks.


I've noticed that h2o code has a single na string of ""NA_h2o"" so I'm not sure why it also handles empties the same way.


How can I get around this?","['r', 'h2o']",Simon R,https://stackoverflow.com/users/9869324/simon-r,11
50581125,50581125,2018-05-29T09:40:02,2019-01-04 23:18:46Z,233,"I wanted to execute the DeepLearning example by using H2O. But it went wrong when running 
""DeepLearningV3 dlBody = h2o.train_deeplearning(dlParams);""


The error message:


 Illegal argument for field: 
 hidden of schema: 
 DeepLearningParametersV3: 
 cannot convert """"200"""" to type int



This is my code, I used the default value of dlParam except 
""responseColumn""
. After it went wrong, I added one line to set value of 
""hidden""
, but the results didn't change.


   private void DL() throws IOException {
    //H2O start
    String url = ""http://localhost:54321/"";
    H2oApi h2o = new H2oApi(url);

    //STEP 0: init a session
    String sessionId = h2o.newSession().sessionKey;

    //STEP 1: import raw file
    String path = ""hdfs://kbmst:9000/user/spark/datasets/iris.csv"";
    ImportFilesV3 importBody = h2o.importFiles(path, null);
    System.out.println(""import: "" + importBody);

    //STEP 2: parse setup
    ParseSetupV3 parseSetupParams = new ParseSetupV3();
    parseSetupParams.sourceFrames = H2oApi.stringArrayToKeyArray(importBody.destinationFrames, FrameKeyV3.class);
    ParseSetupV3 parseSetupBody = h2o.guessParseSetup(parseSetupParams);
    System.out.println(""parseSetupBody: "" + parseSetupBody);

    //STEP 3: parse into columnar Frame
    ParseV3 parseParams = new ParseV3();
    H2oApi.copyFields(parseParams, parseSetupBody);
    parseParams.destinationFrame = H2oApi.stringToFrameKey(""iris.hex"");
    parseParams.blocking = true;
    ParseV3 parseBody = h2o.parse(parseParams);
    System.out.println(""parseBody: "" + parseBody);

    //STEP 4: Split into test and train datasets
    String tmpVec = ""tmp_"" + UUID.randomUUID().toString();
    String splitExpr =
            ""(, "" +
                    ""  (tmp= "" + tmpVec + "" (h2o.runif iris.hex 906317))"" +
                    ""  (assign train "" +
                    ""    (rows iris.hex (<= "" + tmpVec + "" 0.75)))"" +
                    ""  (assign test "" +
                    ""    (rows iris.hex (> "" + tmpVec + "" 0.75)))"" +
                    ""  (rm "" + tmpVec + ""))"";
    RapidsSchemaV3 rapidsParams = new RapidsSchemaV3();
    rapidsParams.sessionId = sessionId;
    rapidsParams.ast = splitExpr;
    h2o.rapidsExec(rapidsParams);

    // STEP 5: Train the model
    // (NOTE: step 4 is polling, which we don't require because we specified blocking for the parse above)
    DeepLearningParametersV3 dlParams = new DeepLearningParametersV3();
    dlParams.trainingFrame = H2oApi.stringToFrameKey(""train"");
    dlParams.validationFrame = H2oApi.stringToFrameKey(""test"");

    dlParams.hidden=new int[]{200,200};

    ColSpecifierV3 responseColumn = new ColSpecifierV3();
    responseColumn.columnName = ""class"";
    dlParams.responseColumn = responseColumn;

    System.out.println(""About to train DL. . ."");
    DeepLearningV3 dlBody = h2o.train_deeplearning(dlParams);
    System.out.println(""dlBody: "" + dlBody);

    // STEP 6: poll for completion
    JobV3 job = h2o.waitForJobCompletion(dlBody.job.key);
    System.out.println(""DL build done."");

    // STEP 7: fetch the model
    ModelKeyV3 model_key = (ModelKeyV3)job.dest;
    ModelsV3 models = h2o.model(model_key);
    System.out.println(""models: "" + models);
    DeepLearningModelV3 model = (DeepLearningModelV3)models.models[0];
    System.out.println(""new DL model: "" + model);


    // STEP 8: predict!
    ModelMetricsListSchemaV3 predict_params = new ModelMetricsListSchemaV3();
    predict_params.model = model_key;
    predict_params.frame = dlParams.trainingFrame;
    predict_params.predictionsFrame = H2oApi.stringToFrameKey(""predictions"");

    ModelMetricsListSchemaV3 predictions = h2o.predict(predict_params);
    System.out.println(""predictions: "" + predictions);

    // STEP 9: end the session
    h2o.endSession();
}



I found the relative source code, but I can't understand why it goes wrong.
This is the definition of hidden.


public class DeepLearningParametersV3 extends ModelParametersSchemaV3 {{
    /**
     * Hidden layer sizes (e.g. [100, 100]).
    */
    public int[] hidden;
    //other params
}



And this is the code where the error message showed.It was the line 
String msg = ""Illegal argument for field: "" + field_name + "" of schema: "" +  schemaClass.getSimpleName() + "": cannot convert \"""" + s + ""\"" to type "" + fclz.getSimpleName();


static <E> Object parse(String field_name, String s, Class fclz, boolean required, Class schemaClass) {
if (fclz.isPrimitive() || String.class.equals(fclz)) {
  try {
    return parsePrimitve(s, fclz);
  } catch (NumberFormatException ne) {
    String msg = ""Illegal argument for field: "" + field_name + "" of schema: "" +  schemaClass.getSimpleName() + "": cannot convert \"""" + s + ""\"" to type "" + fclz.getSimpleName();
    throw new H2OIllegalArgumentException(msg);
  }
}
// An array?
if (fclz.isArray()) {
  // Get component type
  Class<E> afclz = (Class<E>) fclz.getComponentType();
  // Result
  E[] a = null;
  // Handle simple case with null-array
  if (s.equals(""null"") || s.length() == 0) return null;
  // Handling of ""auto-parseable"" cases
  if (AutoParseable.class.isAssignableFrom(afclz))
    return gson.fromJson(s, fclz);
  // Splitted values
  String[] splits; // """".split("","") => {""""} so handle the empty case explicitly
  if (s.startsWith(""["") && s.endsWith(""]"") ) { // It looks like an array
    read(s, 0, '[', fclz);
    read(s, s.length() - 1, ']', fclz);
    String inside = s.substring(1, s.length() - 1).trim();
    if (inside.length() == 0)
      splits = new String[]{};
    else
      splits = splitArgs(inside);
  } else { // Lets try to parse single value as an array!
    // See PUBDEV-1955
    splits = new String[] { s.trim() };
  }

  // Can't cast an int[] to an Object[].  Sigh.
  if (afclz == int.class) { // TODO: other primitive types. . .
    a = (E[]) Array.newInstance(Integer.class, splits.length);
  } else if (afclz == double.class) {
    a = (E[]) Array.newInstance(Double.class, splits.length);
  } else if (afclz == float.class) {
    a = (E[]) Array.newInstance(Float.class, splits.length);
  } else {
    // Fails with primitive classes; need the wrapper class.  Thanks, Java.
    a = (E[]) Array.newInstance(afclz, splits.length);
  }

  for (int i = 0; i < splits.length; i++) {
    if (String.class == afclz || KeyV3.class.isAssignableFrom(afclz)) {
      // strip quotes off string values inside array
      String stripped = splits[i].trim();

      if (""null"".equals(stripped.toLowerCase()) || ""na"".equals(stripped.toLowerCase())) {
        a[i] = null;
        continue;
      }

      // Quotes are now optional because standard clients will send arrays of length one as just strings.
      if (stripped.startsWith(""\"""") && stripped.endsWith(""\"""")) {
        stripped = stripped.substring(1, stripped.length() - 1);
      }

      a[i] = (E) parse(field_name, stripped, afclz, required, schemaClass);
    } else {
      a[i] = (E) parse(field_name, splits[i].trim(), afclz, required, schemaClass);
    }
  }
  return a;
}

// Are we parsing an object from a string? NOTE: we might want to make this check more restrictive.
if (! fclz.isAssignableFrom(Schema.class) && s != null && s.startsWith(""{"") && s.endsWith(""}"")) {
  return gson.fromJson(s, fclz);
}

if (fclz.equals(Key.class))
  if ((s == null || s.length() == 0) && required) throw new H2OKeyNotFoundArgumentException(field_name, s);
  else if (!required && (s == null || s.length() == 0)) return null;
  else
    return Key.make(s.startsWith(""\"""") ? s.substring(1, s.length() - 1) : s); // If the key name is in an array we need to trim surrounding quotes.

if (KeyV3.class.isAssignableFrom(fclz)) {
  if ((s == null || s.length() == 0) && required) throw new H2OKeyNotFoundArgumentException(field_name, s);
  if (!required && (s == null || s.length() == 0)) return null;

  return KeyV3.make(fclz, Key.make(s.startsWith(""\"""") ? s.substring(1, s.length() - 1) : s)); // If the key name is in an array we need to trim surrounding quotes.
}

if (Enum.class.isAssignableFrom(fclz)) {
  return EnumUtils.valueOf(fclz, s);
}

// TODO: these can be refactored into a single case using the facilities in Schema:
if (FrameV3.class.isAssignableFrom(fclz)) {
  if ((s == null || s.length() == 0) && required) throw new H2OKeyNotFoundArgumentException(field_name, s);
  else if (!required && (s == null || s.length() == 0)) return null;
  else {
    Value v = DKV.get(s);
    if (null == v) return null; // not required
    if (!v.isFrame()) throw H2OIllegalArgumentException.wrongKeyType(field_name, s, ""Frame"", v.get().getClass());
    return new FrameV3((Frame) v.get()); // TODO: version!
  }
}

if (JobV3.class.isAssignableFrom(fclz)) {
  if ((s == null || s.length() == 0) && required) throw new H2OKeyNotFoundArgumentException(s);
  else if (!required && (s == null || s.length() == 0)) return null;
  else {
    Value v = DKV.get(s);
    if (null == v) return null; // not required
    if (!v.isJob()) throw H2OIllegalArgumentException.wrongKeyType(field_name, s, ""Job"", v.get().getClass());
    return new JobV3().fillFromImpl((Job) v.get()); // TODO: version!
  }
}

// TODO: for now handle the case where we're only passing the name through; later we need to handle the case
// where the frame name is also specified.
if (FrameV3.ColSpecifierV3.class.isAssignableFrom(fclz)) {
  return new FrameV3.ColSpecifierV3(s);
}

if (ModelSchemaV3.class.isAssignableFrom(fclz))
  throw H2O.fail(""Can't yet take ModelSchemaV3 as input."");
/*
  if( (s==null || s.length()==0) && required ) throw new IllegalArgumentException(""Missing key"");
  else if (!required && (s == null || s.length() == 0)) return null;
  else {
  Value v = DKV.get(s);
  if (null == v) return null; // not required
  if (! v.isModel()) throw new IllegalArgumentException(""Model argument points to a non-model object."");
  return v.get();
  }
*/
throw H2O.fail(""Unimplemented schema fill from "" + fclz.getSimpleName());
} // parse()",['h2o'],Unknown,,N/A
50571600,50571600,2018-05-28T18:06:38,2018-06-22 23:43:40Z,0,"I want to build an autoencoder model with the Caret package with the following features:


1) Build an unsupervised neural network model using deep learning autoencoders


2) Using the autoencoder model in (1) as a pre-training input for a supervised model.


Online examples on using autoencoder in caret are quite few and far in between, offering no real insight into practical use cases.


I'm under data privacy and resource constraints so I'm unable to use H2o or Keras for neural networks.


Sample data for the model can be found at:

https://www.kaggle.com/nodarokroshiashvili/credit-card-fraud/data


An example of this in H2o is at this link:

https://shiring.github.io/machine_learning/2017/05/01/fraud


Any help or pointers in the right direction in this regard will be appreciated.


EDIT:
Thanks to Lauren and Erin, staff at H20 commenting that data privacy should not be a concern because H20 creates a cluster which is located on premise and not in an 'H20.cloud'","['r', 'neural-network', 'r-caret', 'h2o', 'autoencoder']",David Heckmann,https://stackoverflow.com/users/4420776/david-heckmann,"2,939"
50534675,50534675,2018-05-25T18:03:52,2018-05-28 14:45:06Z,0,"I used h2o.relevel to reorder the levels of a factor df$x. But, when I tried to get the min or max using h2o.which_min(df$x) and h2o.which_max, the output was: NAN. This tells me that h2o.relevel does not set a increasing order for instance. 


Example:
x: factor w/4 levels ""B"" ""D"" ""A"" ""C"". df is the dataframe.


I tried this: With h2o.relevel(df$x, levels = c(""A"", ""B"", ""C"", ""D"")), I'm able to rearrange the levels TO ""A"", ""B"", ""C"", ""D"", but A is not the minimum and D is not the maximum. h2o.which_min(df$x) and h2o.which_max return NAN.
How can I make A the min value and D the max value? Please help. Thank you","['r', 'rstudio', 'h2o']",Unknown,,N/A
50533113,50533113,2018-05-25T16:05:20,2018-05-25 16:35:41Z,0,"Working with h2o in python, how do you extract the model coefficients from a 'glm' multiclassification model?


For a binary model, you simply use .coef() or .coef_norm() methods, but these both return an error with multiclassification models. 


In the R version of h2o, it is very simple: model@model$coefficients_table will work just fine.


So im not sure why its so much more difficult in Python. Is there a different method I need to be using, or do you have to somehow manually iterate over the base one-vs-all models and extract the model coefficients somehow?


Thanks","['python', 'h2o']",Nate Thompson,https://stackoverflow.com/users/4008123/nate-thompson,635
50484826,50484826,2018-05-23T09:35:25,2018-05-23 10:01:42Z,95,"Can anyone tell me which kind of auto encoder (sparse, denoising etc.) h2o implements by design or depends this only by the used options?


Second Quesition:
Whats the difference between H2ODeepLearningEstimator() with autoencoder enabled and H2OAutoEncoderEstimator?


Thanks in advance.","['python', 'h2o', 'autoencoder']",Unknown,,N/A
50476210,50476210,2018-05-22T20:46:22,2018-08-14 02:47:43Z,0,"I'm trying to set an order for the levels of a factor in R H2O. Example 
x: factor w/5 levels ""3"" ""4"" ""5"" ""1"" ""2""
. 
df
 is the dataframe.


I tried this:
With 
h2o.setLevels(df$x, levels = c(""1"", ""2"", ""3"", ""4"", ""5""))
, I'm able to rearrange the levels TO ""1"", ""2"", ""3"", ""4"", ""5"", but not set the order I need with 1 as lowest and 5 as highest.
Any help would be appreciated! Thank you","['r', 'h2o']",ozanstats,https://stackoverflow.com/users/9684157/ozanstats,"2,846"
50458586,50458586,2018-05-22T01:31:54,2018-08-01 01:03:04Z,0,"I can't tell from documentation whether or not the 
predict.H2OModel()
 function from the 
h2o
 package in R gives OOB predictions for random forest models built using 
h2o.randomForest()
.  


In fact, in the 3-4 examples I've tried, it seems the results of 
predict.H2OModel()
 are closer to the non-OOB predictions from 
predict.randomForest()
 from the 
randomForest
 package than the OOB ones.  


Does anyone know if they are OOB predictions?  If not, do you know how to get OOB predictions for 
h2o.randomForest()
 models?


Example: 


set.seed(123)
library(randomForest)
library(h2o)

data(mtcars)
d = mtcars[,c('mpg', 'cyl', 'disp', 'hp', 'wt' )]

## define some common settings for both random forests
n.trees=1000
mtry = 3  
min.node = 3

## prep for h2o.randomForest
h2o.init()  
d.h2o= as.h2o(d) 
x.names = colnames(d)[2:5] ## predictors

## fit both models
set.seed(123); 
rf  =     randomForest(mpg ~ .,                      data = d    ,  ntree=n.trees,   mtry = mtry, nodesize=min.node)
h2o = h2o.randomForest(y='mpg', x=x.names, training_frame = d.h2o, ntrees=n.trees, mtries = mtry, min_rows=min.node)

## Correct way and incorrect way of getting OOB predictions for a randomForest model. Not sure about h2o model. 
d$rf.oob.pred =           predict(rf)                  ## Gives OOB predictions
d$rf.pred     =           predict(rf , newdata=d    )  ## Doesn't give OOB predictions.
d$h2o.pred    = as.vector(predict(h2o, newdata=d.h2o)) ## Not sure if this is OOB or not.  

## d$h2o.pred seems more similar to d$rf.pred than d$rf.oob.pred, 
## suggesting that predict.H2OModel() might not give OOB predictions.
mean((d$rf.pred     - d$h2o.pred)^2)
mean((d$rf.oob.pred - d$h2o.pred)^2)","['r', 'random-forest', 'h2o']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
50447863,50447863,2018-05-21T11:27:19,2018-05-21 14:12:18Z,0,"I've trained a linear model in R and exported a POJO which I've embedded in a (Java) SpringBoot webapp. Wrapping the POJO using the 
EasyPredictModelWrapper
 and instantiating an object is well described in the tutorials:


hex.genmodel.GenModel rawModel = (hex.genmodel.GenModel) Class.forName(modelClassName).newInstance();
model = new EasyPredictModelWrapper(rawModel);



I can assemble a 
RowData
 object:


RowData row = new RowData();
row.put(""feature1"", this.feature1);



and obtain a prediction for that row:


BinomialModelPrediction p = model.predictBinomial(row);



I've also exported the same model as a MOJO - the deployment procedure is almost identical.


My use case is that I receive a variable-length list of objects which need scoring, so I iterate the list and construct a row for each object, pass that to the wrapper and receive the prediction. So far so good.


Now I want to use a more sophisticated model that considers all the rows simultaneously. In R it's a case of passing a dataframe to the model, rather than a row (and receiving a dataframe back).


My question is: how can I pass a dataframe to a generated model object? I've gone over the source in 
the h2o-3 repo
, read the title of every SO post in the 
h2o
 tag and scrolled through the 
JIRA board
 until I got RSI, and the closest thing I've found is a DriverlessAI class called 
MojoFrame
, though the example given is a transform rather than a prediction.


I'm fairly sure that the 
EasyPredictModelWrapper
 doesn't support multirow input - the source is all about the 
RowData
 class. It would be great if it does but it's not necessarily a showstopper.


I'm less sure about the underlying model implementation: the 
MojoFrame
 and 
FrameBuilder
 classes tend to imply that it's supported within DriverlessAI, which may or may not use the same MOJO. Also the 
H2OFrame
 class crops up now and then in the context of scoring from Spark/R/Python, so although that's for a native model rather than the generated Java object it gives me hope that the model format can support it, and that a possible route might be to extend the support classes.


Options I've considered but haven't tested yet:




modify 
EasyPredictModelWrapper
 to accept Frames (lots of effort to understand the MOJO format)


renormalise the input list into a single row (breaks the frame-based paradigm for training the model, which probably breaks the whole process)




Has anyone done multi-row / frame-based input with the generated POJO or MOJO artifacts?",['h2o'],xenoclast,https://stackoverflow.com/users/1770618/xenoclast,"1,635"
50416866,50416866,2018-05-18T17:55:31,2018-05-22 16:53:57Z,526,"I am not able to create H2OContext in Spark Databricks- using pysparkling. It is giving the following error. 


Code:from pysparkling import *

Code:import h2o

Code:h2oConf = H2OConf(spark)

Code:h2oConf.set(""spark.ui.enabled"", True)


Out[2]: Sparkling Water configuration:
  backend cluster mode : internal
  workers              : None
  cloudName            : Not set yet, it will be set automatically before starting H2OContext.
  flatfile             : true
  clientBasePort       : 54321
  nodeBasePort         : 54321
  cloudTimeout         : 60000
  h2oNodeLog           : INFO
  h2oClientLog         : INFO
  nthreads             : -1
  drddMulFactor        : 10


Code: h2oContext = H2OContext.getOrCreate(spark, h2oConf)


Error: java.lang.NoSuchFieldError: quasibinomial


Here is all the details of the cluster:

1. Cluster:

Runtime version: Spark 2.1 (Auto updating, Scala 2.11)
Type: Standard 
Workers: 4 




Libraries attached with above cluster: 
h2o_pysparkling_2.1,

h2o-genmodel.jar","['apache-spark', 'pyspark', 'h2o', 'databricks', 'sparkling-water']",Marvania Mehul,https://stackoverflow.com/users/5378027/marvania-mehul,165
50407731,50407731,2018-05-18T09:09:40,2018-12-15 16:34:35Z,0,"Is there any ""simple"" way to plot trees from an H2O random forest model. I am also interestred in extracting the resulting rules ?","['python', 'random-forest', 'h2o']",Ala Ham,https://stackoverflow.com/users/4048081/ala-ham,169
50400887,50400887,2018-05-17T21:40:50,2018-05-17 22:16:45Z,0,"I have trained a binary classifier model using 
h2o.GLM
. I have around 5-10 features. I am wondering which would be faster in production?




Coding the logistic regression in my java code. 


Using the POJO by h2o. 


Using the MOJO generated by h2o.




Will this answer change if I trained a Random Forest model instead of GLM?


I need to score ~100 million rows. I am already distributing scoring of different observations.","['java', 'h2o']",deepAgrawal,https://stackoverflow.com/users/2796123/deepagrawal,723
50377307,50377307,2018-05-16T17:57:03,2018-05-20 14:55:41Z,0,"I am trying start embedded H2o in a Java application and train a model. However I don't get what exactly explained in the documentation (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/faq/java.html
). Can anyone help me by providing an example?


Thanks,","['java', 'machine-learning', 'h2o', 'sparkling-water']",Esildor,https://stackoverflow.com/users/669363/esildor,169
50367769,50367769,2018-05-16T09:57:09,2018-05-17 07:05:16Z,0,"The H2O-R-Package is providing functions to extract/convert entries of an H2OFrame object from milliseconds to :




Year
h2o.year() 


Month
h2o.month()


Hour
h2o.hour()


Day
h2o.day()    


Day of the week
h2o.dayOfWeek()


but nothing for minutes ...




How to convert the entries of an H2OFrame object from milliseconds to Minutes since the begining of Hour ?


data.hex = h2o.importFile(filetoload, sep = "","" )
date.hex = data.hex[,3] 

#Number of minutes since the begining of Hour
date_epoch = as.data.frame(date.hex)
date_formated = apply(date_epoch , 1, function(x){
    date_format =  as.POSIXlt(x, origin=""1970-01-01"", tz=""HKT"")
    return(date_format)
} )

minu =  unlist(lapply(date_formated, function(x){

    return(x$min)
}))
minu.hex = as.h2o(minu)



The computing time of this code is extremely long compare to :


#Hour of day
heure.hex = hour(date.hex)



Is there any better solutions?
Why there is not any h2o.minute() fonction?","['r', 'h2o']",Sam Bst,https://stackoverflow.com/users/9799082/sam-bst,33
50361561,50361561,2018-05-16T02:17:43,2018-05-16 17:50:41Z,84,"Based on this 
document
 it seems like I could just run the H20 jar file in several pods with K8s services in front of them. I can then reference the K8s services in the IP address file. Has anyone tried anything like that before?","['kubernetes', 'h2o']",mdornfe1,https://stackoverflow.com/users/2145297/mdornfe1,"2,130"
50278965,50278965,2018-05-10T18:05:18,2018-05-10 22:56:00Z,86,"I have been working with building the h2o4gpu Docker image, and after getting through the build process for the CentOS image, have found myself with a few more questions:




What is the difference between the Dockerfile_runtime, and Dockerfile-build-centos.x86-64-centos7-cuda9.0? Near as I can tell, the Dockerfile_runtime is intended for an Ubuntu image, but is there a preferred one to use?


After building the Centos Dockerfile, I have been using the below code to try and run it:


Docker run -it --rpm -p 8888:8888 dockerimage




When I do, I get the error:


Error response from daemon: oci runtime error: container linux.go:247: starting container process caused ""exec \""./run.sh\"": stat ./run.sh: no such file or directory



I did try adding in the included run.sh file, partially since I saw that it was included in the Dockerfile_runtime:


COPY run.sh /run.sh
ENTRYPOINT [""./run.sh""]
CMD [""/run.sh]



However, this didn't have any visible changes on the output, so I've been scratching my head what to do next.


I appreciate any insight or thoughts offered!


Thank you!","['docker', 'h2o']",strudlelion,https://stackoverflow.com/users/8184497/strudlelion,15
50274481,50274481,2018-05-10T13:49:40,2018-05-10 13:58:04Z,0,"I am running RStudio 1.0.136 on a server through my browser. I installed the package and can call the library. But when I type h2o.init(), it takes a while and then it gives me the following error:


h2o.init()



H2O is not running yet, starting it now...


Note:  In case of errors look at the following log files:
    /tmp/RtmplsTxlS/h2o_aelard_started_from_r.out
    /tmp/RtmplsTxlS/h2o_aelard_started_from_r.err


openjdk version ""1.8.0_141""
OpenJDK Runtime Environment (build 1.8.0_141-b16)
OpenJDK 64-Bit Server VM (build 25.141-b16, mixed mode)


Starting H2O JVM and connecting: localhost/127.0.0.1 some of the required ports 54321, 54322 are not available, change -port PORT and try again. 
 ""localhost""
 54321
 FALSE
 502


Error in h2o.init() : H2O failed to start, stopping execution.","['r', 'h2o']",Hootan Kamran,https://stackoverflow.com/users/4592221/hootan-kamran,11
50267961,50267961,2018-05-10T07:40:42,2018-05-21 06:43:43Z,57,"I was running sparkling water when this occured.


This my version,totally meets the document's requirement.


jdk1.7.0_67
scala-2.11.5
hadoop-2.6.5
spark-2.1.2-bin-hadoop2.6
sparkling-water-2.1.27



I used the command, started it successfully :


bin/sparkling-shell --conf ""spark.executor.memory=1g""



And it shows this:




But when I use the next command, it went wrong.


Exception in thread ""H2O Launcher thread"" 
java.lang.UnsupportedClassVersionError: 
ai/h2o/extensions/stacktrace/StackTraceExtension: 
Unsupported major.minor version 52.0



This is the document provied by H2O.","['apache-spark', 'hadoop', 'h2o']",liyuhui,https://stackoverflow.com/users/7124383/liyuhui,"1,250"
50254178,50254178,2018-05-09T13:06:41,2018-05-09 20:29:27Z,252,"When training a model in h2o v3.10 using the python h2o library, I am seeing an error when trying to set 
one_hot_explicit
 as a choice for the 
categorical_encoding
 parameter. 


encoding = ""enum""

gbm = H2OGradientBoostingEstimator(
        categorical_encoding = encoding)

gbm.train(x, y,train_h2o_df,test_h2o_df)



Works fine and the model uses 
enum
 categorical_encoding, but when: 


encoding = ""one_hot_explicit""



or


encoding = ""OneHotExplicit""



the following error is raised: 


gbm Model Build progress: | (failed)
....
OSError: Job with key $03017f00000132d4ffffffff$_bde8fcb4777df7e0be1199bf590a47f9 failed with an exception: java.lang.AssertionError
stacktrace: 
java.lang.AssertionError
at hex.ModelBuilder.init(ModelBuilder.java:958)
at hex.tree.SharedTree.init(SharedTree.java:78)
at hex.tree.gbm.GBM.init(GBM.java:57)
at hex.tree.SharedTree$Driver.computeImpl(SharedTree.java:159)
at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:169)
at water.H2O$H2OCountedCompleter.compute(H2O.java:1203)
at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



Is there some dependency I'm missing or is this a bug?","['python', 'python-3.x', 'h2o']",joshi123,https://stackoverflow.com/users/5665505/joshi123,865
50227205,50227205,2018-05-08T06:26:09,2018-05-08 15:19:46Z,86,"I was running H2O on Hardoop when this error occured.
This is the doc of how to install and running H2O on Hardoop:





First, I download H2O for my version of Hadoop.


Then I unpack the zip file and run this command




hadoop jar h2odriver.jar -nodes 1 -mapperXmx 6g -output /user/root/lyh2/h2o_output




But the console shows error info:




ERRR: Cannot find free port for /168.2.4.54 from baseport = 54321
EmbeddedH2OConfig: exit called (-1)
EmbeddedH2OConfig: after bwt.start()




I check the port 54321, no other application is using it.




Does anyone know how to fix it?","['linux', 'hadoop', 'h2o']",liyuhui,https://stackoverflow.com/users/7124383/liyuhui,"1,250"
50162987,50162987,2018-05-03T20:00:22,2018-05-03 20:44:14Z,0,"I am very used to the h2o framework from R, but having some trouble getting adjusted to certain aspects of h2o within python. 


I know that you can create a copy of a pandas dataframe using the .copy() method, so that when you update the new dataframe you dont update the original one as well. Do h2o frames have similar functionality? What makes it even more complicated is that h2o frames seem to not behave according to function local / global environment rules. 


Below is an example, and it seems that if only I could create a .copy of the frame, or have the function local environment not update my global environment it would solve my issue. If I create this same exact thing within R, then it behaves exactly as expected and doesnt actually modify the column in my original h2o frame, so how can I get python to work the same way?


##### A FUNCTION TO CHANGE THE VALUE OF A COLUMN
def test_func(train_df,
              var):

    train_df[var] = train_df[var].log()

    return(train_df)

##### TRY TO CREATE A NEW COPY OF THE FRAME WITH THE COLUMN CHANGED
new_df = test_func(train_df = old_df,
                   var = 'target')

##### THE COLUMN IN BOTH new_df AND old_df has both been changed.","['python', 'h2o']",Nate Thompson,https://stackoverflow.com/users/4008123/nate-thompson,635
50141622,50141622,2018-05-02T18:54:08,2018-05-03 23:58:20Z,146,"I am trying to set up H20 on Hadoop ,so we have a HDP2.5 Hadoop cluster and i have downloaded and unzipped : h2o-3.18.0.8-hdp2.5 on my edgenode  .


I ran :
 hadoop jar h2odriver.jar -nodes 1 -mapperXmx 6g -output /user//h20test


I see that its getting launched ,but the ui seems to binding to 127.0.0.1 and when i go to browser as expected its not working.Can someone please help


Sample Log from above command :


Job name 'H2O_84301' submitted
JobTracker job ID is 'job_1523015956637_0262'
For YARN users, logs command is 'yarn logs -applicationId application_1523015956637_0262'
Waiting for H2O cluster to come up...
H2O node 127.0.0.1:54321 requested flatfile
Sending flatfiles to nodes...
    [Sending flatfile to node 127.0.0.1:54321]
H2O node 127.0.0.1:54321 reports H2O cluster size 1 [leader is 127.0.0.1:54321]
H2O cluster (1 nodes) is up
(Note: Use the -disown option to exit the driver after cluster formation)


Open H2O Flow in your web browser: 
http://127.0.0.1:54321


(Press Ctrl-C to kill the cluster)
Blocking until the H2O cluster shuts down...","['hadoop', 'h2o']",Unknown,,N/A
50127250,50127250,2016-11-10T17:24:26,2021-10-18 17:58:52Z,0,"I'm evaluating tools for production ML based applications and one of our options is Spark MLlib , but I have some questions about how to serve a model once its trained? 


For example in Azure ML, once trained, the model is exposed as a web service which can be consumed from any application, and it's a similar case with Amazon ML.


How do you serve/deploy ML models in Apache Spark ?","['apache-spark', 'machine-learning', 'apache-spark-mllib']",eliasah,https://stackoverflow.com/users/3415409/eliasah,40.3k
50124080,50124080,2018-05-01T21:27:17,2018-05-08 13:40:34Z,0,"I am trying to run 
h2o.xgboost()
 in R and was able to use that successfully in 
3.14.0.3 version
. But, I recently updated to 
3.18.0.8
 version and I am getting below error. I tried lot of things but was not able to find reason. Any help will be appreciated.


Error:


DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.NullPointerException

DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.NullPointerException
    at water.MRTask.getResult(MRTask.java:478)
    at water.MRTask.getResult(MRTask.java:486)
    at water.MRTask.doAll(MRTask.java:390)
    at water.MRTask.doAll(MRTask.java:386)
    at ml.dmlc.xgboost4j.java.XGBoostScoreTask.runScoreTask(XGBoostScoreTask.java:45)
    at hex.tree.xgboost.XGBoostModel.makePreds(XGBoostModel.java:367)
    at hex.tree.xgboost.XGBoostModel.makeMetrics(XGBoostModel.java:343)
    at hex.tree.xgboost.XGBoostModel.makeMetrics(XGBoostModel.java:337)
    at hex.tree.xgboost.XGBoostModel.doScoring(XGBoostModel.java:387)
    at hex.tree.xgboost.XGBoost$XGBoostDriver.doScoring(XGBoost.java:470)
    at hex.tree.xgboost.XGBoost$XGBoostDriver.scoreAndBuildTrees(XGBoost.java:376)
    at hex.tree.xgboost.XGBoost$XGBoostDriver.buildModelImpl(XGBoost.java:335)
    at hex.tree.xgboost.XGBoost$XGBoostDriver.buildModel(XGBoost.java:262)
    at hex.tree.xgboost.XGBoost$XGBoostDriver.computeImpl(XGBoost.java:252)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:206)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1263)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
Caused by: java.lang.NullPointerException
    at hex.tree.xgboost.XGBoostUtils.dense(XGBoostUtils.java:311)
    at hex.tree.xgboost.XGBoostUtils.convertChunksToDMatrix(XGBoostUtils.java:279)
    at ml.dmlc.xgboost4j.java.XGBoostScoreTask.map(XGBoostScoreTask.java:139)
    at water.MRTask.compute2(MRTask.java:657)
    at water.MRTask.compute2(MRTask.java:591)
    at water.MRTask.compute2(MRTask.java:591)
    at water.MRTask.compute2(MRTask.java:591)
    at water.MRTask.compute2(MRTask.java:591)
    at water.MRTask.compute2(MRTask.java:591)
    at water.MRTask.compute2(MRTask.java:591)
    at water.H2O$H2OCountedCompleter.compute1(H2O.java:1266)
    at ml.dmlc.xgboost4j.java.XGBoostScoreTask$Icer.compute1(XGBoostScoreTask$Icer.java)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1262)
    ... 5 more

Error: DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.NullPointerException



My Code:


my_xgb1 <- h2o.xgboost(x = num_x,
                       y = num_y,
                       training_frame = hex,
                       distribution = ""bernoulli"",
                       ntrees = 50,
                       max_depth = 3,
                       min_rows = 2,
                       learn_rate = 0.2,
                       nfolds = nfolds,
                       fold_assignment = ""Modulo"",
                       keep_cross_validation_predictions = TRUE,
                       seed = 1)



Note: Same code works with 
3.14.0.3","['r', 'dataframe', 'h2o', 'xgboost']",Unknown,,N/A
50115886,50115886,2018-05-01T11:42:23,2019-06-07 17:25:29Z,0,"I just installed h2o module on python 3.6.1... 
(Java version 8 Update 74 (build 1.8.0_74-b02)
windows version 10 pro )


When I am giving the command h2o init() , it causing me connection error.








import h2o


h2o.init()








Checking whether there is an H2O instance running at 
http://localhost:54321
..... not found.
Attempting to start a local H2O server...
Traceback (most recent call last):
  File ""C:\Users\nshirsat\AppData\Local\Programs\Python\Python36\lib\site-packages\h2o\h2o.py"", line 252, in init
    ""connected."", ""not found.""))
  File ""C:\Users\nshirsat\AppData\Local\Programs\Python\Python36\lib\site-packages\h2o\backend\connection.py"", line 318, in open
    conn._cluster = conn._test_connection(retries, messages=_msgs)
  File ""C:\Users\nshirsat\AppData\Local\Programs\Python\Python36\lib\site-packages\h2o\backend\connection.py"", line 588, in _test_connection
    % (self._base_url, max_retries, ""\n"".join(errors)))
h2o.exceptions.H2OConnectionError: Could not establish link to the H2O cloud 
http://localhost:54321
 after 5 retries
[08:43.85] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError(': Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))
[08:46.12] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError(': Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))
[08:48.44] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError(': Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))
[08:50.75] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError(': Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))
[08:53.03] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError(': Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))


During handling of the above exception, another exception occurred:


Traceback (most recent call last):
  File """", line 1, in 
    h2o.init()
  File ""C:\Users\nshirsat\AppData\Local\Programs\Python\Python36\lib\site-packages\h2o\h2o.py"", line 261, in init
    min_mem_size=mmin, ice_root=ice_root, port=port, extra_classpath=extra_classpath)
  File ""C:\Users\nshirsat\AppData\Local\Programs\Python\Python36\lib\site-packages\h2o\backend\server.py"", line 121, in start
    mmax=max_mem_size, mmin=min_mem_size)
  File ""C:\Users\nshirsat\AppData\Local\Programs\Python\Python36\lib\site-packages\h2o\backend\server.py"", line 250, in _launch_server
    jver_bytes = subprocess.check_output([java, ""-version""], stderr=subprocess.STDOUT)
  File ""C:\Users\nshirsat\AppData\Local\Programs\Python\Python36\lib\subprocess.py"", line 336, in check_output
    **kwargs).stdout
  File ""C:\Users\nshirsat\AppData\Local\Programs\Python\Python36\lib\subprocess.py"", line 418, in run
    output=stdout, stderr=stderr)
subprocess.CalledProcessError: Command '['C:\WINDOWS\system32\java.exe', '-version']' returned non-zero exit status 2.


===============================


I am using office laptop and I am not getting how to resolve the error.
I search on internet with no luck.


I really appreciate your help in this.


Thanks & Regards,


Naresh","['python-3.x', 'h2o']",Naresh,https://stackoverflow.com/users/8295764/naresh,41
50093334,50093334,2018-04-30T02:43:27,2018-07-26 19:32:07Z,0,"Given a 
h2o
 dataframe df with a numeric column col, the sort of df by col works if the column is defined specifically:


h2o.arrange(df, ""col"")



But the sort doesn't work when I passed a dynamic variable name:


var <- ""A""
h2o.arrange(df, var)



I do not want to hard-coded the column name. Is there any way to solve it? Thanks.


added an example per Darren's request


library(h2o)
h2o.init()

df <- as.h2o(cars)

var <- ""dist""

h2o.arrange(df, var) # got error

h2o.arrange(df, ""dist"") # works","['r', 'h2o']",Unknown,,N/A
50052656,50052656,2018-04-26T22:21:23,2018-05-02 23:05:54Z,0,"I've always used a Mac and don't remember encountering this in the past so I'm not sure what I'm doing wrong.   Theres a bit of q's on this on the internet already but I didn't find a solution yet for me.


I just got a new mac and I'm trying to use rJava & h2o specifically.  Below is a couple of things I ran and the output from them.   Can someone help me out?


Java --version





java --version java 10.0.1 2018-04-17 Java(TM) SE Runtime Environment
  18.3 (build 10.0.1+10) Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode)




Javac --version





javac 10.0.1




/usr/libexec/java_home -V





Matching Java Virtual Machines (2):
    10.0.1, x86_64: ""Java SE 10.0.1""    /Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home
      1.8.0_171, x86_64:    ""Java SE 8"" /Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk/Contents/Home


/Library/Java/JavaVirtualMachines/jdk-10.0.1.jdk/Contents/Home




Sys.getenv('JAVA_HOME')





""/Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk""




library(h2o)
h2o.init()





H2O is not running yet, starting it now...  Error in value[3L] :    You have a 32-bit
  version of Java. H2O works best with 64-bit Java. Please download the
  latest Java SE JDK 7 from the following URL:
  
http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html




I definitely have the 64 bit of java installed as I re-installed it to be sure and see above the x64 notification from the output. Also, I have gotten this to work on another computer and so I don't understand what is going on  with this one. 


I also did 


vim ~/.bash_profile
EXPORT JAVA_HOME = ""/Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk""



but this doesn't seem to help me target the Java 8.","['java', 'r', 'macos', 'h2o', 'rjava']",Unknown,,N/A
50027950,50027950,2018-04-25T17:16:48,2020-05-07 10:44:58Z,593,"I trained a GBM in h2o using early stopping and setting 
ntrees=10000
. I want to retrieve the number of trees are actually in the model. But if I called 
model.params['ntrees']
 (where 
model
 is the best model from a gridsearch) I get


{'default': 50, 'actual': 10000}



where the 
10000
 is the parameter I set during training but not the actual number of trees that ended up in the model.


If I call 
model.score_history()
 then i can see that early stopping kicked in at 
280
 trees. But surely there is a more direct way to find out the actual number of trees in the model than this hack:


best_model.score_history()['number_of_trees'].max()","['python', 'h2o', 'hyperparameters']",Unknown,,N/A
50007751,50007751,2018-04-24T17:36:02,2018-10-12 03:31:22Z,0,"after running 
automl
 (classification of 3 classes), I can see a list of models as follows:


model_id                                                mean_per_class_error
StackedEnsemble_BestOfFamily_0_AutoML_20180420_174925   0.262355
StackedEnsemble_AllModels_0_AutoML_20180420_174925      0.262355
XRT_0_AutoML_20180420_174925                            0.266606
DRF_0_AutoML_20180420_174925                            0.278428
GLM_grid_0_AutoML_20180420_174925_model_0               0.442917



but 
mean_per_class_error
 is not a good metric for my case, where classes are unbalanced (one class has very small population). How to fetch details of non-leader models and calculate other metrics? Thanks.


python version: 3.6.0


h2o version:    3.18.0.5","['h2o', 'automl']",slowD,https://stackoverflow.com/users/2177373/slowd,339
49985533,49985533,2018-04-23T16:11:10,2018-06-13 01:31:52Z,0,"I'm trying to run H2O's AutoML in a for loop but I'm having trouble saving the results after each iteration. 


My plan is to run AutoML for each group in my data set, and saving the leaderboard for each of the groups. I already created subsetted datasets by group, so essentially what I want to do is loop through a list of h2o frames and build out automl for each of the datasets.  Below is the code I'm using:


#list of h2o frames 
dfs = c(df.UPR00015, df.UPR00019, df.UPR00020)
#list of frame id's
df_id = c(""df.UPR00015"", ""df.UPR00019"", ""df.UPR00020"")
#counter
count = 1



I then loop through the dataframes, and try to save each autoML object as follows: 


for (df in dfs){

#run auto for each data frames and name it as id
assign(sprintf(""aml_%s"", df_id[count]) , h2o.automl(x = x,
         y = y,
         training_frame = df,
         max_models = 5))

 #increase iterator
 count = count + 1

}



This almost works. It returns 3 correctly named H2oAutoML objects (aml_df.UPR00015, aml_df.UPR00019, aml_df.UPR00020), but they all contain the same information in the leaderboard. This means that the same leaderboard gets created for the 3 different subsets. In my real application I have 40 different groups to build models for, but only listed 3 for simplicity's sake. 


Can anyone please help me figure out where I'm going wrong, or perhaps share with me a better way to run AutoML for each data frame, or for each group in a dataset, in a loop? After I have a list of H2OAutoML bjects, I can easily loop through to retrieve the best models and all the metrics that comes along with that. 


Thanks in advance,
Jurgen","['r', 'loops', 'machine-learning', 'data-science', 'h2o']",Jurgen De Jager,https://stackoverflow.com/users/4901542/jurgen-de-jager,55
49980180,49980180,2018-04-23T11:36:45,2020-10-29 17:15:41Z,0,"I'm currently working with some insurance data to predict in what kind of insured sum class a customer will fall. To achieve this I'm using the AutoML function of the H2O package in R. Now that I have my model I'd like to be able to see which variables/features in my data contribute the most to the predictions the model makes. Is such a thing possible with H2O? If not, what would be another good option to achieve this with R? Thanks!","['r', 'variables', 'h2o']",Machavity,https://stackoverflow.com/users/2370483/machavity,31.5k
49942257,49942257,2018-04-20T12:59:39,2018-04-20 13:05:09Z,71,"I'm trying to set up H2O steam on a Linux VM. 


What I did so far: 




Set up VM with Ubuntu


Download and deployed file as described on official page: 

http://docs.h2o.ai/steam/latest-stable/Installation.html


Start jetty with the following command:


java -jar var/master/assets/jetty-runner.jar var/master/assets/ROOT.war


Start Steam with the following command:


steam serve master --superuser-name=admin --superuser-password=admin012




(official page seems outdated, it suggests run with option 
--admin=admin
, which gives error. After some search, I think 
--superuser
 is the new parameter?)


This seems to work, service starts with no error.


When I try to open 
http://localhost:9000
 in Chrome to get the UI and do stuff, I'm prompted for username and passwort. Trying admin / admin012 doesn't work, but gives no error, too. UI just don't open. 


Here I'm stuck. 


If anyone would have step-by-step instruction, on how to get current version of H2O steam work, would be great. 


Best regards,
Jochen",['h2o'],Unknown,,N/A
49932493,49932493,2018-04-20T01:24:43,2018-04-20 11:15:23Z,189,"I just installed h2o (python) on my linux VM. The server admin doesnt allow use of webserver on VM, hence I can't use a localhost. Is there a way i can still use H2o on my VM.  Can I create a fake local server or something to tricks h2o into believing that these is a web server.


Whenever i try to do a h2o.init() i get the following error.","['python', 'webserver', 'rhel', 'h2o']",Arslán,https://stackoverflow.com/users/4622664/arsl%c3%a1n,"1,773"
49920880,49920880,2018-04-19T12:15:03,2018-04-19 12:49:40Z,80,"I'm trying to productionize Python Sparkling Water application and I want to unify logging formats from my app, Spark and H2O.
I was able to modify log4j.properties in Spark home and achieve it with Spark logs, however, H2O logs doesn't have format applied (i.e. timestamp, severity are missing).


How to do it?","['python', 'log4j', 'h2o', 'sparkling-water']",Unknown,,N/A
49911639,49911639,2018-04-19T02:10:26,2018-04-19 02:10:26Z,86,"running an H2O Flow I've already successfully run in the past on a Mac OS with the following configuration:


04-18 22:50:02.722 192.168.0.11:54321    684    main      INFO: ----- H2O started  -----
04-18 22:50:02.723 192.168.0.11:54321    684    main      INFO: Build git branch: rel-wolpert
04-18 22:50:02.723 192.168.0.11:54321    684    main      INFO: Build git hash: 6903ba4b4cfcd912f15cf61965f88bb13bbc4890
04-18 22:50:02.723 192.168.0.11:54321    684    main      INFO: Build git describe: jenkins-3.18.0.3-6-g6903ba4
04-18 22:50:02.723 192.168.0.11:54321    684    main      INFO: Build project version: 3.18.0.4 (latest version: 3.18.0.7)
04-18 22:50:02.723 192.168.0.11:54321    684    main      INFO: Build age: 1 month and 10 days
04-18 22:50:02.723 192.168.0.11:54321    684    main      INFO: Built by: 'jenkins'
04-18 22:50:02.723 192.168.0.11:54321    684    main      INFO: Built on: '2018-03-08 19:15:57'
04-18 22:50:02.723 192.168.0.11:54321    684    main      INFO: Watchdog Build git branch: (unknown)
04-18 22:50:02.724 192.168.0.11:54321    684    main      INFO: Watchdog Build git hash: (unknown)
04-18 22:50:02.724 192.168.0.11:54321    684    main      INFO: Watchdog Build git describe: (unknown)
04-18 22:50:02.724 192.168.0.11:54321    684    main      INFO: Watchdog Build project version: (unknown)
04-18 22:50:02.724 192.168.0.11:54321    684    main      INFO: Watchdog Built by: (unknown)
04-18 22:50:02.724 192.168.0.11:54321    684    main      INFO: Watchdog Built on: (unknown)
04-18 22:50:02.724 192.168.0.11:54321    684    main      INFO: XGBoost Build git branch: (unknown)
04-18 22:50:02.724 192.168.0.11:54321    684    main      INFO: XGBoost Build git hash: (unknown)
04-18 22:50:02.725 192.168.0.11:54321    684    main      INFO: XGBoost Build git describe: (unknown)
04-18 22:50:02.725 192.168.0.11:54321    684    main      INFO: XGBoost Build project version: (unknown)
04-18 22:50:02.725 192.168.0.11:54321    684    main      INFO: XGBoost Built by: (unknown)
04-18 22:50:02.725 192.168.0.11:54321    684    main      INFO: XGBoost Built on: (unknown)
04-18 22:50:02.725 192.168.0.11:54321    684    main      INFO: KrbStandalone Build git branch: (unknown)
04-18 22:50:02.725 192.168.0.11:54321    684    main      INFO: KrbStandalone Build git hash: (unknown)
04-18 22:50:02.725 192.168.0.11:54321    684    main      INFO: KrbStandalone Build git describe: (unknown)
04-18 22:50:02.726 192.168.0.11:54321    684    main      INFO: KrbStandalone Build project version: (unknown)
04-18 22:50:02.726 192.168.0.11:54321    684    main      INFO: KrbStandalone Built by: (unknown)
04-18 22:50:02.726 192.168.0.11:54321    684    main      INFO: KrbStandalone Built on: (unknown)
04-18 22:50:02.726 192.168.0.11:54321    684    main      INFO: Processed H2O arguments: []
04-18 22:50:02.726 192.168.0.11:54321    684    main      INFO: Java availableProcessors: 4
04-18 22:50:02.726 192.168.0.11:54321    684    main      INFO: Java heap totalMemory: 245.5 MB
04-18 22:50:02.726 192.168.0.11:54321    684    main      INFO: Java heap maxMemory: 10.67 GB
04-18 22:50:02.726 192.168.0.11:54321    684    main      INFO: Java version: Java 1.8.0_162 (from Oracle Corporation)
04-18 22:50:02.726 192.168.0.11:54321    684    main      INFO: JVM launch parameters: [-Xmx12g]
04-18 22:50:02.726 192.168.0.11:54321    684    main      INFO: OS version: Mac OS X 10.12.6 (x86_64)
04-18 22:50:02.726 192.168.0.11:54321    684    main      INFO: Machine physical memory: 16.00 GB
04-18 22:50:02.727 192.168.0.11:54321    684    main      INFO: X-h2o-cluster-id: 1524102601065



My dataset has around 600k observations and ~60 features.
When building a simple GBM model with CV5 the job stops with the following trace:


java.lang.NullPointerException
    at hex.Model$Parameters.read_unlock_frames(Model.java:331)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:209)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1263)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



it always stops at 84% progress.


any clue why and what to do?


thanks
Carmelo","['java', 'nullpointerexception', 'h2o', 'gbm']",italiano,https://stackoverflow.com/users/9350818/italiano,21
49909866,49909866,2018-04-18T22:21:10,2018-04-19 16:40:15Z,314,"I'm looking for a way to set beta in prior to the model run in H2O GeneralizedLinearEstimator? Beta which can be used as a starting point for the model? It is called beta constraints as per the documentation below. Could someone help me with this.


http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/beta_constraints.html


Here is the example what I'm trying


1)   Model 1: I ran a model with 20 iterations and used the betas from this model run and saved it as a data frame


2)   Model 2: I ran a model with everything the same as the first Model (including 20 iterations), additionally I specified beta constraints as the coefficient from the previous model. Trying to warm start this model so it starts where the previous model ended.


3)   Model 3: I ran a model with everything the same as the first model but with 40 iteration


As explained in the documentation I wanted to compare and see that the model 2 betas to be same as the betas from a model 3


Code with beta constraints specification


    model2 =  H2OGeneralizedLinearEstimator(family = ""poisson"",
                                            alpha=0,
                                            solver = ""L-BFGS"", 
                                            max_iterations=20, 
                                            gradient_epsilon=1e-8, 
                                            objective_epsilon=1e-8, 
                                            beta_epsilon=1e-8, 
                                            beta_constraints=bc)","['pyspark', 'glm', 'h2o']",Balaji Srinivasan Ramanathan,https://stackoverflow.com/users/9631563/balaji-srinivasan-ramanathan,21
49907249,49907249,2018-04-18T19:05:24,2020-01-30 16:59:20Z,533,I'm able to deploy a h2o cluster with ec2 instances and having the private ip in the flatfile. Doing the same with docker works but I can't figure out what to enter into the flatfile so they can create the cluster. Private IP the container is running on is not working,"['docker', 'distributed', 'h2o', 'marathon', 'dcos']",Nick Anderson,https://stackoverflow.com/users/7460022/nick-anderson,11
49895721,49895721,2018-04-18T09:13:41,2018-04-18 10:35:42Z,760,"According to H2O docs in FAQ of the DRF section, 
this note is mentioned
 on the ""How does the algorithm handle missing values during training?"" FAQ:




Note:
 Unlike in GLM, in DRF numerical values are handled the same way as categorical values. Missing values are not imputed with the mean, as is done by default in GLM.




I use a DRF Algorithm to solve a regression problem, but when I saw this note, I felt strange.  If I convert all numerical value to categorical value to solve regression problem, I think that it is nonsense. 


Here is My question.




Do I need to convert all numerical values to categorical values to use DRF algorithm?
 




or




Do I not need to convert all numerical values to categorical values to use DRF algorithm?




Thank you to read my question.","['python', 'machine-learning', 'random-forest', 'h2o']",Unknown,,N/A
49885093,49885093,2018-04-17T18:29:09,2018-04-18 11:37:26Z,216,"I have recently built the h2o4gpu docker image using the Dockerfile-runtime, and managed to run it and log into the Jupyter notebooks.


However, when trying to run 


import h2o4gpu



I get the error that there is no h2o4gpu module. After, I tried installing by adding the below command to the dockerfile.


pip install --extra-index-url https://pypi.anaconda.org/gpuopenanalytics/simple h2o4gpu 
pip install h2o4gpu-0.2.0-cp36-cp36m-linux_x86_64.whl



This also failed, so I was wondering if there were other changes I should make, or if I should be making the docker file from scratch.


Thank you","['h2o', 'h2o4gpu']",strudlelion,https://stackoverflow.com/users/8184497/strudlelion,15
49866372,49866372,2018-04-16T21:17:04,2018-04-23 19:02:50Z,0,"I am relatively new to the machine learning ocean, please excuse me if some of my questions are really basic.


Current situation: The overall goal was trying to improve some code for h2o package in r running on the supercomputer cluster. However, since the data is too large that single node with h2o really takes more than a day, therefore, we have decided to use multiple nodes to run the model. I came up with an idea: 


(1) Distribute each node to build (nTree/num_node) trees and saved into a model; 


(2) running on the cluster at each node for (nTree/num_node) number of trees in the forest; 


(3) Merging the trees back together and reform the original forest, and using the measurement results in average.


I later realized this could be risky. But I cannot find the actual support or against statement since I am not machine learning focused programmer. 


Questions:




if this way of handling random forest will result in some risk, please reference me the link so I can have a basic idea why this is not right.


If this way is actually an ""ok"" way to do so. What should I be do to merge the trees, is there a package or method I can borrow from?


If this is actually a solved problem, please reference me the link, I may have searched the wrong keywords, and thank you!




The real number-involved example I can present here is:


I have a random forest task with 80k rows and 2k columns and wanted the number of trees are 64. What I have done is put 16 trees on each node running with the whole dataset, and each one of four nodes come up with an RF model. I am now trying to merge the trees from each model into this one big RF model and average the measurements (from each of those four models).","['r', 'machine-learning', 'parallel-processing', 'h2o']",Unknown,,N/A
49864021,49864021,2018-04-16T18:30:26,2018-04-16 20:35:41Z,442,"I have figured out how to download models as a MOJO, and how to use that to then make predictions onto new datasets that are in .csv format. I wanted to understand a little better how the .zip file of the model and the h2o-genmodel.jar files relate to each other. Here are my questions? 




1.) What is the exact difference about what each of these are storing / doing.


2.) Is the genmodel.jar file specific to that model in question, or can it be executed against a different .zip MOJO model file? 


If I am going to be downloading off multiple models, does each one need its own genmodel.jar file, along with the .zip file of the model?




Thanks!",['h2o'],Nate Thompson,https://stackoverflow.com/users/4008123/nate-thompson,635
49859063,49859063,2018-04-16T13:55:14,2018-04-16 13:55:14Z,340,"I would like to build a do my performance evaluation on top 3 predictions (by that, I mean, top N classes that are the most probable to assign an item, within a cross-validation).


Of course, I can keep the cross-validation result, go through each and look into the probabilities of assignment, sort them and pick the top 3 ones, and then build my confusion matrix. But I wonder if the functionality already exist?","['h2o', 'confusion-matrix', 'multiclass-classification']",Areza,https://stackoverflow.com/users/702846/areza,"6,030"
49824276,49824276,2018-04-13T19:58:09,2018-04-13 21:36:50Z,0,Trying to get the top 3 columns from my h2o GBM model using lime which has the highest significance in model prediction.,"['python-2.7', 'h2o', 'gbm']",Shreya Jain,https://stackoverflow.com/users/9643354/shreya-jain,3
49823178,49823178,2018-04-13T18:30:09,2022-12-02 20:16:17Z,0,"I am using h2o to perform predictive modeling from python.
I have loaded some data from a csv using pandas, specifying some column types:


dtype_dict = {'SIT_SSICCOMP':'object',
              'SIT_CAPACC':'object',
              'PTT_SSIRMPOL':'object',
              'PTT_SPTCLVEI':'object',
              'cap_pad':'object',
              'SIT_SADNS_RESP_PERC':'object',
              'SIT_GEOCODE':'object',
              'SIT_TIPOFIRMA':'object',
              'SIT_TPFRODESI':'object',
              'SIT_CITTAACC':'object',
              'SIT_INDIRACC':'object',
              'SIT_NUMCIVACC':'object'
              }
date_cols = [""SIT_SSIDTSIN"",""SIT_SSIDTDEN"",""PTT_SPTDTEFF"",""PTT_SPTDTSCA"",""SIT_DTANTIFRODE"",""PTT_DTELABOR""]


columns_to_drop = ['SIT_TPFRODESI','SIT_CITTAACC',
       'SIT_INDIRACC', 'SIT_NUMCIVACC', 'SIT_CAPACC', 'SIT_LONGITACC',
       'SIT_LATITACC','cap_pad','SIT_DTANTIFRODE']


comp='mycomp'

file_completo = os.path.join(dataDir,""db4modelrisk_""+comp+"".csv"")
db4scoring = pd.read_csv(filepath_or_buffer=file_completo,sep="";"", encoding='latin1',
                          header=0,infer_datetime_format =True,na_values=[''], keep_default_na =False,
                          parse_dates=date_cols,dtype=dtype_dict,nrows=500e3)
db4scoring.drop(labels=columns_to_drop,axis=1,inplace =True)



Then, after I set up a h2o cluster I import it in h2o using 
db4scoring_h2o = H2OFrame(db4scoring)
 and I convert categorical predictors in factor for example:


db4scoring_h2o[""SIT_SADTPROV""]=db4scoring_h2o[""SIT_SADTPROV""].asfactor()
db4scoring_h2o[""PTT_SPTFRAZ""]=db4scoring_h2o[""PTT_SPTFRAZ""].asfactor()



When I check data types using db4scoring.dtypes I notice that they are properly set but when I import it in h2o I notice that h2oframe performs some unwanted conversions to enum (eg from float or from int). I wonder if is is a way to specify the variable format in H2OFrame.","['python', 'pandas', 'casting', 'h2o']",Giorgio Spedicato,https://stackoverflow.com/users/1259856/giorgio-spedicato,"2,483"
49780303,49780303,2018-04-11T16:33:51,2018-04-12 15:35:24Z,0,"Having got the implementation of MOJO prediction working on local install, we migrated the code to Centos 7 and are now having issues with the code, despite the only real difference between local windows and centos being java versions. Code as follows. Error we get on Centos;


Error in system2(java, args, stdout = TRUE, stderr = TRUE) : 
  error in running command


This is version info from both platforms;




VM – Centos 7 openjdk version ""1.8.0_161"" OpenJDK Runtime Environment
  (build 1.8.0_161-b14) OpenJDK 64-Bit Server VM (build 25.161-b14,
  mixed mode) H2O cluster version:        3.16.0.2  R Version:

  R version 3.4.3 (2017-11-30) Local – Windows 7 64bit java version
  ""1.8.0_151"" Java(TM) SE Runtime Environment (build 1.8.0_151-b12) Java
  HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode) H2O
  cluster version:        3.16.0.2  R Version:                  R
  version 3.4.1 (2017-06-30)




library(RJDBC)
library(dplyr)
library(h2o)
library(jsonlite)

# set up h2o
options(java.parameters = ""- Xmx2400m"")
Sys.unsetenv(""http_proxy"")
localh20=h2o.init(ip           = ""localhost"",
                  nthreads     = -1, # how many CPU cores to use
                  port         = 54321,
                  max_mem_size = ""32g"") # how much memory to use
h2o.removeAll() 

df<- data.frame(VAR1=1,VAR2=1,VAR3=1,VAR4=1,VAR5=1,VAR6=1,VAR7=""'METAL'"",VAR8 = ""'LONDON & SE'"" )
dfstr <- sapply(1:ncol(df), function(i) paste(paste0('\""', names(df)[i], '\""'), df[1,i], sep = ':'))
json <- paste0('{', paste0(dfstr, collapse = ','), '}')
dataPredict <- as.data.frame(h2o.predict_json(model = ""D:\\GBM_model_0_CMP.zip"", json = json, genmodelpath = ""D:\\h2o-genmodel.jar"", labels = TRUE))
dataPredict <-dataPredict[,c(3,4)]
names(dataPredict) <- c(""Score"", ""Class"")","['java', 'r', 'h2o']",user2554330,https://stackoverflow.com/users/2554330/user2554330,44k
49774823,49774823,2018-04-11T12:12:38,2018-04-11 16:03:35Z,0,"I'm trying to use H2O Sparkling Water on Google DataProc.  I've successfully run Sparkling Water on a standalone Spark, and now moved on to use it on DataProc.  Initially, I got an error about 
spark.dynamicAllocation.enabled
 not being supported, so I've gone on the master and started like this...


pyspark \
   --conf spark.ext.h2o.fail.on.unsupported.spark.param=false \
   --conf spark.dynamicAllocation.enabled=false



The interaction to start Sparkling Water looks like this, once the stage gets to around 30000, it starts to grind, and then after 30 mins or so, there's a string of errors:


>>> from pysparkling import *
>>> import h2o
>>> hc = H2OContext.getOrCreate(spark)
18/04/11 11:56:08 WARN org.apache.spark.h2o.backends.internal.InternalH2OBackend: Increasing 'spark.locality.wait' to value 30000
18/04/11 11:56:08 WARN org.apache.spark.h2o.backends.internal.InternalH2OBackend: Due to non-deterministic behavior of Spark broadcast-based joins
We recommend to disable them by
configuring `spark.sql.autoBroadcastJoinThreshold` variable to value `-1`:
sqlContext.sql(""SET spark.sql.autoBroadcastJoinThreshold=-1"")
[Stage 0:=================>                               (35346 + 11) / 100001]



I've tried a variety of things like:
- Deploying small (3 nodes).
- Deploying a 30 worker cluster.
- Tried running DataProc image 1.1 (Spark 2.0), 1.2 (Spark 2.2) and preview (Spark 2.2).


Also tried a variety of Spark options:


spark.ext.h2o.fail.on.unsupported.spark.param=false \
spark.ext.h2o.nthreads=2
spark.ext.h2o.cluster.size=2
spark.ext.h2o.default.cluster.size=2
spark.ext.h2o.hadoop.memory=50m
spark.ext.h2o.repl.enabled=false
spark.ext.h2o.flatfile=false
spark.dynamicAllocation.enabled=false
spark.executor.memory=700m



Anyone have any luck with H2O on Google DataProc?


Detailed errors are:


18/04/11 12:08:40 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1523445048432_0005_01_000006 on host: cluster-dev-w-0.c.trust-networks.internal. Exit status: 1. Diagnostics: Exception from container-launch.
Container id: container_1523445048432_0005_01_000006
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
    at org.apache.hadoop.util.Shell.runCommand(Shell.java:972)
    at org.apache.hadoop.util.Shell.run(Shell.java:869)
    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170)
    at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)

Container exited with a non-zero exit code 1

18/04/11 12:08:48 ERROR org.apache.spark.network.server.TransportRequestHandler: Error sending result RpcResponse{requestId=5571077381947066483, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /10.154.0.12:59387; closing connection
java.nio.channels.ClosedChannelException
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)



and later:


Exception in thread ""task-result-getter-3"" java.lang.OutOfMemoryError: GC overhead limit exceeded
    at java.lang.Class.newReflectionData(Class.java:2513)
    at java.lang.Class.reflectionData(Class.java:2503)
    at java.lang.Class.privateGetDeclaredConstructors(Class.java:2660)
    at java.lang.Class.getConstructor0(Class.java:3075)
    at java.lang.Class.newInstance(Class.java:412)
    at sun.reflect.MethodAccessorGenerator$1.run(MethodAccessorGenerator.java:403)
    at sun.reflect.MethodAccessorGenerator$1.run(MethodAccessorGenerator.java:394)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.reflect.MethodAccessorGenerator.generate(MethodAccessorGenerator.java:393)
    at sun.reflect.MethodAccessorGenerator.generateSerializationConstructor(MethodAccessorGenerator.java:112)","['h2o', 'google-cloud-dataproc', 'sparkling-water']",cybermaggedon,https://stackoverflow.com/users/9630329/cybermaggedon,31
49773582,49773582,2018-04-11T11:11:17,2018-08-17 06:12:08Z,0,"I randomly encounter the same error whenever I run XGBoost model (both the normal run and grid search). The error message says this:


H2OConnectionError: Local server has died unexpectedly. RIP.


I don't know what happens, I tried to change versions but didn't work. I'm currently using the version 3.18.0.5. Does anyone have any idea what is happening? Thanks in advance","['python', 'machine-learning', 'artificial-intelligence', 'h2o', 'xgboost']",Cœur,https://stackoverflow.com/users/1033581/c%c5%93ur,38.5k
49752125,49752125,2018-04-10T11:10:15,2020-07-18 03:36:08Z,0,"I tried to run xgboost on my local machine with Windows OS. But the following error : 


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

java.lang.AssertionError: Unregistered algorithm xgboost



here is my code sample : 


library(h2o)
h2o.init(enable_assertions = TRUE)
localH2O=h2o.init(nthreads = 8)

train.h2o <- h2o.importFile(""train.csv"")
test.h2o <- h2o.importFile(""test.csv"")

# Number of CV folds (to generate level-one data for stacking)
nfolds <- 5

y <- get_index(train.h2o,""loss"")
x <- setdiff(1:length(train.h2o), y)
x=h2o.colnames(train.h2o[,x])
y=h2o.colnames(train.h2o[,y])


my_xgb1 <- h2o.xgboost(x = x,
                       y = y,
                       training_frame = train.h2o,
                       ntrees = 50,
                       max_depth = 3,
                       min_rows = 2,
                       learn_rate = 0.2,
                       nfolds = nfolds,
                       fold_assignment = ""Modulo"",
                       keep_cross_validation_predictions = TRUE,
                       seed = 1)



when I run it, I get the following error : 




ERROR: Unexpected HTTP Status code: 500 Server Error (url =
  
http://localhost:54321/3/ModelBuilders/xgboost
)


java.lang.AssertionError  [1] ""java.lang.AssertionError: Unregistered
  algorithm xgboost""

  [2] ""    hex.ModelBuilder.make(ModelBuilder.java:149)""

  [3] ""

  water.api.ModelBuildersHandler.fetch(ModelBuildersHandler.java:35)""

  [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""


....




I found here that xgboost for h2o work on some specific dev environments. 


MY QUESTION is:
 how can I make it work on Windows OS ? 


Thank you.","['r', 'machine-learning', 'h2o', 'xgboost']",smerllo,https://stackoverflow.com/users/7362261/smerllo,"3,325"
49716815,49716815,2018-04-08T10:18:43,2018-06-04 19:51:23Z,0,"I am building a shiny app that scores the data of the deep learning model using 
h2o
 engine.


I could achieve my goal by simply placing my predicting operation into the function. In this function I would typically start my deep learning machine, make calculations and stop it. This is unfortunately slow.


My goal is to start 
h2o
 in the beginning, when user starts 
shiny app
 from 
R-Server
 and then to make sure the 
h2o
 virtual machine will shutdown when user closes the browser.


I would ask to suggest the most optimal way to do that because I am not fully satisfied with this method taken from 
here
 where I just placed these lines of code into 
global.R
 script:


#global.R
library(h2o)
h2o.init(nthreads = 2)
onStop(function() {
  # shut down the h2o on app exit see 
  h2o.shutdown(prompt = FALSE)
})



It seems that sometimes my h2o instance is stopped before as I got an error 
Error in h2o.shutdown(prompt = FALSE) : There is no H2O instance running.


... I am now testing it in the browser but I just want to make sure there would be no consequences on the R-Server


Any help is appreciated!","['r', 'shiny', 'shiny-server', 'h2o']",topchef,https://stackoverflow.com/users/59470/topchef,19.8k
49712632,49712632,2018-04-07T22:35:30,2018-04-13 06:39:36Z,856,"When I use H2O 3.19, I want to save the training data's performance 
matplotlib.pyplot
 graphic (ROC) at server side, how can I do it?


Here we can see the source code of 
plot()
 in 
h2o/model/metrics_base.py
:


def plot(self, type=""roc"", server=False):
    """"""
    Produce the desired metric plot.

    :param type: the type of metric plot (currently, only ROC supported).
    :param server: if True, generate plot inline using matplotlib's ""Agg"" backend.
    :returns: None
    """"""
    # TODO: add more types (i.e. cutoffs)
    assert_is_type(type, ""roc"")
    # check for matplotlib. exit if absent.
    try:
        imp.find_module('matplotlib')
        import matplotlib
        if server: matplotlib.use('Agg', warn=False)
        import matplotlib.pyplot as plt
    except ImportError:
        print(""matplotlib is required for this function!"")
        return

    if type == ""roc"":
        plt.xlabel('False Positive Rate (FPR)')
        plt.ylabel('True Positive Rate (TPR)')
        plt.title('ROC Curve')
        plt.text(0.5, 0.5, r'AUC={0:.4f}'.format(self._metric_json[""AUC""]))
        plt.plot(self.fprs, self.tprs, 'b--')
        plt.axis([0, 1, 0, 1])
        if not server: plt.show()



plot(type=""roc"", server=False)
 just examines the existence of 
matplotlib.pyplot
 and does not return the 
plt
 object, so I cannot call 
plt.savefig()
. What can I do?","['python', 'matplotlib', 'h2o']",Unknown,,N/A
49711455,49711455,2018-04-07T19:59:50,2018-04-09 09:27:04Z,0,"I am trying to understand how deep features are made in an autoencoder.


I created an autoencoder with 
h2o.deeplearning
 and then I tried to
calculate the deepfeatures manually.


The autoencoder


fit = h2o.deeplearning(
x = names(x_train),
training_frame = x_train,
activation = ""Tanh"",
autoencoder = TRUE,
hidden = c(25,10),
epochs = 100,
export_weights_and_biases = TRUE,
)    



I used as activation function Tanh and 2 hidden layers with no dropout, to
make the things simple.


Calculating hidden layer 1 deep features manually


Then I extracted the weighs and biases that goes from the input layer to the hidden layer 1


w12 = as.matrix(h2o.weights(fit, 1))
b12 = as.matrix(h2o.biases (fit,1))



I prepared the training data for the operations normalizing it between the
compact interval of [-0.5 , 0.5] because h2o does that automatically in 
autoencoders.


normalize = function(x) {(((x-min(x))/(max(x)-min(x))) - 0.5)}
d.norm =  apply(d, 2, normalize)`



Then I calculated manually the deepfeatures of the first layer


a12 = d.norm %*% t(w12)
b12.rep = do.call(rbind, rep(list(t(b12)), nrow(d.norm)))
z12 = a12 + b12.rep
f12 = tanh(z12)



When I compared those values with hidden layer 1 deep features, they didnt match


hl1.output = as.matrix(h2o.deepfeatures(fit, x_train, layer = 1))
all.equal(
as.numeric(f12[,1]),
hl1.output[, 1],
check.attributes = FALSE,
use.names = FALSE,
tolerance = 1e-04
)
[1] ""Mean relative difference: 0.4854887""



Calculating hidden layer 2 deep features manually


Then I tried to do the same thing to calculate manually the deep features of
the hiddem layer 2 from the deep features of the hidden layer 1


a23 = hl1.output %*% t(w23)
b23.rep = do.call(rbind, rep(list(t(b23)), nrow(a23)))
z23 = a23 + b23.rep
f23 = tanh(z23)



Comparing these values with the deep features of the hidden layer 2
I saw that they match perfecly


hl2.output = as.matrix(h2o.deepfeatures(fit,x_train,layer = 2))
all.equal(
as.numeric(f23[,1]),
hl2.output[, 1],
check.attributes = FALSE,
use.names = FALSE,
tolerance = 1e-04
)
[1] TRUE



Calculating the output layer features manually


I tried the same thing for the output layer


a34 = hl2.output %*% t(w34)
b34.rep = do.call(rbind, rep(list(t(b34)), nrow(a34)))
z34 = a34 + b34.rep
f34 = tanh(z34)



I compared the result with the output I had and I could not get the same result


all.equal(
as.numeric(f34[1,]),
output[1,],
check.attributes = FALSE,
use.names = FALSE,
tolerance = 1e-04
)
[1] ""Mean relative difference: 3.019762""



The questions


I think that I am not normalizing data in the correct way because I can recreate the deep features of the hidden layer 2 with the features of the hidden layer 1. I do not understand what is wrong, because with 
autoencoder = TRUE
 h2o should normalize the data between[-0.5:0.5]


I dont understand why the manual calculation of the output layer does not 
work


1) How to calculate manually the deep features of the hidden layer 1?


2) How to calculate manually the output features?","['r', 'deep-learning', 'h2o', 'autoencoder']",JHKimKim,https://stackoverflow.com/users/9612631/jhkimkim,23
49692830,49692830,2018-04-06T12:28:07,2020-01-05 15:16:51Z,290,"I have a Jupyter notebook connected to a Sparkling Water instance, running on a Hadoop cluster.


This is my assumption about how the processing works:




The user code from the notebook is submitted to the running Sparkling Water instance.


Sparkling Water translates it to use Spark API commands.


It is submitted as a Spark job to the cluster. 


Spark executes it as any other job.




Am I right?

Is this how it works?


The bigger topic I am trying to explain is whether Sparkling Water runs the H2O algorithms in a distributed manner and utilizes the available cluster resources.","['apache-spark', 'h2o']",Marek Grzenkowicz,https://stackoverflow.com/users/95/marek-grzenkowicz,17.3k
49681116,49681116,2018-04-05T20:37:55,2018-04-12 12:22:55Z,0,"Closed
. This question needs to be more 
focused
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Update the question so it focuses on one problem only by 
editing this post
.






Closed 
6 years ago
.















                        Improve this question
                    








Problem:


I have multiple datasets of transactional data that I use to predict an event (binary classification as outcome). One of them has 10,587,989 rows with 23 columns. I am attempting to run 
gradient boosting
 with 10 fold cv and ctree (
package:party
) but every time I run these models my system crashes. 


Hardware:


16 cores, 48 gig of RAM, 48 gig of SWAP


Question:
 


What causes R to crash while working with large data sets even after utilizing parallel processing, adding more memory, bouncing the system? 


Things I have tried:




Enabled parallel processing through 
doParallel
, execute xgBoost
through 
caret
, I see every core lighting up and RAM and swap being
fully utilized through 
top
 function in linux but it eventually
crashes everytime.


Bounced the RStudio server, rebooted the system as initial
maneuvering but problem persists.




I did find people commenting about H2O. I also reached out to a vendor and asked him for a solution, he suggested Sparkly but you need Hadoop layer in your server to run Sparkly.","['r', 'apache-spark', 'h2o']",Unknown,,N/A
49671640,49671640,2018-04-05T11:48:02,2018-04-05 11:51:47Z,259,"I already have a Deep Learning model.I am trying to run scoring on streaming data. For this I am reading data from kafka using spark structured streaming api.When I try to convert the received dataset to H20Frame I am getting below error:




Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Queries with streaming sources must be executed with writeStream.start();





Code Sample


Dataset<Row> testData=sparkSession.readStream().schema(testSchema).format(""kafka"").option(""kafka.bootstrap.servers"", ""localhost:9042"").option(""subscribe"", ""topicName"").load();
H2OFrame h2oTestFrame = h2oContext.asH2OFrame(testData.toDF(), ""test_frame"");



Is there any example that explains sparkling water using spark structured streaming with streaming source?","['apache-spark', 'h2o', 'sparkling-water', 'spark-structured-streaming']",Apeksha Agnihotri,https://stackoverflow.com/users/8364291/apeksha-agnihotri,11
49634547,49634547,2018-04-03T16:06:07,2018-04-04 15:54:11Z,0,"What my question isnt:    




Efficient way to maintain a h2o data frame


H2O running slower than data.table R


Loading data bigger than the memory size in h2o




Hardware/Space:    




32 Xeon threads w/ ~256 GB Ram


~65 GB of data to upload. (about 5.6 billion cells)




Problem:

It is taking hours to upload my data into h2o.  This isn't any special processing, only ""as.h2o(...)"".  


It takes less than a minute using ""fread"" to get the text into the space and then I make a few row/col transformations (diff's, lags) and try to import.  


The total R memory is ~56GB before trying any sort of ""as.h2o"" so the 128 allocated shouldn't be too crazy, should it?


Question:

What can I do to make this take less than an hour to load into h2o?  It should take from a minute to a few minutes, no longer.


What I have tried:
 




bumping ram up to 128 GB in 'h2o.init'


using slam, data.table, and options( ...


convert to ""as.data.frame"" before ""as.h2o""


write to csv file (r write.csv chokes and takes forever.  It is writing a lot of GB though, so I understand).


write to sqlite3, too many columns for a table, which is weird.  


Checked drive cache/swap to make sure there are enough GB there.  Perhaps java is using cache.  (still working)




Update:

So it looks like my only option is to make a giant text file and then use ""h2o.importFile(...)"" for it.  I'm up to 15GB written.  


Update2:

It is a hideous csv file, at ~22GB (~2.4Mrows, ~2300 cols).  For what it was worth, it took from 12:53pm until 2:44PM to write the csv file.  Importing it was substantially faster, after it was written.","['r', 'import', 'sqlite', 'h2o']",Unknown,,N/A
49615505,49615505,2018-04-02T17:07:40,2018-04-02 17:07:40Z,345,"While trying to build a H2O Random Forest via the Python API, I got a flaky error.  (I've saved the .err file which is empty and the .out file in case somebody wants to look at it.)


 ""java.lang.AssertionError: Can't unlock: Not locked!""



in two out of the three times I tried.  One failure showed progress up to 86% and the other 90%.  On the third try, I got all the way through.  I restarted the H2O server after the first failure.


Running on x86_64 x86_64 x86_64 GNU/Linux, Linux 4.4.0-101-generic (Ubuntu), H2O 3.18.0.4, Python 2.7.12


There are about 33K training set examples for a multi-nominal classifier of about 86 classes and 137 numeric input features.  Previously, we had no problem (other than some time out issues) with the system using similar data using much older versions of H2O.


Here's the output to stdout from running my program.


Attempting to start a local H2O server...
  Java Version: java version ""1.8.0_144""; Java(TM) SE Runtime Environment (build 1.8.0_144-b01); Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)
  Starting server from /home/ubuntu/django-env/lib/python2.7/site-packages/h2o/backend/bin/h2o.jar
  Ice root: /tmp/tmpViqbs4
  JVM stdout: /tmp/tmpViqbs4/h2o_ubuntu_started_from_python.out
  JVM stderr: /tmp/tmpViqbs4/h2o_ubuntu_started_from_python.err
  Server is running at http://127.0.0.1:54321
Connecting to H2O server at http://127.0.0.1:54321... successful.
--------------------------  ----------------------------------------
H2O cluster uptime:         08 secs
H2O cluster timezone:       Etc/UTC
H2O data parsing timezone:  UTC
H2O cluster version:        3.18.0.4
H2O cluster version age:    24 days
H2O cluster name:           H2O_from_python_ubuntu_x4p9wv
H2O cluster total nodes:    1
H2O cluster free memory:    6.545 Gb
H2O cluster total cores:    8
H2O cluster allowed cores:  8
H2O cluster status:         accepting new members, healthy
H2O connection url:         http://127.0.0.1:54321
H2O connection proxy:
H2O internal security:      False
H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4
Python version:             2.7.12 final
--------------------------  ----------------------------------------



[stuff omitted]


Parse progress: |█████████████████████████████████████████████████████████████████████████████| 100%
Running trainh2o()..
drf Model Build progress: |█████████████████████████████████████████████████████████▉ (failed)|  86%
Traceback (most recent call last):
  File ""/home/ubuntu/XXX/webapp/XXX/classify_unified/buildpc.py"", line 130, in <module>
    print(""\nTrain Error: {}"".format(pc.train()))
  File ""/home/ubuntu/XXX/webapp/XXX/classify_unified/ProvisionClassifier.py"", line 2130, in train
    self._trainh2o()
  File ""/home/ubuntu/XXX/webapp/XXX/classify_unified/ProvisionClassifier.py"", line 2090, in _trainh2o
    training_frame=self._combo_h2odf)
  File ""/home/ubuntu/django-env/local/lib/python2.7/site-packages/h2o/estimators/estimator_base.py"", line 232, in train
    model.poll(verbose_model_scoring_history=verbose)
  File ""/home/ubuntu/django-env/local/lib/python2.7/site-packages/h2o/job.py"", line 77, in poll
    ""\n{}"".format(self.job_key, self.exception, self.job[""stacktrace""]))
EnvironmentError: Job with key $03017f00000132d4ffffffff$_8f9d9edcff82420eefea1d6cff0f4396 failed with an exception: java.lang.AssertionError: Can't unlock: Not locked!
stacktrace: 
java.lang.AssertionError: Can't unlock: Not locked!
    at water.Lockable$Unlock.atomic(Lockable.java:197)
    at water.Lockable$Unlock.atomic(Lockable.java:187)
    at water.TAtomic.atomic(TAtomic.java:17)
    at water.Atomic.compute2(Atomic.java:56)
    at water.Atomic.fork(Atomic.java:39)
    at water.Atomic.invoke(Atomic.java:31)
    at water.Lockable.unlock(Lockable.java:181)
    at water.Lockable.unlock(Lockable.java:176)
    at hex.tree.SharedTree$Driver.computeImpl(SharedTree.java:358)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:206)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1263)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

    H2O session _sid_9f4c closed.",['h2o'],Clem Wang,https://stackoverflow.com/users/2263303/clem-wang,739
49595290,49595290,2018-04-01T06:09:25,2018-04-01 09:08:23Z,0,"after installing the h2o package in the Rstudio, when I'm initiating 
h2o.init()
 it is having this problem




In normalizePath(path.expand(path), winslash, mustWork) :path[1]=""C:\Program Files\Java\jdk1.8.0_162\bin/bin/java.exe"": The system cannot find the path specified




I'm facing this problem even after setting my 
java/jdk1.8/bin
 as 
PATH
 in the environment variables.


can anyone help me out to solve this problem","['r', 'data-science', 'h2o']",user2554330,https://stackoverflow.com/users/2554330/user2554330,44k
49592926,49592926,2018-03-31T22:11:54,2018-04-02 21:04:48Z,419,"I am running a Python program that calls H2O for deep learning (training and testing). The program runs in a loop of 20 iterations and in each loop calls 
H2ODeepLearningEstimator()
 4 times and associated 
predict()
 and 
model_performance()
. I am doing 
h2o.remove_all()
 and cleaning up all data-related Python objects after each iteration. 


Data size: training set 80,000 with 122 features (all float) with 20% for validation (10-fold CV). test set 20,000. Doing binary classification.


Machine 1: Windows 7, 4 core, Xeon, each core 3.5GHz, Memory 32 GB
Takes about 24 hours to complete


Machine 2: CentOS 7, 20 core, Xeon, each core 2.0GHz, Memory 128 GB
Takes about 17 hours to complete
I am using h2o.init(nthreads=-1, max_mem_size = 96)


So, the speed-up is not that much.
My questions:
 1) Is the speed-up typical?
 2) What can I do to achieve substantial speed-up?
        2.1) Will adding more cores help?
        2.2) Are there any H2O configuration or tips that I am missing?


Thanks very much.
- Mohammad,
  Graduate student",['h2o'],Jared Forth,https://stackoverflow.com/users/8869277/jared-forth,"1,627"
49576850,49576850,2018-03-30T15:11:41,2018-04-05 07:17:25Z,162,"H2O in spark cluster mode giving different predictions from spark local mode. H2O in spark local is giving better than spark cluster why it is happening ,can you help me? Tell me whether it's H2O behaviour.
     Two Data set are being used. One for training the model and another for scoring.

    trainingData.csv : 1.8MB (number of rows are 2211),

    testingData.csv : 1.8MB (number of rows are 2211),

    Driver Memory : 1G,

    Executors Memory: 1G,

    Number Of Executors : 1

    The following command is being used over cluster :=>

    nohup /usr/hdp/current/spark2-client/bin/spark-submit --class com.inn.sparkrunner.h2o.GradientBoostingAlgorithm --master yarn --driver-memory 1G --executor-memory 1G --num-executors 1 --deploy-mode cluster spark-runner-1.0.jar > tool.log &


1)Main Method  


    public static void main(String args[]) {   
              SparkSession sparkSession = getSparkSession();
              H2OContext h2oContext = getH2oContext(sparkSession);
              UnseenDataTestDRF(sparkSession, h2oContext);  
}



2)h2o context is being created.


    private static H2OContext getH2oContext(SparkSession sparkSession) {  
      H2OConf h2oConf = new H2OConf(sparkSession.sparkContext()).setInternalClusterMode();
    H2OContext orCreate = H2OContext.getOrCreate(sparkSession.sparkContext(), h2oConf);    
                     return orCreate;  
}          



3)spark session is being created.


    public static SparkSession getSparkSession() {  
    SparkSession spark = SparkSession.builder().appName(""Java Spark SQL basic example"").master(""yarn"")
            .getOrCreate();  
    return spark;  
}    



4)Setting GBM parameters. 


    private static GBMParameters getGBMParam(H2OFrame asH2OFrame) {     
    GBMParameters gbmParam = new GBMParameters();           
    gbmParam._response_column = ""high"";      
    gbmParam._train = asH2OFrame._key;      
    gbmParam._ntrees = 10;      
    gbmParam._seed = 1;    
    return gbmParam;           
}","['h2o', 'sparkling-water']",Unknown,,N/A
49538287,49538287,2018-03-28T15:17:57,2018-03-28 15:17:57Z,402,"Having the following issue with Sparkling-water version 2.2.9.  My Hadoop cluster is running CDH 5.13.  As per the H2o documentation, I should have roughly 4x the memory as the data size in the H2o/Sparkling-water cluster.


I can import a 750 GB data file (CSV) of size onto a sparkling-water cluster with 4 TB of memory (40 executors, 100GB each).  However having problems loading a larger data file.  This (CSV) file is roughly 2.2 TB in size (also have it in Parquet/Snappy format, 550GB in size).  I have created a Sparkling-water cluster with 100 executors of 100GB/executor.  The ""parsing"" step runs for about 60-70% and then the containers start failing with Error Code 143 and 255.  I have bumped up the memory to about 12 TB, still no success.


The python code is:


import h2o
h2o.init(ip='hdchdp01v03', port=9500, strict_version_check=False)
ls_hdfs=""hdfs://HDCHDP01ns/h2o_test/csv_20171004""
print(""Reading files from "", ls_hdfs)
sum_df = h2o.import_file(path = ls_hdfs, destination_frame=""sum_df"")



Has anyone run into similar issues? My Hadoop cluster only has 20 TB of memory, so hogging 12 TB memory itself would be a stretch most of the time.


With my first file, I see once the data is imported into the cluster, it seemed to take roughly double the file size in memory, but not sure how to recover the 4x memory I have allocated until the sparkling-water cluster comes down.


So, are there any other workarounds I could do to load this data into H2o for analysis with some due diligence on the available cluster memory?


Shankar","['apache-spark', 'hadoop', 'pyspark', 'h2o', 'sparkling-water']",VShankar,https://stackoverflow.com/users/6552685/vshankar,151
49535272,49535272,2018-03-28T12:59:27,2018-03-29 08:22:00Z,0,"Newby question in R/H2O, I want to access H2O both from Flow (the web) and from R. I've created a dataframe from R and can see it on H2O. I have created another dataframe in H2O directly, and asking how to get a handle to it in R?","['r', 'h2o']",Oren,https://stackoverflow.com/users/1614089/oren,"1,006"
49515618,49515618,2018-03-27T14:30:29,2018-03-27 19:56:32Z,0,"My goal is to export an h2o model trained on spark with scala (using sparkling-water), such that I can import it in an application without Spark.


Thus:




using scala (the documentation only shows examples for r and python)


export a model which is build using sparkling-water (h2o with spark)


import a model in scala (without spark nor h2o cluster, only the 
hex-genmodel
 package)




I'm therefore using the 
ModelSerializationSupport
 to export, and the 
MojoModel.load
 to import


val gbmParams = new GBMParameters()
gbmParams._train = train
gbmParams._response_column = ""target""
gbmParams._ntrees = 5
gbmParams._valid = valid
gbmParams._nfolds = 3 
gbmParams._min_rows = 1
gbmParams._distribution = DistributionFamily.multinomial
val gbm = new GBM(gbmParams)
val gbmModel = gbm.trainModel.get
val mojoPath = ""./model.zip""
ModelSerializationSupport.exportMOJOModel(gbmModel, new File(mojoPath).toURI, force = true)
val simpleModel = new EasyPredictModelWrapper(MojoModel.load(mojoPath))



Fails with 


error in opening zip file
java.util.zip.ZipException: error in opening zip file
at java.util.zip.ZipFile.open(Native Method)
at java.util.zip.ZipFile.<init>(ZipFile.java:220)
at java.util.zip.ZipFile.<init>(ZipFile.java:150)
at java.util.zip.ZipFile.<init>(ZipFile.java:121)
at hex.genmodel.ZipfileMojoReaderBackend.<init>(ZipfileMojoReaderBackend.java:13)
at hex.genmodel.MojoModel.load(MojoModel.java:33)
...



It seems that the mojo exporter doesn't use the same format as expected in the 
hex.genmodel
 (a zip apparently)


Running on h2o 2.1.23 (2.1.24 fails when building the cluster, as reported on 
https://0xdata.atlassian.net/browse/SW-776
) and spark 2.1


--
update:


Using the ModelSerializationSupport class to load it's own export fails too with the same exception:


ModelSerializationSupport.loadMOJOModel(new File(mojoPath).toURI)



H2OModel export and load

Loading back as H2OModel (thus with sparkling-water) does work:


val h2oModelPath = ""./model_h2o""
ModelSerializationSupport.exportH2OModel(gbmModel, new File(h2oModelPath).toURI, force = true)
val loadedModel: GBMModel = ModelSerializationSupport.loadH2OModel(new File(h2oModelPath).toURI)



H2OMOJOModel export and load

Loading it back with 
H2OMOJOModel
 does work (copied from implementation of 
H2OGBM
):


val mojoModel = new H2OMOJOModel(ModelSerializationSupport.getMojoData(gbmModel))
mojoModel.write.overwrite.save(mojoPath)
H2OMOJOModel.load(mojoPath) 



H2OGBM export with MojoModel import

Attempting to import using regular 
MojoModel
 fails though :


val gbm = new H2OGBM(gbmParams)(h2oContext, myspark.sqlContext)
val gbmModel = gbm.trainModel(gbmParams)
val mojoPath = ""./models.zip""
gbmModel.write.overwrite.save(mojoPath)
MojoModel.load(mojoPath)



with the following exception:


./models.zip/model.ini (No such file or directory)
java.io.FileNotFoundException: ./models.zip/model.ini (No such file or directory)","['scala', 'h2o', 'sparkling-water']",Unknown,,N/A
49510134,49510134,2018-03-27T10:11:16,2021-09-28 08:50:25Z,0,"I am trying to run h2o.automl() but it keeps failing because i am running out of ncpus.


I initiate my h20 session by requesting 47 threads: 
h2o.init(nthreads=47)


I am providing a sufficent amount of ncpus and memory at the start:




R is connected to the H2O cluster:
      H2O cluster uptime:         2 seconds 286 milliseconds
      H2O cluster timezone:       Europe/London
      H2O data parsing timezone:  UTC
      H2O cluster version:        3.18.0.4
      H2O cluster version age:    18 days
      H2O cluster name:           H2O_started_from_R_cmorgan1_gvi181
      H2O cluster total nodes:    1
      H2O cluster total memory:   26.67 GB
      H2O cluster total cores:    40
      H2O cluster allowed cores:  40
      H2O cluster healthy:        TRUE
      H2O Connection ip:          localhost
      H2O Connection port:        54321
      H2O Connection proxy:       NA
      H2O Internal Security:      FALSE
      H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4
      R Version:                  R version 3.4.1 (2017-06-30)




however, after a while (38% completion) it cuts out and tells me i do not have enough ncpus.




|======================================================================| 100%   |====   |=======   |=========   |==========   |==============

  |================   |=================   |===========   |===

  |===========================

  |  38%=>> PBS: job killed: ncpus 33.43 exceeded limit 32 (sum)


============================================


    Job resource usage summary

             Memory (GB)    NCPUs  Requested  :        45            48  Used       :        12 (peak)  36.00 (ave)





Has anyone come across this before and do you have a work around? I do not believe my data is abnormally sized, it has 29 scaled parameters and 94,000 rows of data.


Thanks in advace,","['r', 'h2o', 'automl']",user2528935,https://stackoverflow.com/users/2528935/user2528935,25
49508870,49508870,2018-03-27T09:16:25,2018-03-27 10:38:55Z,0,"Both functions are available in version 3.18.0.4 and the only apparent difference is that h2o.saveMojo(force = T) allows you to overwrite an existing file with the same name. Why are there 2? Any relative (dis)advantage?
Cheers","['r', 'h2o']",Unknown,,N/A
49506194,49506194,2018-03-27T06:55:34,2018-03-29 11:55:54Z,458,"I use h2o Deep Learning with Python. My problem is a time series forecasting problem as I want to predict the evolution of the number of sunspots. Here are all the values of sunspots since 1749 : 
http://www.sidc.be/silso/DATA/SN_ms_tot_V2.0.txt
. 


I want to use a sliding window of 43 months hence my dataset is now composed of 44 columns and 3170 rows (the value I want to predict is the 44th, based on the 43rd previous month).


My data looks like that : 


135.90, 137.90, 140.20, 143.80, 146.40 ... 68.10, 63.60, 60.40


137.90, 140.20, 143.80, 146.40, 147.90, ... 63.60, 60.40, 61.10


140.20, 143.80, 146.40, 147.90, 148.40, ... 60.40, 61.10, 59.70


...


99.0, 104.6, 107.0, 106.9, 107.6, ... 27.80, 26.50, 25.70


I have divided my data set into training (first 80% rows) and validation (last 20%). See my code below :


import h2o
from h2o.estimators.deeplearning import H2ODeepLearningEstimator

h2o.init()

test=h2o.import_file(""validationSet_43month.txt"")
train=h2o.import_file(""trainingSet_43month.txt"")
l=train.shape[1] 
x=train.names[0:l-1] 
y=train.names[l-1]

Factiv=""Tanh""
HiddenLayer=[100,100]
Nepochs=2000

model=H2ODeepLearningEstimator(
    activation=Factiv,
    hidden=HiddenLayer,
    epochs=Nepochs,
    reproducible=True,
    stopping_rounds=0, #I want to see an eventual overfitting on scoring history
    seed=123456789)
model.train(x=x,y=y,training_frame=train,validation_frame=test)



I want to draw the scoring history in order to know the optimum number of epochs to use but my scoring history seems to have a lot of 
noise, with peaks
 (see pictures).

Scoring history on 10,000 epochs


zoom on 2,000 epochs for validation deviance 


I thought I would get this type of scoring history instead : 

Normal scoring history




1) I tried to use Tanh instead of Rectifier with [8] hidden neurons but the noise is still here so it is probably not due to numerical instability. 




Scoring history 43-8-1 ; Activation=Tanh ; epochs=2600




2) Then I tried to add some hidden neurons ([100,100]) and still use Tanh. I still have a lot of noise : 




Scoring history 43-100-100-1 Tanh




3) I tried to use Random forest and Gradient boosting with default parameters, scoring_history looks good : 




Scoring history Random forest


Scoring history GBM


Does anybody have an explanation for the look of my scoring history ?","['python', 'deep-learning', 'h2o', 'scoring']",Unknown,,N/A
49501795,49501795,2018-03-26T22:45:08,2018-03-26 22:59:17Z,0,"When fitting a poisson GLM to a dataset - I receive this error code (Error: DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.NegativeArraySizeException) and am stumped about why this is happening.  I am posting a reproducible example below with a toy dataset that is broadly similar to my dataset and the arguments I am using:


df <- data.frame(count = c(512,1025,234,324),gene = c(""gene_1"",""gene_2"",""gene_1"",""gene_2""),factor = c(1,2,3,4),factor_2 = c(""a"",""b"",""c"",""d""),bound = c(""bound"",""unbound"",""bound"",""unbound""))
df <- as.h2o(df)
intx_terms_suppress <- c(""factor_2.a"",""factor_2.b"",""factor_2.c"",""factor_2.d"",""bound.bound"",""bound.unbound"")
constraints <- data.frame(names=intx_terms_suppress, 
                      lower_bounds=0, 
                      upper_bounds=0, 
                      beta_given=0)
y <- 1
train <- c(""gene"",""factor"",""factor_2"",""bound"")
interactions <- list(c(""factor_2"", ""bound""))
df[,1] <- as.numeric(df[,1])
df[,2:5] <- as.factor(df[,2:5])
fit <- h2o.glm(y = y, x = train, training_frame = df, family = ""poisson"", standardize = FALSE, interaction_pairs = interactions, beta_constraints = constraints)



I appreciate any help anyone might have. And thank you!


Edit: Some more info.  This is my session info:


H2O cluster uptime:         2 seconds 363 milliseconds 
H2O cluster timezone:       America/New_York 
H2O data parsing timezone:  UTC 
H2O cluster version:        3.18.0.4 
H2O cluster version age:    17 days  
H2O cluster name:           H2O_started_from_R_ra2816_ywt950 
H2O cluster total nodes:    1 
H2O cluster total memory:   444.44 GB 
H2O cluster total cores:    24 
H2O cluster allowed cores:  24 
H2O cluster healthy:        TRUE 
H2O Connection ip:          localhost 
H2O Connection port:        54321 
H2O Connection proxy:       NA 
H2O Internal Security:      FALSE 
H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
R Version:                  R version 3.4.4 (2018-03-15)","['r', 'glm', 'h2o']",Unknown,,N/A
49501096,49501096,2018-03-26T21:41:43,2018-03-27 04:00:22Z,123,"I am new to H2o. I am calling the predictBinomial method from a Java application and I am getting the correct results back, but it takes a long time to respond. Here is my scenario:


I am exposing a web service method where I receive the name of the class (modelName) and using ClassLoader to load it (the idea is to allow business users to upload their compiled models to the server and be able to call the service without any development required):


ClassLoader classLoader = new URLClassLoader(
                new URL[]{new URL(String.format(""%s%s/"", H2O_MODELS_URI, modelName))}, this.getClass().getClassLoader());



then I instantiate the raw model:


hex.genmodel.GenModel rawModel = (hex.genmodel.GenModel) classLoader.loadClass(modelName).newInstance();



Instantiate the wrapper:


EasyPredictModelWrapper model = new EasyPredictModelWrapper(rawModel);



Prepare the RowData (modelParameters received as service parameters):


RowData row = _h20ParametersMapper.map(modelParameters); 



Until here, everything goes smoothly, but when I call


BinomialModelPrediction binomialPrediction = model.predictBinomial(row);



It takes more than 10 seconds to return a response, which doesn't work for us. Do you see any way I could gain some performance? I am referencing the h2o-genmodel:3.18.0.3 library.


Thanks!",['h2o'],The_Outsider,https://stackoverflow.com/users/7793375/the-outsider,"1,925"
49493980,49493980,2018-03-26T14:33:04,2018-04-09 22:04:44Z,0,"I am using h2o kmeans in R to divide my population. The method need to be audited, so I would like to explain the threshold used in the h2o's kmeans.


In the documentation of h2o kmeans (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/k-means.html
), it is said :




H2O uses proportional reduction in error (PRE) to determine when to
  stop splitting.  The PRE value is calculated based on the sum of
  squares within (SSW).


PRE=(SSW[before split]−SSW[after split])/SSW[before split]


H2O stops splitting when PRE falls below a threshold, which is a
  function of the number of variables and the number of cases as
  described below:


threshold takes the smaller of these two values:


either 0.8 or [0.02 + 10/number_of_training_rows +
  2.5/(number_of_model_features)^2]




The source code (
https://github.com/h2oai/h2o-3/blob/master/h2o-algos/src/main/java/hex/kmeans/KMeans.java
) is given as :




final double rel_improvement_cutoff = Math.min(0.02 + 10. /
  _train.numRows() + 2.5 / Math.pow(model._output.nfeatures(), 2), 0.8);




Where does this threshold come from ? Are there scientific papers about it ?","['r', 'k-means', 'h2o', 'threshold']",Unknown,,N/A
49467713,49467713,2018-03-24T17:28:58,2018-03-29 02:20:10Z,0,"I have a question regarding the use of do.call() on H2O's group_by() function. 


Consider the following example: 


library(h2o)
h2o.init()

# Import the airlines data set and display a summary.
airlinesURL <- ""https://s3.amazonaws.com/h2o-airlines-unpacked/allyears2k.csv""
airlines.hex <- h2o.importFile(path = airlinesURL, 
destination_frame = ""airlines.hex"")


h2o.group_by(data = airlines.hex, 
             by=""Origin"",
             nrow(""Origin""), 
             min(""Origin""), 
             max(""Origin""), 
             gb.control=list(na.methods=""rm""))



The output is shown below: 


  Origin nrow min_Origin max_Origin
1    ABE   59          0          0
2    ABQ  876          1          1
3    ACY   31          2          2
4    ALB   75          3          3
5    AMA   11          4          4
6    ANC    1          5          5



Now, instead of the above direct call to h2o.group_by(), I would like to use a do.call() function to call h2o.group_by(). The reason is that I would like to dynamically build the list of fields to be summarized (i.e. nrow(""Origin""), max(""Origin"") etc.). 


So, I tried this (and many variations of the following code): 


do.call(h2o.group_by, list(data=airlines.hex, 
                           by=""Origin"", 
                           nrow(""Origin""), 
                           min(""Origin""), 
                           max(""Origin""), 
                           gb.control=list(na.methods=""rm"")), 
        quote=FALSE)



I get an error: 


Error in if (ag == ""sd"") ag <- ""\""sdev\"""" : argument is of length zero



Does anyone know how to get the do.call() function to work in this case? Basically, I need to be able to call h2o.group_by() with a dynamically generated set of fields to be summarized. 


In the worst case, I believe that I can use eval and parse to do what I need. In other words: 


eval(parse(text=""
h2o.group_by(data = airlines.hex, 
             by=\""Origin\"",
           nrow(\""Origin\""), 
           min(\""Origin\""), 
           max(\""Origin\""), 
           gb.control=list(na.methods=\""rm\""))
           ""))



I was wondering that do.call might be a better approach to use. 


Any suggestions on how to get do.call to produce the intended result? 


Karthik","['r', 'h2o', 'do.call']",Karthik,https://stackoverflow.com/users/211082/karthik,769
49448791,49448791,2018-03-23T11:36:48,2021-07-26 19:52:37Z,0,"The task to merge prediction frame to h2oframe containing features is not being done by merge method of water.rapids.Merge. 


How to use merge method to merge prediction's frame to features's frame and let me know the parameters description of this method, so method could be called properly?


merge(Frame leftFrame, Frame riteFrame, int[] leftCols, int[] riteCols, boolean allLeft, int[][] id_maps) 

merge(Frame leftFrame, Frame riteFrame, int[] leftCols, int[] riteCols, boolean allLeft, int[][] id_maps, int[] ascendingL, int[] ascendingR) 



what are 
int[][] id_maps
, 
int[] leftCols
, 
int[] riteCols
 parameters?


What is the right way to get merged frame of prediction's frame to features's frame?","['h2o', 'sparkling-water']",James Z,https://stackoverflow.com/users/4420967/james-z,12.3k
49439094,49439094,2018-03-22T21:55:14,2018-03-26 16:18:44Z,113,"Two concurrent h2ocontext created on the same driver seem to conflict with each other. When one is running, the other one will throw errors. Can we do some configuration such that two instances of sparkling water can run in parallel?","['h2o', 'sparkling-water']",user9537361,https://stackoverflow.com/users/9537361/user9537361,11
49431314,49431314,2018-03-22T14:32:16,2018-03-23 16:57:23Z,615,"I created a clustering model using h2o.kmeans(). The modeling dataset was standardized by scale() in R first.


The model has five clusters and the coordinates of the centroids are:


CENTROID    X1  X2  X3  X4  X5  X6  X7  X8  X9  X10 X11 X12 X13 X14 X15 X16 X17 X18 X19 X20 X21 X22
1   -0.646544   -0.6322714  -0.5101907  -0.2980412  -1.6182105  -1.7939725  -1.8194372  -1.82349    -1.8174061  -1.8069266 -2.2213561   -2.2618561  -2.2170297  -2.2004509  -2.196722   -2.2267695  -2.2536694  -2.2653944  -2.1599764  -2.2074994 -1.9114193   -2.78E-16
2   -0.2505012  -0.2582746  -0.2542313  -0.3205136  0.2912933   0.3239872   0.3236214   0.3231876   0.3234663   0.309818 0.362641   0.3800735   0.3615138   0.3542787   0.350817    0.3583391   0.375764    0.3715018   0.3533203   0.3533025   0.2651153 3.72E-15
3   0.4237044   0.4421857   0.408422    0.6620773   0.2371281   0.2592748   0.2597783   0.2782299   0.258803    0.3129833   0.4157714 0.3704712 0.3948566   0.4137049   0.4289137   0.4229101   0.3904031   0.4323851   0.3984215   0.442518    0.5278553   1.00E+00
4   2.2426614   2.2450805   2.0475964   1.5666675   0.2249847   0.2887632   0.3391117   0.3224008   0.3375972   0.3617759 0.5063836 0.4805747   0.5226613   0.5097081   0.5196333   0.5136624   0.4780912   0.4686772   0.4743151   0.5357567   0.5734882 8.24E-01
5   4.4718381   4.5243432   4.8917335   5.223828    0.2374653   0.3096633   0.3215417   0.3326531   0.3189998   0.414707    0.5065842 0.5113028 0.558864    0.5482378   0.543278    0.5436269   0.5204451   0.5341745   0.5096259   0.6486469   0.6595461   9.89E-01



When using the model to make predictions for new data, mostly the result makes sense, which returns the cluster whose centroid has the shortest euclidean distance to the data point; however, sometimes (about 5%) the prediction is off. For example, for a data point as below:


X1  X2  X3  X4  X5  X6  X7  X8  X9  X10 X11 X12 X13 X14 X15 X16 X17 X18 X19 X20 X21 X22
-0.2001578  -0.2485784  -0.3008685  -0.005366991    0.2624246   0.3142725   0.3074037   0.3221539   0.3033765   0.3403944 0.3557642 0.3810387   0.4848038   0.2788213   0.544491    0.2838926   0.2899755   0.3963652   0.2594092   0.3083141   0.463528    1



The prediction is cluster 3; however, the euclidean distance between the data point and centroids are:


cluster 1: 10
cluster 2: 1.11
cluster 3: 1.39
cluster 4: 4.53
cluster 5: 9.97.



Based on the calculation above, the data point should be assigned to cluster 2, not 3.


Is it a bug or h2o.kmeans() uses other methods instead of euclidean distance for prediction?


Thank you.","['cluster-analysis', 'k-means', 'h2o', 'euclidean-distance']",soniCYouth,https://stackoverflow.com/users/9469401/sonicyouth,33
49417911,49417911,2018-03-21T23:03:13,2018-03-21 23:03:13Z,0,"I installed h2o using the AMI on the marketplace.   It installed 3.14, and I am trying to update the version to the latest stable one of h2o.ai so my co-workers can use flow.   How can I best do this?


I have tried uninstalling using 


pip install http://h2o-release.s3.amazonaws.com/h2o/rel-wolpert/4/Python/h2o-3.18.0.4-py2.py3-none-any.whl



And directly by SSH-ing into my instance via terminal.  However, even if it says ""successfully uninstalled"", it seems to keep reverting to the version 3.14.  


I suspect there is a script at startup that is reinstalling and loading 3.14, but I can't figure it out.  Any help is appreciated.","['amazon-web-services', 'h2o']",Chris Hawkins,https://stackoverflow.com/users/9531566/chris-hawkins,11
49414312,49414312,2018-03-21T18:43:55,2018-03-21 23:09:40Z,75,"I am using H2o with R to train an autoencoder using h2o.deeplearning. My training data used to fit the model is 10000x1000 so there is a real possibility of overfitting (because I have only 10 data cases per variable). The purpose of training the autoencoder is to detect outliers in the test data.


What I am finding is reasonable performance on the training data (MSE~0.015) but a strong trend upwards in errors on the test data.




I have experimented with both dropout and l1/l2 regularization parameters in an attempt to generalize the learning (that is not have the model too precisely memorizing the input data). I have: 


anomaly_model <- h2o.deeplearning(
x = colnames(obs_h2o),
training_frame = obs_h2o,
activation = ""RectifierWithDropout"", 
autoencoder = TRUE,
hidden = c(100),   
reproducible = FALSE,  
standardize = TRUE,
seed = 7,
mini_batch_size = 2,
epochs = 10,          # number of times whole dataset is iterated
input_dropout_ratio   = 0.3,
hidden_dropout_ratios = 0.9,
average_activation    = 1.9,
sparse = TRUE,   
l2 = 1e-3
)



I have tried tweaking parameters but this increase in error with time on the test data persists.


Any ideas related to this strange occurrence?","['deep-learning', 'h2o', 'autoencoder']",Unknown,,N/A
49414212,49414212,2018-03-21T18:37:35,2018-03-22 10:49:41Z,0,"I have a data frame in H2O (called df1.hex) and I am trying to add new columns to this data frame using h2o.cbind. I am using h2o 3.18.0.4. 


The code that I have shown below is only a simplified version of what I am trying to do. In reality, I am adding new columns to the df1.hex data frame based on various conditions. The bottomline is that I would like to be able to use 'h2o.cbind' whenever I need to append new columns to df1.hex. So, I would have to call h2o.cbind multiple times during my program. The real dataset that I am operating on is too big for me to do all this in R and then export it into h2o. 


Consider the code below:


# Let's load H2O and start up an H2O cluster
library(h2o)
h2o.init()

# Initialize a data frame with a column 'y'
df1 = data.frame(y=c('A', 'B', 'C'))
df1.hex = as.h2o(df1)
print(df1.hex)

# Need to append additional columns to df1.hex named x1, x2 etc...
for (i in 1:2) {
  df2 = data.frame(x=c(1*i, 2*i, 3*i))
  colnames(df2) = c(paste(""x"", i, sep='')) # x1, x2 etc...
  df2.hex = as.h2o(df2)
  print(paste(""Iteration: "", i, "": Adding df2.hex..."", sep=''))
  print(df2.hex)
  df1.hex = h2o.cbind(df1.hex, df2.hex) # Append x(i) to df1.hex data frame
}

print(""The final dataset df1.hex: "")
print(df1.hex)

h2o.shutdown(prompt=FALSE)



The output is as follows: 


> print(df1.hex)
      y
    1 A
    2 B
    3 C

[1] ""Iteration: 1: Adding df2.hex...""
  x1
1  1
2  2
3  3

[1] ""Iteration: 2: Adding df2.hex...""
  x2
1  2
2  4
3  6

[3 rows x 1 column] 

[1] ""The final dataset df1.hex: ""
> print(df1.hex)
  y x2 x20
1 A  2   2
2 B  4   4
3 C  6   6



Even though I was appending two new columns named x1 and x2, the final version of df1.hex contains two columns named x2 and x20. Why did that happen? 


Also, the x1 column completely disappeared. I only see the column x2 appearing twice. 


How can I fix my code to name my columns x1 and x2 and have the correct values in those columns as I originally intended? 


Thanks.


Karthik.","['r', 'h2o']",Unknown,,N/A
49413724,49413724,2018-03-21T18:09:15,2018-03-22 14:03:58Z,57,I just have a POJO model file and the genModel.jar delivered to me. I need to figure out a way to output the individual tree results for that. Please guide me which wrapper and methods to use if this is supported in the POJO model.,"['h2o', 'gbm']",Kᴏɴsᴛᴀɴᴛɪɴ Sʜɪʟᴏᴠ,https://stackoverflow.com/users/3563993/k%e1%b4%8f%c9%b4s%e1%b4%9b%e1%b4%80%c9%b4%e1%b4%9b%c9%aa%c9%b4-s%ca%9c%c9%aa%ca%9f%e1%b4%8f%e1%b4%a0,13k
49411393,49411393,2018-03-21T16:12:03,2018-04-11 16:38:18Z,0,"I think I have exhausted the entire internet looking for an example / answer to my query regarding implementing a h2o mojo model to predict within RShiny. We have created a bunch of models, and wish to predict scores in a RShiny front end where users enter values. However, with the following code to implement the prediction we get an error of 




Warning: Error in checkForRemoteErrors: 6 nodes produced errors; first
  error: No method asJSON S3 class: H2OFrame




dataInput <- dfName
dataInput <- toJSON(dataInput)

rawPred <- as.data.frame(h2o.predict_json(model= ""folder/mojo_model.zip"",  json = dataInput, genmodelpath = ""folder/h2o-genmodel.jar""))



Can anyone help with some pointers?
Thanks,
Siobhan","['r', 'rstudio', 'h2o']",Siobhan,https://stackoverflow.com/users/9099971/siobhan,173
49403802,49403802,2018-03-21T10:25:40,2018-03-27 11:04:16Z,165,"Tools used:




Spark 2


Sparkling Water (H2O)


Zeppeling notebook


Pyspark Code




I'm starting H2O in INTERNAL mode from my Zeppelin notebook, since my environment is YARN. I'm using the basic command:


from pysparkling import *
hc = H2OContext.getOrCreate(spark)
import h2o



My problem is that I have the zeppelin server installed on a weak machine and when I run my code FROM ZEPPELIN the H2O cluster starts on that machine using its IP automatically. The driver runs on there and i'm limited by the driver memory which H2O consumes. I have 4 strong worker node machines with 100GB and many cores and the cluster uses them while I run my models, but I would like the H2O cluster to start on one of these worker machines and run the driver there, but I didn't find a way to force H2O to do that.


I wonder if there is a solution, or if I must install the zeppelin server on a worker machine.


Help will be appreciated if a solution is possible","['apache-spark', 'h2o', 'apache-zeppelin', 'sparkling-water']",orryk,https://stackoverflow.com/users/9261598/orryk,1
49393343,49393343,2018-03-20T20:13:13,2018-03-20 21:20:52Z,0,"I am trying to follow the suggestion on question: 
""Coerce multiple columns to factors at once""
, but it does not work for an 
H2OFrame
 object, for example:


data <- data.frame(matrix(sample(1:40), 4, 10, dimnames = list(1:4, LETTERS[1:10])))
data.hex <- as.h2o(data, destination_frame = ""data.hex"")
cols <- c(""A"", ""C"", ""D"", ""H"")
data.hex[cols] <- lapply(data.hex[cols], factor)



Produces the following error message:


Error in `[<-.H2OFrame`(`*tmp*`, cols, value = list(1L, 1L, 1L, 1L, 1L,  : 
  `value` can only be an H2OFrame object or a numeric or character vector
In addition: 
Warning message:
In if (is.na(value)) value <- NA_integer_ else if (!is.numeric(value) &&  :


the condition has length > 1 and only the first element will be used



If I try to coerce as factor one by one, it works. Another workaround is to coerce as factor first the 
data.frame
, then convert it into 
H2OFrame
 object, for example:


data[cols] <- lapply(data[cols], factor)
data.hex <- as.h2o(data, destination_frame = ""data.hex"")



Any explanation why it happens or any better workaround?","['r', 'h2o']",David Leal,https://stackoverflow.com/users/6237093/david-leal,"6,739"
49391938,49391938,2018-03-20T18:42:11,2018-03-20 20:59:00Z,414,I am currently using h2o automl feature in R environment and I see the leaderboard showing up none in the logs even though I have provided the dataset to it. Appreciate if someone can answer,"['h2o', 'automl']",rknimmakayala,https://stackoverflow.com/users/8366375/rknimmakayala,21
49368280,49368280,2018-03-19T16:50:06,2018-03-19 16:50:06Z,714,"I'm trying to write a sample program in Scala/Spark/H2O. The program compiles, but throws an exception in 
H2OContext.getOrCreate
:


object App1 extends App{

         val conf = new SparkConf()
         conf.setAppName(""AppTest"")
         conf.setMaster(""local[1]"")
         conf.set(""spark.executor.memory"",""1g"");
         val sc = new SparkContext(conf)

         val spark = SparkSession.builder
            .master(""local"")
            .appName(""ApplicationController"")
            .getOrCreate()

         import spark.implicits._  

         val h2oContext = H2OContext.getOrCreate(sess) // <--- error here
         import h2oContext.implicits._

         val rawData = sc.textFile(""c:\\spark\\data.csv"")        
         val data = rawData.map(line => line.split(',').map(_.toDouble))
         val response: RDD[Int] = data.map(row => row(0).toInt)

         val str = ""count: "" + response.count()
         val h2oResponse: H2OFrame = response.toDF

         sc.stop
         spark.stop
}



This is the exception log:




Exception in thread ""main""
  java.lang.RuntimeException: When using the Sparkling Water as Spark
  package via --packages option, the 'no.priv.garshol.duke:duke:1.2'
  dependency has to be specified explicitly due to a bug in Spark
  dependency resolution.    at
  org.apache.spark.h2o.H2OContext.init(H2OContext.scala:117)","['scala', 'apache-spark', 'h2o']",ps0604,https://stackoverflow.com/users/1362485/ps0604,"1,211"
49353079,49353079,2018-03-18T21:38:17,2018-03-19 05:13:19Z,82,"The code below is throwing an error when assigning the H2OFrame, most likely something is wrong in the implicit conversion. The error is:




type mismatch; found : org.apache.spark.h2o.RDD[Int] (which expands
  to) org.apache.spark.rdd.RDD[Int] required:
  org.apache.spark.h2o.H2OFrame (which expands to) water.fvec.H2OFrame




and the code:


import org.apache.spark.h2o._

import org.apache.spark._
import org.apache.spark.SparkContext._

object App1 extends App{

         val conf = new SparkConf()
         conf.setAppName(""Test"")
         conf.setMaster(""local[1]"")
         conf.set(""spark.executor.memory"",""1g"");

         val sc = new SparkContext(conf)

         val rawData = sc.textFile(""c:\\spark\\data.csv"")        
         val data = rawData.map(line => line.split(',').map(_.toDouble))    
         val response: RDD[Int] = data.map(row => row(0).toInt)

         val h2oResponse: H2OFrame = response   // <-- this line throws the error
         sc.stop

}","['scala', 'apache-spark', 'h2o']",ps0604,https://stackoverflow.com/users/1362485/ps0604,"1,211"
49347630,49347630,2018-03-18T12:01:24,2019-03-30 11:37:28Z,0,"I don't meet with the problem when using Gridsearch in xgboost by Python coding. But today when I tried to use Gridsearch in H2O's xgboost (also using H2O's Gridsearch function), it didn't let me pass. Below is the code:


xgboost_hyperparameters ={ 'max_depth' : range(2,10)
               ,'min_rows' : range(1,9)                                      #min_child_weight
               ,'sample_rate' : [i/10 for i in range (5,10)]}                 #subsample  
               ,'col_sample_rate_per_tree' : [i/10 for i in range (5,10)]}    #colsample_bytree


param = {'booster': 'gbtree', 
     'col_sample_rate': 1,                     #colsample_bylevel
     'keep_cross_validation_predictions': True,
     'learn_rate' : 0.1,         
     'max_abs_leafnode_pred': 1.0,        
     'nfolds': 10,
     'ntrees' : 24,
     'reg_alpha': 0.0,
     'reg_lambda': 5.0

    }

xgboost_grid1 = H2OGridSearch(model = H2OXGBoostEstimator(**param),
                         grid_id = 'xgboost_grid1',
                         hyper_params = xgboost_hyperparameters)



it's passed in Jupyter Notebook, but when I started to train the model using below code, it's report error:


xgboost_grid1.train(x=x, y=y,
           training_frame=train,
           validation_frame=valid)



the error's message:


H2OResponseError                          Traceback (most recent call last)
<ipython-input-15-b1393b94399c> in <module>()
      1 xgboost_grid1.train(x=x, y=y,
      2                    training_frame=train,
----> 3                    validation_frame=valid)
      4 

~/anaconda3/lib/python3.6/site-packages/h2o/grid/grid_search.py in train(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, **params)
    206         x = list(xset)
    207         parms[""x""] = x
--> 208         self.build_model(parms)
    209 
    210 

~/anaconda3/lib/python3.6/site-packages/h2o/grid/grid_search.py in build_model(self, algo_params)
    221         if is_auto_encoder and y is not None: raise ValueError(""y should not be specified for autoencoder."")
    222         if not is_unsupervised and y is None: raise ValueError(""Missing response"")
--> 223         self._model_build(x, y, training_frame, validation_frame, algo_params)
    224 
    225 

~/anaconda3/lib/python3.6/site-packages/h2o/grid/grid_search.py in _model_build(self, x, y, tframe, vframe, kwargs)
    243         rest_ver = kwargs.pop(""_rest_version"") if ""_rest_version"" in kwargs else None
    244 
--> 245         grid = H2OJob(h2o.api(""POST /99/Grid/%s"" % algo, data=kwargs), job_type=(algo + "" Grid Build""))
    246 
    247         if self._future:

~/anaconda3/lib/python3.6/site-packages/h2o/h2o.py in api(endpoint, data, json, filename, save_to)
    101     # type checks are performed in H2OConnection class
    102     _check_connection()
--> 103     return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
    104 
    105 

~/anaconda3/lib/python3.6/site-packages/h2o/backend/connection.py in request(self, endpoint, data, json, filename, save_to)
    400                                     auth=self._auth, verify=self._verify_ssl_cert, proxies=self._proxies)
    401             self._log_end_transaction(start_time, resp)
--> 402             return self._process_response(resp, save_to)
    403 
    404         except (requests.exceptions.ConnectionError, requests.exceptions.HTTPError) as e:

~/anaconda3/lib/python3.6/site-packages/h2o/backend/connection.py in _process_response(response, save_to)
    723         # Client errors (400 = ""Bad Request"", 404 = ""Not Found"", 412 = ""Precondition Failed"")
    724         if status_code in {400, 404, 412} and isinstance(data, (H2OErrorV3, H2OModelBuilderErrorV3)):
--> 725             raise H2OResponseError(data)
    726 
    727         # Server errors (notably 500 = ""Server Error"")

H2OResponseError: Server error water.exceptions.H2OIllegalArgumentException:
  Error: Can't parse the hyper_parameters dictionary; got error: com.google.gson.stream.MalformedJsonException: Expected ':' at line 1 column 28 path $. for raw value: {'max_depth': range(2, 10), 'min_rows': range(1, 9), 'sample_rate': [0.5, 0.6, 0.7, 0.8, 0.9]}
  Request: POST /99/Grid/xgboost
    data: {'hyper_parameters': ""{'max_depth': range(2, 10), 'min_rows': range(1, 9), 'sample_rate': [0.5, 0.6, 0.7, 0.8, 0.9]}"", 'booster': 'gbtree', 'col_sample_rate': '1', 'keep_cross_validation_predictions': 'True', 'learn_rate': '0.1', 'max_abs_leafnode_pred': '1.0', 'nfolds': '10', 'ntrees': '24', 'reg_alpha': '0.0', 'reg_lambda': '5.0', 'training_frame': 'py_4_sid_80f1', 'validation_frame': 'py_5_sid_80f1', 'response_column': 'label', 'grid_id': 'xgboost_grid1'}



Need help on it because few documents I could find in H2O's website and here.","['python', 'h2o', 'xgboost', 'grid-search']",David Kwok,https://stackoverflow.com/users/9511739/david-kwok,21
49319432,49319432,2018-03-16T11:14:45,2018-03-26 13:00:35Z,0,"The following script, reproduces an equivalent problem as it was stated in h2o Help (
Help -> View Example Flow
 or 
Help -> Browse Installed packs.. -> examples -> Airlines Delay.flow
, 
download
), but using h2o R-package and a fixed seed (
123456
):


library(h2o)
# To use avaliable cores
h2o.init(max_mem_size = ""12g"", nthreads = -1)

IS_LOCAL_FILE = switch(1, FALSE, TRUE)
if (IS_LOCAL_FILE) {
    data.input <- read.csv(file = ""allyears2k.csv"", stringsAsFactors = F)
    allyears2k.hex <- as.h2o(data.input, destination_frame = ""allyears2k.hex"")
} else {
    airlinesPath <- ""https://s3.amazonaws.com/h2o-airlines-unpacked/allyears2k.csv""
    allyears2k.hex <- h2o.importFile(path = airlinesPath, destination_frame = ""allyears2k.hex"")
}

response <- ""IsDepDelayed""
predictors <- setdiff(names(allyears2k.hex), response)

# Copied and pasted from the flow, then converting to R syntax
predictors.exc = c(""DayofMonth"", ""DepTime"", ""CRSDepTime"", ""ArrTime"", ""CRSArrTime"",
    ""TailNum"", ""ActualElapsedTime"", ""CRSElapsedTime"",
    ""AirTime"", ""ArrDelay"", ""DepDelay"", ""TaxiIn"", ""TaxiOut"",
    ""Cancelled"", ""CancellationCode"", ""Diverted"", ""CarrierDelay"",
    ""WeatherDelay"", ""NASDelay"", ""SecurityDelay"", ""LateAircraftDelay"",
    ""IsArrDelayed"")

predictors <- setdiff(predictors, predictors.exc)
# Convert to factor for classification
allyears2k.hex[, response] <- as.factor(allyears2k.hex[, response])

# Copied and pasted from the flow, then converting to R syntax
fit1 <- h2o.glm(
    x = predictors,
    model_id=""glm_model"", seed=123456, training_frame=allyears2k.hex,
    ignore_const_cols = T, y = response,
    family=""binomial"", solver=""IRLSM"",
    alpha=0.5,lambda=0.00001, lambda_search=F, standardize=T,
    non_negative=F, score_each_iteration=F,
    max_iterations=-1, link=""family_default"", intercept=T, objective_epsilon=0.00001,
    beta_epsilon=0.0001, gradient_epsilon=0.0001, prior=-1, max_active_predictors=-1
)
# Analysis
confMatrix <- h2o.confusionMatrix(fit1)
print(""Confusion Matrix for training dataset"")
print(confMatrix)
print(summary(fit1))
h2o.shutdown()



This is the Confusion Matrix for the training set:


 Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
       NO   YES    Error          Rate
NO      0 20887 1.000000  =20887/20887
YES     0 23091 0.000000      =0/23091
Totals  0 43978 0.474942  =20887/43978



And the metrics:


H2OBinomialMetrics: glm
** Reported on training data. **

MSE:  0.2473858
RMSE:  0.4973789
LogLoss:  0.6878898
Mean Per-Class Error:  0.5
AUC:  0.5550138
Gini:  0.1100276
R^2:  0.007965165
Residual Deviance:  60504.04
AIC:  60516.04



On contrary the result of h2o flow has a better performance:




and Confusion Matrix for max f1 threshold:



The h2o flow performance is much better than running the same algorithm using the equivalent R-package function
.


Note
: For sake of simplicity I am using Airlines Delay problem, that is a well-known problem using h2o, but I realized that such kind of significant difference are found in other similar situations using 
glm
 algorithm.


Any thought about why these significant differences occur


Appendix A: Using default model parameters


Following the suggestion from @DarrenCook answer, just using default building parameters except for excluding columns and seed:


h2o flow


Now the 
buildModel
 is invoked like this:


buildModel 'glm', {""model_id"":""glm_model-default"",
  ""seed"":""123456"",""training_frame"":""allyears2k.hex"",
  ""ignored_columns"": 
     [""DayofMonth"",""DepTime"",""CRSDepTime"",""ArrTime"",""CRSArrTime"",""TailNum"",
      ""ActualElapsedTime"",""CRSElapsedTime"",""AirTime"",""ArrDelay"",""DepDelay"",
      ""TaxiIn"",""TaxiOut"",""Cancelled"",""CancellationCode"",""Diverted"",
      ""CarrierDelay"",""WeatherDelay"",""NASDelay"",""SecurityDelay"",
      ""LateAircraftDelay"",""IsArrDelayed""],
   ""response_column"":""IsDepDelayed"",""family"":""binomial""



}


and the results are:




and the training metrics:




Running R-Script


The following script allows for an easy switch into default configuration (via 
IS_DEFAULT_MODEL
 variable) and also keeping the configuration as it states in the Airlines Delay example:


library(h2o)
h2o.init(max_mem_size = ""12g"", nthreads = -1) # To use avaliable cores

IS_LOCAL_FILE    = switch(2, FALSE, TRUE)
IS_DEFAULT_MODEL = switch(2, FALSE, TRUE)
if (IS_LOCAL_FILE) {
    data.input <- read.csv(file = ""allyears2k.csv"", stringsAsFactors = F)
    allyears2k.hex <- as.h2o(data.input, destination_frame = ""allyears2k.hex"")
} else {
    airlinesPath <- ""https://s3.amazonaws.com/h2o-airlines-unpacked/allyears2k.csv""
    allyears2k.hex <- h2o.importFile(path = airlinesPath, destination_frame = ""allyears2k.hex"")
}

response <- ""IsDepDelayed""
predictors <- setdiff(names(allyears2k.hex), response)

# Copied and pasted from the flow, then converting to R syntax
predictors.exc = c(""DayofMonth"", ""DepTime"", ""CRSDepTime"", ""ArrTime"", ""CRSArrTime"",
    ""TailNum"", ""ActualElapsedTime"", ""CRSElapsedTime"",
    ""AirTime"", ""ArrDelay"", ""DepDelay"", ""TaxiIn"", ""TaxiOut"",
    ""Cancelled"", ""CancellationCode"", ""Diverted"", ""CarrierDelay"",
    ""WeatherDelay"", ""NASDelay"", ""SecurityDelay"", ""LateAircraftDelay"",
    ""IsArrDelayed"")

predictors <- setdiff(predictors, predictors.exc)
# Convert to factor for classification
allyears2k.hex[, response] <- as.factor(allyears2k.hex[, response])

if (IS_DEFAULT_MODEL) {
    fit1 <- h2o.glm(
        x = predictors, model_id = ""glm_model"", seed = 123456,
        training_frame = allyears2k.hex, y = response, family = ""binomial""
    )
} else { # Copied and pasted from the flow, then converting to R syntax
    fit1 <- h2o.glm(
        x = predictors,
        model_id = ""glm_model"", seed = 123456, training_frame = allyears2k.hex,
        ignore_const_cols = T, y = response,
        family = ""binomial"", solver = ""IRLSM"",
        alpha = 0.5, lambda = 0.00001, lambda_search = F, standardize = T,
        non_negative = F, score_each_iteration = F,
        max_iterations = -1, link = ""family_default"", intercept = T, objective_epsilon = 0.00001,
        beta_epsilon = 0.0001, gradient_epsilon = 0.0001, prior = -1, max_active_predictors = -1
    )
}

# Analysis
confMatrix <- h2o.confusionMatrix(fit1)
print(""Confusion Matrix for training dataset"")
print(confMatrix)
print(summary(fit1))
h2o.shutdown()



It produces the following results:


MSE:  0.2473859
RMSE:  0.497379
LogLoss:  0.6878898
Mean Per-Class Error:  0.5
AUC:  0.5549898
Gini:  0.1099796
R^2:  0.007964984
Residual Deviance:  60504.04
AIC:  60516.04

Confusion Matrix (vertical: actual; across: predicted) 
for F1-optimal threshold:
       NO   YES    Error          Rate
NO      0 20887 1.000000  =20887/20887
YES     0 23091 0.000000      =0/23091
Totals  0 43978 0.474942  =20887/43978



Some metrics are close, but the Confusion Matrix is quite diferent, the R-Script predict all flights as delayed.


Appendix B: Configuration


Package: h2o
Version: 3.18.0.4
Type: Package
Title: R Interface for H2O
Date: 2018-03-08



Note: I tested the R-Script also under 3.19.0.4231 with the same results


This is the cluster information after running the R:


> h2o.init(max_mem_size = ""12g"", nthreads = -1)

R is connected to the H2O cluster: 
H2O cluster version:        3.18.0.4 
...
H2O API Extensions:         Algos, AutoML, Core V3, Core V4 
R Version:                  R version 3.3.3 (2017-03-06)","['r', 'h2o']",Unknown,,N/A
49318981,49318981,2018-03-16T10:51:53,2018-03-16 10:51:53Z,983,"val airlinesDf = spark.read.csv(""input file"") 
val airlinesData : H2OFrame = airlinesDf 
val airlinesTable: RDD[Airlines] = asRDD[Airlines](airlinesDf) 
val flightsToORD = airlinesTable.filter(f => f.Dest == Some(""ORD"")) 
flightsToORD.count()



when executing any function on flightsToORD we are getting below error message.
we can only create Spark Data Frame from input file here, that is why we are creating spark data frame and converting it to H2O


I am using sparkling-water-2.0.25 with Spark 2.0.2


airlinesDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ActualElapsedTime: string, AirTime: string ... 29 more fields]
airlinesTable: org.apache.spark.h2o.RDD[org.apache.spark.examples.h2o.Airlines] = H2ORDD[1420] at RDD at H2ORDD.scala:52
flightsToORD: org.apache.spark.rdd.RDD[org.apache.spark.examples.h2o.Airlines] = MapPartitionsRDD[1421] at filter at <console>:412
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 253.0 failed 4 times, most recent failure: Lost task 0.3 in stage 253.0 (TID 2351, dn4.h2oitc.ad.phemi.com): java.lang.IllegalArgumentException: Operation not allowed on string vector.
at water.fvec.CStrChunk.at8_impl(CStrChunk.java:89)
at water.fvec.Chunk.at8(Chunk.java:266)
at org.apache.spark.h2o.backends.internal.InternalReadConverterCtx.longAt(InternalReadConverterCtx.scala:60)
at org.apache.spark.h2o.backends.internal.InternalReadConverterCtx.intAt(InternalReadConverterCtx.scala:59)
at org.apache.spark.h2o.backends.internal.InternalReadConverterCtx.intAt(InternalReadConverterCtx.scala:29)
at org.apache.spark.h2o.converters.ReadConverterCtx$$anonfun$ExtractorsTable$4.apply(ReadConverterCtx.scala:108)
at org.apache.spark.h2o.converters.ReadConverterCtx$$anonfun$ExtractorsTable$4.apply(ReadConverterCtx.scala:108)
at org.apache.spark.h2o.backends.internal.InternalReadConverterCtx$$anonfun$returnOption$2.apply(InternalReadConverterCtx.scala:47)
at org.apache.spark.h2o.backends.internal.InternalReadConverterCtx$$anonfun$returnOption$2.apply(InternalReadConverterCtx.scala:46)
at scala.Option$WithFilter.flatMap(Option.scala:208)
at org.apache.spark.h2o.backends.internal.InternalReadConverterCtx.returnOption(InternalReadConverterCtx.scala:46)
at org.apache.spark.h2o.converters.ReadConverterCtx$$anonfun$org$apache$spark$h2o$converters$ReadConverterCtx$$OptionReadersMap$1$$anonfun$apply$1.apply(ReadConverterCtx.scala:120)
at org.apache.spark.h2o.converters.ReadConverterCtx$$anonfun$org$apache$spark$h2o$converters$ReadConverterCtx$$OptionReadersMap$1$$anonfun$apply$1.apply(ReadConverterCtx.scala:120)
at org.apache.spark.h2o.converters.H2ORDD$H2ORDDIterator$$anonfun$5$$anonfun$apply$1.apply(H2ORDD.scala:129)
at org.apache.spark.h2o.converters.H2ORDD$H2ORDDIterator$$anonfun$6.apply(H2ORDD.scala:133)
at org.apache.spark.h2o.converters.H2ORDD$H2ORDDIterator$$anonfun$6.apply(H2ORDD.scala:133)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
at org.apache.spark.h2o.converters.H2ORDD$H2ORDDIterator.extractRow(H2ORDD.scala:133)
at org.apache.spark.h2o.converters.H2ORDD$H2ORDDIterator.readOneRow(H2ORDD.scala:189)
at org.apache.spark.h2o.converters.H2ORDD$H2ORDDIterator.hasNext(H2ORDD.scala:157)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763)
at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134)
at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)
at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
at org.apache.spark.scheduler.Task.run(Task.scala:86)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)","['scala', 'apache-spark', 'apache-spark-sql', 'h2o', 'sparkling-water']",Naveen,https://stackoverflow.com/users/7243994/naveen,31
49314440,49314440,2018-03-16T06:26:24,2023-02-03 11:05:57Z,0,"I'm having a hard time understanding why the output for various metrics on my models differs when I use h2o.  


For example,  if I use 'h2o.grid' then the logloss measure is 
different
 when I look at the mean model$cross_validation_metrics_summary.  It is the same as   model$cross_validation_metrics_summary.   What is the reasoning behind this difference?  What one should I report on?


library(mlbench) 
  library(h2o)
 data(Sonar)

h2o.init() Sonarhex <- as.h2o(Sonar) h2o.grid(""gbm"", grid_id = ""gbm_grid_id0"", x = c(1:50), y = 'Class',
         training_frame = Sonarhex, hyper_params = list(ntrees = 50, learn_rate = c(.1, .2, .3)), nfolds = 5, seed=1234)

grid <- h2o.getGrid(""gbm_grid_id0"", sort_by = 'logloss')

first_model = h2o.getModel(grid@model_ids[[1]]) first_model@model$cross_validation_metrics_summary first_model@model$cross_validation_metrics","['r', 'cross-validation', 'h2o']",runningbirds,https://stackoverflow.com/users/3788557/runningbirds,"6,565"
49268055,49268055,2018-03-14T00:36:03,2018-03-14 01:57:54Z,0,"I've been having trouble with the H2o package in R lately. What first started out as an update has turned into a mess.


As mentioned, I had to update the H2o package from H2o version 3.16, to the lastest version (3.19). The first issue, was getting H2o to connect with java version 1.8. using this 
post
 from a user who had a similar problem, I managed to fix the 
/~.Renviron
  and successfully implemented a 
Sys.getenv(""JAVA_HOME"")
, that way, a link could be created to find the java 1.8 library.  


Now I am still getting this error, and I am not sure why.  For some reason the error 
Network address/interface is not reachable in 150m
 is the most predominate error that keeps repeating. I don't know if this is analogous to the way I set up my 
JAVA_HOME
 in the 
/~.Renviron
. Also the package version is clearly updated. 


Here is the full contents of the error:


======================================================================

> h2o.init(nthreads = -1)

 


 H2O is not running yet, starting it now...

 Note:  In case of errors look at the following log files:


  /var/folders/j5/77z5k0l92kg18q8svpxl_n640000gn/T//RtmpS0HsXx/h2o_andrewlewis_started_from_r.out
/var/folders/j5/77z5k0l92kg18q8svpxl_n640000gn/T//RtmpS0HsXx/h2o_andrewlewis_started_from_r.err

 java version ""1.8.0_45""
 Java(TM) SE Runtime Environment (build 1.8.0_45-b14)
 Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)

 Starting H2O JVM and connecting: 
 ............................................................Cannot load 
 library from path lib/osx_64/libxgboost4j_gpu.dylib
 Cannot load library from path lib/libxgboost4j_gpu.dylib
 Failed to load library from both native path and jar!
 Cannot load library from path lib/osx_64/libxgboost4j_omp.dylib
 Cannot load library from path lib/libxgboost4j_omp.dylib
 Failed to load library from both native path and jar!
 Found XGBoost backend with library: xgboost4j_minimal
 Your system supports only minimal version of XGBoost (no GPUs, no 
 multithreading)!
 ----- H2O started  -----
 Build git branch: master
 Build git hash: d0520285e973c062e57c88c1c865164006358fda
 Build git describe: jenkins-master-4230-2-gd052028
 Build project version: 3.19.0.4231 (latest version: 3.18.0.4)
 Build age: 13 hours and 11 minutes
 Built by: 'jenkins'
 Built on: '2018-03-13 06:31:09'
 Watchdog Build git branch: (unknown)
 Watchdog Build git hash: (unknown)
 Watchdog Build git describe: (unknown)
 Watchdog Build project version: (unknown)
 Watchdog Built by: (unknown)
 Watchdog Built on: (unknown)
 XGBoost Build git branch: (unknown)
 XGBoost Build git hash: (unknown)
 XGBoost Build git describe: (unknown)
 XGBoost Build project version: (unknown)
 XGBoost Built by: (unknown)
 XGBoost Built on: (unknown)
 KrbStandalone Build git branch: (unknown)
 KrbStandalone Build git hash: (unknown)
 KrbStandalone Build git describe: (unknown)
 KrbStandalone Build project version: (unknown)
 KrbStandalone Built by: (unknown)
 KrbStandalone Built on: (unknown)
 Processed H2O arguments: [-name, H2O_started_from_R_andrewlewis_zkx063, -ip, 
 localhost, -port, 54321, -ice_root, 
 /var/folders/j5/77z5k0l92kg18q8svpxl_n640000gn/T//RtmpS0HsXx]
 Java availableProcessors: 4
 Java heap totalMemory: 61.5 MB
 Java heap maxMemory: 910.5 MB
 Java version: Java 1.8.0_45 (from Oracle Corporation)
 JVM launch parameters: [-ea]
 OS version: Mac OS X 10.13.1 (x86_64)
 Machine physical memory: 4.00 GB
 X-h2o-cluster-id: 1520984534744
 User name: 'andrewlewis'
 IPv6 stack selected: false
 Network address/interface is not reachable in 150ms: 
 /fe80:0:0:0:a86b:c976:882a:7803%utun1/name:utun1 (utun1)
 Network address/interface is not reachable in 150ms: 
 /fe80:0:0:0:89dc:1b0f:6ed4:90f9%utun0/name:utun0 (utun0)
 Network address/interface is not reachable in 150ms: 
 /fe80:0:0:0:d093:b2ff:fe8c:74be%awdl0/name:awdl0 (awdl0)
 Network address/interface is not reachable in 150ms: 
 /fe80:0:0:0:10c0:35b:627d:a197%en1/name:en1 (en1)
 Network address/interface is not reachable in 150ms: /192.168.0.9/name:en1 (en1)
 Network address/interface is not reachable in 150ms: 
 /fe80:0:0:0:0:0:0:1%lo0/name:lo0 (lo0)
 Possible IP Address: lo0 (lo0), 0:0:0:0:0:0:0:1
 Network address/interface is not reachable in 150ms: /127.0.0.1/name:lo0 (lo0)
 IP address not found on this machine 
 [1] ""localhost""
 [1] 54321
 [1] TRUE
 [1] -1
 [1] ""Failed to connect to localhost port 54321: Connection refused""
   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                             Dload  Upload   Total   Spent    Left  Speed
    0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     
 0curl: (7) Failed to connect to localhost port 54321: Connection refused
 [1] 7
  Error in h2o.init(nthreads = -1) : 
   H2O failed to start, stopping execution.`



I do not know what to do from here, I am not sure if other packages need to be updated or if there is a slight change that needs to be implemented for this to work. I'm just trying to get H2o to work.","['r', 'h2o']",Welz,https://stackoverflow.com/users/9240845/welz,235
49262383,49262383,2018-03-13T17:30:41,2018-03-13 23:53:59Z,0,"I'm having problems implementing a simple balancing for an H2ORandomForestEstimator, I'm trying to reproduce a simple example found in Darren Cook's book written in R ('Practical Machine Learning with H2O - pag. 107).


Working on the Iris Dataset, firstly I artificially unbalance the target variable cutting out a good share of virginica keeping first 120 rows.


Then I build 3 models, a vanilla one, one where I set balance_classes as True, and a last one where I set balance_classes as True and I input a list for class_sampling_factors to oversample the virginica one. List is [1.0,1.0,2.5], referred to columns sorted alphabetically.


I train them, and then output confusion matrix for train for each one.


I'm expecting an unbalanced output for the first one, and a balanced one for the last two, while I have always the same result. I checked 
the documentation example in Python
, and I can't see anything wrong (I may be tired as well).


This is my code:


data_unb = data[1:120,:]  # messing up with target variable
train, valid = data_unb.split_frame([0.8], seed=12345)

m1 = h2o.estimators.random_forest.H2ORandomForestEstimator(seed=12345)
m2 = h2o.estimators.random_forest.H2ORandomForestEstimator(balance_classes=True, seed=12345)
m3 = h2o.estimators.random_forest.H2ORandomForestEstimator(balance_classes=True, class_sampling_factors=[1.0,1.0,2.5], seed=12345)

m1.train(x=list(range(4)),y=4,training_frame=train,validation_frame=valid,model_id='RF_defaults')
m2.train(x=list(range(4)),y=4,training_frame=train,validation_frame=valid,model_id='RF_balanced')
m3.train(x=list(range(4)),y=4,training_frame=train,validation_frame=valid,model_id='RF_class_sampling',)

m1.confusion_matrix(train)
m2.confusion_matrix(train)
m3.confusion_matrix(train)



This is my output:


my confusion matrices (wrong)


this is my expected output.


expected confusion matrices


What am I evidently missing? Thanks in advance.","['python', 'machine-learning', 'h2o']",Daniele,https://stackoverflow.com/users/9486953/daniele,21
49261738,49261738,2018-03-13T16:55:21,2018-03-13 23:46:17Z,510,"I have been accustomed to use the base margin parameter in standard xgboost to allow for offset, starting (transformed) prediction (see this SO question 
SO xgboost exposure question
. I wonder if it were possible to perform the same in the h2o implementation of xgboost. In particular I see an offset parameter, but I wonder wether it has been really truly implemented.","['h2o', 'xgboost']",Giorgio Spedicato,https://stackoverflow.com/users/1259856/giorgio-spedicato,"2,483"
49211864,49211864,2018-03-10T17:19:22,2018-03-12 00:51:03Z,87,I am using the H2O machine learning suite for unsupervised modeling. I see that there is a validation_frame option. But I wonder if it is really used on the h2o pca algorithm.,"['pca', 'h2o']",Giorgio Spedicato,https://stackoverflow.com/users/1259856/giorgio-spedicato,"2,483"
49203443,49203443,2018-03-09T23:00:54,2018-03-24 15:25:18Z,218,"When starting Hadoop h2o (YARN h2o) with the following command:


hadoop jar ./h2o-3.18.0.4-cdh5.13/h2odriver.jar -nodes 10 -mapperXmx 5g   -output junk/tmp1



I seem to sometimes get an issue bringing up the h2o cluster.  This is the error I see on console:


ERROR: Timed out waiting for H2O cluster to come up (120 seconds)
ERROR: (Try specifying the -timeout option to increase the waiting time limit)
Attempting to clean up hadoop job...
Killed.



This is the error I see in YARN logs:


03-09 14:50:35.118 x.x.x.56:54321    37628  #49:54321 ERRR: Got IO error when sending batch UDP bytes: java.net.ConnectException: Connection refused",['h2o'],Daniel Cheung,https://stackoverflow.com/users/9469978/daniel-cheung,1
49199204,49199204,2018-03-09T17:35:36,2018-10-22 17:03:08Z,0,"I'm struggling to figure out conda virtual environments on windows. All I want is to be able to have different versions of h2o installed at the same time because of their insane decision to not allow you to be able to load files saved in even the most minor different version.


I created a virtual environment by cloning my base anaconda:


conda create -n h203_14_0_7 --clone base



I then activated the virtual environment like so:


C:\ProgramData\Anaconda3\Scripts\activate h203_14_0_7



Now that I'm in the virtual environment (I see the 
(h203_14_0_7)
 at the beginning of the prompt), i want to uninstall the version of h2o in this virtual environment so I tried:


pip uninstall h2o



But this output




which to me looks like it's going to uninstall the global h2o rather than the virtual environment h2o. So I think it's using the global pip instead of the pip it should have cloned off the base. So how to I use the virtual environment pip to uninstall h2o just for my virtual environment and how can I be sure that it's doing the right thing?


I then ran


conda intall pip



and it seems that after that I was able to use pip to uninstall h2o only from the virtual environment (I hope). I then downloaded the older h2o version from here: 
https://github.com/h2oai/h2o-3/releases/tag/jenkins-rel-weierstrass-7


but when I try install it I get


(h203_14_0_7) C:\ProgramData\Anaconda3\envs\h203_14_0_7>pip install C:\Users\dan25\Downloads\h2o-3-jenkins-rel-weierstrass-7.tar.gz
Processing c:\users\dan25\downloads\h2o-3-jenkins-rel-weierstrass-7.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""C:\ProgramData\Anaconda3\envs\h203_14_0_7\lib\tokenize.py"", line 452, in open
        buffer = _builtin_open(filename, 'rb')
    FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\dan25\\AppData\\Local\\Temp\\pip-sf7r_6pm-build\\setup.py'



So what now?","['windows', 'pip', 'virtualenv', 'conda', 'h2o']",Unknown,,N/A
49189487,49189487,2018-03-09T08:28:07,2018-03-12 05:40:58Z,92,"the h2o DKV dose not have persisting to disk support, if the cluster crashed it cannot restore the frames and data, so why they use it, is it possible to replace the DKV to redis?",['h2o'],Unknown,,N/A
49177418,49177418,2018-03-08T16:01:35,2018-03-08 23:53:53Z,0,"I have a large 
.rds
 file saved and I trying to directly import 
.rds
 file to h2o frame using some functionality, because it is not feasible for me to read that file in R enviornment and then use 
as.h2o
 function to convert. 
I am looking for some fast and efficient way to deal with it.


My attempts:




I have tried to read that file and then convert it into h2o frame. But, it is way much time consuming process.


I tried saving file in .csv format and using 
h2o.import()
 with 
parse=T
.
Due to memory constraint I was not able to save complete dataframe.




Please suggest me any efficient way to do it.


Any suggestions would be highly appreciated.","['r', 'dataframe', 'datatable', 'dplyr', 'h2o']",Rushabh Patel,https://stackoverflow.com/users/7120667/rushabh-patel,"2,764"
49174052,49174052,2018-03-08T13:18:01,2018-07-29 07:10:28Z,100,"An error occurred when running the following code: 


    model = H2ODeepWaterEstimator(epochs=10, activation=""Rectifier"", hidden=[200,200],
                          ignore_const_cols=False, mini_batch_size=256,
                          input_dropout_ratio=0.1, hidden_dropout_ratios=[0.5,0.5],
                          stopping_rounds=3, stopping_tolerance=0.05,
                          stopping_metric=""misclassification"",score_interval=2,
                          score_duty_cycle=0.5,score_training_samples=1000,
                          score_validation_samples=1000, nfolds=5, gpu=True,
                          backend = ""tensorflow"",
                          seed=1234)



The error message is


    H2OConnectionError: Local server has died unexpectedly. RIP.



The log in that folder shows


     JRE version: Java(TM) SE Runtime Environment (8.0_161-b12) (build 1.8.0_161-b12)
     Java VM: Java HotSpot(TM) 64-Bit Server VM (25.161-b12 mixed mode linux-amd64 compressed oops)
     Problematic frame:
     C  [libtensorflow_jni.so1630a261-826b-43a7-a1e1-8025861cbde7+0x211992d]



An answer to a similar question says it occurs when the requirements of deepwater are not met, but I'm using Ubuntu 16.04 and I believe I have installed cuda, cudnn and tensorflow successfully


    zpm@zpm-Lenovo-IdeaPad-Y500:~$ nvcc -V
    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2016 NVIDIA Corporation
    Built on Tue_Jan_10_13:22:03_CST_2017
    Cuda compilation tools, release 8.0, V8.0.61

    zpm@zpm-Lenovo-IdeaPad-Y500:~$ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
    #define CUDNN_MAJOR      5
    #define CUDNN_MINOR      1
    #define CUDNN_PATCHLEVEL 10

    tensorflow-gpu            1.1.0                     <pip>



So how can I fix this error?


By the way I install tensorflow with


   pip install tensorflow-gpu==1.1.0



I don't know whether it will cause this problem.",['h2o'],Nope,https://stackoverflow.com/users/9462280/nope,1
49174040,49174040,2018-03-08T13:16:59,2018-03-08 13:16:59Z,0,"Using the fold_column parameter leads to an error when using h2o.grid for any algorithm / method. Does anybody know why that is the case? h2o.grid works when I just use nfolds and the estimation - not using the grid function - also works when fold_column is used, but not when I combin h2o.grid and fold_column even though is should work (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html
)


library(h2o)

#create random data
N = 1000

y = rnorm(N)
x1 = y + rnorm(N)
x2 = -y + 0.5*rnorm(N)
x3 = rnorm(N)

Data = data.table(y = y, 
                  x1 = x1, 
                  x2 = x2, 
                  x3 = x3, 
                  Split = rep(1:5, N/5), 
                  Weight = runif(N))
setkey(Data, Split)


#h2o
h2o.init(
  nthreads=-1,            ## -1: use all available threads
  max_mem_size = ""4G"")    ## specify the memory size for the H2O cloud
h2o.removeAll() # Clean slate - just in case the cluster was already running


bla = h2o.assign(as.h2o(Data), ""bla.hex"")


#make simple grid
Grid = list(ntrees = seq(1, 5, by=1))


#not working: grid with fold_column
GridResult = h2o.grid(""gbm"", 
                      x = c(""x1"", ""x2"", ""x3""), 
                      y = ""y"",
                      training_frame = bla,
                      hyper_params = Grid,
                      keep_cross_validation_predictions = TRUE,
                      weights_column = ""Weight"",
                      fold_column = ""Split""
)

#working: grid with nfolds
GridResult = h2o.grid(""gbm"", 
                      x = c(""x1"", ""x2"", ""x3""), 
                      y = ""y"",
                      training_frame = bla,
                      hyper_params = Grid,
                      keep_cross_validation_predictions = TRUE,
                      weights_column = ""Weight"",
                      nfolds = 5
)



My key information is
R version 3.3.1 (2016-06-21)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1
h2o_3.8.2.6 


Error: 


[2018-03-08 14:14:38] failure_details: NA 
[2018-03-08 14:14:38] failure_stack_traces: java.lang.NullPointerException
    at hex.ModelBuilder.nFoldWork(ModelBuilder.java:209)
    at hex.ModelBuilder.computeCrossValidation(ModelBuilder.java:224)
    at hex.ModelBuilder.trainModelNested(ModelBuilder.java:186)
    at hex.grid.GridSearch.startBuildModel(GridSearch.java:329)
    at hex.grid.GridSearch.buildModel(GridSearch.java:311)
    at hex.grid.GridSearch.gridSearch(GridSearch.java:215)
    at hex.grid.GridSearch.access$000(GridSearch.java:69)
    at hex.grid.GridSearch$1.compute2(GridSearch.java:136)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1194)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)","['r', 'h2o']",jovogt,https://stackoverflow.com/users/7429864/jovogt,11
49161283,49161283,2018-03-07T21:12:58,2018-03-07 23:06:52Z,0,"Closed.
 This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet 
Stack Overflow guidelines
. It is not currently accepting answers.
                                
                            
























 We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.






Closed 
6 years ago
.















                        Improve this question
                    








I am successfully using 
h2o.word2vec
 function to load pre-trained GloVe embeddings in R.


embed_model <- h2o.word2vec(pre_trained = glove, vec_size = 300)
df_doc_vecs <- h2o.transform(embed_model, tokenized_words, aggregate_method = ""AVERAGE"")


However, because my production environment will not support H2O, I need to find another package with similar functionality. Specifically I just need to be able to replace 
h2o.word2vec
 and 
h2o.transform
 with something else to:


[1] Use GloVe or any other pre-trained embeddings


[2] Create sentence or document level vectors out of them


Any suggestions? Has to be in R...","['r', 'text-mining', 'word2vec', 'h2o', 'word-embedding']",Unknown,,N/A
49153340,49153340,2018-03-07T13:40:12,2018-03-07 16:11:04Z,0,"When scheduling an R script with crontab scheduler that initiates an H2o instance locally it fails with the following error:


Starting H2O JVM and connecting: 
............................................................
[1] ""localhost""
[1] 54321
[1] TRUE
[1] -1
[1] ""Failed to connect to localhost port 54321: Connection refused""
[1] 7



When I run it without cron, it is working just fine. 
Any ideas why is this happening?","['r', 'cron', 'ubuntu-16.04', 'h2o']",Unknown,,N/A
49107634,49107634,2018-03-05T09:56:43,2021-01-07 10:36:48Z,0,"I have model built in H2O (say, GLM model)
Now, I want to import that model in Python to use for other apps. 


How can I do it ?","['python', 'model', 'h2o']",Hari Krishna,https://stackoverflow.com/users/9445005/hari-krishna,41
49106838,49106838,2018-03-05T09:11:51,2018-03-05 19:21:47Z,135,"We use H2O over zeppelin(with sparkling water).

Zeppelin works on the edge machine with low resources.

While running H2O from Zeppelin I can see that the Zeppelin interpreter process is doing all the work (I suppose it's the algorithm processing) and takes a lot of resources from the edge node while the data node shows no action.


Is there a way to spread the H2O process to the data node?

Or, in the way H2O is working, can I use spark only to read the data and then process the algorithm on the client?","['h2o', 'apache-zeppelin']",zx485,https://stackoverflow.com/users/1305969/zx485,29k
49106333,49106333,2018-03-05T08:36:57,2018-03-06 21:42:15Z,554,"In H2O KMeans Cluster. is there a way to calculate the actual distances from the cluster centroids for each point in the data set?
Currently H2o Gives the predicted Cluster for the data passed but what the best way of getting the distance of a point from its cluster centroid.


I intend to this for anomaly detection where points found far from the centroid are seen as anomalies. I have dont this using Apache Spark but Intend to try this using Sparking Water but the H2o Api does not seem to show the best way to get distances for each point from the cluster centroid.","['python', 'k-means', 'h2o', 'anomaly-detection', 'sparkling-water']",Nkuli Thangelane,https://stackoverflow.com/users/7606428/nkuli-thangelane,1
49075982,49075982,2018-03-02T19:21:36,2018-03-02 21:14:44Z,0,"I am trying to run stacked ensemble in h2o using h2o.Ensemble and getting this error .




Error in h2o.randomForest(x = x, y = y, training_frame = training_frame,  : 
    unused argument (offset_column = offset_column)","['r', 'machine-learning', 'h2o']",Cristik,https://stackoverflow.com/users/1974224/cristik,32.8k
49002384,49002384,2018-02-27T06:24:05,2018-10-16 13:14:54Z,0,"I installed h2o for Python 2 using below code in Azure Notebook IDE:


!pip install h2o



Then imported it using:


import h2o



However, I get the following error:


H2OConnectionError: Could not establish link to the H2O cloud      http://127.0.0.1:54321 after 20 retries
[07:03.57] H2OServerError: HTTP 503 Service Unavailable:
[07:04.78] H2OServerError: HTTP 503 Service Unavailable:
[07:05.99] H2OServerError: HTTP 503 Service Unavailable:



Can I get some help from anyone please?","['python', 'azure', 'h2o']",m00am,https://stackoverflow.com/users/2862719/m00am,"6,248"
49000226,49000226,2018-02-27T02:34:04,2018-03-16 20:41:19Z,913,"Do 
h2o
 estimators need to have the input data set have the same column names that they were trained on (regardless of if some columns were ignored) or is it the order that matters (in which case, can the ignored columns be replaced with other data)? 


Eg. When predicting on a data set with an 
h2o
 model, suppose the training frame for the a 
DRF
 model in the 
h2o flow
 UI was of the form:


fa | fb | fc | meta_a | meta_b | response
---------------------------------------
fa1| fb1| fc1| meta_a1| meta_b1| response1
fa2| fb2| fc2| meta_a2| meta_b2| response2
....



where I specify for the flow UI to ignore the meta_... columns.


Now say I load that model in python and want to predict with it on a new data set 
frame_in
 like:


est = h2o.load_model('/path/to/exported/model/file')
preds = est.predict(frame_in)



Where frame_in is a 
pandas
 dataframe of the form:


Fa | Fc | Fb | meta_c | meta_d | response
---------------------------------------
fa1| fb1| fc1| meta_a1| meta_b1| response1
fa2| fb2| fc2| meta_a2| meta_b2| response2
.... 



where the actual column names have been changed so fa=Fa, fb=Fb, fc=Fc (though they represent to same features) and the meta_... features are totally different data. 


I imagine in this case there will be an error, but for which reason? The ordering of the columns (which again makes me ask if the meta_... columns that were ignored during training can then be totally different data here) or the changes in the column names? Is there any documentation that clarifies this (looking 
here
 I was not able to tell)? Thanks.","['python', 'h2o']",Unknown,,N/A
48989990,48989990,2018-02-26T13:44:32,2020-04-20 09:32:58Z,0,"I am having trouble running h2o from within R. Just installed following R instructions on this page: 
http://h2o-release.s3.amazonaws.com/h2o/rel-wolpert/2/index.html
. Then, 


library(h2o)
h2o.init(nthreads = -1, max_mem_size = '2g', ip = ""127.0.0.1"") 



h2o server wouldn't start:


    H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    /var/folders/7y/p9rdbryd5zg5nvtjgdzyshx40000gt/T//RtmpvbbPTR/h2o_arman_started_from_r.out
    /var/folders/7y/p9rdbryd5zg5nvtjgdzyshx40000gt/T//RtmpvbbPTR/h2o_arman_started_from_r.err

java version ""1.8.0_111""
Java(TM) SE Runtime Environment (build 1.8.0_111-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode)

Starting H2O JVM and connecting: ............................................................Cannot load library from path lib/osx_64/libxgboost4j_gpu.dylib
Cannot load library from path lib/libxgboost4j_gpu.dylib
Failed to load library from both native path and jar!
Cannot load library from path lib/osx_64/libxgboost4j_omp.dylib
Cannot load library from path lib/libxgboost4j_omp.dylib
Failed to load library from both native path and jar!
Found XGBoost backend with library: xgboost4j_minimal
Your system supports only minimal version of XGBoost (no GPUs, no multithreading)!
----- H2O started  -----
Build git branch: master
Build git hash: a0f976c49acfa9750b7887b1cda6408d298913f4
Build git describe: jenkins-master-4214
Build project version: 3.19.0.4215 (latest version: 3.18.0.2)
Build age: 16 hours and 15 minutes
Built by: 'jenkins'
Built on: '2018-02-26 07:35:56'
Watchdog Build git branch: (unknown)
Watchdog Build git hash: (unknown)
Watchdog Build git describe: (unknown)
Watchdog Build project version: (unknown)
Watchdog Built by: (unknown)
Watchdog Built on: (unknown)
XGBoost Build git branch: (unknown)
XGBoost Build git hash: (unknown)
XGBoost Build git describe: (unknown)
XGBoost Build project version: (unknown)
XGBoost Built by: (unknown)
XGBoost Built on: (unknown)
KrbStandalone Build git branch: (unknown)
KrbStandalone Build git hash: (unknown)
KrbStandalone Build git describe: (unknown)
KrbStandalone Build project version: (unknown)
KrbStandalone Built by: (unknown)
KrbStandalone Built on: (unknown)
Processed H2O arguments: [-name, H2O_started_from_R_arman_nbd116, -ip, 127.0.0.1, -port, 2341, -ice_root, /var/folders/7y/p9rdbryd5zg5nvtjgdzyshx40000gt/T//RtmpvbbPTR]
Java availableProcessors: 8
Java heap totalMemory: 245.5 MB
Java heap maxMemory: 1.78 GB
Java version: Java 1.8.0_111 (from Oracle Corporation)
JVM launch parameters: [-Xmx2g, -ea]
OS version: Mac OS X 10.13.1 (x86_64)
Machine physical memory: 16.00 GB
X-h2o-cluster-id: 1519649491546
User name: 'arman'
IPv6 stack selected: false
Network address/interface is not reachable in 150ms: /fe80:0:0:0:75d0:fa45:58ee:6f07%utun0/name:utun0 (utun0)
Network address/interface is not reachable in 150ms: /fe80:0:0:0:5448:2bff:fec7:2829%awdl0/name:awdl0 (awdl0)
Network address/interface is not reachable in 150ms: /fe80:0:0:0:1846:22ba:65a6:e16b%en0/name:en0 (en0)
Network address/interface is not reachable in 150ms: /192.168.0.9/name:en0 (en0)
Network address/interface is not reachable in 150ms: /fe80:0:0:0:0:0:0:1%lo0/name:lo0 (lo0)
Network address/interface is not reachable in 150ms: /0:0:0:0:0:0:0:1/name:lo0 (lo0)
Network address/interface is not reachable in 150ms: /127.0.0.1/name:lo0 (lo0)
IP address not found on this machine 
[1] ""127.0.0.1""
[1] 2341
[1] TRUE
[1] -1
[1] ""Failed to connect to 127.0.0.1 port 2341: Connection refused""
[1] 0
Error in h2o.init(nthreads = -1, max_mem_size = ""2g"", ip = ""127.0.0.1"",  : 
  H2O failed to start, stopping execution.



On the other hand, I can run the server from the terminal


java -jar /Users/arman/Library/R/3.4/library/h2o/java/h2o.jar
# Retrieved the location of jar file in R using  h2o:::.h2o.downloadJar()



The above command works. I can continue running from terminal, but h2o.init provides many convenience options as well as code consistency. 


Any ideas why h2o fails to initialize from R? Also, tried from Python and had the same problem. 


OS:
 Mac OS X ver 10.13.1


java
 version ""1.8.0_111""
Java(TM) SE Runtime Environment (build 1.8.0_111-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode)


h2o
 version: 3.19.0.4215


R
 version: 3.4.3",['h2o'],arman,https://stackoverflow.com/users/1166362/arman,332
48985905,48985905,2018-02-26T10:09:00,2018-02-26 10:09:00Z,805,"I have an sklearn pipeline with h2o preprocessors and h2o estimators. Please see below.


pipeline = Pipeline([(""standardize"", h2o.transforms.preprocessing.H2OScaler()),
                     (""pca"", h2o.transforms.decomposition.H2OPCA(k=2)),
                     (""drf"", h2o.estimators.random_forest.H2ORandomForestEstimator(ntrees=200))])
pipeline.fit(iris_df[:4],iris_df[4])



I am unable to save this pipeline model in any way described in the h2o documentation. I have tried POJO, MOJO, pickle, dill etc.


Please help me in solving this issue. It would be helpful for me, if I could save the model using pickle.


Thanks,
Anup.","['scikit-learn', 'save', 'pipeline', 'h2o', 'pre-trained-model']",Anup,https://stackoverflow.com/users/9102053/anup,63
48978718,48978718,2018-02-25T21:36:19,2018-02-25 22:36:35Z,255,"I am confused about how H2O calculates R^2. I created a dummy dataframe used H2O's RandomForestEstimator:


df = pd.DataFrame({'x':[1,2,3,4,5],'y':[3,9,2,8,1]})
h2o_df=h2o.H2OFrame(df)
rf = H2ORandomForestEstimator()
rf.train('x','y',h2o_df)
rf.r2()



This returns -0.667, which would indicate a pretty poor fit! But I calculated R^2 with the predict method:


y_true = df.y
y_pred = rf.predict(h2o_df).as_data_frame().predict
SSE = sum((y_pred-y_true)**2)
SST = sum((y_true-y_true.mean())**2)
r2 = 1-(SSE/SST)
r2



This returns 0.727, which makes a lot more sense. What is happening internally with the .r2() method?","['machine-learning', 'random-forest', 'h2o']",Logan Wilson,https://stackoverflow.com/users/8485963/logan-wilson,51
48956020,48956020,2018-02-23T20:49:17,2018-02-24 09:14:10Z,228,"When you one-hot encode categorical variables, you usually drop one of the variables before modeling. That way, you don't have a redundant feature that is linearly dependent on the others. 


Is there a way to specify a level of the categorical variable that should not be used in fitting? 


From 
the documentation
:
""We strongly recommend avoiding one-hot encoding categorical columns with any levels into many binary columns, as this is very inefficient. This is especially true for Python users who are used to expanding their categorical variables manually for other frameworks.",['h2o'],Sepehr,https://stackoverflow.com/users/529800/sepehr,23
48954878,48954878,2018-02-23T19:24:47,2018-03-01 14:07:32Z,128,"I'm trying to start a H2O cluster as external backend for Sparkling Water manually. By following the documentation 
here
 it says I need to use the parameter 'name' with the extended H2O driver. But by doing so it says that the parameter 'name' doesn't exist. How can I set the cloud name so that I can use it to identify the cluster in the sparkling-shell? The help of the driver also doesn't mention a way to set the cloud name. 


Any help would be appreciated.


Best regards,


Markus","['h2o', 'sparkling-water']",Markus Wilhelm,https://stackoverflow.com/users/1984860/markus-wilhelm,171
48954257,48954257,2018-02-23T18:35:43,2021-11-29 19:36:49Z,590,"Closed.
 This question needs 
debugging details
. It is not currently accepting answers.
                                
                            
























 Edit the question to include 
desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem
. This will help others answer the question.






Closed 
6 years ago
.















                        Improve this question
                    








I am trying to use the following code to apply function to each of the rows in a particular column of H2O frame, but getting error.


df[""decision""] = df[""resp_cd""].apply(lambda x:1 if x in [""00"", 01"", ""11] else 0, axis=1)





Is there some other way to use user defined functions in H2O?


How should we use 
apply
 function in H2O? I expect it to be similar to 
apply
 function in Pandas but that doesn't seem to be true.","['python', 'h2o']",learnToCode,https://stackoverflow.com/users/8553795/learntocode,383
48951557,48951557,2018-02-23T15:46:58,2018-02-23 16:05:43Z,373,"pysparkling 2.1


I run the following code:


hc = H2OContext.getOrCreate(spark)
h2o_frame = h2o.import_file('hdfs:path/to/my/file.csv')
spark_frame = hc.as_spark_frame(h2o_frame)



and it works just fine, just like in the documentation.


But then when I try the following code:


hc = H2OContext.getOrCreate(spark)
h2o_frame = h2o.H2OFrame(some_list)
spark_frame = hc.as_spark_frame(h2o_frame) #error at this line



I get the following error:


File ""/my_path/my_file.py"", line 530, in _convert_and_append
spark_frame = hc.as_spark_frame(h2o_frame)  
File ""/my_path/.virtualenv/lib/python2.7/site-packages/pysparkling/context.py"", line 196, in as_spark_frame
j_h2o_frame = h2o_frame.get_java_h2o_frame()
File ""/my_path/.virtualenv/lib/python2.7/site-packages/pysparkling/context.py"", line 38, in get_java_h2o_frame
self._java_frame = hc._jhc.asH2OFrame(self.frame_id)
File ""/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__
File ""/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco
File ""/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o67.asH2OFrame.
 : java.lang.NullPointerException
    at water.fvec.H2OFrame.<init>(H2OFrame.scala:38)
    at water.fvec.H2OFrame.<init>(H2OFrame.scala:46)
    at org.apache.spark.h2o.H2OContext.asH2OFrame(H2OContext.scala:234)
    at org.apache.spark.h2o.JavaH2OContext.asH2OFrame(JavaH2OContext.java:111)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:280)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:214)
    at java.lang.Thread.run(Thread.java:745)



The only difference is the way I initialize the H2OFrame. What is causing this discrepancy? Is there something I'm missing? Why does it matter how the H2OFrame is created?


Any help appreciated


EDIT:


some_list
:


<type 'list'>
[(u'petal_length', 15800.0, 1.0, 0.42857966682031484), (u'petal_width', 14200.0, 0.8987341772151899, 0.3851791942309159), (u'sepal_length', 4808.2783203125, 0.30432141267800633, 0.13042596965182748), (u'sepal_width', 2057.6796875, 0.13023289161392404, 0.05581516929694175), (u'id', 0.0, 0.0, 0.0)]","['python', 'apache-spark', 'pyspark', 'h2o', 'sparkling-water']",Unknown,,N/A
48935263,48935263,2018-02-22T18:59:47,2018-02-22 23:02:26Z,0,"I have a model where some of the input features are calculated from the training dataset (e.g. average or median of a value). I am trying to perform n-fold cross validation on this model, but that means that the values for these features would be different depending on the samples selected for training/validation for each fold. Is there a way in h2o (I'm using it in R) to perhaps pass a funtion that calculates those features once the training set has been determined?


It seems like a pretty intuitive feature to have, but I have not been able to find any documentation on something like it out-of-the-box. Does it exist? If so, could someone point me to a resource?","['r', 'cross-validation', 'h2o']",Abraham Escalante,https://stackoverflow.com/users/4308969/abraham-escalante,45
48927229,48927229,2018-02-22T12:17:28,2018-02-22 12:17:28Z,56,"I'm trying to start a H2O cluster in the Steam web interface. The cluster is starting (I can open it after it started), but Steam thinks it failed and doesn't add it to the list of active clusters. It seems it getting a Java Timeout exception after disowning the cluster. So maybe I need to increase the timeout for the cluster startup?


Is there an option in Steam to increase the timeout? The documentation doesn't say anything about it.


Best regards,


Markus",['h2o'],Markus Wilhelm,https://stackoverflow.com/users/1984860/markus-wilhelm,171
48925902,48925902,2018-02-22T11:07:34,2018-02-22 16:01:46Z,0,"I have already read this question: 
How should we interpret the results of the H2O predict function?

Still don't understand if p1 is the probability between [0,1] and could be used equally as it 's a regression and i can apply my own threshold 


edit: 
thank you for your answer still have some confusion about it, let's dig it suppose my outcome Y is [0,1], if  Y is numeric i run it as REGRESSION and i have a single column as response. On the other hand if Y is factor run it as CLASSIFICATION and the output is: prediction/p0/p1. NOW, is p1 the same as use Y as numeric? 
Also 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/calibrate_model.html
  calibrate_model parameter affects logloss but now the max F1 is still used as threshold on P0 P1 or on the calibrated probabilities? Can i use the calibrated probabilities for regression as the logloss is supposed less?",['h2o'],Unknown,,N/A
48915750,48915750,2018-02-21T21:26:00,2018-02-21 21:26:00Z,383,"I would like to do cross validation for 5 folds. In each fold, I have a training and valid set. However, due to data issue, I need to transform my data. First, I transform the training data, train the model,apply the transformation rule to the validation data, and then test the model. I need to redo the transformation for every fold. How would I do that in H2O? I can't find away to separate the transformation part out. Does anyone have any suggestion?","['transformation', 'cross-validation', 'h2o']",Hua,https://stackoverflow.com/users/9016110/hua,31
48915519,48915519,2018-02-21T21:09:32,2018-02-21 23:02:50Z,0,We have our data stored in hive text files and parquet files is there anyway to load directly from these into H2O or do we have to go through an intermediate step like csv or pandas dataframe?,"['hadoop', 'hive', 'h2o']",Alex Brown,https://stackoverflow.com/users/9393231/alex-brown,19
48907176,48907176,2018-02-21T13:26:31,2018-02-23 08:05:45Z,0,"I am trying to explain the decision taken by h2o GBM model. based on idea:
https://medium.com/applied-data-science/new-r-package-the-xgboost-explainer-51dd7d1aa211

I want to calculate the contribution by each feature into making a certain decision at test time.
Is it possible to get each individual tree from the ensable along with the log-odds at every node? 
also be needing the path traverse for each tree by model while making the prediction.","['decision-tree', 'h2o', 'ensemble-learning']",Unknown,,N/A
48907003,48907003,2018-02-21T13:18:08,2018-02-26 09:35:11Z,0,"I encountered the known issue of not being able to save the xgboost model and load it later to obtain predictions and it was supposedly changed in h2o 3.18 (the problem was in 3.16). I updated the package from h2o's website (downloadable zip) and now the model that had no problem gives the following error:


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  : 
  Unexpected CURL error: Failed to connect to localhost port 54321: Connection refused



This is only in the case of xgboost (binary classification), as other models I use work fine. Of course h2o is initialised and a previous model estimates without problems. Does anyone have any idea what can be the issue? 


EDIT: Here is a reproducible example (based on Erin's answer) that produces the error:


library(h2o)
library(caret)
h2o.init()

# Import a sample binary outcome train set into H2O
train <- h2o.importFile(""https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv"")

# Identify predictors and response
y <- ""response""
x <- setdiff(names(train), y)

# Assigning fold column
set.seed(1)
cv_folds <- createFolds(as.data.frame(train)$response,
                        k = 5,
                        list = FALSE,
                        returnTrain = FALSE)

# version 1
train <- train %>%
    as.data.frame() %>% 
    mutate(fold_assignment = cv_folds) %>%
    as.h2o()

# version 2
train <- h2o.cbind(train, as.h2o(cv_folds))
names(train)[dim(train)[2]] <- c(""fold_assignment"")


# For binary classification, response should be a factor
train[,y] <- as.factor(train[,y])

xgb <- h2o.xgboost(x = x,
                   y = y, 
                   seed = 1,
                   training_frame = train,
                   fold_column = ""fold_assignment"",
                   keep_cross_validation_predictions = TRUE,
                   eta = 0.01,
                   max_depth = 3,
                   sample_rate = 0.8,
                   col_sample_rate = 0.6,
                   ntrees = 500,
                   reg_lambda = 0,
                   reg_alpha = 1000,
                   distribution = 'bernoulli') 



Both versions of creating the train data.frame result in the same error.","['r', 'io', 'h2o', 'xgboost']",Unknown,,N/A
48901169,48901169,2018-02-21T08:28:06,2018-02-21 20:59:23Z,502,"Is there a way to save h2o flow cells after training, in form of a HTML or PDF report? I would like to achieve similar result to Jupyter Notebook, full of figures, tables etc. - a static overview of what happened. ""Save flow"" option saves static text, but removes figures and pictures.


So far I've tried to use primitive approach - just ""save as HTML full page"" in Chrome/Firefox/Safari/Edge, but the result is either empty page, or just a first cell with missing graphic elements.


I was using static Jupyter notebooks as a form of report for my Clients, and I'd like to use H2O Flow in a similar way. 


Thank you in advance","['jupyter', 'h2o']",Animattronic,https://stackoverflow.com/users/1389197/animattronic,375
48897340,48897340,2018-02-21T02:34:46,2018-05-04 18:53:03Z,427,"I'm trying to read data out of our Hadoop HDFS filesystem using the 
h2o.import_file
 python function. I've set the HADOOP_CONF_DIR environment variable as so:


import os
os.environ[""HADOOP_CONF_DIR""] = ""/etc/hadoop/conf""



When I try to read a file using the 
hdfs:///path/to/my/file.txt
 syntax, H2O gives me an error stating 
Error: java.net.UnknownHostException: nameservice1
. 
nameservice1
 is the configured name of our high-availability name service. It's not a single host. The 
hdfs-site.xml
 configuration defines how to contact the name nodes for 
nameservice1
. How do I get H2O to access HDFS in this configuration?","['python', 'hdfs', 'h2o']",aL_eX,https://stackoverflow.com/users/3938208/al-ex,"1,449"
48869016,48869016,2018-02-19T15:11:53,2019-10-25 21:52:41Z,260,"I'm trying to install 
3.14.0.1
 to match the server's version. 


PyPi has a limited set listed at:


https://pypi.python.org/pypi/h2o


The documentation only offers a (current?) install command using a wheel


http://h2o-release.s3.amazonaws.com/h2o/rel-wolpert/1/index.html


pip install http://h2o-release.s3.amazonaws.com/h2o/rel-wolpert/1/Python/h2o-3.18.0.1-py2.py3-none-any.whl


but there doesn't seem to be a list of previous versions ...


For reference, I found Anaconda  packages listed at (but doesn't have version I was looking for):


https://anaconda.org/h2oai/h2o/files


and Github project doesn't publish wheels directly:


https://github.com/h2oai/h2o-3/releases","['python', 'h2o']",gliptak,https://stackoverflow.com/users/304690/gliptak,"3,660"
48837328,48837328,2018-02-17T02:33:56,2018-06-17 21:44:21Z,913,"I downloaded the latest release of H2O (3.18.0.1) and XGboost keeps failing. I am not sure whether to post to the JIRA issues or here. 


h2o.init()
from h2o.estimators import H2OXGBoostEstimator
is_xgboost_available = H2OXGBoostEstimator.available()
train_path = 'https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/higgs_train_imbalance_100k.csv'
test_path = 'https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/higgs_test_imbalance_100k.csv'

df_train = h2o.import_file(train_path)
df_test = h2o.import_file(test_path)

# Transform first feature into categorical feature
df_train[0] = df_train[0].asfactor()
df_test[0] = df_test[0].asfactor()

param = {
      ""ntrees"" : 500
}

model = H2OXGBoostEstimator(**param)
model.train(x = list(range(1, df_train.shape[1])), y = 0, training_frame = df_train)



I can run random forest, GBM without an issue but xgboost keeps failing.


I am running on Ubuntu 16.04. Java Version: java version ""1.8.0_161""; Java(TM) SE Runtime Environment (build 1.8.0_161-b12); Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode). Anaconda Python 3.6


I reinstalled Anaconda and reinstalled JRE, but am still having the same issue.


It keeps giving me the following error:


    xgboost Model Build progress: |████████████████████████████████████████
    ---------------------------------------------------------------------------
    ConnectionResetError                      Traceback (most recent call last)
    ~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
        600                                                   body=body, headers=headers,
    --> 601                                                   chunked=chunked)
        602 

    ~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
        386                     # otherwise it looks like a programming error was the cause.
    --> 387                     six.raise_from(e, None)
        388         except (SocketTimeout, BaseSSLError, SocketError) as e:

    ~/anaconda3/lib/python3.6/site-packages/urllib3/packages/six.py in raise_from(value, from_value)

    ~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
        382                 try:
    --> 383                     httplib_response = conn.getresponse()
        384                 except Exception as e:

    ~/anaconda3/lib/python3.6/http/client.py in getresponse(self)
       1330             try:
    -> 1331                 response.begin()
       1332             except ConnectionError:

    ~/anaconda3/lib/python3.6/http/client.py in begin(self)
        296         while True:
    --> 297             version, status, reason = self._read_status()
        298             if status != CONTINUE:

    ~/anaconda3/lib/python3.6/http/client.py in _read_status(self)
        257     def _read_status(self):
    --> 258         line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
        259         if len(line) > _MAXLINE:

    ~/anaconda3/lib/python3.6/socket.py in readinto(self, b)
        585             try:
    --> 586                 return self._sock.recv_into(b)
        587             except timeout:

    ConnectionResetError: [Errno 104] Connection reset by peer

    During handling of the above exception, another exception occurred:

    ProtocolError                             Traceback (most recent call last)
    ~/anaconda3/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
        439                     retries=self.max_retries,
    --> 440                     timeout=timeout
        441                 )

    ~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
        638             retries = retries.increment(method, url, error=e, _pool=self,
    --> 639                                         _stacktrace=sys.exc_info()[2])
        640             retries.sleep()

    ~/anaconda3/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
        356             if read is False or not self._is_method_retryable(method):
    --> 357                 raise six.reraise(type(error), error, _stacktrace)
        358             elif read is not None:

    ~/anaconda3/lib/python3.6/site-packages/urllib3/packages/six.py in reraise(tp, value, tb)
        684         if value.__traceback__ is not tb:
    --> 685             raise value.with_traceback(tb)
        686         raise value

    ~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
        600                                                   body=body, headers=headers,
    --> 601                                                   chunked=chunked)
        602 

    ~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
        386                     # otherwise it looks like a programming error was the cause.
    --> 387                     six.raise_from(e, None)
        388         except (SocketTimeout, BaseSSLError, SocketError) as e:

    ~/anaconda3/lib/python3.6/site-packages/urllib3/packages/six.py in raise_from(value, from_value)

    ~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
        382                 try:
    --> 383                     httplib_response = conn.getresponse()
        384                 except Exception as e:

    ~/anaconda3/lib/python3.6/http/client.py in getresponse(self)
       1330             try:
    -> 1331                 response.begin()
       1332             except ConnectionError:

    ~/anaconda3/lib/python3.6/http/client.py in begin(self)
        296         while True:
    --> 297             version, status, reason = self._read_status()
        298             if status != CONTINUE:

    ~/anaconda3/lib/python3.6/http/client.py in _read_status(self)
        257     def _read_status(self):
    --> 258         line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
        259         if len(line) > _MAXLINE:

    ~/anaconda3/lib/python3.6/socket.py in readinto(self, b)
        585             try:
    --> 586                 return self._sock.recv_into(b)
        587             except timeout:

    ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

    During handling of the above exception, another exception occurred:

    ConnectionError                           Traceback (most recent call last)
    ~/anaconda3/lib/python3.6/site-packages/h2o/backend/connection.py in request(self, endpoint, data, json, filename, save_to)
        399                                     headers=headers, timeout=self._timeout, stream=stream,
    --> 400                                     auth=self._auth, verify=self._verify_ssl_cert, proxies=self._proxies)
        401             self._log_end_transaction(start_time, resp)

    ~/anaconda3/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs)
         57     with sessions.Session() as session:
    ---> 58         return session.request(method=method, url=url, **kwargs)
         59 

    ~/anaconda3/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
        507         send_kwargs.update(settings)
    --> 508         resp = self.send(prep, **send_kwargs)
        509 

    ~/anaconda3/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs)
        617         # Send the request
    --> 618         r = adapter.send(request, **kwargs)
        619 

    ~/anaconda3/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
        489         except (ProtocolError, socket.error) as err:
    --> 490             raise ConnectionError(err, request=request)
        491 

    ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

    During handling of the above exception, another exception occurred:

    H2OConnectionError                        Traceback (most recent call last)
    <ipython-input-22-37b26d4dfbfd> in <module>()
          1 start = time.time()
    ----> 2 model.train(x = list(range(1, df_train.shape[1])), y = 0, training_frame = df_train)
          3 end = time.time()
          4 print(end - start)

    ~/anaconda3/lib/python3.6/site-packages/h2o/estimators/estimator_base.py in train(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, max_runtime_secs, ignored_columns, model_id, verbose)
        229             return
        230 
    --> 231         model.poll(verbose_model_scoring_history=verbose)
        232         model_json = h2o.api(""GET /%d/Models/%s"" % (rest_ver, model.dest_key))[""models""][0]
        233         self._resolve_model(model.dest_key, model_json)

    ~/anaconda3/lib/python3.6/site-packages/h2o/job.py in poll(self, verbose_model_scoring_history)
         56                 pb.execute(self._refresh_job_status, print_verbose_info=lambda x: self._print_verbose_info() if int(x * 10) % 5 == 0  else "" "")
         57             else:
    ---> 58                 pb.execute(self._refresh_job_status)
         59         except StopIteration as e:
         60             if str(e) == ""cancelled"":

    ~/anaconda3/lib/python3.6/site-packages/h2o/utils/progressbar.py in execute(self, progress_fn, print_verbose_info)
        167                 # Query the progress level, but only if it's time already
        168                 if self._next_poll_time <= now:
    --> 169                     res = progress_fn()  # may raise StopIteration
        170                     assert_is_type(res, (numeric, numeric), numeric)
        171                     if not isinstance(res, tuple):

    ~/anaconda3/lib/python3.6/site-packages/h2o/job.py in _refresh_job_status(self)
         91     def _refresh_job_status(self):
         92         if self._poll_count <= 0: raise StopIteration("""")
    ---> 93         jobs = h2o.api(""GET /3/Jobs/%s"" % self.job_key)
         94         self.job = jobs[""jobs""][0] if ""jobs"" in jobs else jobs[""job""][0]
         95         self.status = self.job[""status""]

    ~/anaconda3/lib/python3.6/site-packages/h2o/h2o.py in api(endpoint, data, json, filename, save_to)
        101     # type checks are performed in H2OConnection class
        102     _check_connection()
    --> 103     return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
        104 
        105 

    ~/anaconda3/lib/python3.6/site-packages/h2o/backend/connection.py in request(self, endpoint, data, json, filename, save_to)
        408             else:
        409                 self._log_end_exception(e)
    --> 410                 raise H2OConnectionError(""Unexpected HTTP error: %s"" % e)
        411         except requests.exceptions.Timeout as e:
        412             self._log_end_exception(e)

    H2OConnectionError: Unexpected HTTP error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",['h2o'],David Comfort,https://stackoverflow.com/users/6287730/david-comfort,153
48833097,48833097,2018-02-16T18:47:59,2021-10-08 13:42:20Z,0,"I am trying to save the model build using the function: 
h2o.saveModel()
, based on function description on page 159 of the 
H2O user manual for R
, the arguments only consider 
path
. I looked at other similar function such as: 
h2o.saveModelDetails()
 but it uses the same argument. Please advise if there any another way to specify the name of the model.",['h2o'],David Leal,https://stackoverflow.com/users/6237093/david-leal,"6,739"
48832512,48832512,2018-02-16T18:03:29,2018-02-20 02:49:16Z,0,"have been trying to install h2o for use on R and have hit multiple buffers....


I seem to be able to install the file successfully by using:


install.packages(""~/Desktop/h2o_3.18.0.1.tar.gz"", repos = NULL, type = ""source"")



Out


installing *source* package ‘h2o’ ...
R
demo
inst
preparing package for lazy loading
help
installing help indices
building package indices
testing if installed package can be loaded
DONE (h2o)



1. Then when I call h2o, I get an error (see below), ... I understand this may be
because it only works with earlier versions of java. Is this correct?


2. If so .... what should I do to get R to run an alternative version of
java?
 


(I have seen that there are people who can do this and are
describing this but I wondered if there were any concise instructions?)


Error: package or namespace load failed for ‘h2o’ in get(Info[i, 1],
envir = env):
 lazy-load database
'/Library/Frameworks/R.framework/Versions/3.4/Resources/library/h2o/R/h2o.rdb'
is corrupt
In addition: Warning message:
In get(Info[i, 1], envir = env) : internal error -3 in R_decompress1","['java', 'r', 'h2o']",user8369515,https://stackoverflow.com/users/8369515/user8369515,525
48830659,48830659,2018-02-16T16:09:16,2018-02-16 16:09:16Z,109,"The autoML stops on a clock.  I compared two auto-ML's where one used a subset of what the other had to make the same predictions, and at 3600 seconds runtime the fuller model looked better.  I repeated this with a 5000 second re-run, and the subset model looked better.  They traded places, and that isn't supposed to happen.


I think it is convergence.  
Is there any way to track convergence-history of stacked ensemble learners to determine either if they are relatively stable?
  We have that for parallel and series CART ensembles.  I don't see why a heterogeneous ensemble wouldn't do the same.


I have plenty of data, and especially with cross-validation, I would like to not think that the difference was because of the training vs. validation set random draws.  


I'm running on relatively high performance hardware so I don't think it is a ""too short runtime"".  My ""all"" model count is between hundreds and a thousand, for what it's worth.","['h2o', 'ensemble-learning', 'automl']",EngrStudent,https://stackoverflow.com/users/2259468/engrstudent,"1,972"
48830541,48830541,2018-02-16T16:02:04,2018-02-16 19:26:43Z,619,"AutoML makes two learners, one that includes ""all"" and the other that is a subset that is ""best of family"".


Is there any way to not-manually save the components and stacked ensemble aggregator to disk so that that ""best of family"", treated as a standalone black-box, can be stored, reloaded, and used without requiring literally 1000 less valuable learners to exist in the same space?


If so, how do I do that?","['h2o', 'ensemble-learning', 'automl']",EngrStudent,https://stackoverflow.com/users/2259468/engrstudent,"1,972"
48810326,48810326,2018-02-15T15:06:54,2018-02-15 15:34:37Z,0,"Here is what is happening:


> h2o.init(startH2O = FALSE)
 Connection successful!


ERROR: Unexpected HTTP Status code: 301 Moved Permanently (url = http://localhost:54321/3/Cloud?skip_ticks=true)

Error: lexical error: invalid char in json text.
                                       <HTML>  <HEAD><TITLE>Redirectio
                     (right here) ------^



I've attempted numerous solutions related to 
this
 and 
this
 without success.


I'm using OSX 10.12.6. Here are a few notes on my configuration:


$ java -version
java version ""1.8.0_144""
Java(TM) SE Runtime Environment (build 1.8.0_144-b01)
Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)



In R (version 3.4.3):


> packageVersion(""h2o"")
[1] ‘3.16.0.2’
> Sys.getenv(""JAVA_HOME"") # Based on my manual setting in ~/.Renviron
[1] ""/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home""
> update.packages() # everything up-to-date
> 



Any ideas for what to check for would be appreciated!","['r', 'h2o']",zkurtz,https://stackoverflow.com/users/2232265/zkurtz,"3,270"
48805063,48805063,2018-02-15T10:32:54,2020-10-21 22:09:26Z,0,"I would like to build a GBM model with H2O. My data set is imbalanced, so I am using the balance_classes parameter. For grid search (parameter tuning) I would like to use 5-fold cross validation. I am wondering how H2O deals with class balancing in that case. Will only the training folds be rebalanced? I want to be sure the test-fold is not rebalanced.","['machine-learning', 'cross-validation', 'h2o', 'gbm', 'imbalanced-data']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
48795297,48795297,2018-02-14T20:02:07,2018-02-14 20:02:07Z,750,"Based on the question and subsequent answer 
here
: When starting an 
h2o
 instance running on a hadoop cluster, (with say 
hadoop jar h2odriver.jar -nodes 4 -mapperXmx 6g -output hdfsOutputDir
) the callback IP address used to connect to the h2o instance is selected by the hadoop runtime. So in most of the cases the IP address and the port is select by the Hadoop run time to find best available and looks like


....
H2O node 172.18.4.63:54321 reports H2O cluster size 4
H2O node 172.18.4.67:54321 reports H2O cluster size 4
H2O cluster (4 nodes) is up
(Note: Use the -disown option to exit the driver after cluster formation)
Open H2O Flow in your web browser: http://172.18.4.67:54321
Connection url output line: Open H2O Flow in your web browser: http://172.18.4.67:54321



The recommended way of using 
h2o
 is to start and stop individual instances each time you want to use it (sorry, can't currently find the supporting documentation). The problem here is that if you want your python code to start up and connect to a 
h2o
 instance automatically, it is not going to know what IP to connect to until the 
h2o
 instance is already up and running. Thus, a common way to start H2O cluster on Hadoop is to let the Hadoop decide the cluster, then to parse the output for the line


Open H2O Flow in your web browser: x.x.x.x:54321



to get/extract the IP address.


The problem here is that 
h2o
 is a blocking process who's output prints as a 
stream
 of text lines as the instance starts up rather than in bulk, this made it hard for me to get the final output line needed using basic python 
Popen
 logic to capture output. Is there a way to capture the output as it is being generated to get the line with the connection IP?","['python', 'h2o']",lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
48761938,48761938,2018-02-13T07:54:27,2018-06-14 12:18:32Z,0,"I am trying to run algorithms in H2o as the dataset is quite large and its a regression problem


I am competing in a kernel only competition named 
Mercari Price suggestion  challenge
  and thus it requires  to run and check the code only in Kaggle Kernels.


I am using R language with an 8 GB RAM  


Initially  I  was   able to run glm model and save output csv 
with the following code 


library(glm2)
glm.model2 <- h2o.glm( y = y.dep, x = x.indep, training_frame = train1.h2o, validation_frame = valid1.h2o
,family = ""gaussian"")



Glm runs quickly in 12 sec without producing error but as soon as I try to run 


either gbm or  basic deep learning  model  it produces error


library(gbm)
h2o.gbm(y=y.dep, x=x.indep, training_frame = train1.h2o,validation_frame = valid1.h2o, ntrees = 2000, max_depth = 4, learn_rate = 0.01)

library(randomForest)
rforest.model <- h2o.randomForest(y=y.dep, x=x.indep, training_frame = train1.h2o,validation_frame = valid1.h2o, ntrees = 1000, mtries = 3, max_depth = 4, seed = 1122)


 dlearning.model <- h2o.deeplearning(y = y.dep,
                                      x = x.indep,
                                       training_frame = train1.h2o,
                                       validation_frame = valid1.h2o,
                                       epoch = 60,
                                       hidden = c(100,100),
                                       activation = ""Rectifier"",
                                       seed = 1122
  )



I get the following error time and again.
Please suggest what can be done to solve this problem as glm is running very fine but all other are not at all running


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix, : Unexpected CURL error: Failed to connect to localhost port 54321: Connection refused
Traceback:



It fails even after reaching  10 to 11 percent for both  models and I want to know is there any hack or any measure so I can at least run these algorithms so that I can submit my result. I am unable to do built an ensemble model because of all this.


Any measure that can be used as I have run them in kaggle kernel only","['r', 'machine-learning', 'random-forest', 'h2o', 'kaggle']",Unknown,,N/A
48750531,48750531,2018-02-12T16:00:05,2018-02-13 01:40:10Z,0,"I have a question regarding 
h2o.stackedEnsemble
 in R. When I try to create an ensemble from GLM models (or any other models and GLM) I get the following error: 


DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.NullPointerException

DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.NullPointerException
    at water.MRTask.getResult(MRTask.java:478)
    at water.MRTask.getResult(MRTask.java:486)
    at water.MRTask.doAll(MRTask.java:390)
    at water.MRTask.doAll(MRTask.java:396)
    at hex.StackedEnsembleModel.predictScoreImpl(StackedEnsembleModel.java:123)
    at hex.StackedEnsembleModel.doScoreMetricsOneFrame(StackedEnsembleModel.java:194)
    at hex.StackedEnsembleModel.doScoreOrCopyMetrics(StackedEnsembleModel.java:206)
    at hex.ensemble.StackedEnsemble$StackedEnsembleDriver.computeMetaLearner(StackedEnsemble.java:302)
    at hex.ensemble.StackedEnsemble$StackedEnsembleDriver.computeImpl(StackedEnsemble.java:231)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:206)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1263)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
Caused by: java.lang.NullPointerException

Error: DistributedException from localhost/127.0.0.1:54321: 'null', caused by java.lang.NullPointerException



The error does not occur when I stack any other models, only appears with the GLM. Of course I use the same folds for cross-validation.


Some sample code for training the model and the ensemble:


glm_grid <- h2o.grid(algorithm = ""glm"",
                     family = 'binomial',
                     grid_id = ""glm_grid"",
                     x = predictors,
                     y = response,
                     seed = 1,
                     fold_column = ""fold_assignment"",
                     training_frame = train_h2o,
                     keep_cross_validation_predictions = TRUE,
                     hyper_params = list(alpha = seq(0, 1, 0.05)),
                     lambda_search = TRUE,
                     search_criteria = search_criteria,
                     balance_classes = TRUE,
                     early_stopping = TRUE)

glm <- h2o.getGrid(""glm_grid"",
                  sort_by=""auc"",
                  decreasing=TRUE)

ensemble <- h2o.stackedEnsemble(x = predictors,
                                y = response,
                                training_frame = train_h2o,
                                model_id = ""ens_1"",
                                base_models = glm@model_ids[1:5])","['java', 'r', 'h2o', 'ensemble-learning']",Unknown,,N/A
48744565,48744565,2018-02-12T10:45:18,2018-02-12 10:45:18Z,0,"I am using h2o.glm to carry out a prediction. The model requirement is as many true positives as possible at the cost of many false positives. 
In terms of accuracy or (AUC) the result of h2o.glm is pretty good, however, due to the requirements, I need to play with the cost matrix. C5.0 has a way to introduce cost matrix, for example.


I read the documentation 

https://www.rdocumentation.org/packages/h2o/versions/3.8.1.3/topics/h2o.glm


and checked many other resources and did not come across with a solution.


Perhaps some of you tried it. I would appreciate a hint.","['r', 'h2o']",may,https://stackoverflow.com/users/1865782/may,61
48706347,48706347,2018-02-09T12:54:27,2018-02-11 19:20:59Z,124,"I used


h2o.predict_leaf_node_assignment(model, frame) 



to get the 
leaf node assignments
 of my 
gbm
 model. Is it possible that I store this function as an 
h2o
-object and then use it for deployment on new data entries?","['python-3.x', 'h2o', 'gbm']",Skandix,https://stackoverflow.com/users/4464653/skandix,"1,984"
48698385,48698385,2018-02-09T03:34:34,2018-08-16 02:18:37Z,0,"I tried to use H2O to create some machine learning models for binary classification problem, and the test results are pretty good. But then I checked and found something weird. I tried to print the prediction of the model for the test set out of curiosity. And I found out that my model actually predicts 0 (negative) all the time, but the AUC is around 0.65, and precision is not 0.0. Then I tried to use Scikit-learn just to compare the metrics scores, and (as expected) they’re different. The Scikit learn yielded 0.0 precision and 0.5 AUC score, which I think is correct. Here's the code that I used:


model = h2o.load_model(model_path)
predictions = model.predict(Test_data).as_data_frame()

# H2O version to print the AUC score
auc = model.model_performance(Test_data).auc()

# Python version to print the AUC score
auc_sklearn = sklearn.metrics.roc_auc_score(y_true, predictions['predict'].tolist())



Any thought? Thanks in advance!","['python', 'machine-learning', 'scikit-learn', 'classification', 'h2o']",Aryo Pradipta Gema,https://stackoverflow.com/users/7676016/aryo-pradipta-gema,"1,230"
48697292,48697292,2018-02-09T01:12:27,2018-02-09 05:32:16Z,158,"I am curious about the cluster configuration environment in terms of the ML Training performance of H2O.


If there are three nodes, is there a performance difference between configuring a generic H2O Multi-node Cluster and configuring an H2O Spark Cluster based on Spark?


From our experiments, we conclude that there is no obvious performance difference between the two.


However, many of the H2O documents tell me that H2O Sparkling Water is more effective at ML Training.




Reference




H2O Multi-node Cluster: 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/starting-h2o.html#flatfile","['cluster-computing', 'h2o', 'sparkling-water']",fool-dev,https://stackoverflow.com/users/4172515/fool-dev,"7,767"
48676228,48676228,2018-02-08T01:51:57,2018-02-08 02:32:14Z,323,"I am using the h2o package in R to fit a GLM via the 
h2o.glm()
 fucntion. One reasonable way to assess feature importance in a GLM with the l1 regularization penalty is to monitor the order that parameters enter the linear predictor (i.e. the model) as the l1 penalty weight decreases over each successive 
lambda
. I cannot find in the h2o documentation if it possible to extract this information from a returned model object. 


Does anyone know if it is possible to view the fitted model form after each successive 
lambda
?


Thanks,","['glm', 'h2o']",Vypa,https://stackoverflow.com/users/3788582/vypa,71
48656028,48656028,2018-02-07T04:28:11,2018-02-07 18:04:20Z,450,"I would like to find the ensemble performance of the training data in H2O stacked ensemble. In the link -python code, it showed the performance (AUC) of the test data, however, I need to see the performance of the train data (Mean of the results of the each fold). Is there any options available to do that?

code","['python', 'cross-validation', 'h2o', 'ensemble-learning']",Unknown,,N/A
48645255,48645255,2018-02-06T14:24:38,2018-02-08 00:59:23Z,940,"I'm performing a gridsearch for GBM in h2o for a continuous outcome with continuous predictors. I'm using cross validation for training and then predict on a test set.


I'm using the function .predict_leaf_node_assignment:


best_gbm.predict_leaf_node_assignment(test_frame_h2o)
(where best_gbm is the best gbm model I got from gridsearch)


and get the following table where we can see the leaf node assignments per tree T1, T2, T3 etc.




Question 1:


How can I get the values of T1, T2, T3 etc. per leaf in the below table and not the location of the leaf?


Question 2:


If there is a way to get the values for T1, T2, T3 etc. what do they actually reflect? Is the T1 the first prediction and then T2, T3, T4 are the corrections? Or T1 is the prediction and then T2 is T1 corrected etc.?


Thanks.


Edit:
 I tried to download mojo in python as explained in this page so that I can look into the different trees.

http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html?highlight=mojo


In ""Step 2: Compile and run the MOJO"" the 2nd part of this step is given only in R:
""Create your main program in the experiment folder by creating a new file called main.java (for example, using “vim main.java”). Include the following contents. Note that this file references the GBM model created above using R.""


Can I do this in python?
I have tried to copy for example the command ""import java.io.*"" in the jupyter notebook but it throws an error (ModuleNotFoundError: No module named 'java').","['python-3.x', 'h2o', 'gbm']",Community,https://stackoverflow.com/users/-1/community,1
48642605,48642605,2018-02-06T12:00:46,2018-02-09 12:50:20Z,328,"I've set up a Spark Standalone cluster (1 master and 2 slaves, to start with) and like to use Pysparkling on in. My understanding is that I have to install h2o_pysparkling_2.0 (I'm running Spark 2.0.2), requests, tabulate, colorama and future on the master instance (I'm using Anaconda distribution, so that I shouldn't care about numpy and the like):


pip install h2o_pysparkling_2.0
pip install requests 
pip install tabulate
pip install future
pip install colorama



However, I don't know whether I must install the same packages on the slave instances. I think so, but I'd like to know whether any of you have some information on this. Available 
official documentation
 and 
blog posts
 do not provide specific information on this issue.","['python', 'apache-spark', 'pyspark', 'h2o']",Alper t. Turker,https://stackoverflow.com/users/3269809/alper-t-turker,35.1k
48639220,48639220,2018-02-06T09:09:44,2018-02-06 16:22:48Z,0,"I'm trying to create a pre-trained embedding layer, using 
h2o.word2vec
, i'm looking to extract each word in the model and its equivalent embedded vector.


Code:


library(data.table)
library(h2o)
h2o.init(nthreads = -1)

comment <- data.table(comments='ExplanationWhy the edits made under my username Hardcore Metallica 
                      Fan were reverted They werent vandalisms just closure on some GAs after I voted 
                      at New York Dolls FAC And please dont remove the template from the talk page since Im retired now')

comments.hex <- as.h2o(comment, destination_frame = ""comments.hex"", col.types=c(""String""))

words <- h2o.tokenize(comments.hex$comments, ""\\\\W+"")

vectors <- 3 # Only 10 vectors to save time & memory
w2v.model <- h2o.word2vec(words
                          , model_id = ""w2v_model""
                          , vec_size = vectors
                          , min_word_freq = 1
                          , window_size = 2
                          , init_learning_rate = 0.025
                          , sent_sample_rate = 0
                          , epochs = 1) # only a one epoch to save time
print(h2o.findSynonyms(w2v.model, ""the"",2))



The 
h2o
 API enables me to get the cosine of two word, but i'm just looking to get the vector of each work in my vocabulary, how can i get it? couldn't find any simple method in the API that gives it


Thanks in advance","['python', 'r', 'nlp', 'h2o']",Yehoshaphat Schellekens,https://stackoverflow.com/users/3386991/yehoshaphat-schellekens,"2,365"
48633281,48633281,2018-02-05T23:39:45,2018-02-06 00:30:02Z,0,"In H2O Steam Overview: Data scientists can publish Python and R code as REST APIs and easily integrate with production applications.


I don't know how to public R code as REST APIs in H2O steam.
Please help me?","['r', 'h2o']",Unknown,,N/A
48617076,48617076,2018-02-05T06:41:10,2018-02-07 13:34:00Z,0,"I am working on a multiclass text classification problem. I have build a gradient boosting model for the same.


About the dataset:


The dataset has two columns: ""Test_name"" and  ""Description""


There are six labels in the Test_Name column and their corresponding description in the  ""Description"" column.


My approach towards the problem


DATA PREPARATION




Creat a word vector for description.




Build a corpus using the word vector.




Pre-processing tasks such as removing number, whitespaces, stopwords and conversion to lower case.




Build a document term matrix (dtm).




Remove sparse words from the above dtm.




The above step leads to a count frequency matrix showing the frequency of each word in its coressponding column.




Tranform count frequency matrix to a binary instance matrix, which shows occurences of a word in a document as either 0 or 1, 1 for being present and 0 for absent.




Append the label column from the original notes dataset with the transformed dtm. The label column has 6 labels.






Model Building


Using H2o package, build a gbm model.


Results obtained


Four of the class labels are classified well but the rest two are poorly classified.


below is the output:


Extract training frame with `h2o.getFrame(""train"")`
MSE: (Extract with `h2o.mse`) 0.1197392
RMSE: (Extract with `h2o.rmse`) 0.3460335
Logloss: (Extract with `h2o.logloss`) 0.3245868
Mean Per-Class Error: 0.3791268
Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)

Body Fluid Analysis =   401 / 2,759
Cytology Test       =   182 / 1,087
Diagnostic Imaging  =   117 / 3,907
Doctors Advice      =      32 / 752
Organ Function Test =     461 / 463
Patient Related     =     101 / 113
Totals              = 1,294 / 9,081



The misclassification errors for organ function test and patient related are relatively higher. How can i fix this?","['r', 'h2o', 'text-classification', 'gbm', 'multiclass-classification']",Community,https://stackoverflow.com/users/-1/community,1
48588299,48588299,2018-02-02T17:51:13,2018-02-05 21:00:19Z,237,how should I use H2O DAI on 2 target columns? Current version (AMI ID: h2oai-driverless-ai-1.0.19 (ami-46e5dd3c)) only allows 1 target column. The 2 target columns of interest are both float64 type. Thanks.,"['h2o', 'driverless-ai']",slowD,https://stackoverflow.com/users/2177373/slowd,339
48580085,48580085,2018-02-02T09:57:00,2018-02-06 15:14:29Z,0,"It seems like h2o's MOJO zip files for its random forest algorithm contain ""auxiliary"" tree data that account for roughly 75% of the total MOJO file size.




What is this auxiliary tree data exactly used for?


Is it necessary to have this data for deployed models that only perform forecasting?


If not, is it possible to omit storing the auxiliary data when calling 
h2o.download_mojo
?




Thanks for your help.","['python', 'r', 'random-forest', 'h2o']",cryo111,https://stackoverflow.com/users/983028/cryo111,"4,474"
48569509,48569509,2018-02-01T18:29:42,2018-06-13 01:27:21Z,279,"I am using H2O AutoML for modelling in R. I found that AutoML supports 
keep_cross_validation_predictions
 option on h2o web interface page (i.e. Flow) and it doesn't support it when we use R interface to run. Please help me to know why such thing is happening.",['h2o'],Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
48553191,48553191,2018-01-31T23:34:50,2018-02-01 01:46:12Z,0,It would be really great if some one can tell me how to retain an ID variable as an indicator variable but not a predictor while creating (training & testing models). I am using H2o through R. Appreciate if someone can respond with their thoughts.,"['r', 'machine-learning', 'h2o']",nickhar,https://stackoverflow.com/users/1714488/nickhar,20.6k
48538135,48538135,2018-01-31T08:57:48,2018-02-01 00:26:55Z,148,"Using H2o 3.16.0.4 to parse the Kaggle Toxic data classifier competition. The data is not getting parsed even after using parser as csv and separator as "","". Let me know if this is a product related bug or some configuration is missing.",['h2o'],Arun Lakhotia,https://stackoverflow.com/users/7984336/arun-lakhotia,51
48531506,48531506,2018-01-30T22:23:14,2018-01-31 18:28:50Z,47,"We started a POC of Sparkling water and realized they are internally using the 
duke library
. Nor sure what features of Duke Library H2o.


Duke allows to use custom comparators? Does h2o exposes this feature ?


I have gone through the source code and the only reference i see is during the H2oContext Initialization they are making sure the Duke Library Comparator is loaded. How ever i didn't see any other places where they are actually using the comparators.


Thanks","['h2o', 'sparkling-water']",Unknown,,N/A
48525653,48525653,2018-01-30T15:59:27,2019-11-02 23:06:49Z,0,"I'm new to H2O project but have a lot of interest to use it for my web application. I've made a few tutorials building a model with 
h2o
 library in R and then exporting it as 
POJO
 model.


My problem is that the data I need to predict is in different format I used for train dataset. I am wondering is it possible to use my R code with data transformation and export it as 
POJO
 (or similar) so I could prepare a new data for prediction in real time.


I've found some examples in ""Productionizing H2O"" section on H2O docs page (
link
), like 
Consumer loan application
 and 
Storm bolt
. But in first case they use the same set of features and in the second one data prep is done with .java code (not sure on that, I have a poor programming skills outside of R). So I am not able to figure out how to use my transformation code in R on a new dataset.


Any suggestions?","['r', 'h2o']",Blundering Ecologist,https://stackoverflow.com/users/6674698/blundering-ecologist,"1,265"
48521538,48521538,2018-01-30T12:29:34,2018-02-02 10:31:41Z,656,"I am new to h2o in python. I'm fitting a GBM with cross-validation on my training set and then get predictions on a holdout set. My outcome is CONTINOUOUS and for every prediction I would like to have a measure of uncertainty.  I'm not interested in prediction intervals, I simply look for an uncertainty score for every prediction that I make. Any ideas are more than welcome. 


Thanks in advance!","['python-3.x', 'prediction', 'h2o', 'gbm', 'uncertainty']",Unknown,,N/A
48490900,48490900,2018-01-28T20:20:58,2018-01-30 09:46:09Z,0,"I have searched for different portals and even in h2o ensemble documentation and all I have got ensemble examples for only classification problem binary in nature but not a single  example showing how to implement general stacking or h2o ensembling for a simple regression problem in r 


I request anyone to please share working code on how to implement h2o ensemble or stacking only for regression  problem in R 


OR 


simple ensembling only meant for regression in R.


Only want to know how ensembling/stacking is implemented for regression with varying weights.","['r', 'h2o', 'ensemble-learning']",jatin singh,https://stackoverflow.com/users/7496453/jatin-singh,123
48470944,48470944,2018-01-26T23:21:02,2020-01-22 10:31:52Z,0,"I'm using H2O's Machine Learning package (Random Forest).  


Occasionally, I get this error:



H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/PostFile (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f176f9f39d0>: Failed to establish a new connection: [Errno 111] Connection refused',))



Some combination of restarting the H2O server and/or retrying my command (I'm not sure which one since it's buried deep in my code but it's after 
h2o.init()
 and before building the H2O data frame) seems to eventually works around the error.


Is there a way to up the Max retries?


My configuration.  Ubuntu 16.04-2 x86_64 GNU/Linux.  I'm using the Python API.


H2O:


Checking whether there is an H2O instance running at http://localhost:54321..... not found.
Attempting to start a local H2O server...
  Java Version: openjdk version ""1.8.0_151""; OpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12); OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)
  Starting server from /usr/local/lib/python2.7/dist-packages/h2o/backend/bin/h2o.jar
  Ice root: /tmp/tmpGnu_MH
  JVM stdout: /tmp/tmpGnu_MH/h2o_clem_started_from_python.out
  JVM stderr: /tmp/tmpGnu_MH/h2o_clem_started_from_python.err
  Server is running at http://127.0.0.1:54321
Connecting to H2O server at http://127.0.0.1:54321... successful.

H2O cluster uptime:     05 secs
H2O cluster version:    3.16.0.4
H2O cluster version age:    10 days
H2O cluster name:   H2O_from_python_clem_30vfd8
H2O cluster total nodes:    1
H2O cluster free memory:    13.95 Gb
H2O cluster total cores:    8
H2O cluster allowed cores:  7
H2O cluster status:     accepting new members, healthy
H2O connection url:     http://127.0.0.1:54321
H2O connection proxy:   None
H2O internal security:  False
H2O API Extensions:     XGBoost, Algos, AutoML, Core V3, Core V4
Python version:     2.7.12 final



(Added)  I'm actually having the problem in making the dataframe:


/usr/local/lib/python2.7/dist-packages/h2o/frame.pyc in __init__(self, python_obj, destination_frame, header, separator, column_names, column_types, na_strings)
    100         if python_obj is not None:
    101             self._upload_python_object(python_obj, destination_frame, header, separator,
--> 102                                        column_names, column_types, na_strings)
    103 
    104     @staticmethod

/usr/local/lib/python2.7/dist-packages/h2o/frame.pyc in _upload_python_object(self, python_obj, destination_frame, header, separator, column_names, column_types, na_strings)
    141             csv_writer.writerows(data_to_write)
    142         tmp_file.close()  # close the streams
--> 143         self._upload_parse(tmp_path, destination_frame, 1, separator, column_names, column_types, na_strings)
    144         os.remove(tmp_path)  # delete the tmp file
    145 

/usr/local/lib/python2.7/dist-packages/h2o/frame.pyc in _upload_parse(self, path, destination_frame, header, sep, column_names, column_types, na_strings)
    315 
    316     def _upload_parse(self, path, destination_frame, header, sep, column_names, column_types, na_strings):
--> 317         ret = h2o.api(""POST /3/PostFile"", filename=path)
    318         rawkey = ret[""destination_frame""]
    319         self._parse(rawkey, destination_frame, header, sep, column_names, column_types, na_strings)

/usr/local/lib/python2.7/dist-packages/h2o/h2o.pyc in api(endpoint, data, json, filename, save_to)
    101     # type checks are performed in H2OConnection class
    102     _check_connection()
--> 103     return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
    104 
    105 

/usr/local/lib/python2.7/dist-packages/h2o/backend/connection.pyc in request(self, endpoint, data, json, filename, save_to)
    408             else:
    409                 self._log_end_exception(e)
--> 410                 raise H2OConnectionError(""Unexpected HTTP error: %s"" % e)
    411         except requests.exceptions.Timeout as e:
    412             self._log_end_exception(e)

H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/PostFile (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcc564de750>: Failed to establish a new connection: [Errno 111] Connection refused',))","['python', 'h2o']",aL_eX,https://stackoverflow.com/users/3938208/al-ex,"1,449"
48470043,48470043,2018-01-26T21:47:47,2018-02-02 02:58:39Z,0,"I'm trying to use H2O's Random Forest for a multinominal classification into 71 classes with 38,000 training set examples.  I have one features that is a string that in many cases are predictive, so I want to use it as a categorical feature.


The hitch is that even after canonicalizing the strings (uppercase, stripping out numbers, punctuation, etc.), I still have 7,000 different strings (some due to spelling or OCR errors, etc.)  I have code to remove strings that are relatively rare, but I'm not sure what a reasonable cut off value is.  (I can't seem to find any help in the documentation.)


I'm also not sure what to due with nbin_cats hyperparameter.  Should I make it equal to the number of different categorical variables I have?  [added: default for nbin_cats is 1024 and I'm well below that at around 300 different categorical values, so I guess I don't have to do anything with this parameter]


I'm also thinking perhaps if a categorical value is associated with too many different categories that I'm trying to predict, maybe I should drop it as well.


I'm also guessing I need to increase the tree depth to handle this better. 


Also, is there a special value to indicate ""don't know"" for the strings that I am filtering out?  (I'm mapping it to a unique string but I'm wondering if there is a better value that indicates to H2O that the categorical value is unknown.) 


Many thanks in advance.","['machine-learning', 'h2o', 'feature-selection', 'categorical-data']",Unknown,,N/A
48469361,48469361,2018-01-26T20:53:26,2018-01-26 21:42:10Z,0,"I am using 
H2o
 in 
R
 on linux server and when multiple users try to use 
H2o
 it crashes with errors similar to this. Can we use multiple instances of h2o? If not, is there a workaround which will not start 
H2o
 if an instance is already running?


Would this crash still happen if we start one instance from 
R
 and another instance from 
python
?


 ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://localhost:54321/3/ModelBuilders/word2vec)



java.lang.NullPointerException
[1] ""java.lang.NullPointerException""                                                                              
 [2] ""    hex.word2vec.Word2Vec.init(Word2Vec.java:38)""                                                            
 [3] ""    water.api.ModelBuilderHandler.handle(ModelBuilderHandler.java:60)""                                       
 [4] ""    water.api.ModelBuilderHandler.handle(ModelBuilderHandler.java:17)""                                       
 [5] ""    water.api.RequestServer.serve(RequestServer.java:448)""                                                   
 [6] ""    water.api.RequestServer.doGeneric(RequestServer.java:297)""                                               
 [7] ""    water.api.RequestServer.doPost(RequestServer.java:223)""                                                  
 [8] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                            
 [9] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                            
[10] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                  
[11] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                              
[12] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                      
[13] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                               
[14] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                       
[15] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                           
[16] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                   
[17] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                         
[18] ""    water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:183)""                                               
[19] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                   
[20] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                         
[21] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                 
[22] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""          
[23] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""           
[24] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)""                
[25] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)""
[26] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)""                                        
[27] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)""                                   
[28] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                  
[29] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""            
[30] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                        
[31] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                         
[32] ""    java.lang.Thread.run(Thread.java:748)""                                                                   

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Caught exception: java.lang.NullPointerException","['r', 'h2o']",deepAgrawal,https://stackoverflow.com/users/2796123/deepagrawal,723
48463961,48463961,2018-01-26T15:00:43,2018-01-26 19:32:53Z,0,"I have a matrix containing non-ascii character in a column name:


df <- replicate(3, rnorm(5)) 
colnames(df) <- c('A', 'B', 'Č')
df

              A          B          Č
[1,]  1.6882234 0.37369538  0.1412783
[2,] -1.4538027 0.37603834 -0.2108820
[3,]  0.2878318 0.52661834 -0.4106152
[4,]  1.0373949 1.41206911  0.5056488
[5,] -2.3852925 0.05160573 -1.1288920



When I run the following, the result has one additional row and column name changes:


library(h2o)
h2o.init()
df_h2o <- as.h2o(df)
df_h2o

           A          B    ""ÄŹĹĽËť
1        NaN        NaN        NaN
2  1.6882234 0.37369538  0.1412783
3 -1.4538027 0.37603834 -0.2108820
4  0.2878318 0.52661834 -0.4106152
5  1.0373949 1.41206911  0.5056488
6 -2.3852925 0.05160573 -1.1288920

[6 rows x 3 columns] 



Is there an elegant way to fix this besides fixing input column names?","['r', 'h2o']",Unknown,,N/A
48441368,48441368,2018-01-25T11:05:02,2018-10-27 16:45:04Z,73,"My question might be dumb or anything else. But I was wondering : 




I want to do structured streaming


I want to both aggregate and score the data with a Sparkling Water model




So I have this 


val data_processed = data_raw
      .withWatermark(""timestamp"", ""10 minutes"")
      .groupBy(window(col(""timestamp""),""1 minute""))
      .agg(
       *** all aggregations ***
      )



What I want to add is like :


.withColumn(""row_scored"",scoring(all_others_cols))



So for every row in structured streaming it will score after the aggregation. But I don't think that can be possible. So I'm wondering if you think of another approach.


I'm using Sparkling Water so the scoring functions needs a H2O Frame. I was thinking to create an udf like that :




select all other columns,


create a row and transform it to dataframe


convert the dataframe composed of one row to H2O Frame


predict the H2O Frame of one row


transform the prediction from H20 Frame to dataframe


get the score in the dataframe to double and return it with udf




But I don't think that's quite optimised, maybe you have a fresh approach or remarks that will make see another way to do this.


Thanks in advance","['scala', 'apache-spark', 'h2o', 'spark-structured-streaming']",zero323,https://stackoverflow.com/users/1560062/zero323,329k
48413425,48413425,2018-01-24T01:17:21,2018-01-25 02:03:02Z,140,"I tried to import my data in csv format but it took forever to import and I cannot do anything except waiting. 
The number of rows and columns of data is 1,705 and 502, respectively. All variables except target and date are numeric type. The data size is only 12MB. 


I do not know how many hours I will have to wait to import the data.
Please advise what I can do to try this product on my data.","['h2o', 'driverless-ai']",Hyo Yong Jeong,https://stackoverflow.com/users/9259652/hyo-yong-jeong,1
48410708,48410708,2018-01-23T20:59:27,2018-02-15 18:57:09Z,58,"Running sparkling-shell (tried versions 2.2.2 - 2.2.6) on with Spark2 (under CDH 5.13 under Linux 7.2).  CSV and ZIP files import fine, but when I tried to import a Parquet file, it reads it as CSV and garbles the data.


Anyone has any suggestions?


Shankar","['parquet', 'h2o', 'sparkling-water']",Rob,https://stackoverflow.com/users/162698/rob,15.1k
48397934,48397934,2018-01-23T09:12:58,2023-10-10 05:00:09Z,433,"I have data loaded in elasticsearch. 
How can I get elasticsearch data in h2o?","['elasticsearch', 'h2o']",José Antonio Blanco,https://stackoverflow.com/users/9255774/jos%c3%a9-antonio-blanco,13
48392125,48392125,2018-01-22T23:57:26,2018-01-23 00:07:11Z,460,"When trying AutoML, with 
nfolds = 3
 and a specified 
fold_column
 I get an error: 


Cannot specify fold_column and a non-default nfolds value at the same time


Anyone have a workaround?  Or am I stuck with the default?",['h2o'],David Faivre,https://stackoverflow.com/users/79113/david-faivre,"2,342"
48387458,48387458,2018-01-22T17:50:11,2018-01-22 19:01:00Z,0,"Is there a way to assign a new id to an existing h2o model obtained by AutoML/grid?


AUTO <- h2o.automl(training_frame = train,
                            x = input, y = ""Sales"",
                            fold_column = ""Week"",
                            seed = 1, max_models = 8)
model <- AUTO@leader



Neither of these options work:


model@model_id <- ""new_model_id""
model@parameters$model_id <- ""new_model_id""
model@allparameters$model_id <- ""new_model_id""
attr(model, ""model_id"") <- ""new_model_id""



When I try to save the model, h2o throws an error:


model_path <- h2o.saveModel(model, path = getwd(), force = T)

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 

ERROR MESSAGE:

Object 'new_model_id' not found for argument: model_id



Is what I'm trying to do even possible? Thank you!","['r', 'h2o']",Pedro Schuller,https://stackoverflow.com/users/9095133/pedro-schuller,290
48375715,48375715,2018-01-22T06:25:23,2018-01-22 06:50:22Z,566,"There may be an obvious solution to this, as we're new to the H2O platform, though we've been unable to find any conclusive information.


We're saving our (H2O-XGBoost) models via Python:


h2o.save_model(model=model, path=/path/to/our/models, force=True)



Then subsequently loading our models (loaded after starting the server cleanly if that matters):


model = h2o.load_model(""/path/to/our/models/modelname"")



From here the model looks good and is loaded within the Flow UI.


We load our prediction frame:


pf = h2o.H2OFrame(python_obj=data_list, destination_frame='PREDICTION_FRAME', header=-1)



We attempt to make a prediction against the prediction frame we just loaded:


model.predict(pf)



Error Message (same result within Flow UI & Python):


ERROR MESSAGE:

DistributedException from /127.0.0.1:54321: 'null'

 (water.util.DistributedException)
  DistributedException from /127.0.0.1:54321: 'null', caused by java.lang.NullPointerException
      water.MRTask.getResult(MRTask.java:478)
      water.MRTask.getResult(MRTask.java:486)
      water.MRTask.doAll(MRTask.java:390)
      water.MRTask.doAll(MRTask.java:396)
      hex.Model.predictScoreImpl(Model.java:1280)
      hex.Model.score(Model.java:1145)
      water.api.ModelMetricsHandler.predict(ModelMetricsHandler.java:420)
      sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
      sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
      sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
      java.lang.reflect.Method.invoke(Method.java:498)
      water.api.Handler.handle(Handler.java:63)
      water.api.RequestServer.serve(RequestServer.java:451)
      water.api.RequestServer.doGeneric(RequestServer.java:296)
      water.api.RequestServer.doPost(RequestServer.java:222)
      javax.servlet.http.HttpServlet.service(HttpServlet.java:755)
      javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
      org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
      org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)
      org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
      org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)
      org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
      org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
      org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)
      org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
  Caused by:java.lang.NullPointerException
      hex.tree.xgboost.XGBoostModel.score0(XGBoostModel.java:462)
      hex.Model.score0(Model.java:1432)
      hex.Model$BigScore.map(Model.java:1377)
      water.MRTask.compute2(MRTask.java:657)
      water.H2O$H2OCountedCompleter.compute1(H2O.java:1266)
      hex.Model$BigScore$Icer.compute1(Model$BigScore$Icer.java)
      water.H2O$H2OCountedCompleter.compute(H2O.java:1262)
      jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
      jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
      jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
      jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
      jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



If we do not load the saved model and just build it on the fly, predictions work fine on the same prediction frame. So, it's something to do with using a saved model it seems.


My initial thought was the frames used for training and validation for the model in question needed to be present, so I exported those and imported them along with the saved model, but received the same error.


Some relevant information:


python --version
Python 2.7.12

H2O Build git branch    master
H2O Build git hash  7cb70c6a5909257868f72f87da27c07670837f09
H2O Build git describe  jenkins-master-4171-1-g7cb70c6-dirty
H2O Build project version   3.17.0.99999
H2O Build age   8 days
H2O Built by    root
H2O Built on    2018-01-13 13:26:13
H2O Internal Security   Disabled
Flow version    0.7.12

getCloud
56ms    
H2O_from_python_root_e9226d
CLOUD STATUS
HEALTHY CONSENSUS LOCKED
Version Started Nodes (Used / All)
3.17.0.99999    a few seconds ago   1 / 1
NODES
Show advanced
    Name    Ping    Cores   Load    My CPU %    Sys CPU %   GFLOPS  Memory Bandwidth    Data (Used/Total)   Data (% Cached) GC (Free / Total / Max) Disk (Free / Max)   Disk (% Free)
127.0.0.1:54321 a few seconds ago   24  0.080   5   5   NaN - / s   - / NaN undefined   NaN%    17.76 GB / NaN undefined / 17.78 GB 53.26 GB / 109.88 GB    48%
TOTAL   -   24  0.080   -   -   NaN - / s   - / NaN undefined   NaN%    17.76 GB / NaN undefined / 17.78 GB 53.26 GB / 109.88 GB    48%



Any suggestions or guidance would be greatly appreciated.","['python', 'h2o']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
48330026,48330026,2018-01-18T20:47:21,2019-01-09 03:09:38Z,0,"In my problem dataset response variable is extremely skewed to the left. I have tried to fit the model with 
h2o.randomForest()
 and 
h2o.gbm()
 as below. I can give tune 
min_split_improvement
 and 
min_rows
 to avoid overfitting in these two cases. But with these models, I see very high errors on the tail observations. I have tried using  
weights_column
 to oversample the tail observations and undersample other observations, but it does not help.


h2o.model <- h2o.gbm(x = predictors, y = response, training_frame = train,valid = valid, seed = 1,
                              ntrees =150, max_depth = 10, min_rows = 2, model_id = ""GBM_DD"", balance_classes = T, nbins = 20, stopping_metric = ""MSE"", 
                     stopping_rounds = 10, min_split_improvement = 0.0005)


h2o.model <- h2o.randomForest(x = predictors, y = response, training_frame = train,valid = valid, seed = 1,ntrees =150, max_depth = 10, min_rows = 2, model_id = ""DRF_DD"", balance_classes = T, nbins = 20, stopping_metric = ""MSE"", 
                     stopping_rounds = 10, min_split_improvement = 0.0005)



I have tried the 
h2o.automl()
 function of h2o package for the problem for better performance. However, I see significant overfitting. I don't know of any parameters in 
h2o.automl()
 to control overfitting. 


Does anyone know of a way to avoid overfitting with 
h2o.automl()
?


EDIT


The distribution of the 
log
 transformed response is given below. After the suggestion from 
Erin




EDIT2:
 
Distribution of original response.","['r', 'h2o', 'automl']",Unknown,,N/A
48317921,48317921,2018-01-18T09:33:06,2022-05-24 08:40:23Z,0,"I'm a newbie in python.


I have an h2o frame table having 
1000
 rows and 
25
 columns, I would like to convert this table to numpy array and reshape to (5,5)


I used this code:


mynarray=np.array([np.array(nrows).astype(np.float32).reshape(5,5) for nrows in myh2oframe])



Error I received is cannot copy sequence with size 1604 to array axis with dimension 1","['python', 'h2o']",Dan,https://stackoverflow.com/users/1011724/dan,45.7k
48309192,48309192,2018-01-17T20:07:44,2018-01-17 21:17:54Z,0,"Background:

In R, using a linear model, I could write a formula


est <- lm(Y~1+A+B+C+D:A+E:D+E:F+B:A+B:D+C:A+C:D+C:B, data=mydata)



If ""Y"" happens to be binomial, then I can also write:


est <- glm(Y~1+A+B+C+D:A+E:D+E:F+B:A+B:D+C:A+C:D+C:B, data=mydata, family = binomial)



But... 
When I go to h2o.glm, I have to use the ""x=.., y=.."" form.


my_glm.hex <- h2o.glm(y=y_idx,x=x_idx,
                  training_frame = ""my_train"",
                  validation_frame = ""my_valid"",
                  model_id = ""my_glm.hex"",
                  family = ""binomial"",
                  lambda_search = TRUE,
                  balance_classes = TRUE)



Question:

How do I add a formula that allows me to fit a generalized linear model (glm) with interactions using h2o.glm?


Addendum:

I'm not sure what tags outside of 'r', 'h2o', and 'fitting' should be used here.  If you think of something relevant, could you suggest it in comments?","['r', 'glm', 'h2o', 'interaction']",data princess,https://stackoverflow.com/users/8414180/data-princess,"1,160"
48306967,48306967,2018-01-17T17:41:00,2018-07-30 19:19:04Z,0,"I don't see an option to set the arguments


keep_cross_validation_predictions
 


and 


keep_cross_validation_fold_assignment
 


for 
h2o.automl()
 in h2o R package.


Is there another way to access cross validation dataset used in 
h2o.automl()
 call?


The reason for this need is that the response variable used in the model is log-transformed, and cross validation error calculated may be misleading. If we have access to cross validation dataset, then we'll be able to know which observations were used, and then used the un-transformed response to compare against predicted values from the model used in a particular fold.","['r', 'cross-validation', 'h2o']",croxy,https://stackoverflow.com/users/5453249/croxy,"4,150"
48306081,48306081,2018-01-17T16:45:26,2018-07-16 23:34:03Z,0,"I want to use PCA on H2O. In sklearn, we can apply 
fit
 on train set and then 
transform
 can be applied on test set. Here I am trying to follow the same logic in H2O. In the FAQ, it says:




After the PCA model has been built using h2o.prcomp, use h2o.predict
  on the original data frame and the PCA model to produce the
  dimensionality-reduced representation. Use cbind to add the predictor
  column from the original data frame to the data frame produced by the
  output of h2o.predict. At this point, you can build supervised
  learning models on the new data frame.




Based on this I tried the below:


from h2o.transforms.decomposition import H2OPCA

trbb_pca = H2OPCA(k = 5, transform = ""NORMALIZE"", pca_method=""GramSVD"",
                   use_all_factor_levels=True, impute_missing=True,seed=24)

trbb_pca.train(x=trbb_cols, training_frame=train_h2o)

train_h2o_pca = train_h2o.cbind(trbb_pca.predict(train_h2o))
test_h2o_pca = test_h2o.cbind(trbb_pca.predict(test_h2o))



Is it the way to implement PCA on train and test set in H2O?","['python-3.x', 'pca', 'h2o']",mlee_jordan,https://stackoverflow.com/users/3198674/mlee-jordan,842
48274614,48274614,2018-01-16T05:27:20,2018-01-16 15:38:38Z,50,"In AWS, I followed the instruction in 
here
 and launched a g2.2xlarge EC2 using the community AMI ami-97591381 (h2o version: 3.13.0.356).


This is my code, which you can run as I made the S3 links public:


library(h2o)
library(jsonlite)
library(curl)

localH2O = h2o.init()

df.truth <- h2o.importFile(""https://s3.amazonaws.com/nw.data.test.us.east/df.truth.zeroed"", header = T, sep="","")
df.truth$isFemale <- h2o.asfactor(df.truth$isFemale)
hotnames.truth <- fromJSON(""https://s3.amazonaws.com/nw.data.test.us.east/hotnames.json"", simplifyVector = T)

# Training and validation sets
splits <- h2o.splitFrame(df.truth, c(0.9), seed=1234)
train.truth <- h2o.assign(splits[[1]], ""train.truth.hex"")   
valid.truth <- h2o.assign(splits[[2]], ""valid.truth.hex"")

# Train a model using non-GPU deeplearning
dl.2 <- h2o.deeplearning(         
  training_frame = train.truth, model_id=""dl.2"",
  validation_frame = valid.truth,      
  x=setdiff(hotnames.truth[1:(length(hotnames.truth)/2)], c(""isFemale"", ""nwtcs"")),
  y=""isFemale"", stopping_metric = ""AUTO"", seed = 1,
  sparse = F, mini_batch_size = 20)

# Train a model using GPU-enabled deepwater
dw.2 <- h2o.deepwater(         
  training_frame = train.truth, model_id=""dw.2"", 
  validation_frame = valid.truth,         
  x=setdiff(hotnames.truth[1:(length(hotnames.truth)/2)], c(""isFemale"", ""nwtcs"")),
  y=""isFemale"", stopping_metric = ""AUTO"", seed = 1,
  sparse = F, mini_batch_size = 20) 



When I inspect the two models, to my surprise I saw 
large
 difference in logloss:


Non-GPU


print(dl.2)
Model Details:
==============

H2OBinomialModel: deeplearning
Model ID:  dl.2
Status of Neuron Layers: predicting isFemale, 2-class classification, bernoulli distribution, CrossEntropy loss, 160,802 weights/biases, 2.0 MB, 1,041,465 training samples, mini-batch size 1
  layer units      type dropout       l1       l2 mean_rate rate_rms momentum
1     1   600     Input  0.00 %
2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.104435 0.102760 0.000000
3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.031395 0.055490 0.000000
4     4     2   Softmax         0.000000 0.000000  0.001541 0.001438 0.000000
  mean_weight weight_rms mean_bias bias_rms
1
2    0.018904   0.144034  0.150630 0.415525
3   -0.023333   0.081914  0.545394 0.251275
4    0.029091   0.295439 -0.004396 0.357609

H2OBinomialMetrics: deeplearning
** Reported on training data. **
** Metrics reported on temporary training frame with 9877 samples **

MSE:  0.1213733
RMSE:  0.3483868
LogLoss:  0.388214
Mean Per-Class Error:  0.2563669
AUC:  0.8433182
Gini:  0.6866365

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
          0    1    Error        Rate
0      6546 1079 0.141508  =1079/7625
1       836 1416 0.371226   =836/2252
Totals 7382 2495 0.193885  =1915/9877

H2OBinomialMetrics: deeplearning
** Reported on validation data. **
** Metrics reported on full validation frame **

MSE:  0.126671
RMSE:  0.3559087
LogLoss:  0.4005941
Mean Per-Class Error:  0.2585051
AUC:  0.8309913
Gini:  0.6619825

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
           0    1    Error         Rate
0      11746 3134 0.210618  =3134/14880
1       1323 2995 0.306392   =1323/4318
Totals 13069 6129 0.232160  =4457/19198



GPU-enabled


print(dw.2)
Model Details:
==============

H2OBinomialModel: deepwater
Model ID:  dw.2b
Status of Deep Learning Model: MLP: [200, 200], 630.8 KB, predicting isFemale, 2-class classification, 1,708,160 training samples, mini-batch size 20
  input_neurons     rate momentum
1           600 0.000369 0.900000


H2OBinomialMetrics: deepwater
** Reported on training data. **
** Metrics reported on temporary training frame with 9877 samples **

MSE:  0.1615781
RMSE:  0.4019677
LogLoss:  0.629549
Mean Per-Class Error:  0.3467246
AUC:  0.7289561
Gini:  0.4579122

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
          0    1    Error        Rate
0      4843 2782 0.364852  =2782/7625
1       740 1512 0.328597   =740/2252
Totals 5583 4294 0.356586  =3522/9877

H2OBinomialMetrics: deepwater
** Reported on validation data. **
** Metrics reported on full validation frame **

MSE:  0.1651776
RMSE:  0.4064205
LogLoss:  0.6901861
Mean Per-Class Error:  0.3476629
AUC:  0.7187362
Gini:  0.4374724

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
          0    1    Error         Rate
0      8624 6256 0.420430  =6256/14880
1      1187 3131 0.274896   =1187/4318
Totals 9811 9387 0.387697  =7443/19198



As seen above, the difference in logloss is huge between non-GPU and GPU models:


Logloss
+----------------------------------+
|                 | non-GPU | GPU  |
+----------------------------------+
| training data   | 0.39    | 0.63 |
+----------------------------------|
| validation data | 0.40    | 0.69 |
+----------------------------------+



I understand that due to the stochastic nature of the training I will get different results, but I won't expect such a huge difference between non-GPU and GPU.",['h2o'],Patrick Ng,https://stackoverflow.com/users/5552903/patrick-ng,180
48273808,48273808,2018-01-16T03:47:46,2018-07-29 07:11:54Z,126,"In AWS, I followed the instruction in 
here
 and launched a g2.2xlarge EC2 using the community AMI ami-97591381


On the docker image, I can run a simple deepwater tutorial without a problem.  However, when I tried to train a  deepwater model using my own data (which worked ok with a non-GPU deeplearning model), h2o gave me this exception:


java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: 0 <= 186393 < 170807
    at water.Futures.blockForPending(Futures.java:88)
    at hex.deepwater.DeepWaterDatasetIterator.Next(DeepWaterDatasetIterator.java:99)
    at hex.deepwater.DeepWaterTask.setupLocal(DeepWaterTask.java:168)
    at water.MRTask.setupLocal0(MRTask.java:550)
    at water.MRTask.dfork(MRTask.java:456)
    at water.MRTask.doAll(MRTask.java:389)
    at water.MRTask.doAll(MRTask.java:385)
    at hex.deepwater.DeepWater$DeepWaterDriver.trainModel(DeepWater.java:345)
    at hex.deepwater.DeepWater$DeepWaterDriver.buildModel(DeepWater.java:205)
    at hex.deepwater.DeepWater$DeepWaterDriver.computeImpl(DeepWater.java:118)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:173)
    at hex.deepwater.DeepWater$DeepWaterDriver.compute2(DeepWater.java:111)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1256)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0 <= 186393 < 170807
    at water.fvec.Vec.elem2ChunkIdx(Vec.java:925)
    at water.fvec.Vec.chunkForRow(Vec.java:1063)
    at hex.deepwater.DeepWaterDatasetIterator$FrameDataConverter.compute2(DeepWaterDatasetIterator.java:76)
    ... 6 more



This is my code, which you can run as I made the S3 links public:


library(h2o)
library(jsonlite)
library(curl)

h2o.init()
df.truth <- h2o.importFile(""https://s3.amazonaws.com/nw.data.test.us.east/df.truth.zeroed"", header = T, sep="","")
df.truth$isFemale <- h2o.asfactor(df.truth$isFemale)
hotnames.truth <- fromJSON(""https://s3.amazonaws.com/nw.data.test.us.east/hotnames.json"", simplifyVector = T)

# Training and validation sets
splits <- h2o.splitFrame(df.truth, c(0.9), seed=1234)
train.truth <- h2o.assign(splits[[1]], ""train.truth.hex"")   
valid.truth <- h2o.assign(splits[[2]], ""valid.truth.hex"")

dl.2.balanced <- h2o.deepwater(         
  training_frame = train.truth, model_id=""dl.2.balanced"",      
  x=setdiff(hotnames.truth[1:(length(hotnames.truth)/2)], c(""isFemale"", ""nwtcs"")),
  y=""isFemale"", stopping_metric = ""AUTO"", seed = 1000000,
  sparse = F, 
  balance_classes = T,
  mini_batch_size = 20) 



The h2o version is 3.13.0.356.


Update:


I think I found the h2o bug.  If I set 
balance_classes
 to FALSE, then it will run w/o crashing.",['h2o'],Unknown,,N/A
48235159,48235159,2018-01-12T22:57:37,2018-01-12 23:21:47Z,0,"I am using the h2o.glm module (in R). I tried to find the 'weights_column' specification value in the outputting h2o GLM model object but I can not find it. I looked into model@allparameters and model@parameters, none of these two objects contain the weights column information. Is the weight information saved anywhere in the model object?","['r', 'h2o']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
48213625,48213625,2018-01-11T18:24:44,2018-01-12 20:08:34Z,307,"I'm installing H20 Driverless AI on Google Cloud Platform on Ubuntu 16.04.


I'm following these instructions: 

http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/UsingDriverlessAI.pdf


It goes well - or so I think - until step 15, the last one. 


I type the following


docker run \
> --rm \
> -u `id -u`:`id -g` \
> -p 12345:12345 \
> -p 9090:9090 \
> -v `pwd`/data:/data \
> -v `pwd`/log:/log \
> -v `pwd`/license:/license \
> -v `pwd`/tmp:/tmp \
> opsh2oai/h2oai-runtime



And get:


mkdir: cannot create directory '/log/20180111-180304': Permission denied



20180111-180304 corresponds to the timestamp of the action.


When ls, here is the list of the files and folders present on the virtual machine: 


data  demo  driverless-ai-docker-runtime-rel-1.0.5.gz  install.sh  jupyter  license  log  scripts  tmp



I'd be keen to hear if you've encountered a similar error or understand what I am doing wrong.


I've also tried 
sudo docker run \
; similar outcome",['h2o'],Makoto,https://stackoverflow.com/users/1079354/makoto,106k
48212530,48212530,2018-01-11T17:12:30,2018-01-17 15:22:17Z,0,"I have a set of CSVs with a result column to train, and a set of test CSVs without the result column. 


library(h2o)
h2o.init()

train <- read.csv(train_file, header=T)
train.h2o <- as.h2o(train)
y <- ""Result""
x <- setdiff(names(train.h2o), y)

model <- h2o.deeplearning(x = x,
                          y = y,
                          training_frame = train.h2o,
                          model_id = ""my_model"",
                          epochs = 5000,
                          hidden = c(50),
                          stopping_rounds=5,
                          stopping_metric=""misclassification"", 
                          stopping_tolerance=0.001,
                          seed = 1)



test <- read.csv(test_file, header=T)
test.h2o <- as.h2o(test)

pred <- h2o.predict(model,test.h2o)



When I try to predict the outcome with test data, I get a bunch of errors like:


1: In doTryCatch(return(expr), name, parentenv, handler) :
Test/Validation dataset column 'ColumnName' has levels not trained on: [ABCD, BCDE]



H2O used to be able to handle data present in test but not during training. I found some posts online where they say they do. But it is not working for me. 


How can I avoid these errors, and predict a value for the test data?","['r', 'machine-learning', 'classification', 'h2o']",user7792598,https://stackoverflow.com/users/7792598/user7792598,197
48208691,48208691,2018-01-11T13:53:25,2018-09-21 07:29:44Z,0,"Checking whether there is an 
H2O
 instance running at 
http://localhost:54321
..... not found.


Attempting to start a local H2O server...




Java HotSpot(TM) 64-Bit Server VM (build 9.0.1+11, mixed mode)

  Starting server from
  C:\Users\Ramakanth\Anaconda2\lib\site-packages\h2o\backend\bin\h2o.jar
  Ice root: c:\users\ramaka~1\appdata\local\temp\tmpeaff8n   JVM stdout:
  c:\users\ramaka~1\appdata\local\temp\tmpeaff8n\h2o_Ramakanth_started_from_python.out
  JVM stderr:
  c:\users\ramaka~1\appdata\local\temp\tmpeaff8n\h2o_Ramakanth_started_from_python.err
  Traceback (most recent call last):   File """", line 1, in
     File
  ""C:\Users\Ramakanth\Anaconda2\lib\site-packages\h2o\h2o.py"", line 262,
  in init
      min_mem_size=mmin, ice_root=ice_root, port=port, extra_classpath=extra_classpath)   File
  ""C:\Users\Ramakanth\Anaconda2\lib\site-packages\h2o\backend\server.py"",
  line 121, in start
      mmax=max_mem_size, mmin=min_mem_size)   File ""C:\Users\Ramakanth\Anaconda2\lib\site-packages\h2o\backend\server.py"",
  line 317, in _launch_server
      raise H2OServerError(""Server process terminated with error code %d"" % proc.returncode) h2o.exceptions.H2OServerError: Server process
  terminated with error code 1",['h2o'],Maykel Llanes Garcia,https://stackoverflow.com/users/1266824/maykel-llanes-garcia,506
48140279,48140279,2018-01-07T18:31:12,2021-11-06 22:54:48Z,0,"I was trying to train using 
h2o.automl()
. But the training exited due to timeout. I know 
max_runtime_secs
 can be set to higher numbers. But it would be great if we can train for 1 hour, then save it somewhere. Again train it next day from where it was left in day 1.


How to do that ?


I have tried by setting 
project_name
 - but nothing is saved on exit. So if we turn off pc and restart, it is of no use.


I have used the following code for this :


library( h2o )

h2o.init( nthreads = -1, max_mem_size = '10240m' )

train = h2o.importFile( 'train.csv' )

automl_model = h2o.automl( y = 'outcome', training_frame = train, nfolds = 3, max_runtime_secs = 1800,

                           project_name = 'automl_aus_tennis' )



Link to train.csv : 
http://www.mediafire.com/file/qj7yiju15ncgnax/train.csv","['r', 'machine-learning', 'h2o', 'automl']",Nicolás Ozimica,https://stackoverflow.com/users/677022/nicol%c3%a1s-ozimica,"9,708"
48134000,48134000,2018-01-07T03:09:15,2018-06-20 10:48:41Z,0,"After running a hyperparameter search and extracting the best model from the grid, is it possible to use the model object to train on a new data set? The only way I see now is to manually create a call to a train function (e.g. h2o.gbm()) with the parameters from the best model, but this is very cumbersome.","['r', 'h2o']",Edward,https://stackoverflow.com/users/9183024/edward,23
48130735,48130735,2018-01-06T18:53:20,2018-12-07 12:36:20Z,364,"I'm trying to find if one can connect to teradata using H2O. Upon reading some of the basic documentation on H2O, i found that H2O has the ability to connect to relational databases provided they supply a JDBC driver.


Link: 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/getting-data-into-h2o.html?highlight=jdbc


However, the documentation suggests: ""Currently supported SQL databases are MySQL, PostgreSQL, and MariaDB""


So I'm wondering if H2O can connect to other databases like Teradata because they do have a jdbc driver


Link: 
https://downloads.teradata.com/download/connectivity/jdbc-driver
 


-Suhail","['teradata', 'h2o']",Suhail,https://stackoverflow.com/users/9182115/suhail,11
48103924,48103924,2018-01-04T21:35:51,2018-01-04 23:33:57Z,0,"I am looking for a method similar to the 'apply' function in pandas. I tried 


my_H2Oframe.apply(lambda x: my_function(x), axis=1)



But this doesn't work.




ValueError: Unimplemented: op < my_function > not bound in H2OFrame




I found 
this question
. It seems we can only use those functions that have already been defined by H2O. I think there must be a method similar to the apply function because this is a common operation. Does anyone have a solution?",['h2o'],Ben,https://stackoverflow.com/users/1933981/ben,675
48094985,48094985,2018-01-04T12:08:11,2018-01-04 13:22:15Z,94,"I've got several (Binominal)-DRF-Models and I'd like to get the ModelMatricsBinominalV3 object to extract the 
thresholds_and_metric_scores
 variable. I've implemented a solution without retrofit and bindings, but I want to use h2o-bindings to be able to send and receive pojos since my current solution is not very elegant and very fault-prone. Has anybody done something like this and can share his code? Especially I'm interested in extracting Accuracy, F1-Score, Recall etc. for a given threshold.


My current approach calling:


h2oApi.predict(ModelMetricsListSchemaV3)



Works - but is not containing 
thresholds_and_metric_scores
 


Whereas calling POST /3/Predictions/models/{model}/frames/{frame} within PostMan works fine and is returning 
thresholds_and_metric_scores
 within the json String. How can that be since h2oApi calls POST /3/Predictions/models/{model}/frames/{frame} internally?!


This is my old implementation:


public String getModelMetrics(String modelId, String frameId, double threshold){
  String url = buildHttpPath(""/3/Predictions/models/"" + modelId + ""/frames/"" + frameId);
  int metricIndex = Integer.MIN_VALUE;

  HttpClient client = HttpClientBuilder.create().build();
  HttpPost post = new HttpPost(url);
  HttpResponse response;
  String json = """";

  try
  {
     response = client.execute(post);
     json = EntityUtils.toString(response.getEntity());
  }
  catch (IOException exception)
  {
     LOG.error(exception.toString());
  }

  JsonObject var1 = new Gson().fromJson(json, JsonObject.class);
  JsonArray var2 = var1.getAsJsonArray(""model_metrics"");
  JsonElement var3 = var2.get(0);
  JsonElement var4 = ((JsonObject) var3).get(""thresholds_and_metric_scores"");
  JsonElement var5 = ((JsonObject) var4).get(""data"");
  JsonArray var6 = (JsonArray) ((JsonArray) var5).get(0);
  Double min = Double.MAX_VALUE;

  for (int i = 0; i < var6.size(); i++)
  {
     Double currentElement = var6.get(i).getAsDouble();
     Double diff = Math.abs(currentElement - threshold);

     if (diff < min)
     {
        min = diff;
        metricIndex = i;
     }
  }

  LOG.info(""Received threshold is: "" + threshold);
  LOG.info(""Nearest Threshold is: "" + var6.get(metricIndex).getAsDouble());

  JsonArray accuracyColumn = (JsonArray) ((JsonArray) var5).get(4);
  JsonArray f1Column = (JsonArray) ((JsonArray) var5).get(1);
  JsonArray recallColumn = (JsonArray) ((JsonArray) var5).get(6);
  JsonArray precisionColumn = (JsonArray) ((JsonArray) var5).get(5);
  JsonArray tpColumn = (JsonArray) ((JsonArray) var5).get(14);
  JsonArray tnColumn = (JsonArray) ((JsonArray) var5).get(11);
  JsonArray fpColumn = (JsonArray) ((JsonArray) var5).get(13);
  JsonArray fnColumn = (JsonArray) ((JsonArray) var5).get(12);

  Double accuracy = accuracyColumn.get(metricIndex).getAsDouble();
  Double f1 = f1Column.get(metricIndex).getAsDouble();
  Double recall = recallColumn.get(metricIndex).getAsDouble();
  Double precision = precisionColumn.get(metricIndex).getAsDouble();
  int tp = tpColumn.get(metricIndex).getAsInt();
  int tn = tnColumn.get(metricIndex).getAsInt();
  int fp = fpColumn.get(metricIndex).getAsInt();
  int fn = fnColumn.get(metricIndex).getAsInt();

  return accuracy.toString() + "";"" + f1.toString() + "";"" + recall.toString() + "";"" + precision.toString() + "";"" + tp + "";"" + tn + "";"" + fp + "";"" + fn;}



Thank you in advance!",['h2o'],Unknown,,N/A
48090102,48090102,2018-01-04T06:52:21,2018-03-02 01:34:04Z,0,"While trying to train a lenet model for multiclass classification using h2o deepwater using mxnet backed I am getting the following errors:


Loading H2O mxnet bindings.
Found CUDA_HOME or CUDA_PATH environment variable, trying to connect to GPU devices.
Loading CUDA library.
Loading mxnet library.
Loading H2O mxnet bindings.
Done loading H2O mxnet bindings.
Constructing model.
Done constructing model.
Building network.
mxnet data input shape: (32,100)
[10:40:16] /home/jenkins/slave_dir_from_mr-0xb1/workspace/deepwater-master/thirdparty/mxnet/dmlc-core/include/dmlc/logging.h:235: [10:40:16] src/operator/./convolution-inl.h:349: Check failed: (dshape.ndim()) == (4) Input data should be 4D in batch-num_filter-y-x
[10:40:16] src/symbol.cxx:189: Check failed: (MXSymbolInferShape(GetHandle(), keys.size(), keys.data(), arg_ind_ptr.data(), arg_shape_data.data(), &in_shape_size, &in_shape_ndim, &in_shape_data, &out_shape_size, &out_shape_ndim, &out_shape_data, &aux_shape_size, &aux_shape_ndim, &aux_shape_data, &complete)) == (0)
 


The details of my setup :

* Ubuntu : 16.04

* Ram : 12gb

* Graphics card : Nvidia 920mx driver version : 384.90

* Cuda : 8.0.61

* cudnn : 6.0

* R version : 3.4.3

* H2o version : 3.15.0.393 & h2o-R package : 3.16.0.2

* mxnet : 0.11.0

* Train data size : 400mb (when converting to the h2o frame object it comes around 822mb)  


Things I have done :

1.) Gave enough memory to java heap while running h2o cluster (java -Xmx9g -jar h2o.jar)

2.) Build the mxnet from source for gpu

3.) Monitored the gpu and system via nvidia-smi and system monitor. At no point do they eat up all the ram to show ""out of memory"" issue. I still will be having around 2-3gb free before the error shows up

4.) Have tried with tensorflow-gpu(build from source). Checking the pip list made sure that its installed but during model creation in R it gives the error :


Error: java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: null

 5.) The only method I got it the h2o deepwater to work with all the backend and w/wo GPU is through docker setup provided in the installation tutorials. 


I wanted the same functionality on my laptop instead of using Docker. Also is there any way to run deepwater using just CPU? The link 
Is it possible to build Deep Water/TensorFlow model in H2O without CUDA
 doesn't provide any helpful answers. Any help or advice will be greatly appreciated!","['r', 'nlp', 'h2o', 'tensorflow', 'mxnet']",Unknown,,N/A
48086448,48086448,2018-01-03T23:09:57,2019-04-17 15:46:17Z,644,"On Linux, H2O 3.16.0.2 - Exception occured while running GBM with hyperparameter search.


The below information is about H2O and its giving exception.




H2O cluster uptime:         3 hours 17 mins


H2O cluster version:        3.16.0.2 


H2O cluster version age:    1 month and 4 days


H2O cluster name:           beast


H2O cluster total nodes:    1


H2O cluster free memory:    25.46 Gb


H2O cluster total cores:    32


H2O cluster allowed cores:  32


H2O cluster status:         locked, healthy


H2O connection url:         
http://localhost:54321


H2O connection proxy:


H2O internal security:      False


H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4


Python version:             2.7.12 final


gbm Grid Build progress: [################################################] 100%
Errors/Warnings building gridsearch model


Hyper-parameter: col_sample_rate, 0.44
Hyper-parameter: col_sample_rate_change_per_level, 1.03
Hyper-parameter: col_sample_rate_per_tree, 0.38
Hyper-parameter: histogram_type, QuantilesGlobal
Hyper-parameter: max_depth, 4
Hyper-parameter: min_rows, 128.0
Hyper-parameter: min_split_improvement, 1e-06
Hyper-parameter: nbins, 256
Hyper-parameter: nbins_cats, 512
Hyper-parameter: sample_rate, 0.55
failure_details: None
failure_stack_traces: java.lang.NullPointerException
    at hex.Model$Parameters.read_unlock_frames(Model.java:322)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:209)
    at hex.ModelBuilder.trainModelNested(ModelBuilder.java:262)
    at hex.grid.GridSearch.startBuildModel(GridSearch.java:332)
    at hex.grid.GridSearch.buildModel(GridSearch.java:314)
    at hex.grid.GridSearch.gridSearch(GridSearch.java:213)
    at hex.grid.GridSearch.access$000(GridSearch.java:68)
    at hex.grid.GridSearch$1.compute2(GridSearch.java:135)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1263)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)


Traceback (most recent call last):
  File ""malware_detection.py"", line 167, in 
  validation_frame = valid_split)
  File ""/home/beast/local/lib/python2.7/site-packages  /h2o/grid/grid_search.py"", line 189, in train
self.build_model(parms)
File ""/home/beast/local/lib/python2.7/site-packages   /h2o/grid/grid_search.py"", line 204, in build_model
self._model_build(x, y, training_frame, validation_frame, algo_params)
File ""/home/beast/local/lib/python2.7/site-packages  /h2o/grid/grid_search.py"", line 250, in _model_build
failure_messages_stacks += error_message+'\n'
TypeError: unsupported operand type(s) for +: 'NoneType' and 'unicode'
H2O session _sid_af98 closed.",['h2o'],XentneX,https://stackoverflow.com/users/1928229/xentnex,117
48076913,48076913,2018-01-03T11:55:48,2020-02-27 10:04:57Z,581,"I am having trouble using an h2o model (in mojo format) on a Spark cluster, but only when I try to run it in parallel, not when I use 
collect
 and run it on the driver.


Since the dataframe I am predicting on has > 100 features, I am using the following function to convert dataframe rows to RowData format for h2o (from 
here
):


def rowToRowData(df: DataFrame, row: Row): RowData = {
  val rowAsMap = row.getValuesMap[Any](df.schema.fieldNames)
  val rowData = rowAsMap.foldLeft(new RowData()) { case (rd, (k,v)) =>
    if (v != null) { rd.put(k, v.toString) }
    rd
  }
  rowData
}



Then, I import the mojo model and create an easyPredictModel wrapper


val mojo = MojoModel.load(""/path/to/mojo.zip"")
val easyModel = new EasyPredictModelWrapper(mojo)



Now, I can make predictions on my dataframe (
df
) by mapping over the rows if I collect it first, so the following works:


val predictions = df.collect().map { r =>
  val rData = rowToRowData(df, r) . // convert row to RowData using function
  val prediction = easyModel.predictBinomial(rData).label
  (r.getAs[String](""id""), prediction.toInt)
  }
  .toSeq
  .toDF(""id"", ""prediction"")



However, I wish to do this in parallel on the cluster since the final df will be too large to collect on the driver. But if I try to run the same code without collecting first:


val predictions = df.map { r =>
  val rData = rowToRowData(df, r)
  val prediction = easyModel.predictBinomial(rData).label
  (r.getAs[String](""id""), prediction.toInt)
}
  .toDF(""id"", ""prediction"")



I get the following errors:


18/01/03 11:34:59 WARN TaskSetManager: Lost task 0.0 in stage 118.0 (TID 9914, 213.248.241.182, executor 0): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD
    at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2133)
    at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1305)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2024)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
    at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
    at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:80)
    at org.apache.spark.scheduler.Task.run(Task.scala:108)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)



So it looks like a datatype mismatch. I have tried converting the dataframe to an rdd first (i.e. 
df.rdd.map
, but get the same errors), doing 
df.mapPartition
, or placing the 
rowToData
 function code within the map, but nothing has worked so far. 


Any ideas on the best way to achieve this?","['scala', 'apache-spark', 'h2o']",renegademonkey,https://stackoverflow.com/users/7469564/renegademonkey,467
48075535,48075535,2018-01-03T10:27:51,2018-01-03 12:43:33Z,0,"Closed
. This question needs to be more 
focused
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Update the question so it focuses on one problem only by 
editing this post
.






Closed 
6 years ago
.















                        Improve this question
                    








I have been trying to build a model for the dataset which contains 70 million records in 
R
. I tried every possible option to build a model like 
clustering
, using 
ff
 library, 
h2o
 (which is throwing me error) and 
bigmemory
 and 
biganalytics
 package as well. I couldn't handle such huge data using 
R
 with the options I tried.


Could you please suggest me any working option other than this, so that I can use it to build model. My laptop is 4GB RAM and 64bits processor.","['r', 'h2o', 'r-bigmemory']",Blundering Ecologist,https://stackoverflow.com/users/6674698/blundering-ecologist,"1,265"
48064171,48064171,2018-01-02T16:08:03,2018-01-04 21:20:08Z,0,"dI'm new to R and ML but have a focused question that I am trying to answer.


I'm using my own data but following Matt Dancho's example here to predict attrition: 
http://www.business-science.io/business/2017/09/18/hr_employee_attrition.html


I have removed zero variance and scaled variables as per his update.


My issue is running the explain() on explainer step. I get variations of both errors below (in bold) when I run the former original code and the latter variation. Everything else runs up to that point.


explanation <- lime::explain(
as.data.frame(test_h2o[1:10,-1]), 
explainer    = explainer, 
n_labels     = 1, 
n_features   = 4,
kernel_width = 0.5)



gives:


Error during wrapup: arguments imply differing number of rows: 50000, 0



While


explanation <- lime::explain(
as.data.frame(test_h2o[1:500,-1]), 
explainer    = explainer, 
n_labels     = 1, 
n_features   = 5,
kernel_width = 1)



Gives:


ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://localhost:54321/3/PostFile?destination_frame=C%3A%2FUsers%2Fsim.s%2FAppData%2FLocal%2FTemp%2FRtmpykNkl1%2Ffileb203a8d4a58.csv_sid_afd3_26)
Error: lexical error: invalid char in json text.
<html> <head> <meta http-equiv=
                 (right here) ------^



Please let me know if you have any ideas or insights for this problem, or need additional info from me.","['r', 'h2o', 'lime']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
48059291,48059291,2018-01-02T10:27:59,2018-01-02 20:51:42Z,912,"Mathematically, how does the 
offset_column
 parameter work during training and during prediction for the random forest algorithm in H2O?


From the docs:




Note: Offsets are per-row “bias values” that are used during model training. For Gaussian distributions, they can be seen as simple corrections to the response (y) column. Instead of learning to predict the response (y-row), the model learns to predict the (row) offset of the response column. For other distributions, the offset corrections are applied in the linearized space before applying the inverse link function to get the actual response values. For more information, refer to the following link.




Considering that random forests do not have the same concept of 'linearized space', is this any different to applying the offset to the response independently?","['statistics', 'regression', 'random-forest', 'h2o']",SlyFox,https://stackoverflow.com/users/4610057/slyfox,180
48057200,48057200,2018-01-02T07:33:39,2018-01-04 09:21:52Z,0,"When I use the 
predict
 method on my trained model I get an output that is 1 row and 206 columns. it seems to have 206 values ranging in values from 0-1. This sort of makes sense as the model's output is categorical variable with values 0 and 1 as possible values. But I don't get the 206 values, as I understand it the output should be a value of 0 or 1. What do the 206 values mean? 


I've spent the past hour or so browsing h2o documentation but can't seem to find an explanation of how to explain the 206 values outputted by predict when I was expecting one value that is either a 0 or 1.


thanks.","['python', 'random-forest', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
48055082,48055082,2018-01-02T02:54:26,2020-09-26 02:48:03Z,0,"I was trying to install h2o (3.10.0.9) on Anaconda 3 in windows 10, 64 bit. The installation seemed to proceeded, and after it shows 
     ""executing transaction:done""
what i got in the prompt window were a few command lines:


(base) C:\WINDOWS\system32>set ""JAVA_HOME=""  
(base) C:\WINDOWS\system32>set ""JAVA_HOME_CONDA_BACKUP=""  
(base) C:\WINDOWS\system32>set ""JAVA_HOME=C:\Users\Xu\Anaconda3\Library""



And at this point, I still cannot import h2o in jupyter notebook (no module named h2o).  Note that the installation of h2o and installation of openjdk was done at the same time. I downloaded JAVA outside anaconda, and did some manual changes in the environmental variables, but it seems that this is not the problem.


Anaconda Navigator shows that h2o is installed, while h2o-py is not. Now, the problem with h2o-py is that if I install it, it will show this error: 


UnsatisfiableError: The following specifications were found to be in conflict:    
- h2o-py    
- zict
Use ""conda info <package>"" to see the dependencies for each package.



Now, I checked all the dependencies:


colorama
future
patsy
python 3.5*
requests
tabulate



I installed future, and then tried installing tabulate, but it also shows the same UnsatisfiableError ... in conflict with zict. Then I removed zict package, and then both the installations of tabulate and h2o-py show another similar error, but this time in conflict with xlrd package. It took me too long time that I really don't want to waste any more on this.


Could anybody tell to me what I can do in order to be able to simply use h2o in anaconda? I would very much appreciate it!","['python', 'installation', 'anaconda', 'h2o']",Unknown,,N/A
48054670,48054670,2018-01-02T01:40:05,2018-01-02 01:40:05Z,808,"I have a large csv I am trying to read into h2o and train a model based on. When h2o is parsing the large csv though my program keeps crashing and giving me this strange http 500 error that I am not sure how to interpret. 


The line of code that causes the crash is:


data = h2o.import_file(""data/Data_Labeled.csv"", col_types=col_types)



But when the code runs and gets to this line it looks like it almost completes parsing the file and then crashes with the following trace:


Traceback (most recent call last):
File ""vairal.py"", line 12, in <module>
bb.train_model()
File ""/Users/mark/Desktop/vairal/vairal/reddit_model.py"", line 15, in train_model
data = h2o.import_file(""data/Data_Labeled.csv"", col_types=col_types)
...
File ""/Users/mark/Downloads/myenv2/lib/python2.7/site-packages/h2o/backend/connection.py"", line 730, in _process_response
raise H2OServerError(""HTTP %d %s:\n%r"" % (status_code, response.reason, data))
h2o.exceptions.H2OServerError: HTTP 500 Server Error:
u'<html>\n<head>\n<meta http-equiv=""Content-Type"" content=""text/html;charset=ISO-8859-1""/>\n<title>Error 500 Server Error</title>\n</head>\n<body>\n<h2>HTTP ERROR: 500</h2>\n<p>Problem accessing /3/Frames/Reddit_Data_Labeled.hex. Reason:\n<pre>    Server Error</pre></p>\n<hr /><i><small>Powered by Jetty://</small></i>\n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n                                                \n</body>\n</html>\n'



I have 0 idea what this means and would love some context. I can't tell if the formating of the csv is bad somewhere or if maybe it's just crashing because my computer can't handle the parse- either way the crash message seems unhelpful to me. 


Appreciate any help!","['python', 'import', 'h2o', 'http-status-code-500']",sometimesiwritecode,https://stackoverflow.com/users/6539531/sometimesiwritecode,"3,183"
48044253,48044253,2017-12-31T16:50:23,2018-05-03 23:43:19Z,354,"When I try to install:


sudo pip3 install h2o4gpu-0.1.0-py36-none-any.whl



I get the (not so helpful) error msg:


h2o4gpu-0.1.0-py36-none-any.whl is not a supported wheel on this platform.



I suspect that this is because I'm running python 3.5 instead of 3.6.  However, when I try to upgrade to python 3.6, I get a stern/scary warning, which I don't know what to do about...


sudo add-apt-repository ppa:jonathonf/python-3.6
A plain backport of *just* Python 3.6. System extensions/Python libraries may or may not work.

Don't remove Python 3.5 from your system - it will break.
More info: https://launchpad.net/~jonathonf/+archive/ubuntu/python-3.6
Press [ENTER] to continue or ctrl-c to cancel adding it



Googling around for answer, I did come across this, which might indicate that Python 3.5 isn't going to support a 3.6 package:


python3
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pip
>>> print(pip.pep425tags.get_supported())
[('cp35', 'cp35m', 'manylinux1_x86_64'), ('cp35', 'cp35m', 'linux_x86_64'), ('cp35', 'abi3', 'manylinux1_x86_64'), ('cp35', 'abi3', 'linux_x86_64'), ('cp35', 'none', 'manylinux1_x86_64'), ('cp35', 'none', 'linux_x86_64'), ('cp34', 'abi3', 'manylinux1_x86_64'), ('cp34', 'abi3', 'linux_x86_64'), ('cp33', 'abi3', 'manylinux1_x86_64'), ('cp33', 'abi3', 'linux_x86_64'), ('cp32', 'abi3', 'manylinux1_x86_64'), ('cp32', 'abi3', 'linux_x86_64'), ('py3', 'none', 'manylinux1_x86_64'), ('py3', 'none', 'linux_x86_64'), ('cp35', 'none', 'any'), ('cp3', 'none', 'any'), ('py35', 'none', 'any'), ('py3', 'none', 'any'), ('py34', 'none', 'any'), ('py33', 'none', 'any'), ('py32', 'none', 'any'), ('py31', 'none', 'any'), ('py30', 'none', 'any')]



Other system info:




Ubuntu 16.04


Cuda 8


cudnn 6


pip 9.0.1


Python 3.5


x86_64


NVidia 1080




Other notes.  
https://github.com/h2oai/h2o4gpu
 isn't clear about which Python it requires, although I suspect the file name makes this clear.  (I also guess that this means h2o4gpu won't work on Python 2.7).


The README file is not explicit about what nccl is.  From Google, I'm guessing that it means nccl is for having more than one GPU (I have only one.)


Any help would be greatly appreciated.  (Sorry I could add h2o4gpu tag because I don't have enough reputation points.)","['h2o', 'python-wheel']",Clem Wang,https://stackoverflow.com/users/2263303/clem-wang,739
48020172,48020172,2017-12-29T09:16:03,2017-12-30 04:15:27Z,0,"I have been trying to run Spark 2.2, 
master=yarn
 with h2o(
rsparkling
) but when I run 
h2o_context(sc)
 I get exception:


Error: java.lang.NoSuchMethodError: org.apache.spark.util.Utils$.getUserJars(Lorg/apache/spark/SparkConf;Z)Lscala/collection/Seq;
    at org.apache.spark.repl.h2o.H2OInterpreter.createSettings(H2OInterpreter.scala:66)
    at org.apache.spark.repl.h2o.BaseH2OInterpreter.initializeInterpreter(BaseH2OInterpreter.scala:101)
    at org.apache.spark.repl.h2o.BaseH2OInterpreter.<init>(BaseH2OInterpreter.scala:291)
    at org.apache.spark.repl.h2o.H2OInterpreter.<init>(H2OInterpreter.scala:42)
    at water.api.scalaInt.ScalaCodeHandler.createInterpreterInPool(ScalaCodeHandler.scala:100)
    at water.api.scalaInt.ScalaCodeHandler$$anonfun$initializeInterpreterPool$1.apply(ScalaCodeHandler.scala:94)
    at water.api.scalaInt.ScalaCodeHandler$$anonfun$initializeInterpreterPool$1.apply(ScalaCodeHandler.scala:93)
    at scala.collection.immutable.Range.foreach(Range.scala:160)
    at water.api.scalaInt.ScalaCodeHandler.initializeInterpreterPool(ScalaCodeHandler.scala:93)
    at water.api.scalaInt.ScalaCodeHandler.<init>(ScalaCodeHandler.scala:37)
    at water.api.scalaInt.ScalaCodeHandler$.registerEndpoints(ScalaCodeHandler.scala:132)
    at water.api.CoreRestAPI$.registerEndpoints(CoreRestAPI.scala:32)
    at water.api.RestAPIManager.register(RestAPIManager.scala:39)
    at water.api.RestAPIManager.registerAll(RestAPIManager.scala:31)
    at org.apache.spark.h2o.backends.internal.InternalH2OBackend.init(InternalH2OBackend.scala:117)
    at org.apache.spark.h2o.H2OContext.init(H2OContext.scala:121)
    at org.apache.spark.h2o.H2OContext$.getOrCreate(H2OContext.scala:352)
    at org.apache.spark.h2o.H2OContext$.getOrCreate(H2OContext.scala:387)
    at org.apache.spark.h2o.H2OContext.getOrCreate(H2OContext.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at sparklyr.Invoke$.invoke(invoke.scala:102)
    at sparklyr.StreamHandler$.handleMethodCall(stream.scala:97)
    at sparklyr.StreamHandler$.read(stream.scala:62)
    at sparklyr.BackendHandler.channelRead0(handler.scala:52)
    at sparklyr.BackendHandler.channelRead0(handler.scala:14)
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293)
    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:267)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
    at java.lang.Thread.run(Thread.java:745)



I've also tried spark 2.0.0 (installed via 
sparklyr
 function: 
spark_install
). And then match with rsparkling and h2o. It worked when I set 
master=""local""
, however it gave the same error when I set 
master=""yarn""


Similarly I tried Spark 1.6 and it worked fine (also 
master=yarn
).


Any ideas?


Here's my code:


library(sparklyr)
library(rsparkling)
library(h2o)


Sys.setenv(SPARK_HOME='/usr/hdp/2.6.3.0-235/spark2')

sc <- spark_connect(master = ""yarn"")
h2o_context(sc)



I've tried to install various different versions of h2o using(similar) install.packages(""h2o"", type = ""source"", repos = ""
http://h2o-release.s3.amazonaws.com/h2o/rel-tverberg/2/R
"") but each time the error doesn't change.","['r', 'hadoop', 'apache-spark', 'h2o', 'sparklyr']",xhudik,https://stackoverflow.com/users/1408096/xhudik,"2,434"
47989027,47989027,2017-12-27T08:48:48,2017-12-28 03:05:51Z,236,"I am trying to initialise a h2o context using Spark on yarn and hbase and also by Livy. My POM file is like below:


    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <java.version>1.8</java.version>
        <gson.version>2.8.0</gson.version>
        <java.home>${env.JAVA_HOME}</java.home>
    </properties>

    <dependencies>

        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.10 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.2.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql_2.10 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.2.0</version>

        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.11</artifactId>
            <version>2.2.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_2.11</artifactId>
            <version>2.2.0</version>
        </dependency>
        <dependency>
            <groupId>com.databricks</groupId>
            <artifactId>spark-csv_2.11</artifactId>
            <version>1.5.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/ai.h2o/h2o-core -->
        <dependency>
            <groupId>ai.h2o</groupId>
            <artifactId>h2o-core</artifactId>
            <version>3.14.0.7</version>
            <!-- <scope>runtime</scope> -->
        </dependency>
        <!-- https://mvnrepository.com/artifact/ai.h2o/h2o-algos -->
        <dependency>
            <groupId>ai.h2o</groupId>
            <artifactId>h2o-algos</artifactId>
            <version>3.14.0.7</version>
            <!-- <scope>runtime</scope> -->
        </dependency>
        <!-- https://mvnrepository.com/artifact/ai.h2o/h2o-genmodel -->
        <dependency>
            <groupId>ai.h2o</groupId>
            <artifactId>h2o-genmodel</artifactId>
            <version>3.14.0.7</version>
            <!-- <scope>runtime</scope> -->
        </dependency>
        <!-- https://mvnrepository.com/artifact/ai.h2o/sparkling-water-core_2.10 -->
        <dependency>
            <!-- <groupId>ai.h2o</groupId> <artifactId>sparkling-water-core_2.10</artifactId> 
                <version>1.6.11</version> -->

            <groupId>ai.h2o</groupId>
            <artifactId>sparkling-water-core_2.11</artifactId>
            <version>2.1.16</version>
        </dependency>
        <dependency>
            <groupId>com.google.code.gson</groupId>
            <artifactId>gson</artifactId>
            <version>${gson.version}</version>
        </dependency>
        <dependency>
            <groupId>com.cloudera.livy</groupId>
            <artifactId>livy-client-http</artifactId>
            <version>0.3.0</version>
        </dependency>
        <dependency>
            <groupId>com.cloudera.livy</groupId>
            <artifactId>livy-api</artifactId>
            <version>0.3.0</version>
        </dependency>
        <dependency>
            <groupId>it.unimi.dsi</groupId>
            <artifactId>fastutil</artifactId>
            <version>7.1.0</version>
        </dependency>
         <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>2.11.8</version>
        </dependency> 
<dependency>
    <groupId>org.apache.hbase</groupId>
    <artifactId>hbase-spark</artifactId>
    <version>1.1.2.2.6.2.14-5</version>
</dependency>



The code which I initialise the h2o context is as follow:


public RegressionMetric call(JobContext ctx) throws Exception {

        Dataset<Row> sensordataDF = this.InitializeH2OModel(ctx);

        HiveContext hc = ctx.hivectx();
        // Save the H2OContext so that we can extract the H2oFrames later
        H2OContext h2oContext = H2OContext.getOrCreate(ctx.sqlctx().sparkContext());
        // h2oContext._conf().set(""sparkext.h2o.cluster.size"",""15"");
        SQLContext sqlContext = new SQLContext(ctx.sqlctx().sparkContext());
...



I use livy to run the job but I am getting, the parameters whihc I am using to initilize it is as fooolow:


.setConf(""spark.executor.instances"", ""10"")
            .setConf(""spark.driver.memory"", ""10g"")
            .setConf(""spark.driver.cores"", ""5"")
            .setConf(""spark.executor.memory"", ""5g"") // memory per executor
            .setConf(""spark.executor.cores"", ""5"")
            .setConf(""spark.yarn.executor.memoryOverhead"",
                    ""5000"")
            .setConf(""spark.rdd.compress"",""true"")
            .setConf(""spark.default.parallelism"", ""3000"")
            .setConf(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
            .setConf(""spark.driver.extraJavaOptions"", ""-XX:+UseG1GC -Xss5000m"")
            .setConf(""spark.executor.extraJavaOptions"", ""-XX:+UseG1GC -Xss5000m"")
            .setConf(""spark.sql.shuffle.partitions"", ""5000"")
            .setConf(""spark.kryoserializer.buffer.max"", ""1g"")
            .setConf(""spark.ext.h2o.cluster.size"", ""-1"")
            .setConf(""spark.ext.h2o.cloud.timeout"", ""6000"")
            .setConf(""spark.ext.h2o.spreadrdd.retries"", ""-1"")
            .setConf(""spark.ext.h2o.nthreads"", ""-1"")
            .setConf(""spark.ext.h2o.disable.ga"", ""true"")
            .setConf(""spark.ext.h2o.dummy.rdd.mul.factor"", ""10"")
            .setConf(""spark.ext.h2o.fail.on.unsupported.spark.param"",
                    ""false"")
            .setConf(""spark.ext.h2o.client.network.mask"",""10.0.0.0/8"")  
            .setConf(""spark.ext.h2o.node.network.mask"",""10.0.0.0/8"")
            .setConf(""spark.ext.h2o.repl.enabled"",""false"")  
            .setConf(""spark.ext.h2o.repl.enabled"",""false"")  
            .setConf(""spark.driver.extraClassPath"", ""/usr/hdp/current/phoenix-client/phoenix-client.jar:/usr/hdp/current/phoenix-client/phoenix-server.jar:/usr/hdp/current/phoenix/lib/phoenix-spark2.jar:/usr/hdp/current/hbase/lib/hbase-common.jar:/usr/hdp/current/hbase/lib/hbase-server.jar"")
            .setConf(""spark.executor.extraClassPath"", ""/usr/hdp/current/phoenix-client/phoenix-client.jar:/usr/hdp/current/phoenix-client/phoenix-server.jar:/usr/hdp/current/phoenix/lib/phoenix-spark2.jar:/usr/hdp/current/hbase/lib/hbase-common.jar:/usr/hdp/current/hbase/lib/hbase-server.jar"")
            .setConf(""spark.shuffle.compress"", ""true"")
            .setConf(""spark.shuffle.spill.compress"", ""true"")
            .setConf(""spark.driver.maxResultSize"", ""0"")
            .setConf(""spark.network.timeout"",""600s"")
            .setConf(""spark.executor.heartbeatInterval"",""40s"")      
            .build();



but I get below exception


java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 22.0 failed 4 times, most recent failure: Lost task 1.3 in stage 22.0 (TID 35134, auper01-02-30-05-1.prod.vroc.com.au, executor 9): java.lang.AssertionError: assertion failed: SpreadRDD failure - IPs are not equal: (10,auper01-02-30-03-0.prod.vroc.com.au,-1) != (9, auper01-02-30-05-1.prod.vroc.com.au)
    at scala.Predef$.assert(Predef.scala:170)
    at org.apache.spark.h2o.backends.internal.InternalBackendUtils$$anonfun$7.apply(InternalBackendUtils.scala:101)
    at org.apache.spark.h2o.backends.internal.InternalBackendUtils$$anonfun$7.apply(InternalBackendUtils.scala:100)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
    at scala.collection.Iterator$class.foreach(Iterator.scala:893)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
    at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)","['apache-spark', 'h2o']",Unknown,,N/A
47984428,47984428,2017-12-26T22:35:48,2017-12-27 12:29:12Z,130,"I've been working with the h2o.ai automl function on a few problems with quite a bit of success, but have come across a bit of a roadblock.


I've got a problem that uses 500-odd predictors (all float) to map onto 6 responses (again all float.)




Required Data Parameters


y: This argument is the name (or index) of the response column.




3.16 docs


It seems that the automl library only handles a single response. Am I missing something? Perhaps in the terminology even?


In the case that I'm not, my plan is to build 6 separate leaderboards, one for each response, and use the results to kick-start a manual network search.


In theory I guess I could actually run the 6 automl models individually to get the vector response, but that feels like an odd approach.


Any insight would be appreciated,
Cheers.",['h2o'],Paul Hannah,https://stackoverflow.com/users/7504062/paul-hannah,11
47983519,47983519,2017-12-26T20:43:20,2017-12-27 12:53:57Z,349,"How does H2O determine the weights for base learners? For exp. here in the 
example
, are all the base learners equally weighted? And do I have a chance to use regularization parameters (e.g. ridge) in metalearner_algorithm? What would be the best way to avoid overfitting?","['python', 'h2o', 'ensemble-learning']",Unknown,,N/A
47943530,47943530,2017-12-22T15:01:55,2018-03-21 15:42:22Z,130,"I'm currently using H2O steam version 1.1.6 to deploy model endpoints which is working great!
However, steam uses the 
/tmp
 directory to store these deploys, which is actually only meant for temporary files. Because the 
/tmp
 has been cleared on my server, I've lost some deploys.


Is there a way to change where these files are stored?


Additionally it's also not possible to delete the deployments through the steam UI because the files are gone, is there a way to delete these as well?",['h2o'],mtricht,https://stackoverflow.com/users/4193448/mtricht,452
47942490,47942490,2017-12-22T13:38:49,2017-12-22 15:20:40Z,0,"I am trying to launch a Sparkling Water cloud within Spark using Databricks. I've attached the H2O library (3.16.0.2), PySparkling (pysparkling 0.4.6), and the Sparkling Water jar (sparkling-water-assembly_2.11-2.1.10-all.jar) to the cluster I'm running (Spark 2.1, Auto-updating Scala 1.1.1). 


I succesfully import the required libraries below: 


from pysparkling import *
import h2o



Yet when I try to initialize the Sparkling Water cloud using the following commands: 


hc = H2OContext.getOrCreate(spark)
 


or 


H2OContext.getOrCreate(sc)


I get the same error: 


NameError: name 'H2OContext' is not defined


NameError                                 Traceback (most recent call last)
<command-4043510449425708> in <module>()
----> 1 H2OContext.getOrCreate(sc)

NameError: name 'H2OContext' is not defined



For what it's worth I can initialize the Sparkling Water cloud using this 
Scala documentation
: 


%scala
import org.apache.spark.h2o._
val h2oConf = new H2OConf(sc).set(""spark.ui.enabled"", ""false"")
val h2oContext = H2OContext.getOrCreate(sc, h2oConf)

import org.apache.spark.h2o._
h2oConf: org.apache.spark.h2o.H2OConf =
Sparkling Water configuration:
  backend cluster mode : internal
  workers              : None
  cloudName            : sparkling-water-root_app-20171222131625-0000
  flatfile             : true
  clientBasePort       : 54321
  nodeBasePort         : 54321
  cloudTimeout         : 60000
  h2oNodeLog           : INFO
  h2oClientLog         : WARN
  nthreads             : -1
  drddMulFactor        : 10
h2oContext: org.apache.spark.h2o.H2OContext =

Sparkling Water Context:
 * H2O name: sparkling-water-root_app-20171222131625-0000
 * cluster size: 1
 * list of used nodes:
  (executorId, host, port)
  ------------------------
  (x,xx.xxx.xxx.x,54321)
  ------------------------

  Open H2O Flow in browser: http://xx.xxx.xxx.xxx:54321 (CMD + click in Mac OSX)



but this pipeline may not always use Databricks so it needs to be all in PySpark and Databricks doesn't have a corresponding PySpark example. 


Thanks in advance.","['pyspark', 'h2o', 'databricks', 'sparkling-water']",Frank B.,https://stackoverflow.com/users/2630758/frank-b,"1,873"
47927169,47927169,2017-12-21T14:36:43,2017-12-22 09:16:37Z,0,"I am using H2O with R. When I am trying to compare two different elements of an H2O frame, I get an environment variable [1 row x 1 column]. I want to convert the type of this object to a numeric or Boolean type.


The reproducible example.


library(h2o)
h2o.init()

>d <- as.data.frame(c(1,2,3,4,5,1))
>d.hex <- as.h2o(d)

>d.hex[1]
c(1, 2, 3, 4, 5, 1)
1                   1
[1 row x 1 column]

>typeof(d.hex[1])
[1] ""environment""



I want a way to convert the following code so it returns Boolean value, or 0 or 1.


>d.hex[1] == d.hex[6]
c(1, 2, 3, 4, 5, 1)
1                   1    
[1 row x 1 column]



Expected Output


[1] True



or


[1] 1","['r', 'types', 'boolean', 'h2o', 'comparison-operators']",Unknown,,N/A
47925196,47925196,2017-12-21T12:38:45,2017-12-22 09:25:02Z,876,"H2O Python demo does not work on jupyter notebook and show exception because these demos are setting key press event.


(press any key)    demo_function(interactive, echo, test)
  File ""/home/administrator/venv/lib/python3.5/site-packages/h2o/demos.py"", line 135, in deeplearning
    _run_demo(demo_body, interactive, echo, testing)
  File ""/home/administrator/venv/lib/python3.5/site-packages/h2o/demos.py"", line 285, in _run_demo
    body_fn(controller)
  File ""/home/administrator/venv/lib/python3.5/site-packages/h2o/demos.py"", line 92, in demo_body
    go()
  File ""/home/administrator/venv/lib/python3.5/site-packages/h2o/demos.py"", line 273, in controller
    key = _wait_for_keypress()
  File ""/home/administrator/venv/lib/python3.5/site-packages/h2o/demos.py"", line 312, in _wait_for_keypress
    oldterm = termios.tcgetattr(fd)
termios.error: (25, 'Inappropriate ioctl for device')



How to resolve this problem?
My code:


import h2o
h2o.init()
h2o.demo('glm')","['python', 'h2o']",Unknown,,N/A
47894205,47894205,2017-12-19T19:48:14,2017-12-20 11:51:27Z,190,"Closed
. This question is 
opinion-based
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Update the question so it can be answered with facts and citations by 
editing this post
.






Closed 
6 years ago
.















                        Improve this question
                    








I've understood that Sparkling Water is H20 executed on a Spark environment and so it can use the Spark Engine (and all Spark distributed structures) to distribute computing, but in term of performances which are the benefits since H2O is already a distributed and scalable library for machine learning?


And more, the standalone version of H2O is really capable of managing a distributed processing over a cluster of computers?","['apache-spark', 'machine-learning', 'h2o', 'sparkling-water']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
47829169,47829169,2017-12-15T09:18:53,2017-12-15 15:48:28Z,180,"I'm trying to run H2O Deepwater using TensorFlow as back end. The 
installation
 to use it with python ran smoothly without any errors. Now I want to run the 
deeplearning_mnist_introduction
 notebook
 but right at the beginning the function that loads the data returns an error.


test_df = h2o.import_file(PATH + ""bigdata/laptop/mnist/test.csv.gz"")

...
/home/my_user_name/anaconda3/envs/h2o-tf-gpu/lib/python2.7/site-packages/h2o/backend/connection.pyc in _process_response(response, save_to)
    723         # Client errors (400 = ""Bad Request"", 404 = ""Not Found"", 412 = ""Precondition Failed"")
    724         if status_code in {400, 404, 412} and isinstance(data, (H2OErrorV3, H2OModelBuilderErrorV3)):
--> 725             raise H2OResponseError(data)
    726 
    727         # Server errors (notably 500 = ""Server Error"")

H2OResponseError: Server error water.exceptions.H2ONotFoundArgumentException:
  Error: File /home/my_user_name/h2o-3/bigdata/laptop/mnist/test.csv.gz does not exist
  Request: GET /3/ImportFiles
    params: {u'path': '/home/my_user_name/h2o-3/bigdata/laptop/mnist/test.csv.gz'}



My guess would be some hard coded URL that is not longer valid.


Is there a way to fix the URL or just point me to the data set so I can download it manually?


My setup is:




CentOS 7.3.1611


Python 2.7.14 (although I can install a different version if necessary)


h2o-3 (built from nightly, as instructed 
here
)


tf 1.4 (pip installed from 
here
)","['python', 'h2o']",Matt,https://stackoverflow.com/users/137794/matt,27.8k
47828510,47828510,2017-12-15T08:34:05,2018-01-26 15:32:03Z,930,"I have a saved h2o model in mojo format, and now I am trying to load it and use it to make predictions on a new dataset (
df
) as part of a spark app written in scala. Ideally, I wish to append a new row to the existing DataFrame containing the class probability based on this model.


I can see how to apply a mojo to an individual row already in a RowData format (as per answer 
here
), but I am not sure how to map over an existing DataFrame so that it is in the right format to make predictions using the mojo model. I have worked with DataFrames a fair bit, but never with the underlying RDDs. 


Also, should this model be serialised / broadcast so that predictions can be done in parallel on a cluster, or will it be available to all executors as part of the map?


I have gotten this far:


# load mojo model and create easy predict model wrapper
val mojo = MojoModel.load(""loca/path/to/mojo/mojo.zip"")
val easyModel = new EasyPredictModelWrapper(mojo)

# map over spark DataFrame, converty to rdd, and make predictions on each row:
df.rdd.map { row =>
   val prediction = easyModel.predictBinomial(row).classProbabilities
   println(prediction)
   }



But my 
row
 variable is not in the right format for this to work. Any suggestions on what to try next? 


EDIT: my DataFrame consists of 70 predictive feature columns which are a mixture of integers and category/factor columns. A very simple sample DataFrame:


val df = Seq(
  (0, 3, ""cat1""),
  (1, 2, ""cat2""),
  (2, 6, ""cat1"")
).toDF(""id"", ""age"", ""category"")","['scala', 'apache-spark', 'h2o', 'sparkling-water']",Unknown,,N/A
47817730,47817730,2017-12-14T16:11:21,2017-12-20 00:47:41Z,0,"I am new to H2O in python. I am trying to model my data using ensemble model following the example codes from H2O's web site. (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html
)


I have applied GBM and RF as base models. And then using stacking, I tried to merge them in ensemble model. In addition, in my training data I created one additional column named 'fold' to be used in 
fold_column = ""fold""


I applied 10 fold cv and I observed that I received results from cv1. However, all the predictions coming from other 9 cvs, they are empty. What am I missing here?


Here is my sample 
data
:




code:


import h2o
from h2o.estimators.random_forest import H2ORandomForestEstimator
from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator
from h2o.grid.grid_search import H2OGridSearch
from __future__ import print_function

h2o.init(port=23, nthreads=6)

train = h2o.H2OFrame(ens_df)
test = h2o.H2OFrame(test_ens_eq)

x = train.drop(['Date','EQUITY','fold'],axis=1).columns
y = 'EQUITY'

cat_cols = ['A','B','C','D']
train[cat_cols] = train[cat_cols].asfactor()
test[cat_cols] = test[cat_cols].asfactor()

my_gbm = H2OGradientBoostingEstimator(distribution=""gaussian"",
                                      ntrees=10,
                                      max_depth=3,
                                      min_rows=2,
                                      learn_rate=0.2,
                                      keep_cross_validation_predictions=True,
                                      seed=1)

my_gbm.train(x=x, y=y, training_frame=train, fold_column = ""fold"")



Then when I check cv results with 




my_gbm.cross_validation_predictions():






Plus when I try the ensemble in the test set I get the warning below:


# Train a stacked ensemble using the GBM and GLM above
ensemble = H2OStackedEnsembleEstimator(model_id=""mlee_ensemble"",
                                       base_models=[my_gbm, my_rf])
ensemble.train(x=x, y=y, training_frame=train)

# Eval ensemble performance on the test data
perf_stack_test = ensemble.model_performance(test)

pred = ensemble.predict(test)
pred

/mgmt/data/conda/envs/python3.6_4.4/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset is missing column 'fold': substituting in a column of NaN
  warnings.warn(w)



Am I missing something about fold_column?","['python-3.x', 'h2o', 'ensemble-learning']",Unknown,,N/A
47784467,47784467,2017-12-13T02:09:42,2017-12-14 08:43:47Z,225,"What do 
h2o
 checkpoints actually do? Does a model created with say


gbm_continued = H2OGradientBoostingEstimator(checkpoint= gbm_orig.model_id, ntrees = 50, seed = 1234)



mean that gbm_continued will have the same parameters and prediction performance as gbm_orig if we were to not train it on any new data?


The 
docs
, say ""
This will build a new model as a continuation of a previously generated model
"", but I am confused as to what a ""continuation"" actually implies. An explanation would be much appreciated. Thanks","['python', 'h2o']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
47775759,47775759,2017-12-12T15:08:50,2017-12-13 12:56:51Z,0,"I am building an ensemble from GLM models with different regularization parameters (alpha, lambda) using the 
h2o
 package. When I try to build an ensemble, following the documentation:


ensemble <- h2o.stackedEnsemble(x = predictors,
                                y = response,
                                training_frame = train,
                                model_id = ""ensemble"",
                                base_models = list(glm_grid@model_ids)) 



Where 
glm_grid@model_ids
 are the models from a grid search to determine the optimal 
alpha
 and 
lambda
 regularization parameters for GLM. I receive the following error:


When creating a StackedEnsemble you must specify one or more models; 24 were specified but none of those were found: [list(""glm_grid_model_6"", glm_grid_model_11, glm_grid_model_7, glm_grid_model_9, glm_grid_model_2, glm_grid_model_21, glm_grid_model_15, glm_grid_model_0""]



Do you know what seems to be the issue?


EDIT: I tried following the documentation and used code analogical to that one:


gbm_grid <- h2o.grid(algorithm = ""gbm"",
                     grid_id = ""gbm_grid_binomial"",
                     x = x,
                     y = y,
                     training_frame = train,
                     ntrees = 10,
                     seed = 1,
                     nfolds = nfolds,
                     fold_assignment = ""Modulo"",
                     keep_cross_validation_predictions = TRUE,
                     hyper_params = hyper_params,
                     search_criteria = search_criteria)

# Train a stacked ensemble using the GBM grid
ensemble <- h2o.stackedEnsemble(x = x,
                                y = y,
                                training_frame = train,
                                model_id = ""ensemble_gbm_grid_binomial"",
                                base_models = gbm_grid@model_ids)



And as per @Erin LeDell I removed the additional 
list()
 and it works now. However, what I would ultimately like to do is to use grids from various models, so something like: 


ensemble <- h2o.stackedEnsemble(x = x,
                                y = y,
                                training_frame = train,
                                model_id = ""my_ensemble_binomial"",
                                base_models = list(my_gbm, my_rf))



EDIT2:


Solved it by using:


model_list <- as.list(c(glm_grid_1@model_ids,
                        glm_grid_2@model_ids))


ensemble <- h2o.stackedEnsemble(x = predictors,
                                y = response,
                                training_frame = train,
                                model_id = ""ensemble1231"",
                                base_models = model_list)","['r', 'machine-learning', 'glm', 'h2o', 'ensemble-learning']",Unknown,,N/A
47775255,47775255,2017-12-12T14:40:27,2017-12-13 03:24:41Z,49,"Does H2O have ability to add model description on build model step or link it after? I need to add some information about model: what it predict, why and etc.",['h2o'],Vasiliy Nerozin,https://stackoverflow.com/users/7849530/vasiliy-nerozin,21
47773965,47773965,2017-12-12T13:34:29,2017-12-13 00:30:48Z,0,"I have a problem with building an ensemble using 
h2o
 and Random Forest as one of the components. I estimate the models as in 
h2o
's documentation with the only difference being the dependent variable being a multiclass variable. When I try to build an ensemble I get the following error:


Error: water.exceptions.H2OIllegalArgumentException: Don't know how to set the distribution for a multinomial Random Forest classifier.



I tried googling it but with no success. Anyone knows what to do in such a case?","['r', 'machine-learning', 'random-forest', 'h2o', 'ensemble-learning']",Unknown,,N/A
47773338,47773338,2017-12-12T13:01:38,2017-12-12 14:34:11Z,494,"So I have trained several GBM models using GridSearch in H2O.


I then trained an ensemble model like so:


from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator

#List of all models in the GridSearch
all_model_ids=grid_search_gbm.model_ids


ensemble = H2OStackedEnsembleEstimator(
    model_id = 'ensemble_gbm' ,
    base_models = all_model_ids
)

ensemble.train(
    x=training_columns, 
    y=response_column,
    training_frame=train,
)



I can't how ever extract only specific models from ""grid_search_gbm"" and feed it into ""H2OStackedEnsembleEstimator()"". Is there a method for extracting this? For example the 3 best models in 'grid_search_gbm'?","['python', 'h2o']",floyergilmour,https://stackoverflow.com/users/3363634/floyergilmour,125
47759418,47759418,2017-12-11T18:48:39,2018-08-30 21:16:40Z,327,"I have the opposite issue to most people with 
as.h2o()
, though the resulting problem is the same. I have to convert and feed a series of single row vectors just 19 columns wide to an h2o autoencoder. Each vector takes 0.29 seconds approx to convert using 
as.h2o()
, which is causing a major bottleneck. 


Can anyone suggest an alternative approach that might be faster?

(For various reasons I have no alternative to sending single row vectors one by one, so aggregating the data in matrices before calling as.h2o is not an option.) 


Many thanks.",['h2o'],Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
47759165,47759165,2017-12-11T18:32:09,2017-12-11 20:00:13Z,0,"after following the instruction in 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html#install-in-python
, I was able to install h2o v 3.16.0.2. I was also able to use command line instructions mentioned in the above website and test that it is working.


python
import h2o
h2o.init()
h2o.demo(""glm"")



However, when I launch anaconda spyder, I am not able to import h2o. How do I link the h2o I have installed and bring it into Spyder python?


Update:
I have already tried {conda install -c anaconda h2o } which is mentioned on Anaconda.org, but that installs older ver 3.10 of h2o and that did not work either.


thanks for your help.","['anaconda', 'h2o']",Unknown,,N/A
47747662,47747662,2017-12-11T06:42:49,2017-12-11 06:47:48Z,318,"By executing the code 
h2o.connect()
 


I am getting the error message 




""raise AttributeError(""Attribute %s cannot be set on H2OCluster (=
  %r)"" % (k, v)) AttributeError: Attribute internal_security_enabled
  cannot be set on H2OCluster (= False)"".




Can someone help me out with this issue? 


Thanks in advance.","['python', 'h2o']",Ankur Alankar Biswal,https://stackoverflow.com/users/2204625/ankur-alankar-biswal,"1,182"
47722933,47722933,2017-12-08T22:13:36,2017-12-10 17:12:29Z,96,"We have our data stored as .npy files. One for the features and the other for all the targets we want to impute (we plan on just selecting one target column at a time to be trained). It doesn't look like .npy is supported (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/getting-data-into-h2o.html#supported-file-formats
) in h2o, but I was wondering if there was a workaround to directly import .npy files, without having to convert them to .csv and basically having to double our storage space. 


When I uploaded the .npy as pandas dataframes, I would get an error at ncols = training_frame.ncols, saying DataFrame object has no attribute 'ncols'.",['h2o'],Christopher Mancuso,https://stackoverflow.com/users/5451490/christopher-mancuso,351
47722047,47722047,2017-12-08T20:59:09,2017-12-09 19:00:09Z,293,"Is there a way to start an h2o instance interface on a specific node of a cluster? For example...


When using the command:


$ hadoop jar h2odriver.jar -nodes 4 -mapperXmx 6g -output hdfsOutputDir



from say in the h2o install directory, in say node 172.18.4.62, I get the (abridged) output:


....
H2O node 172.18.4.65:54321 reports H2O cluster size 1
H2O node 172.18.4.66:54321 reports H2O cluster size 1
H2O node 172.18.4.67:54321 reports H2O cluster size 1
H2O node 172.18.4.63:54321 reports H2O cluster size 1
H2O node 172.18.4.63:54321 reports H2O cluster size 4
H2O node 172.18.4.66:54321 reports H2O cluster size 4
H2O node 172.18.4.67:54321 reports H2O cluster size 4
H2O node 172.18.4.65:54321 reports H2O cluster size 4
H2O cluster (4 nodes) is up
(Note: Use the -disown option to exit the driver after cluster formation)

Open H2O Flow in your web browser: http://172.18.4.65:54321

(Press Ctrl-C to kill the cluster)
Blocking until the H2O cluster shuts down...



And from a python script that wants to connect to the h2o instance, I would do something like:


h2o.init(ip=""172.18.4.65"")



to connect to the h2o instance. However, it would be better to be able to control which address the h2o instance connection sits at. 


Is there a way to do this? Is this question confused/wrong-headed? My overall goal is to have the python script run periodically, start an h2o cluster, do stuff on that cluster then shut the cluster down (not being able to know the address to use to connect to the cluster means the script would never be sure which address to connect to). Any advice would be appreciated. Thanks.",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
47714901,47714901,2017-12-08T12:57:27,2018-09-10 07:25:11Z,671,"I am trying to use h2o REST API to import CSV files that I have on my local server. 


Command:
 




curl -v -X GET
  '
http://127.0.0.1:54321/3/ImportFiles?path=http://127.0.0.1:8083/datasets/tables/csv/RDsTWgcvAjHeWJFnbhCKTCE5rn6aLCjJ.csv
'




Result in following log:




Trying 127.0.0.1... Connected to 127.0.0.1 (127.0.0.1) port 54321 (#0)
  GET
  /3/ImportFiles?path=
http://127.0.0.1:8083/datasets/tables/csv/RDsTWgcvAjHeWJFnbhCKTCE5rn6aLCjJ.csv
 HTTP/1.1 Host: 127.0.0.1:54321 User-Agent: curl/7.47.0 Accept: 
/

  HTTP/1.1 200 OK X-h2o-build-project-version: 3.16.0.2
  X-h2o-rest-api-version-max: 3 X-h2o-cluster-id: 1512722051559
  X-h2o-cluster-good: true X-h2o-context-path: / Content-Type:
  application/json Content-Length: 349 Server: Jetty(8.y.z-SNAPSHOT)

  Connection #0 to host 127.0.0.1 left intact
  {""__meta"":{""schema_version"":3,""schema_name"":""ImportFilesV3"",""schema_type"":""ImportFiles""},""_exclude_fields"":"""",""path"":""
http://127.0.0.1:8083/datasets/tables/csv/RDsTWgcvAjHeWJFnbhCKTCE5rn6aLCjJ.csv
"",""pattern"":null,""files"":[],""destination_frames"":[],""fails"":[""
http://127.0.0.1:8083/datasets/tables/csv/RDsTWgcvAjHeWJFnbhCKTCE5rn6aLCjJ.csv
""],""dels"":[]}




H2O log on TRACE level shows only:




12-08 15:41:59.951 10.8.128.101:54321    36013  #4756-331 INFO: GET
  /3/ImportFiles, parms:
  {path=
http://127.0.0.1:8083/datasets/tables/csv/RDsTWgcvAjHeWJFnbhCKTCE5rn6aLCjJ.csv
}




Is there any way to debug while importing fails? h2o does not asking local server at all.


Commands from other servers work well:




curl -v -X GET ""
http://127.0.0.1:54321/3/ImportFiles?path=http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz
""
  curl -v -X GET ""
https://raw.github.com/h2oai/h2o/master/smalldata/logreg/prostate.csv
""",['h2o'],Unknown,,N/A
47703196,47703196,2017-12-07T20:25:19,2017-12-07 20:25:19Z,38,"I know this is more a machine learning question vs H2O but since I'm running this in H2O, I was wondering if anyone's seen this before. This is what my confusion matrix looks like with training data ( using a GBM model but happens in other linear models too)


        N       Y       Error   Rate
N       6921014.0       2076845.0       0.2308  (2076845.0/8997859.0)
Y       399597.0        8597833.0       0.0444  (399597.0/8997430.0)
Total   7320611.0       10674678.0000000        0.1376  (2476442.0/17995289.0)



For Validation:


N   Y   Error   Rate
N   4697621.0   263504.0    0.0531  (263504.0/4961125.0)
Y   148169.0    167182.0    0.4699  (148169.0/315351.0)
Total   4845790.0   430686.0    0.078   (411673.0/5276476.0)



There is a major jump in Y error rate. I don't see any overfitting attributes unless I'm missing something. Did anyone run into this before?
This seems to happen for Linear models as well.","['python', 'machine-learning', 'h2o']",Bardiya Choupani,https://stackoverflow.com/users/2047526/bardiya-choupani,165
47702323,47702323,2017-12-07T19:23:36,2017-12-14 07:15:32Z,480,"How do you capture unknown values when making predictions on 
h2o
 data frames?


For example, when doing something like:


model.predict(frame_in)



in the h2o python api, a progress bar loads while the model is making predictions and then a series of lists are outputted detailing the unknown labels seen for each of the enum types of the model predictive features. Eg.


/home/mapr/anaconda2/lib/python2.7/site-packages/h2o/job.py:69: UserWarning:
Test/Validation dataset column 'feature1' has levels not trained on: [, <values>] 



Is there any way to get this set of unknown levels as a python object? Thanks. 


When working with 
h2o MOJO
s, there is a 
java method
 called 
getTotalUnknownCategoricalLevelsSeen()
, but I could not find anything like this in the 
h2o python
 docs.","['python', 'h2o']",Unknown,,N/A
47702022,47702022,2017-12-07T19:01:56,2017-12-07 19:25:07Z,381,"Using 
h2o python
 api and have a H2ODataFrame that raises the error




ValueError: unimpl bytecode instr: LOAD_CONST




when trying to use the apply method on the frame.


The frame in question here looks like:


predict     0_good      1_bad
0_NoDenial  0.999593    0.000407184
0_NoDenial  0.999571    0.000428798
0_NoDenial  0.998374    0.00162572
0_NoDenial  0.999343    0.000657361
0_NoDenial  0.999307    0.000693177
0_NoDenial  0.999895    0.000104678
0_NoDenial  0.999495    0.000504838
0_NoDenial  0.999627    0.000373012
0_NoDenial  0.997075    0.00292529
0_NoDenial  0.996358    0.00364194

<class 'h2o.frame.H2OFrame'>



The code that raises the error looks like:


preds['predict'] = preds['1_bad'].apply(lambda x: '1_bad' if x > custom_thresh else '0_good', axis=1)



With the full error looking like:


/home/mapr/anaconda2/lib/python2.7/site-packages/h2o/frame.pyc in apply(self, fun, axis)
   3034         assert_is_type(fun, FunctionType)
   3035         assert_satisfies(fun, fun.__name__ == ""<lambda>"")
-> 3036         res = lambda_to_expr(fun)
   3037         return H2OFrame._expr(expr=ExprNode(""apply"", self, 1 + (axis == 0), *res))
   3038 

/home/mapr/anaconda2/lib/python2.7/site-packages/h2o/astfun.pyc in lambda_to_expr(fun)
    133     code = fun.__code__
    134     lambda_dis = _disassemble_lambda(code)
--> 135     return _lambda_bytecode_to_ast(code, lambda_dis)
    136 
    137 def _lambda_bytecode_to_ast(co, ops):

/home/mapr/anaconda2/lib/python2.7/site-packages/h2o/astfun.pyc in _lambda_bytecode_to_ast(co, ops)
    147         body, s = _opcode_read_arg(s, ops, keys)
    148     else:
--> 149         raise ValueError(""unimpl bytecode instr: "" + instr)
    150     if s > 0:
    151         print(""Dumping disassembled code: "")

ValueError: unimpl bytecode instr: LOAD_CONST



I've never seen this kind of error before and quick googling did not tell much more. Does anyone know why this is happening and what can be done about it? Thanks.",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
47696590,47696590,2017-12-07T13:51:20,2017-12-07 20:54:38Z,0,"I am trying to find an easy way to save all the cross validation models produced by h2o using R.



Running a any kind of model with nfolds = 5 I can see each CV model listed in the web-interface (localhose:54321) looking something like this:


model_id
model_id_cv_1
model_id_cv_2
model_id_cv_3
model_id_cv_4
model_id_cv_5



I've used this to save it:
    h2o.saveModel(model_id, path=""mypath"") gives a 


But h2o.saveModel(model_id_cv_1, path=""mypath"")
But when I reload it I loose all the cross validated models.


It does seem possible to save each CV model as  POJO via the webinterface, but I'd rather be able to do this programatically in R.  It seems that there used to be a 'save_cv' option in earlier versions of h2o.saveModel(), but this seems to have been removed.  


Is this possible?","['r', 'h2o']",AdrianD,https://stackoverflow.com/users/4105884/adriand,51
47656525,47656525,2017-12-05T14:54:59,2017-12-06 07:22:10Z,0,"I'm using 
automl
 function with code snippet shown below


h2o.init()
h2o_train = as.h2o(train)
h2o_test = as.h2o(test)
aml <- h2o.automl(x=x, y=y, training_frame=h2o_train, leaderboard_frame=h2o_test)
print(aml@leaderboard)  # view top models
print(getParms(aml@leader))  #  get related info for top1 model 



After read through the doc, I couldn't find how to load the results for other models, the leaderboard shows their 
model_id
. It would be valuable if we can load those models, or at least see their parameters.","['r', 'h2o', 'automl']",Paul Lo,https://stackoverflow.com/users/2547739/paul-lo,"6,138"
47644657,47644657,2017-12-05T01:27:12,2022-05-04 12:58:37Z,0,"Using 
h2o
 on python in jupyter notebook and getting error message:


...
/home/mapr/anaconda2/lib/python2.7/site-packages/h2o/backend/connection.pyc in _process_response(response, save_to)
    723         # Client errors (400 = ""Bad Request"", 404 = ""Not Found"", 412 = ""Precondition Failed"")
    724         if status_code in {400, 404, 412} and isinstance(data, (H2OErrorV3, H2OModelBuilderErrorV3)):
--> 725             raise H2OResponseError(data)
    726 
    727         # Server errors (notably 500 = ""Server Error"")
H2OResponseError: Server error water.exceptions.H2ONotFoundArgumentException: 
Error: File <path to data file I'm trying to import> does not exist.



when trying to import data with 


train = h2o.import_file(path = os.path.realpath(""relative path to data file""))



Yet the file does in fact exist on the specified path. Why would this be happening?


Details


Following 
h2o
 deeplearning 
example
 for accessing 
h2o
 service from python code in a jupyter notebook. Everything works fine up until the 
part
 where need to import 
.csv
 data, eg.


spiral = h2o.import_file(path = os.path.realpath(""../data/spiral.csv"")) 



At which point the error above is raised. The source code comments that




# In this case, the cluster is running on our laptops. Data files are imported by their 
relative locations
 to this notebook.




Yet, when running 


os.path.exists(os.path.realpath(""./data/<my data csv file>""))



in the notebook, the response is 
true
. So it seems like the relative path is recognized by the python os package*, but there is some problem with the h2o.import_file() method.


What could be going on here? Thanks.


Note: that I'm am using port forwarding from the machine actually running the h2o and jupyter-notebook services with something like:


remote machine:


$jupyter-notebook --no-browser --port=8889



local machine:


$ssh -N -L localhost:8888:localhost:8889 myuser@mnode01



* The directory structure is:


bin  
data
  |
  |_____ mydata.csv  
include  
lib  
remote-h2o.ipynb



UPDATE


Think have found the problem. The h2o python 
docs
 specify that




The path to the data must be a valid path 
for each node
 in the H2O cluster. If some node in the H2O cluster cannot see the file, then an exception will be thrown by the H2O cluster.




This raises the question, that does this mean that all of the cluster nodes need to have the same virtualenv (with same absolute path) that I am running the jupyter notebook and holding the data/mydata.csv in?",['h2o'],Unknown,,N/A
47639493,47639493,2017-12-04T18:15:13,2018-03-30 01:04:07Z,780,"I've seen two H2O demos including Sri's keynote at H2O World 2017, and they have mentioned a Python version of the R data.table package.  However, I have been unable to find any additional info regarding this.


Is this available as a stand alone python module (or planning on becoming one)?","['python', 'h2o']",R Yoda,https://stackoverflow.com/users/4468078/r-yoda,"8,730"
47625766,47625766,2017-12-04T02:40:16,2017-12-13 09:13:11Z,0,"I'm trying to make sure I am not confused with how h2o works with a cross validation and a validation dataset.   I'm sure I am just getting confused on the verbiage that is used to describe this


    library(mlbench)
   library(h2o)

data(Sonar)
dfh2o = as.h2o(Sonar)
splits=h2o.splitFrame(dfh2o)

train = splits[[1]]
valid = splits[[2]]

gbm_no_val_frame <- h2o.gbm(x = colnames(df), y = ""Class"", training_frame = train,
                         nfolds = 5, seed = 1234, ntrees  = 4000, stopping_rounds = 5)


gbm_val_frame <- h2o.gbm(x = colnames(df), y = ""Class"", training_frame = train, validation_frame = valid,
                    nfolds = 5, seed = 1234, ntrees  = 4000, stopping_rounds = 5)            

   h2o.flow()  ### to see the validation frame stopping under models.





What is the validation frame that is used for early stopping in gbm_no_val_frame?   How does this stop to prevent overfitting exactly?


I think I understand how gbm_val_frame works - it stops overfitting when the scores stop improving when the trained fold/final model hits against the 'valid' frame?




Just want to eliminate any doubts I have here..","['r', 'validation', 'cross-validation', 'h2o']",Unknown,,N/A
47609200,47609200,2017-12-02T15:00:17,2018-05-15 17:53:33Z,0,"I'm trying to explore the use of a GBM with 
h2o
 for a classification issue to replace a logistic regression (GLM). The non-linearity and interactions in my data make me think a GBM is more suitable.


I've ran a baseline GBM (see below) and compared the AUC against the AUC of the logistic regression. THe GBM performs much better.


In a classic linear logistic regression, one would be able to see the direction and effect of each of the predictors (x) on the outcome variable (y). 


Now, I would like to evaluate the variable importance of the estimate GBM in the same way. 


How does one obtain the variable importance for each of the (two) classes?


I know that the variable importance is not the same as the estimated coefficient in a logistic regression, but it would help me to understand which predictor impacts what class.  


Others have asked 
similar questions
, but the answers provided won't work for the H2O object.


Any help is much appreciated.


example.gbm <- h2o.gbm(
  x = c(""list of predictors""), 
  y = ""binary response variable"", 
  training_frame = data, 
  max_runtime_secs = 1800, 
  nfolds=5,
  stopping_metric = ""AUC"")","['r', 'machine-learning', 'classification', 'h2o', 'gbm']",wake_wake,https://stackoverflow.com/users/3587303/wake-wake,"1,214"
47568866,47568866,2017-11-30T08:24:43,2017-11-30 08:24:43Z,0,"There is an error at the final stage of this analysis. When running explain() function on an h2o model, I get the following error:
Error: All permutations have no similarity to the original observation. Try setting bin_continuous to TRUE and/or increase kernel_size
I have tried both the suggestions in the error. If I change the bin_continous to TRUE, the lime() function does not work and other kernel sizes do not work either. 


Any thought on how to solve this and therefore be able to get the results with the plot_features() function?


library(readxl)
library(httr)
library(dplyr)
library(h2o)        
library(lime) 


GET(""https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_FnUseC_-HR-Employee-Attrition.xlsx"", 
   write_disk(tf <- tempfile(fileext = "".xls"")))
hr_data_raw <- read_xlsx(tf)


hr_data <- hr_data_raw %>%
mutate_if(is.character, as.factor) %>%
select(Attrition, everything())  


h2o.init()
h2o.no_progress() 

hr_data_h2o <- as.h2o(hr_data)
split_h2o <- h2o.splitFrame(hr_data_h2o, c(0.7, 0.15), seed = 1234 )
train_h2o <- h2o.assign(split_h2o[[1]], ""train"" ) # 70%
valid_h2o <- h2o.assign(split_h2o[[2]], ""valid"" ) # 15%
test_h2o  <- h2o.assign(split_h2o[[3]], ""test"" )  # 15%  


y <- ""Attrition""
x <- setdiff(names(train_h2o), y)  
automl_models_h2o <- h2o.automl(
x = x, 
y = y,
training_frame = train_h2o,
validation_frame = valid_h2o,
leaderboard_frame = test_h2o,
max_runtime_secs  = 30)

automl_leader <- automl_models_h2o@leader 


explainer <- lime::lime(
   as.data.frame(train_h2o[,-1]), 
   model = automl_leader, 
   bin_continuous = F)

explanation <- lime::explain(
   as.data.frame(test_h2o[1:10, -1]), 
   explainer = explainer, 
   n_labels     = 1, 
   n_features   = 4)

  # Error: All permutations have no similarity to the original observation.
  # Try setting bin_continuous to TRUE and/or increase kernel_size

  # Cannot Continue
  plot_features(explanation)","['r', 'h2o', 'lime']",YK95,https://stackoverflow.com/users/9032316/yk95,21
47563536,47563536,2017-11-29T23:24:48,2017-11-30 00:19:36Z,0,"I'm trying to run H2O's K-means algorithm on my data set using H2O 3.16.0.1 with R 3.3.3 as follows:


h2o.kmeans(x=covars, training_frame=mydata.h2o, k=3, fold_column='Fold')



But it results in the error:




ERRR on field: _fold_column: Fold column 'Fold' not found in the training frame




The ""Fold"" column is indeed in the data. After some experimenting, the error goes away only when ""Fold"" is added to the list of covariates in the ""x=covars"" even though it's technically not a covariate. Could this be a bug in the software?","['r', 'k-means', 'h2o']",user1769120,https://stackoverflow.com/users/1769120/user1769120,107
47544498,47544498,2017-11-29T03:20:33,2018-06-06 20:07:21Z,286,"I tried to import my data in json format but it took forever to import and I cannot do anything except waiting.


The files consist of a list of images, and for each image, you can find the following fields:
id - the id of the image
band_1, band_2 - the flattened image data. Each band has 75x75 pixel values in the list, so the list has 5625 elements. Note that these values are not the normal non-negative integers in image files since they have physical meanings - these are float numbers with unit being dB. Band 1 and Band 2 are signals characterized by radar backscatter produced from different polarizations at a particular incidence angle. 
inc_angle - the incidence angle of which the image was taken. Note that this field has missing data marked as ""na"", and those images with ""na"" incidence angles are all in the training data to prevent leakage.
is_iceberg - the target variable, set to 1 if it is an iceberg, and 0 if it is a ship. 


Please advise what I can do to try this product on my data. I want to predicted probability that this image is iceberg.","['h2o', 'driverless-ai']",user1314404,https://stackoverflow.com/users/1314404/user1314404,"1,285"
47514577,47514577,2017-11-27T15:34:36,2017-11-28 02:42:14Z,0,"I want to  do cross join between two H2OFrames. Looking for work around 
Strictly in H2OFrame


col1.1 <- c('A', 'B', 'E', 'C', 'F', 'D')
dummy <- rep(1,6)

d1.hex <- as.h2o( cbind( col1.1, dummy ) )

col2.1 <- c('xx', 'yy', 'zz', 'ww')

dummy <- rep(1,4)

d2.hex <- as.h2o( cbind( col2.1, dummy ) )



If I use 
all =TRUE
 it throws Error : unimplemented


h2o.merge(d1.hex, d2.hex, all = TRUE)



If I use default, joining result is not cross join


h2o.merge(d1.hex, d2.hex )





dummy col1.1 col2.1


1      A     xx


1      B     xx


1      E     xx


1      C     xx


1      F     xx


1      D     xx




I have tried changing data types of joining column to categorical or numeric but no success. Looking for your help in resolving the issue.


Thank you","['r', 'h2o', 'cross-join']",Unknown,,N/A
47513901,47513901,2017-11-27T14:58:29,2017-11-27 15:03:54Z,0,"I'm having trouble understanding whether I need to be consistent with the categorical / factor encodings of variables. With consistency I mean that I need to assure that the encodings from integers and levels should be the same in the training and the new testing sample.


This answer
 seems to suggest that it is not necessary. On the contrary, 
this answer
 suggests that IT is indeed necessary.


Suppose I have a training sample with an 
xcat
 that can take values 
a
, 
b
, 
c
. The expected result is that the 
y
 variable will tend to take values close to 
1
 when 
xcat
 is 
a
, 
2
when 
xcat
 is 
b
, and 
3
 when 
xcat
 is 
c
.


First I'll create the dataframe, pass it to 
h2o
 and then encode with the function 
as.factor
:


library(h2o)
localH2O = h2o.init(ip = ""localhost"", port = 54321, startH2O = TRUE)

n = 20
y <- sample(1:3, size = n, replace = T)
xcat <- letters[y]
xnum <- sample(1:10, size = n, replace = T)
y <- dep + rnorm(0, 0.3, n = 20)

df <- data.frame(xcat=xcat, xnum=xnum , y=y)
df.hex <- as.h2o(df, destination_frame=""df.hex"")

#Encode as factor. You will get: a=1, b=2, c=3
df.hex[ , ""xcat""] = as.factor(df.hex[, ""xcat""])



Now I'll estimate it with an 
glm
 model and predict on the same sample:


x = c(""xcat"", ""xnum"")
glm <- h2o.glm( y = c(""y""), x = x, training_frame=df.hex, 
               family=""gaussian"", seed=1234)

glm.fit <- h2o.predict(object=glm, newdata=df.hex)



glm.fit
 gives the expected results (no surprises here).


Now I'll create a new test dataset that only has 
a
 and 
c
, no 
b
 value:


xcat2 = c(""c"", ""c"", ""a"")
xnum2 = c(2, 3, 1)
y = c(1, 2, 1) #not really needed
df.test = data.frame(xcat=xcat2, xnum=xnum2, y=y)
df.test.hex <- as.h2o(df.test, destination_frame=""df.test.hex"")
df.test.hex[ , ""xcat""] = as.factor(df.test.hex[, ""xcat""])



Running 
str(df.test.hex$xcat)
 shows that this time the factor encoding has assigned 
2
 to 
c
 and  
1
 to 
a
. This looked like it could be trouble, but then the fitting works as expected:


test.fit = h2o.predict(object=glm, newdata=df.test.hex)
test.fit
#gives 2.8, 2.79, 1.21  as expected



What's going on here? Is it that the 
glm
 model carries around the information of levels of the 
x
 variables so it doesn't mind if the internal encoding is different in the training and the new test data? Is that the general case for all 
h2o
 models?


From looking at one of the answers I linked above, it seems that at least some 
R
 models do require consistency.


Thanks and best!","['r', 'h2o', 'categorical-data']",Unknown,,N/A
47475848,47475848,2017-11-24T15:02:11,2017-11-26 03:58:21Z,0,"Code and results below. How is this possible? MSEs are identical with vastly different parameters.


hyper_params <- list(
        ntrees = c(5000, 7000, 10000),
        max_depth = c(15,18,21,24,27),
        min_rows = c(5,7,10,13,16),
        learn_rate = c(0.01,0.03,0.05,0.1),
        col_sample_rate_per_tree = c(0.3,0.5,0.7,0.85),
        min_split_improvement = c(10,20,30,40,50,60,70))

  search_criteria = list(strategy = ""RandomDiscrete"",
                         #stopping_rounds = 3,
                         max_models = num_models)#,
                         #stopping_tolerance=1e-3
  grid <- h2o.grid(
    algorithm = ""gbm"",
    grid_id = ""grid"",
    training_frame = train,
    # validation_frame = valid,
    x = independent_variables,
    y=""NSP"",
    stopping_metric=""MSE"",
    hyper_params = hyper_params,
    search_criteria = search_criteria,
    nfolds = 5,
    fold_assignment = ""Modulo"",
    keep_cross_validation_predictions = TRUE)

Hyper-Parameter Search Summary: ordered by increasing mse
   col_sample_rate_per_tree learn_rate max_depth min_rows min_split_improvement ntrees     model_ids                 mse
1                       0.5       0.01        24      5.0                  60.0   7000  grid_model_2 0.36927908175912655
2                      0.85       0.01        18      5.0                  40.0   5000  grid_model_6 0.36927908175912655
3                       0.3        0.1        24     13.0                  30.0   7000 grid_model_11 0.36927908175912655
4                      0.85       0.03        24     10.0                  50.0   5000  grid_model_4 0.36927908175912655
5                       0.5       0.01        21      5.0                  30.0   7000  grid_model_9 0.36927908175912655
6                      0.85        0.1        24     10.0                  70.0   5000 grid_model_13 0.36927908175912655
7                       0.7        0.1        15     13.0                  10.0  10000 grid_model_14 0.36927908175912655
8                       0.3       0.05        27     13.0                  20.0   7000  grid_model_5 0.36927908175912655
9                      0.85       0.05        27     13.0                  70.0   7000  grid_model_0 0.36927908175912655
10                      0.7       0.05        24     13.0                  60.0   5000  grid_model_7 0.36927908175912655
11                     0.85       0.05        21      7.0                  20.0  10000  grid_model_8 0.36927908175912655
12                      0.5       0.01        15      7.0                  20.0  10000 grid_model_10 0.36927908175912655
13                      0.3       0.01        18      7.0                  30.0   7000  grid_model_1 0.36927908175912655
14                      0.5       0.03        21     13.0                  10.0   5000 grid_model_12 0.36927908175912655
15                     0.85       0.01        21      7.0                  40.0   5000  grid_model_3 0.36927908175912655



Any and all guidance is appreciated.


Thank you.","['r', 'h2o', 'grid-search', 'gbm']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
47466620,47466620,2017-11-24T04:59:56,2017-11-27 02:05:33Z,0,"could someone please let me know if it is possible to specify methods to balance classes while using H2O's Auto ML function for classificatin problems? 


h2o.automl(x, y, training_frame, validation_frame = NULL,
leaderboard_frame = NULL, nfolds = 5, fold_column = NULL,
weights_column = NULL, max_runtime_secs = 3600, max_models = NULL,
stopping_metric = c(""AUTO"", ""deviance"", ""logloss"", ""MSE"", ""RMSE"", ""MAE"",
""RMSLE"", ""AUC"", ""lift_top_group"", ""misclassification"",
""mean_per_class_error""), stopping_tolerance = NULL, stopping_rounds = 3,
seed = NULL, project_name = NULL) 



The existing documentation doesn't seem to specify any parameters. Thanks.","['r', 'machine-learning', 'h2o']",elvikingo,https://stackoverflow.com/users/1823293/elvikingo,987
47445626,47445626,2017-11-22T23:59:52,2017-11-23 00:07:11Z,114,"Relatively new to ML and 
h2o
. Is there a way to do collaborative learning/training with 
h2o
? Would prefer a way that uses the 
flow UI
, else woud be using python. 


My use case is that there would be new feature samples x=[a, b, c, d] periodically coming into a system where an 
h2o
 algorithm (say, running from a java program using a 
MOJO
) assigns a binary class that users should be able to manually reclassify as either good(0) or bad(1), at which point these samples (with their newly assigned responses) get sent back to theh h2o algorithm to be used to further train it.


Thanks",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
47406405,47406405,2017-11-21T06:17:00,2018-01-02 21:31:11Z,255,"Installing and using H2O.ai's Flow UI is great and all - but has anyone tried to use the built models in popular BI tools like Qlik/PowerBI?


I've read a little bit on POJO/MOJO outputs, do these tools support them?",['h2o'],Ivan,https://stackoverflow.com/users/8977501/ivan,21
47404817,47404817,2017-11-21T03:43:32,2017-11-21 07:08:45Z,792,"I built a H2O (v. 3.14) GLM model.  However, when I check the predictions using h2o.predict, I got very different results based on 
how many rows
 I use in the validation set.


Calling h2o.predict on the first 10 rows, I got:


# Predict using the first 10 lines in validation set
h2o.predict(glm.test, df.valid[1:10,])
# Result:
  predict        p0           p1
1       0 0.9999224 7.756014e-05
2       0 0.9962711 3.728930e-03
3       0 0.9997378 2.622195e-04
4       0 0.9999556 4.437544e-05
5       0 0.9998994 1.006037e-04
6       0 0.9999394 6.062479e-05



But if I call h2o.predict on the first 
100
 rows, I got very different result.


h2o.predict(glm.test, df.valid[1:100,])
# Result:
  predict         p0        p1
1       1 0.06196439 0.9380356
2       1 0.15371122 0.8462888
3       1 0.01654756 0.9834524
4       1 0.12830090 0.8716991
5       1 0.07195659 0.9280434
6       1 0.09725532 0.9027447



I have posted the code which repro the problem.  The data set (which is very 
sparse
) can be downloaded from 
https://www.dropbox.com/s/58ul6zrekpmjh20/dt.truth.csv.gz


h2o.removeAll()

# Note: The zipped data file can be downloaded from:
#       https://www.dropbox.com/s/58ul6zrekpmjh20/dt.truth.csv.gz

df.truth <- h2o.importFile(
  path=""data/dt.truth.csv.gz"", sep="","", header=T)

df.truth$isTarget <- h2o.asfactor(df.truth$isTarget)

# Split into train / test
splits <- h2o.splitFrame(df.truth, c(0.7), seed=1234)
df.train <- h2o.assign(splits[[1]], ""df.train.hex"")   
df.valid <- h2o.assign(splits[[2]], ""df.valid.hex"")

# Build a GLM model
glm.test <- h2o.glm(         
  training_frame = df.train,        
  y=""isTarget"",                 
  family = ""binomial"",
  missing_values_handling = ""MeanImputation"",
  seed = 1000000) 

# Predict using the first 10 lines in validation set
h2o.predict(glm.test, df.valid[1:10,])

# Predict using the first 100 lines in validation set.  Got very different result!
h2o.predict(glm.test, df.valid[1:100,])",['h2o'],Unknown,,N/A
47390133,47390133,2017-11-20T10:43:50,2017-11-20 10:49:16Z,501,"I built a GLM model using H2O (ver 3.14) in R.  Please note that the training data contains integers, and also many NA, which I use MeanImputation to handle them.


glm <- h2o.glm(         
    training_frame = train.truth,        
    x=getColNames(train.truth),
    y=""isFemale"",                 
    family = ""binomial"",
    missing_values_handling = ""MeanImputation"",
    seed = 1000000) 



I then use a validation data set to look at the perf, and the Precision looks good to me:


h2o.performance(glm, newdata=valid.truth)%>% h2o.confusionMatrix()

Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.529384526696015:
           0     1    Error         Rate
0      41962   300 0.007099   =300/42262
1        863 13460 0.060253   =863/14323
Totals 42825 13760 0.020553  =1163/56585



I then saved the model as a MOJO:


h2o.download_mojo(glm, path=""models/mojo"", get_genmodel_jar=TRUE)



I exported the validation DF to a CSV file:


dt.valid <- data.table(as.data.frame(valid.truth))
write.table(dt.valid, row.names = F, na="""", file=""models/test.csv"")



I tried to use the saved mojo to do the same prediction by running this on my Linux shell:


java -cp h2o-genmodel.jar hex.genmodel.tools.PredictCsv \
    --mojo GLM_model_R_1511161743608_15 \
    --decimal --mojo GLM_model_R_1511161743608_15.zip \
    --input ../test.csv --output output.csv



However, the result is terrible.  All the records were predicted as 0, which is very different from what I got when I ran the model in R.


I have been stuck in this for a day but I couldn't figure out what went wrong.  Anyone can shed some light on this?",['h2o'],Unknown,,N/A
47376409,47376409,2017-11-19T11:40:55,2017-11-19 11:40:55Z,462,"I have a Keras model, in hdf5 format, and I would like to use it for 
making predictions with
, from inside H2O Deep Water.


(I am using the tensorflow backend on both Keras and Deep Water, and running the Docker cpu version of Deep Water, though I can switch to the gpu version if that is essential. Deep Water is using Tensorflow 1.1.)


I need to specify 
network_definition_file
 and 
network_parameters_file
. In section 6.5.2 of the 
Deep Water booklet pdf
 the former has a .meta extension, and the latter has no extension.


I found 
https://stackoverflow.com/a/44349081/841830
 which led me to 
https://github.com/amir-abdi/keras_to_tensorflow
. They appear to be taking a "".h5"" file and exporting a "".pb"" file. Can I use an "".hdf5"" file equally well? And is the "".pb"" file the same as the no-extension file shown in the Deep Water docs?


For the meta file I think I need to call the 
tf.train.export_meta_graph()
 function, giving it a 
tf.train.Saver()
 object. It looks like 
cell 7 of old version
  is where I could stick that?


The final thing I'm not sure on is how to just use it for prediction. Is the it same as normal H2O deep learning, and you use the 
epochs=0
 trick? If using the Python API, will I still need the call to 
train()
 or is creating the 
H2ODeepWaterEstimator()
 object enough?","['tensorflow', 'keras', 'h2o']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
47371242,47371242,2017-11-18T21:26:09,2017-11-18 21:59:31Z,759,"We have a Cloudera cluster up and running with an h2o instance although it appears to be running off h2o.jar (which as I understand it--please correct me if incorrect) is the stand-alone h2o.  I can connect, but it will not load any files from our HDFS. (all of this i can see via 'ps' on edge node.


So I started an instance with h2odriver.jar


java -jar /path/to/h2odriver.jar -nodes 2 -mapperXmx 5g -output /my/hdfs/dir


I get several output/callback addresses:


[Possible callback IP address: 10.96.243.46:33728]
[Possible callback IP address: 127.0.0.1]
Using mapper->driver callback IP address and port: 10.96.243.46:33728


So I fire up python and try and connect (same thing happens if I use 10.96.243.46):


>>>h2o.connection(ip='127.0.0.1', port='33728')


and get 


'Connecting to H2O server at http://127.0.0.1:33728..... failed.
H2OConnectionError: COuld not estalich link to the H2O cloud http://127.0.0.1:33728 after 5 retries
...
Failed to establish a new connection:[Errno 111] Connection refused',))`



Thing is on my screen with the H2O jar/java job I can see:


`MapperToDriverMessage: Read invalid type (G) from socket, ignoring...
MapperToDriverMessage: read: Unknown Type `



I cannot figure out how to launch h2o in cluster mode and have it access our hdfs system or even connect.  I can connect to the h2o.jar version, but that sees no hdfs (it can see the filesystem of the edgenode).  What is the proper way to launch H2O so that it can see the attached HDFS system (We are running Cloudera 5.7 in a enterprise environment, Python is 3.6, H2O is 3.10.0.6 and I know we have a ton of firewalls/security-- i beleive we are setup through LDAP","['hdfs', 'h2o']",RDS,https://stackoverflow.com/users/5656742/rds,516
47348556,47348556,2017-11-17T10:30:38,2017-11-17 14:21:22Z,0,"I am setting up for the first time Sparkling Water on a standalone cluster running spark 2.2. I have run Sparkling Water on such a cluster before via R (using rsparkling + sparklyr + h2o), but am having issues setting this up as a spark application (in scala).


The app is built with Maven, so I have added the latest sparkling water dependancy:


    <dependency>
        <groupId>ai.h2o</groupId>
        <artifactId>sparkling-water-core_2.11</artifactId>
        <version>2.2.2</version>
    </dependency>



Then the app code is as follows:


package com.me.app

import org.apache.spark.sql.{DataFrame, SparkSession}        
import org.apache.spark.h2o._
import water.Key
import water.fvec.Frame

object sparklingWaterH2o {

  def sparklingWaterH2o(): Unit = {

    val sparkSession = SparkSession
      .builder()
      .master(""spark://clsuter.address:0077"")
      .appName(""sparklingWaterH2o"")
      .config(""spark.executor.memory"", ""32G"")
      .config(""spark.executor.cores"", ""5"")
      .config(""spark.cores.max"", ""40"")
      .config(""spark.ext.h2o.nthreads"", ""40"")
      .config(""spark.jars"", ""/path/to/fat/jar/app-1.0-SNAPSHOT-jar-with-dependencies.jar"")
      .getOrCreate()

    val h2oContext = H2OContext.getOrCreate(sparkSession)

    import h2oContext._

    val df = Seq(
      (1, ""2014/07/31 23:00:01""),
      (1, ""2016/12/09 10:12:43"")).toDF(""id"", ""date"")

    val h2oTrainFrame = h2oContext.asH2OFrame(df)

    println(s""h2oContext = ${h2oContext.toString()}"")



I then compile the fat jar to send to the cluster, however the h2oContext never gets created and the SparkContext gets shut down with 
exit code 255
. The app exiting with no error codes before an h2o context is created - the only potentially useful message is 
IP address not found on this machine
. 


I've tried this with Sparkling Water version 2.2.0 and get the same issues, also tried adding dependencies for 
sparkling-water-ml
 and 
sparkling-water-repl
, as well as adding all the h2o core dependencies (though assuming these are not needed as they are integrated into sparkling water?). See log file below. 


objc[39611]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/bin/java (0x10ab4b4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10bb724e0). One of the two will be used. Which one is undefined.
Usinrg Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/Users/username/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/Users/username/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/11/17 10:16:01 INFO SparkContext: Running Spark version 2.2.0
17/11/17 10:16:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/11/17 10:16:02 INFO SparkContext: Submitted application: sparklingWaterH2o
17/11/17 10:16:02 INFO SecurityManager: Changing view acls to: username
17/11/17 10:16:02 INFO SecurityManager: Changing modify acls to: username
17/11/17 10:16:02 INFO SecurityManager: Changing view acls groups to: 
17/11/17 10:16:02 INFO SecurityManager: Changing modify acls groups to: 
17/11/17 10:16:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(username); groups with view permissions: Set(); users  with modify permissions: Set(username); groups with modify permissions: Set()
17/11/17 10:16:03 INFO Utils: Successfully started service 'sparkDriver' on port 53775.
17/11/17 10:16:03 INFO SparkEnv: Registering MapOutputTracker
17/11/17 10:16:03 INFO SparkEnv: Registering BlockManagerMaster
17/11/17 10:16:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/11/17 10:16:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/11/17 10:16:03 INFO DiskBlockManager: Created local directory at /private/var/folders/gl/vgw262w9227cwqvzk595rbvjygdzh8/T/blockmgr-d29de5c5-9116-4abf-812c-04ca680781fe
17/11/17 10:16:03 INFO MemoryStore: MemoryStore started with capacity 1002.3 MB
17/11/17 10:16:03 INFO SparkEnv: Registering OutputCommitCoordinator
17/11/17 10:16:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/11/17 10:16:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.103.46:4040
17/11/17 10:16:03 INFO SparkContext: Added JAR /path/to/app/target/app-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://192.168.103.46:53775/jars/app-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1510913763424
17/11/17 10:16:03 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://rnd-centos7-ben-31.nominet.org.uk:7077...
17/11/17 10:16:03 INFO TransportClientFactory: Successfully created connection to rnd-centos7-ben-31.nominet.org.uk/XXX.XXX.211.31:7077 after 26 ms (0 ms spent in bootstraps)
17/11/17 10:16:03 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20171117101603-0031
17/11/17 10:16:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171117101603-0031/0 on worker-20171013100055-XXX.XXX.211.30-33565 (XXX.XXX.211.30:33565) with 5 cores
17/11/17 10:16:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20171117101603-0031/0 on hostPort XXX.XXX.211.30:33565 with 5 cores, 32.0 GB RAM
17/11/17 10:16:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171117101603-0031/1 on worker-20171013100055-XXX.XXX.211.33-34424 (XXX.XXX.211.33:34424) with 5 cores
17/11/17 10:16:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20171117101603-0031/1 on hostPort XXX.XXX.211.33:34424 with 5 cores, 32.0 GB RAM
17/11/17 10:16:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171117101603-0031/2 on worker-20171013100055-XXX.XXX.211.31-37513 (XXX.XXX.211.31:37513) with 5 cores
17/11/17 10:16:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20171117101603-0031/2 on hostPort XXX.XXX.211.31:37513 with 5 cores, 32.0 GB RAM
17/11/17 10:16:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171117101603-0031/3 on worker-20171013100054-XXX.XXX.211.32-36797 (XXX.XXX.211.32:36797) with 5 cores
17/11/17 10:16:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20171117101603-0031/3 on hostPort XXX.XXX.211.32:36797 with 5 cores, 32.0 GB RAM
17/11/17 10:16:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171117101603-0031/2 is now RUNNING
17/11/17 10:16:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171117101603-0031/1 is now RUNNING
17/11/17 10:16:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171117101603-0031/3 is now RUNNING
17/11/17 10:16:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171117101603-0031/0 is now RUNNING
17/11/17 10:16:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53777.
17/11/17 10:16:03 INFO NettyBlockTransferService: Server created on 192.168.103.46:53777
17/11/17 10:16:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/11/17 10:16:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.103.46, 53777, None)
17/11/17 10:16:03 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.103.46:53777 with 1002.3 MB RAM, BlockManagerId(driver, 192.168.103.46, 53777, None)
17/11/17 10:16:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.103.46, 53777, None)
17/11/17 10:16:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.103.46, 53777, None)
17/11/17 10:16:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (XXX.XXX.211.31:46906) with ID 2
17/11/17 10:16:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (XXX.XXX.211.30:54738) with ID 0
17/11/17 10:16:05 INFO BlockManagerMasterEndpoint: Registering block manager XXX.XXX.211.31:45376 with 8.4 GB RAM, BlockManagerId(2, XXX.XXX.211.31, 45376, None)
17/11/17 10:16:05 INFO BlockManagerMasterEndpoint: Registering block manager XXX.XXX.211.30:34172 with 8.4 GB RAM, BlockManagerId(0, XXX.XXX.211.30, 34172, None)
17/11/17 10:16:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (XXX.XXX.211.32:53076) with ID 3
17/11/17 10:16:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (XXX.XXX.211.33:47478) with ID 1
17/11/17 10:16:05 INFO BlockManagerMasterEndpoint: Registering block manager XXX.XXX.211.32:34360 with 8.4 GB RAM, BlockManagerId(3, XXX.XXX.211.32, 34360, None)
17/11/17 10:16:05 INFO BlockManagerMasterEndpoint: Registering block manager XXX.XXX.211.33:34342 with 8.4 GB RAM, BlockManagerId(1, XXX.XXX.211.33, 34342, None)
17/11/17 10:16:33 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
17/11/17 10:16:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/path/to/app/spark-warehouse/').
17/11/17 10:16:33 INFO SharedState: Warehouse path is 'file:/path/to/app/spark-warehouse/'.
17/11/17 10:16:34 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
17/11/17 10:16:34 WARN InternalH2OBackend: Increasing 'spark.locality.wait' to value 30000
17/11/17 10:16:34 WARN InternalH2OBackend: Due to non-deterministic behavior of Spark broadcast-based joins
We recommend to disable them by
configuring `spark.sql.autoBroadcastJoinThreshold` variable to value `-1`:
sqlContext.sql(""SET spark.sql.autoBroadcastJoinThreshold=-1"")
17/11/17 10:16:34 INFO InternalH2OBackend: Starting H2O services: Sparkling Water configuration:
  backend cluster mode : internal
  workers              : None
  cloudName            : sparkling-water-username_app-20171117101603-0031
  flatfile             : true
  clientBasePort       : 54321
  nodeBasePort         : 54321
  cloudTimeout         : 60000
  h2oNodeLog           : INFO
  h2oClientLog         : WARN
  nthreads             : 40
  drddMulFactor        : 10
17/11/17 10:16:34 INFO SparkContext: Starting job: collect at SpreadRDDBuilder.scala:105
17/11/17 10:16:34 INFO DAGScheduler: Got job 0 (collect at SpreadRDDBuilder.scala:105) with 41 output partitions
17/11/17 10:16:34 INFO DAGScheduler: Final stage: ResultStage 0 (collect at SpreadRDDBuilder.scala:105)
17/11/17 10:16:34 INFO DAGScheduler: Parents of final stage: List()
17/11/17 10:16:34 INFO DAGScheduler: Missing parents: List()
17/11/17 10:16:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at mapPartitionsWithIndex at SpreadRDDBuilder.scala:102), which has no missing parents
17/11/17 10:16:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 1002.3 MB)
17/11/17 10:16:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1379.0 B, free 1002.3 MB)
17/11/17 10:16:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.103.46:53777 (size: 1379.0 B, free: 1002.3 MB)
17/11/17 10:16:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/11/17 10:16:34 INFO DAGScheduler: Submitting 41 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at mapPartitionsWithIndex at SpreadRDDBuilder.scala:102) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
17/11/17 10:16:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 41 tasks
17/11/17 10:16:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, XXX.XXX.211.31, executor 2, partition 0, PROCESS_LOCAL, 4829 bytes)
17/11/17 10:16:34 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, XXX.XXX.211.30, executor 0, partition 1, PROCESS_LOCAL, 4829 bytes)
...
17/11/17 10:16:34 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19, XXX.XXX.211.33, executor 1, partition 19, PROCESS_LOCAL, 4829 bytes)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on XXX.XXX.211.30:34172 (size: 1379.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on XXX.XXX.211.32:34360 (size: 1379.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on XXX.XXX.211.33:34342 (size: 1379.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on XXX.XXX.211.31:45376 (size: 1379.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added rdd_0_13 in memory on XXX.XXX.211.30:34172 (size: 32.0 B, free: 8.4 GB)
...
17/11/17 10:16:43 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 29 ms on XXX.XXX.211.33 (executor 1) (41/41)
17/11/17 10:16:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/11/17 10:16:43 INFO DAGScheduler: ResultStage 0 (collect at SpreadRDDBuilder.scala:105) finished in 8.913 s
17/11/17 10:16:43 INFO DAGScheduler: Job 0 finished: collect at SpreadRDDBuilder.scala:105, took 9.072610 s
17/11/17 10:16:43 INFO ParallelCollectionRDD: Removing RDD 0 from persistence list
17/11/17 10:16:43 INFO BlockManager: Removing RDD 0
17/11/17 10:16:43 INFO SpreadRDDBuilder: Detected 4 spark executors for 4 H2O workers!
17/11/17 10:16:43 INFO InternalH2OBackend: Launching H2O on following 4 nodes: (0,XXX.XXX.211.30,-1),(1,XXX.XXX.211.33,-1),(2,XXX.XXX.211.31,-1),(3,XXX.XXX.211.32,-1)
17/11/17 10:16:43 INFO SparkContext: Starting job: collect at InternalBackendUtils.scala:163
17/11/17 10:16:43 INFO DAGScheduler: Got job 1 (collect at InternalBackendUtils.scala:163) with 4 output partitions
17/11/17 10:16:43 INFO DAGScheduler: Final stage: ResultStage 1 (collect at InternalBackendUtils.scala:163)
17/11/17 10:16:43 INFO DAGScheduler: Parents of final stage: List()
17/11/17 10:16:43 INFO DAGScheduler: Missing parents: List()
17/11/17 10:16:43 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at map at InternalBackendUtils.scala:100), which has no missing parents
17/11/17 10:16:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 1002.3 MB)
17/11/17 10:16:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2029.0 B, free 1002.3 MB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.103.46:53777 (size: 2029.0 B, free: 1002.3 MB)
17/11/17 10:16:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
17/11/17 10:16:43 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at InternalBackendUtils.scala:100) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
17/11/17 10:16:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks
17/11/17 10:16:43 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 41, XXX.XXX.211.31, executor 2, partition 2, NODE_LOCAL, 4821 bytes)
17/11/17 10:16:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 42, XXX.XXX.211.30, executor 0, partition 0, NODE_LOCAL, 4821 bytes)
17/11/17 10:16:43 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 43, XXX.XXX.211.32, executor 3, partition 3, NODE_LOCAL, 4821 bytes)
17/11/17 10:16:43 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 44, XXX.XXX.211.33, executor 1, partition 1, NODE_LOCAL, 4821 bytes)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on XXX.XXX.211.30:34172 (size: 2029.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on XXX.XXX.211.31:45376 (size: 2029.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on XXX.XXX.211.33:34342 (size: 2029.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on XXX.XXX.211.32:34360 (size: 2029.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 42) in 349 ms on XXX.XXX.211.30 (executor 0) (1/4)
17/11/17 10:16:43 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 41) in 358 ms on XXX.XXX.211.31 (executor 2) (2/4)
17/11/17 10:16:43 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 43) in 394 ms on XXX.XXX.211.32 (executor 3) (3/4)
17/11/17 10:16:43 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 44) in 408 ms on XXX.XXX.211.33 (executor 1) (4/4)
17/11/17 10:16:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/11/17 10:16:43 INFO DAGScheduler: ResultStage 1 (collect at InternalBackendUtils.scala:163) finished in 0.411 s
17/11/17 10:16:43 INFO DAGScheduler: Job 1 finished: collect at InternalBackendUtils.scala:163, took 0.428038 s
17/11/17 10:16:43 INFO SparkContext: Starting job: foreach at InternalBackendUtils.scala:175
17/11/17 10:16:43 INFO DAGScheduler: Got job 2 (foreach at InternalBackendUtils.scala:175) with 4 output partitions
17/11/17 10:16:43 INFO DAGScheduler: Final stage: ResultStage 2 (foreach at InternalBackendUtils.scala:175)
17/11/17 10:16:43 INFO DAGScheduler: Parents of final stage: List()
17/11/17 10:16:43 INFO DAGScheduler: Missing parents: List()
17/11/17 10:16:43 INFO DAGScheduler: Submitting ResultStage 2 (InvokeOnNodesRDD[2] at RDD at InvokeOnNodesRDD.scala:27), which has no missing parents
17/11/17 10:16:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 1832.0 B, free 1002.3 MB)
17/11/17 10:16:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1209.0 B, free 1002.3 MB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.103.46:53777 (size: 1209.0 B, free: 1002.3 MB)
17/11/17 10:16:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/11/17 10:16:43 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 2 (InvokeOnNodesRDD[2] at RDD at InvokeOnNodesRDD.scala:27) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
17/11/17 10:16:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks
17/11/17 10:16:43 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 45, XXX.XXX.211.31, executor 2, partition 2, NODE_LOCAL, 4821 bytes)
17/11/17 10:16:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 46, XXX.XXX.211.30, executor 0, partition 0, NODE_LOCAL, 4821 bytes)
17/11/17 10:16:43 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 47, XXX.XXX.211.32, executor 3, partition 3, NODE_LOCAL, 4821 bytes)
17/11/17 10:16:43 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 48, XXX.XXX.211.33, executor 1, partition 1, NODE_LOCAL, 4821 bytes)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on XXX.XXX.211.31:45376 (size: 1209.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on XXX.XXX.211.33:34342 (size: 1209.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on XXX.XXX.211.32:34360 (size: 1209.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on XXX.XXX.211.30:34172 (size: 1209.0 B, free: 8.4 GB)
17/11/17 10:16:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 46) in 28 ms on XXX.XXX.211.30 (executor 0) (1/4)
17/11/17 10:16:43 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 48) in 28 ms on XXX.XXX.211.33 (executor 1) (2/4)
17/11/17 10:16:43 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 45) in 30 ms on XXX.XXX.211.31 (executor 2) (3/4)
17/11/17 10:16:43 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 47) in 32 ms on XXX.XXX.211.32 (executor 3) (4/4)
17/11/17 10:16:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/11/17 10:16:43 INFO DAGScheduler: ResultStage 2 (foreach at InternalBackendUtils.scala:175) finished in 0.034 s
17/11/17 10:16:43 INFO DAGScheduler: Job 2 finished: foreach at InternalBackendUtils.scala:175, took 0.043737 s
17/11/17 10:16:43 INFO InternalH2OBackend: Starting H2O client on the Spark Driver (192.168.103.46): -name sparkling-water-username_app-20171117101603-0031 -nthreads 40 -ga_opt_out -quiet -log_level WARN -log_dir /path/to/app/h2ologs/app-20171117101603-0031 -baseport 54321 -client -ip 192.168.103.46 -flatfile /var/folders/gl/vgw262w9227cwqvzk595rbvjygdzh8/T/1510913803950-0/flatfile.txt
17/11/17 10:16:44 INFO NativeLibrary: Loaded XGBoost library from lib/osx_64/libxgboost4j.dylib (/var/folders/gl/vgw262w9227cwqvzk595rbvjygdzh8/T/libxgboost4j2584224510491657515.dylib)
Found XGBoost backend with library: xgboost4j
Your system supports only minimal version of XGBoost (no GPUs, no multithreading)!
IP address not found on this machine
17/11/17 10:16:45 INFO SparkContext: Invoking stop() from shutdown hook
17/11/17 10:16:45 INFO SparkUI: Stopped Spark web UI at http://192.168.103.46:4040
17/11/17 10:16:45 INFO StandaloneSchedulerBackend: Shutting down all executors
17/11/17 10:16:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
17/11/17 10:16:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/11/17 10:16:45 INFO MemoryStore: MemoryStore cleared
17/11/17 10:16:45 INFO BlockManager: BlockManager stopped
17/11/17 10:16:45 INFO BlockManagerMaster: BlockManagerMaster stopped
17/11/17 10:16:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/11/17 10:16:45 INFO SparkContext: Successfully stopped SparkContext
17/11/17 10:16:45 INFO ShutdownHookManager: Shutdown hook called
17/11/17 10:16:45 INFO ShutdownHookManager: Deleting directory /private/var/folders/gl/vgw262w9227cwqvzk595rbvjygdzh8/T/spark-51594e29-1ea0-4a4d-9aa0-dd65ef5146dd","['apache-spark', 'h2o', 'sparkling-water']",renegademonkey,https://stackoverflow.com/users/7469564/renegademonkey,467
47334892,47334892,2017-11-16T16:38:26,2017-11-16 22:51:30Z,0,"I need to remove duplicated rows in an 
H2O.Frame
 object.


With a 
data.frame
 
df
 in R I would use


df <- df[!duplicated(df), ]



What is the equivalent in H2O? Thank you.","['r', 'dataframe', 'duplicates', 'h2o']",mac,https://stackoverflow.com/users/7946651/mac,165
47321547,47321547,2017-11-16T04:20:31,2017-11-16 16:28:15Z,855,"I am new to H2O. So far for the train-test split I have used the StratifiedKFold() of sklearn.


skf = StratifiedKFold(n_splits=n, random_state=None, shuffle=False)
for train_index, test_index in skf.split(X, y):               
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]



I need the indexes for some further processing later.


In H2O I can't figure out how to get the indexes while doing cross validation. From what I have gathered via videos and blogs, this is how we do CV in H2O:


gbm_model = H2OGradientBoostingEstimator(model_id = 'gbm_model',nfolds=5)



How do I get the train and test indexes of each fold?


Also, how do I get the indexes while doing a simple split?


data_split = data.split_frame(ratios=[0.8],seed = 1234)
train_df = data_split[0]
test_df = data_split[1]



How do I get the indexes that went into train and test?","['python', 'dataframe', 'indexing', 'h2o']",rj dj,https://stackoverflow.com/users/6738440/rj-dj,290
47310212,47310212,2017-11-15T14:38:57,2017-11-15 16:15:38Z,210,"I have done a few experiments on h2o driverless AI. Was curious to know whether we can explicitly ask the experiment to train using GLM.


Or it will always make model the way it does?","['glm', 'h2o', 'driverless-ai']",IYY,https://stackoverflow.com/users/7031039/iyy,273
47274555,47274555,2017-11-13T22:22:25,2017-11-16 19:57:04Z,0,"I have moderate experience with data science. I have a data set with 9500 observations and more than 4500 features most of which are highly correlated. Here is briefly what I have tried: I have dropped columns where there are less than 6000 non-NAs and have imputed NAs with their corresponding columns' median values when there are at least 6000 non-NAs. As for correlation, I have kept only features having at most 0.7 correlation with others. By doing so, I have reduced the number of features to about 750. Then I have used those features in my binary classification task in random forest. 


My data set is highly unbalanced where ratio of (0:1) is (10:1). So when I apply RF with 10-fold cv, I observe too good results in each cv (AUC of 99%) which is to good to be true and in my test set I got way worse results such as 0.7. Here is my code:


import h2o
from h2o.estimators import H2ORandomForestEstimator

h2o.init(port=23, nthreads=4)

train = fs_rf[fs_rf['Year'] <= '201705']
test = fs_rf[fs_rf['Year'] > '201705']
train = train.drop('Year',axis=1)
test = test.drop('Year',axis=1)
test.head()

train = h2o.H2OFrame(train)
train['BestWorst2'] = train['BestWorst2'].asfactor()

test = h2o.H2OFrame(test)
test['BestWorst2'] = test['BestWorst2'].asfactor()

training_columns = train.drop('BestWorst2',axis=1).col_names
response_column = 'BestWorst2'

model = H2ORandomForestEstimator(ntrees=100, max_depth=20, nfolds=10, balance_classes=True)

model.train(x=training_columns, y=response_column, training_frame=train)

performance = model.model_performance(test_data=test)

print(performance)



How could I avoid this over-fitting? I have tried many different parameters in grid search but none of them improved the results.","['python-2.7', 'random-forest', 'h2o']",mlee_jordan,https://stackoverflow.com/users/3198674/mlee-jordan,842
47238248,47238248,2017-11-11T13:11:08,2017-11-15 13:30:25Z,313,"I am new to machine learning, doing prediction on an Imbalanced dataset using random forest algorithm. I built the model in R, the response variable is a binary categorical one (0,1). The random forest model built in R produces a proper classification, but when this model is been converted to h2o pojo for building an application, the model only returns ""1"" as the response.","['machine-learning', 'pojo', 'h2o']",desertnaut,https://stackoverflow.com/users/4685471/desertnaut,60.1k
47226254,47226254,2017-11-10T15:33:06,2017-11-14 19:18:59Z,0,"I am using H2O (basic version) and it works well. I want to try Deep Water for GPU support. So, I carefully followed the instruction on;

https://www.h2o.ai/deep-water/#try

to install Deep Water. However, it failed to run and showed this error:




Error in h2o.init(nthreads = -1, port = 54323, startH2O = FALSE) :
        Version mismatch! H2O is running version 3.15.0.393 but h2o-R package is version 3.13.0.369.
               Install the matching h2o-R version from - 
http://h2o-release.s3.amazonaws.com/h2o/(HEAD
 detached at c46596cad) 




Where do I get the right version?",['h2o'],Aleks Andreev,https://stackoverflow.com/users/4685428/aleks-andreev,"7,044"
47222985,47222985,2017-11-10T12:39:10,2017-11-10 20:48:43Z,31,"We are using H2O version 3.14.0.3 with Flow (v. 0.7.7) on a Hadoop Cluster with three Nodes. In the flow web interface there are some model defaults (especially for Deep Learning) we adjust every time (e.g. lowering Epochs to 0.1). If we do not adjust the parameter the training takes hours due to the volume of our datasets.


Would I be able to configure the default parameter settings?",['h2o'],Sebastian Hätälä,https://stackoverflow.com/users/2407819/sebastian-h%c3%a4t%c3%a4l%c3%a4,"1,035"
47211420,47211420,2017-11-09T20:58:29,2017-11-12 06:42:00Z,0,"Using 
h2o flow
, is there a way to create a stacked ensemble model based on individual models that may not take the same inputs but predict on the same response labels.


Eg. I am trying to predict for miscoded healthcare claims (ie. charges) and would like to train models for a stacked ensemble of the form:


model1(diagnosis1, diagnosis2, ..., diagnosis5) -> denied or paid (by insurer)
model2(procedure, procedure_detail1, ..., procedure_detail5) -> denied or paid 
model3(service_date, insurance_amount, insurer_id) -> (same)
model4(pat_age, pat_sex, ...) -> (same)
...



Is there a way to do this in 
h2o flow
 (can't tell how to do this with what is presented in the 
h2o flow
 gui for stacked ensemble)? Is this even a sensible way to go about this or is it confused in some way (relatively new to machine learning)? Thanks.","['machine-learning', 'h2o', 'medical']",lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
47201028,47201028,2017-11-09T11:45:26,2017-11-09 12:16:02Z,165,"I am running 
Sparkling water
over 
36 Spark executors
.
Due to Yarn's scheduling, some executors would preempt and comeback later.
Overall, there are 
36 executors
 for the majority of time, just not always.


So far, my experience is that, as soon as 
1 executor
 fails, the entire 
H2o
 instance halts, even if the missing executor comes back to life later.
I wonder if this is how 
Sparkling-water
behaves? Or some preemptive capability needs to be turned on?


Anyone have a clue about this ?","['h2o', 'sparkling-water']",Unknown,,N/A
47158520,47158520,2017-11-07T12:52:52,2017-11-09 00:48:48Z,595,"I would like to plot the decision boundaries of an H20 Random Forest model in Python like so:




All the examples I have found so far has been done with scikit learn.","['python-3.x', 'h2o']",phuclv,https://stackoverflow.com/users/995714/phuclv,41.4k
47155492,47155492,2017-11-07T10:24:37,2017-11-07 16:57:44Z,0,"I am using h2o binomial prediction and converting few string columns like this


X2 <- as.numeric(as.factor(test$X2))
X3 <- as.numeric(as.factor(test$X3))
X4 <- as.numeric(as.factor(test$X4))



and I generated h2o java pojo class for binomial model for Rest API call.


so, how do I convert my 2 string columns to required format in java. Since, the h2o team mentioned like all conversion should be done before h20 api call.","['java', 'r', 'machine-learning', 'deep-learning', 'h2o']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
47140630,47140630,2017-11-06T15:43:12,2021-05-22 18:32:22Z,0,"I updated my h2o to the newest version and then tried to load the pre-trained model by typing:


randomforest = h2o.loadModel('randomforest')



However, it shows:


 Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
 ERROR MESSAGE:
 Found version 3.10.5.3, but running version 3.14.0.7



Does that mean I need to retrained all the model I built before ? That's extremely inconvenient.","['r', 'h2o']",mega6382,https://stackoverflow.com/users/6998123/mega6382,"9,396"
47140334,47140334,2017-11-06T15:27:15,2017-11-07 23:09:29Z,610,"I want to use H2O Autoencoder (Anomaly Detection) for Inference / Prediction in a Java class.


I built the autoencoder example ""ECG Hearbeats"" from H2O DeepLearningBooklet with R and saved it. I can succesfully import the generated Java class and its related h2o-genmodel.jar into my Java project.


Unfortunately, I cannot find an example or documentation how to use it there. 


Here is my first try with some code and some guesses from my experience with other H2O models used for inference in Java code:


private static String modelClassName = ""machinelearning.DeepLearning_model_R_1509973865970_1"";

public static void main(String[] args) throws Exception {

    hex.genmodel.GenModel rawModel;
    rawModel = (hex.genmodel.GenModel) Class.forName(modelClassName).newInstance();
    EasyPredictModelWrapper model = new EasyPredictModelWrapper(rawModel);

    RowData row = new RowData();
    // row.put(key, value); // TODO Add new line of input data, e.g.:
    // 2.10,2.13,2.19,2.28,2.44,2.62,2.80,3.04,3.36,3.69,3.97,4.24,4.53,4.80,5.02,5.21,5.40,5.57,5.71,5.79,5.86,5.92,5.98,6.02,6.06,6.08,6.14,6.18,6.22,6.27,6.32,6.35,6.38,6.45,6.49,6.53,6.57,6.64,6.70,6.73,6.78,6.83,6.88,6.92,6.94,6.98,7.01,7.03,7.05,7.06,7.07,7.08,7.06,7.04,7.03,6.99,6.94,6.88,6.83,6.77,6.69,6.60,6.53,6.45,6.36,6.27,6.19,6.11,6.03,5.94,5.88,5.81,5.75,5.68,5.62,5.61,5.54,5.49,5.45,5.42,5.38,5.34,5.31,5.30,5.29,5.26,5.23,5.23,5.22,5.20,5.19,5.18,5.19,5.17,5.15,5.14,5.17,5.16,5.15,5.15,5.15,5.14,5.14,5.14,5.15,5.14,5.14,5.13,5.15,5.15,5.15,5.14,5.16,5.15,5.15,5.14,5.14,5.15,5.15,5.14,5.13,5.14,5.14,5.11,5.12,5.12,5.12,5.09,5.09,5.09,5.10,5.08,5.08,5.08,5.08,5.06,5.05,5.06,5.07,5.05,5.03,5.03,5.04,5.03,5.01,5.01,5.02,5.01,5.01,5.00,5.00,5.02,5.01,4.98,5.00,5.00,5.00,4.99,5.00,5.01,5.02,5.01,5.03,5.03,5.02,5.02,5.04,5.04,5.04,5.02,5.02,5.01,4.99,4.98,4.96,4.96,4.96,4.94,4.93,4.93,4.93,4.93,4.93,5.02,5.27,5.80,5.94,5.58,5.39,5.32,5.25,5.21,5.13,4.97,4.71,4.39,4.05,3.69,3.32,3.05,2.99,2.74,2.61,2.47,2.35,2.26,2.20,2.15,2.10,2.08

    AutoEncoderModelPrediction p = model.predictAutoEncoder(row);

    System.out.println(p.reconstructedRowData);
    System.out.println(p.reconstructed[0]);
    // TODO How to do get the MSE from object 'p'? 



This code actually compiles and runs. However, I do not really understand how to 




configure EasyPredictModelWrapper correctly (or do I just need to instantiate it?)


add a new single event for prediction as I do not have keys, but only values for the ECG Heartbeats. (I assume the method row.putAll is the best to the various features of the ECG data set ?!)


get the MSE out of the prediction (similar to what we see in the R / Python example with 'recon_error <- as.data.frame(recon_error))' where you can see the reconstruction error for all 23 ECG lines? (then I can define a threshold, and build a real time application which creates an alert if a threshold is reached)




I assume the answer is simple, but without documentation not easy to find :-)


Thanks for help.","['java', 'deep-learning', 'h2o', 'autoencoder', 'anomaly-detection']",Unknown,,N/A
47140224,47140224,2017-11-06T15:22:06,2017-11-12 11:25:15Z,0,"I know you can generate Java POJOs from H2O models built in Python or R, but is there a way to build C# dlls or something similar that can run in a .NET environment? I have seen references to this but nothing concrete.","['c#', 'r', 'h2o']",H Zumaeta,https://stackoverflow.com/users/7710042/h-zumaeta,33
47129613,47129613,2017-11-06T04:10:20,2017-11-08 12:18:27Z,616,"I get below exception in my code when I try to create a h2o contetx by Spark 1.6.3


17/11/06 12:01:39 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[H2O Launcher thread,5,main]
java.lang.NoSuchMethodError: org.joda.time.DateTime.now()Lorg/joda/time/DateTime;
        at water.util.Timer.nowAsLogString(Timer.java:38)
        at water.util.Log.header(Log.java:163)
        at water.util.Log.write0(Log.java:131)
        at water.util.Log.write0(Log.java:124)
        at water.util.Log.write(Log.java:109)
        at water.util.Log.log(Log.java:86)
        at water.util.Log.info(Log.java:72)
        at water.H2OSecurityManager.<init>(H2OSecurityManager.java:57)
        at water.H2OSecurityManager.instance(H2OSecurityManager.java:79)
        at water.H2ONode.<init>(H2ONode.java:127)



EDIT: I have attached the POM file, it is a long file but it shows the dependency. I think there shoud be something wrong with my dependencies.


<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
    <modelVersion>4.0.0</modelVersion>
    <groupId>au.com.vroc.mdm</groupId>
    <artifactId>mdm</artifactId>
    <version>0.0.1-SNAPSHOT</version>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <java.version>1.8</java.version>
        <gson.version>2.8.0</gson.version>
        <java.home>${env.JAVA_HOME}</java.home>
    </properties>

    <dependencies>

        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.10 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql_2.10 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_2.11</artifactId>
            <version>2.1.1</version>
            <!-- <scope>provided</scope> -->
        </dependency>
        <dependency>
            <groupId>com.databricks</groupId>
            <artifactId>spark-csv_2.10</artifactId>
            <version>1.5.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/ai.h2o/h2o-core -->
        <dependency>
            <groupId>ai.h2o</groupId>
            <artifactId>h2o-core</artifactId>
            <version>3.14.0.7</version>
            <!-- <scope>runtime</scope> -->
        </dependency>
        <!-- https://mvnrepository.com/artifact/ai.h2o/h2o-algos -->
        <dependency>
            <groupId>ai.h2o</groupId>
            <artifactId>h2o-algos</artifactId>
            <version>3.14.0.7</version>
            <!-- <scope>runtime</scope> -->
        </dependency>
        <!-- https://mvnrepository.com/artifact/ai.h2o/h2o-genmodel -->
        <dependency>
            <groupId>ai.h2o</groupId>
            <artifactId>h2o-genmodel</artifactId>
            <version>3.14.0.7</version>
            <!-- <scope>runtime</scope> -->
        </dependency>
        <!-- https://mvnrepository.com/artifact/ai.h2o/sparkling-water-core_2.10 -->
        <dependency>
            <!-- <groupId>ai.h2o</groupId> <artifactId>sparkling-water-core_2.10</artifactId> 
                <version>1.6.11</version> -->

            <groupId>ai.h2o</groupId>
            <artifactId>sparkling-water-core_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>com.google.code.gson</groupId>
            <artifactId>gson</artifactId>
            <version>${gson.version}</version>
        </dependency>
        <dependency>
            <groupId>com.cloudera.livy</groupId>
            <artifactId>livy-client-http</artifactId>
            <version>0.3.0</version>
        </dependency>
        <dependency>
            <groupId>com.cloudera.livy</groupId>
            <artifactId>livy-api</artifactId>
            <version>0.3.0</version>
        </dependency>
        <dependency>
            <groupId>it.unimi.dsi</groupId>
            <artifactId>fastutil</artifactId>
            <version>7.1.0</version>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>2.11.5</version>
        </dependency>
        <!-- <dependency> <groupId>jdk.tools</groupId> <artifactId>jdk.tools</artifactId> 
            <scope>system</scope> <version>1.8</version> <systemPath>${java.home}/lib/tools.jar</systemPath> 
            </dependency> -->
        <!-- https://mvnrepository.com/artifact/joda-time/joda-time -->
        <dependency>
            <groupId>org.apache.phoenix</groupId>
            <artifactId>phoenix-spark</artifactId>
            <version>4.7.0-HBase-1.1</version>
            <!-- <scope>provided</scope> -->
        </dependency>

        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-exec</artifactId>
            <version>1.1.0-cdh5.4.0</version>
        </dependency>
    </dependencies>

    <repositories>
        <repository>
            <id>cloudera.repo</id>
            <url>https://repository.cloudera.com/artifactory/cloudera-repos</url>
            <name>Cloudera Repositories</name>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </repository>
        <repository>
            <id>Local repository</id>
            <url>file://${basedir}/lib</url>
        </repository>

    </repositories>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.6.1</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                    <encoding>UTF-8</encoding>
                </configuration>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>2.5.2</version>
                <!-- <version>3.0.0</version> -->
                <configuration>
                    <!-- get all project dependencies -->
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <!--<id>assemble-all</id> -->
                        <!-- bind to the packaging phase -->
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>



Creating of model is simply done by livyclient as follow:


public RegressionMetric call(JobContext ctx) throws Exception {
    if (!checkInputValid()) {
        throw new IllegalArgumentException(""Mandatory parameters are not set"");
    } else {
        RegressionMetric metric = new RegressionMetric();
        Dataset<Row> sensordataDF = this.InitializeH2OModel(ctx);

        SQLContext hc = ctx.sqlctx();

        // Save the H2OContext so that we can extract the H2oFrames later
        H2OContext h2oContext = H2OContext.getOrCreate(ctx.sc().sc());
    //...       
}
}



In above the InitializeH2OModel(ctx) is a complex function which generate the spark frame for training the model. The prgram can run correctly till the line which starts the h2o context ""H2OContext h2oContext = H2OContext.getOrCreate(ctx.sc().sc());""


The configuration parameter which I add to livy is as follow:


    LivyClient client = new LivyClientBuilder().setURI(new URI(livyUrl)).setConf(""spark.executor.instances"", ""9"")
            .setConf(""spark.driver.memory"", ""20g"")
            .setConf(""spark.driver.cores"", ""5"")
            .setConf(""spark.executor.memory"", ""16g"") // memory per executor
            .setConf(""spark.executor.cores"", ""5"")
            .setConf(""spark.yarn.executor.memoryOverhead"", ""7000"")
            .setConf(""spark.rdd.compress"", ""true"")
            .setConf(""spark.default.parallelism"", ""3000"")
            .setConf(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
            .setConf(""spark.driver.extraJavaOptions"", ""-XX:+UseG1GC -XX:MaxPermSize=10000m -Xss5000m"")
            .setConf(""spark.executor.extraJavaOptions"", ""-XX:+UseG1GC -XX:MaxPermSize=10000m -Xss5000m"")
            .setConf(""spark.shuffle.compress"", ""true"")
            .setConf(""spark.shuffle.spill.compress"", ""true"")
            .setConf(""spark.kryoserializer.buffer.max"", ""1g"")
            .setConf(""spark.shuffle.io.maxRetries"", ""6"")
            .setConf(""spark.sql.shuffle.partitions"", ""7000"")
            .setConf(""spark.sql.files.maxPartitionBytes"", ""5000"")
            .setConf(""spark.driver.extraClassPath"",
                    ""/usr/hdp/2.6.2.0-205/phoenix/phoenix-4.7.0.2.6.2.0-205-client.jar:/usr/hdp/2.6.2.0-205/phoenix/phoenix-4.7.0.2.6.2.0-205-server.jar:/usr/hdp/2.6.2.0-205/phoenix/lib/phoenix-spark-4.7.0.2.6.2.0-205.jar:/usr/hdp/2.6.2.0-205/hbase/lib/hbase-common-1.1.2.2.6.2.0-205.jar:/usr/hdp/2.6.2.0-205/hbase/lib/hbase-server-1.1.2.2.6.2.0-205.jar:/usr/hdp/2.6.2.0-205/hbase/lib/hbase-server-1.1.2.2.6.2.0-205"")
            .setConf(""spark.executor.extraClassPath"",
                    ""/usr/hdp/2.6.2.0-205/phoenix/phoenix-4.7.0.2.6.2.0-205-client.jar:/usr/hdp/2.6.2.0-205/phoenix/phoenix-4.7.0.2.6.2.0-205-server.jar:/usr/hdp/2.6.2.0-205/phoenix/lib/phoenix-spark-4.7.0.2.6.2.0-205.jar:/usr/hdp/2.6.2.0-205/hbase/lib/hbase-common-1.1.2.2.6.2.0-205.jar:/usr/hdp/2.6.2.0-205/hbase/lib/hbase-server-1.1.2.2.6.2.0-205.jar:/usr/hdp/2.6.2.0-205/hbase/lib/hbase-server-1.1.2.2.6.2.0-205"")
              .setConf(""spark.ext.h2o.cluster.size"", ""-1"")
              .setConf(""spark.ext.h2o.cloud.timeout"", ""60000"")
              .setConf(""spark.ext.h2o.spreadrdd.retries"", ""-1"")
              .setConf(""spark.ext.h2o.nthreads"", ""-1"")
              .setConf(""spark.ext.h2o.disable.ga"", ""true"")
              .setConf(""spark.ext.h2o.dummy.rdd.mul.factor"", ""10"")
              .setConf(""spark.ext.h2o.fail.on.unsupported.spark.param"", ""false"")
            .setConf(""spark.cassandra.input.split.size_in_mb"", ""64"")
            .setConf(""spark.driver.maxResultSize"", ""3g"")
            .setConf(""spark.network.timeout"", ""1000s"")
            .setConf(""spark.executor.heartbeatInterval"", ""600s"")
            .build();



I am running above on HDP 2.6.2 in cluster mode with Spark 2.1.1.","['h2o', 'sparkling-water']",Unknown,,N/A
47106887,47106887,2017-11-04T02:47:14,2017-11-04 03:13:28Z,819,"I use the h2o deep learning using python on a data of 2 balanced classes ""0"" and ""1"", and adjusted the parameters to be as follows:


prostate_dl = H2ODeepLearningEstimator(
     activation=,""Tanh""
     hidden=[50,50,50],
     distribution=""multinomial"",
    score_interval=10,
    epochs=1000,
    input_dropout_ratio=0.2
    ,adaptive_rate=True
    , rho=0.998, epsilon = 1e-8
    )

prostate_dl .train( 
x=x,
y=y,
training_frame =train,
validation_frame = test) 



Each time the program runs gives different confusion matric and accuarcy results, can anyway explain that? how can the results can be reliable?


Also, all of the runs gives the majority prediction as class ""1"" not ""0"" , is their any suggestion?","['deep-learning', 'h2o']",user8883441,https://stackoverflow.com/users/8883441/user8883441,3
47099605,47099605,2017-11-03T15:45:47,2017-11-06 13:15:58Z,0,"I am using the cut function to divide a continuous variable into intervals (0 -1, 2 - 3, 3 - 4, 4+). I am specifically having issues defining the last interval which I want to have an upper bound of infinity. In base 
R
, I can define an upper bound of infinity using ""Inf""


Here's an example which throws an error while using h2o.


input_data <- as.h2o(seq(0.1, by = 0.1, 100))
names(input_data) <- 'var1'
breaks <- c(0, 1, 2, 3, 4, Inf)
cut(input_data[['var1']], breaks = breaks, right = TRUE)



I receive an error:




""breaks must be a numeric vector""","['r', 'h2o']",Brian Tompsett - 汤莱恩,https://stackoverflow.com/users/4370109/brian-tompsett-%e6%b1%a4%e8%8e%b1%e6%81%a9,"5,875"
47098219,47098219,2017-11-03T14:33:26,2017-11-03 17:56:00Z,273,"public BinomialModelPrediction predictBinomial(RowData data) throws PredictException {
      double[] preds = this.preamble(ModelCategory.Binomial, data);
      BinomialModelPrediction p = new BinomialModelPrediction();
      double d = preds[0];
      p.labelIndex = (int)d;
      String[] domainValues = this.m.getDomainValues(this.m.getResponseIdx());
      p.label = domainValues[p.labelIndex];
      p.classProbabilities = new double[this.m.getNumResponseClasses()];
      System.arraycopy(preds, 1, p.classProbabilities, 0, p.classProbabilities.length);
      if(this.m.calibrateClassProbabilities(preds)) {
          p.calibratedClassProbabilities = new double[this.m.getNumResponseClasses()];
          System.arraycopy(preds, 1, p.calibratedClassProbabilities, 0, p.calibratedClassProbabilities.length);
       }
       return p;
  }





Eg: classProbabilities =[0.82333,0,276666]
      labelIndex = 1
      label  = true
      domainValues = [false,true]




what does this labelIndex signifies and does the class probabilities
order is same as the domain value order ,If order is same then it means  that here probability of false is 0.82333 and probability of true is 0.27666 but why is this labelIndex showing as 1 and label as true. 


Please help me to figure out this issue.",['h2o'],RAHUL TARWAY,https://stackoverflow.com/users/8875604/rahul-tarway,1
47088983,47088983,2017-11-03T05:23:43,2017-11-29 06:18:20Z,0,"I am wondering if it's possible to provide any 
r
 sample code for using word2vec and cnn on text classification in H2O DeepWater R version ? There's very very few documentation on either 
mexnetR
 or 
h2o deep water r


I have already used the 
h2o
 
r
 version package to train my 
word2vec
 
word embedding
 vocabulary lookup table and the document word vector matrix. I am wondering if there's any sample code to combine the lookup table and the original raw text into the  using 
mxnetR
 (custom iterator) CNN classification model, or using 
h2o r
 to build CNN directly.


I am asking because if I convert all data into the array format at once, then my machine will not have enough memory to support it.","['r', 'nlp', 'word2vec', 'h2o', 'mxnet']",Weiye Deng,https://stackoverflow.com/users/8844226/weiye-deng,65
47080618,47080618,2017-11-02T16:50:53,2017-11-09 11:07:40Z,345,"To reproduce, use simplest sparkling water Python example (
https://github.com/h2oai/sparkling-water/blob/rel-2.2/py/examples/scripts/H2OContextInitDemo.py
):


from pysparkling import *
from pyspark.sql import SparkSession
import h2o

# Initiate SparkSession
spark = SparkSession.builder.appName(""App name"").getOrCreate()

# Initiate H2OContext
hc = H2OContext.getOrCreate(spark)

# Stop H2O and Spark services
h2o.shutdown(prompt=False)
spark.stop()



I have SPARK_HOME exported and pointing to Spark 2.2.0.
I have MASTER=""local[4]"".


I have installed (among others):


pyspark (2.2.0)
h2o-pysparkling-2.2 (2.2.2)
h2o (3.14.0.7)



Now, when I'm running this script, I'm getting (Under Python 2.7):


H2O session _sid_9ee5 closed.
/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/pysparkling/context.py:151: UserWarning: Stopping H2OContext. (Restarting H2O is not yet fully supported...) 
  warnings.warn(""Stopping H2OContext. (Restarting H2O is not yet fully supported...) "")
11-02 17:37:43.710 10.0.1.62:54321       21323  Thread-28 INFO: Orderly shutdown:  Shutting down now.
11-02 17:37:43.719 10.0.1.62:54321       21323  Thread-29 INFO: Orderly shutdown:  Shutting down now.
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/py4j/java_gateway.py"", line 883, in send_command
    response = connection.send_command(command)
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/py4j/java_gateway.py"", line 1040, in send_command
    ""Error while receiving"", e, proto.ERROR_ON_RECEIVE)
Py4JNetworkError: Error while receiving
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/atexit.py"", line 24, in _run_exitfuncs
    func(*targs, **kargs)
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/pysparkling/context.py"", line 140, in <lambda>
    atexit.register(lambda: h2o_context.stop_with_jvm())
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/pysparkling/context.py"", line 147, in stop_with_jvm
    self.stop()
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/pysparkling/context.py"", line 153, in stop
    self._jhc.stop(False)
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/py4j/java_gateway.py"", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/py4j/protocol.py"", line 327, in get_return_value
    format(target_id, ""."", name))
Py4JError: An error occurred while calling o32.stop
Error in sys.exitfunc:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/atexit.py"", line 24, in _run_exitfuncs
    func(*targs, **kargs)
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/pysparkling/context.py"", line 140, in <lambda>
    atexit.register(lambda: h2o_context.stop_with_jvm())
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/pysparkling/context.py"", line 147, in stop_with_jvm
    self.stop()
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/pysparkling/context.py"", line 153, in stop
    self._jhc.stop(False)
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/py4j/java_gateway.py"", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/home/user/.virtualenvs/sacred2/local/lib/python2.7/site-packages/py4j/protocol.py"", line 327, in get_return_value
    format(target_id, ""."", name))
py4j.protocol.Py4JError: An error occurred while calling o32.stop



Why am I getting those tracebacks? Return code of script is 0, also in Python 3 but some other tracebacks are thrown. How to clean this up?


Complete log: 
https://gist.github.com/anonymous/163fba371b2a419c2171f4aff83a1ff7","['python', 'h2o', 'sparkling-water']",omikron,https://stackoverflow.com/users/719457/omikron,"2,815"
47062207,47062207,2017-11-01T19:22:59,2019-04-07 12:28:50Z,0,"I've been working with h2o and h2o Flow for the past days and have loved it. Two days ago I exported some models (in binary format) from h2o Flow and imported them with R so I could do further studies. It worked perfectly until today. For some reason, I'm getting the following error when I try the 
h2o.loadModel
 function (which worked just fine before with all my exported binary models). Just to clarify, I already ran successfully the 
h2o.init()
 command to open the H2O cluster. 


ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/99/Models.bin/)

water.exceptions.H2OIllegalArgumentException
 [1] ""water.exceptions.H2OIllegalArgumentException: Illegal argument: dir of function: importModel: H2O/H2O-XX/gbm_grid1_m02""
 [2] ""    water.api.ModelsHandler.importModel(ModelsHandler.java:220)""                                                       
 [3] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                       
 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                                     
 [5] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                             
 [6] ""    java.lang.reflect.Method.invoke(Method.java:498)""                                                                  
 [7] ""    water.api.Handler.handle(Handler.java:63)""                                                                         
 [8] ""    water.api.RequestServer.serve(RequestServer.java:446)""                                                             
 [9] ""    water.api.RequestServer.doGeneric(RequestServer.java:296)""                                                         
[10] ""    water.api.RequestServer.doPost(RequestServer.java:222)""                                                            
[11] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                                      
[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                      
[13] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                            
[14] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                                        
[15] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                                
[16] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                                         
[17] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                                 
[18] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                     
[19] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                             
[20] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                   
[21] ""    water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:189)""                                                         
[22] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                             
[23] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                   
[24] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                           
[25] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                    
[26] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                     
[27] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)""                          
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)""          
[29] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)""                                                  
[30] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)""                                             
[31] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                            
[32] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                      
[33] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                                  
[34] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                   
[35] ""    java.lang.Thread.run(Thread.java:748)""                                                                             

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Illegal argument: dir of function: importModel: H2O/H2O-XX/gbm_grid1_m02



Im using the latest h2o version, 3.15.0.4029, on RStudio, macOS Sierra (10.12.6 (16G29))


UPDATE:
 as requested by Erin, posting how I tried to import the model:


library(h2o)
#h2o.init(nthreads = -1, max_mem_size = ""7G"") # Already initiated 
#h2o.removeAll()   
h2oXX <- h2o.loadModel(""H2O-XX/GBM_grid_0_AutoML_20171102_095446_model_41"")



Downgraded to the latest stable version (h2o-3.14.0.7) and receiving the same error: 
Illegal argument: dir of function: importModel: H2O-XX/GBM_grid_0_AutoML_20171102_095446_model_41


Attached model (you can try to import it): bit.ly/h2odemo","['r', 'load', 'h2o']",Unknown,,N/A
47059131,47059131,2017-11-01T16:13:09,2017-11-02 23:51:21Z,0,"I noticed that there are a couple functions designed to calculate the confidence interval for models built using 
randomForest
 packages, such as 
rfPredVar
 in 
RFinfer
. I am wondering if anyone knows any functions like 
rfPredVar
 that can calculate the confidence interval for RF models built using 
h2o
 package. Many thanks in advance.","['r', 'random-forest', 'h2o']",Davidx,https://stackoverflow.com/users/7547959/davidx,61
47049458,47049458,2017-11-01T06:35:17,2017-11-02 09:31:22Z,106,"I am using the DriverlessAI(with H2O.ai) and trying to use scorer (which is a Diagnosis model made from DriverlessAI), but an error occurred.


When running run_tcp_client.sh, the error 
"" File ""example_client.py"", line 5, in 
    from thrift import Thrift
ModuleNotFoundError: No module named 'thrift'""
has occurred.


In the document(
https://www.h2o.ai/wp-content/uploads/2017/09/driverlessai/scoring-package.html
) it's written that it is enough to run run_tcp_client.sh after runing run_tcp_server.sh.


What can I do about this?","['python', 'h2o', 'driverless-ai']",Mateusz Dymczyk,https://stackoverflow.com/users/217019/mateusz-dymczyk,15.1k
47041404,47041404,2017-10-31T17:37:40,2017-11-09 12:40:01Z,0,"I'm using H2O with a SVMLight sparse matrix of dimensions ~700,000 x ~800,000.  The file size is approximately ~800MB on disk.  But importing it into H2O takes up over 300GB of RAM? The process also takes too long (~15 minutes) to finish. 


I can create and store the sparse matrix in RAM using the Matrix package rather quickly in comparison.  The Sparse Matrix in that case takes ~1.2GB of RAM.


Below is my code:


library(h2o)
h2o.init(nthreads=-1,max_mem_size = ""512g"")

x <- h2o.importFile('test2.svmlight', parse = TRUE)



Here is my system:


openjdk version ""1.8.0_121""
OpenJDK Runtime Environment (build 1.8.0_121-b13)
OpenJDK 64-Bit Server VM (build 25.121-b13, mixed mode)

Starting H2O JVM and connecting: .. Connection successful!

R is connected to the H2O cluster: 
H2O cluster uptime:         2 seconds 76 milliseconds 
H2O cluster version:        3.14.0.3 
H2O cluster version age:    1 month and 8 days  
H2O cluster name:           H2O_started_from_R_ra2816_fhv677 
H2O cluster total nodes:    1 
H2O cluster total memory:   455.11 GB 
H2O cluster total cores:    24 
H2O cluster allowed cores:  24 
H2O cluster healthy:        TRUE 
H2O Connection ip:          localhost 
H2O Connection port:        54321 
H2O Connection proxy:       NA 
H2O Internal Security:      FALSE 
H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
R Version:                  R version 3.4.1 (2017-06-30) 



I would appreciate any advice because I really enjoy H2O and would like to use it for this project.","['r', 'sparse-matrix', 'h2o']",Unknown,,N/A
47025680,47025680,2017-10-30T23:22:51,2017-11-14 17:00:36Z,184,"I am using a DRF model generated with 
h2o flow
. When running fresh input data against this model (using its MOJO in a java program with the 
EasyPredictModelWrapper
), there are a large number of 
UnknownCategoricalLevels
 (checking with the 
getUnknownCategoricalLevelsSeen()
 and 
getUnknownCategoricalLevelsSeenPerColumn()
 
methods
). 


My workaround for this was to only use those predictions that had a prediction confidence above a certain threshold (say 0.90). Ie. the 
classProbability
 selected by the model must be grater than threshold to be used. 


My questions are:




Is this solution wrong-headed (ie. does not actually address/workaround the problem (eg. 
unknownlevels
 don't actually affect the class probability values)) or is it a 
valid
 workaround to the problem?


Is there a better way to address this issue?




Thanks.","['machine-learning', 'h2o']",lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
46977685,46977685,2017-10-27T14:41:34,2017-11-02 00:19:30Z,0,"I am a h2o R version user and I have a question regarding the h2o local cluster. I setup the cluster by execute the command in r,


h2o.init()


However, the cluster will be turned off automatically when I do not use it for a few hours. For example, I run my model during the night, but when I come back to my office in the morning to check on my model. It says,


Error in h2o.getConnection() : No active connection to an H2O cluster. Did you runh2o.init()?


Is there a way to fix or work around it ?","['r', 'h2o']",Weiye Deng,https://stackoverflow.com/users/8844226/weiye-deng,65
46971969,46971969,2017-10-27T09:37:26,2017-11-30 04:57:45Z,0,"I have a Pandas dataframe which has 
Encoding: latin-1
 and is delimited by 
;
. The dataframe is very large almost of 
size: 350000 x 3800
. I wanted to use sklearn initially but my dataframe has missing values (
NAN values
) so i could not use sklearn's random forests or GBM. So i had to use 
H2O's
 Distributed random forests for the Training of the dataset. The main Problem is the dataframe is not efficiently converted when i do 
h2o.H2OFrame(data)
. I checked for the possibility for providing the Encoding Options but there is nothing in the documentation. 


Do anyone have an idea about this? Any leads could help me. I also want to know if there are any other libraries like H2O which can handle NAN values very efficiently? I know that we can impute the columns but i should not do that in my dataset because my columns are values from different sensors, if the values are not there implies that the sensor is not present. I can use only Python","['python', 'machine-learning', 'h2o']",ayaan,https://stackoverflow.com/users/3847509/ayaan,735
46956051,46956051,2017-10-26T13:44:41,2022-05-29 15:48:51Z,0,"I am currently running a script in which several h2o glm and deeplearning models are being generated for several iterations of a Monte-Carlo Cross-Validation. When finished running (which takes about half a day), h2o is saving immense files to the local drive (with sizes up to 8.5 GB). These files are not erased when RStudio or my computer is restarted (as I originally thought). Is there a way to stop h2o from saving these files?","['r', 'logging', 'h2o']",jhearn,https://stackoverflow.com/users/8581033/jhearn,525
46938002,46938002,2017-10-25T16:53:01,2017-10-26 14:42:23Z,752,"I am trying for first time to use h2o package in R. My problem comes when I try to use 
h2o.init()
 function I have the next error message


Error in value[[3L]](cond) : 
You have a 32-bit version of Java. H2O works best with 64-bit Java.
Please download the latest Java SE JDK 7 from the following URL:
http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html
In addition: Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :
path[1]=""C:\Program Files\Java\jre1.8.0_111\bin/bin/java.exe"": El sistema no puede encontrar la ruta especificada



I have followed the solutions 
here
 and 
there
 But I still have the same error things become worst since i do not have admin rights. Does any one can point me somewhere or give me some guide lines to get this solve?? This is driving me crazy.


My sesion info is


R version 3.4.2 (2017-09-28)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1



Java version: 


java version ""1.8.0_131""
JAVA(TM) SE Runtime Environment (build 1.8.0_131-b11)
Java HotSpot(TM) Client VM (build 25.131-b11, mixed mode)","['h2o', 'rjava']",mina,https://stackoverflow.com/users/5687290/mina,195
46937314,46937314,2017-10-25T16:13:30,2017-10-25 16:20:34Z,0,"I'm tuning my parameters by testing many models, and I'm fairly annoyed that I can't do much about the ""Build Progress"" bars that are cluttering up my iPython Notebook. I've skimmed the docs looking for some sort of ""verbose"" setting to turn off, but can't find it. Is there any way to turn this off when I want to train and evaluate dozens of models at once?","['python', 'jupyter-notebook', 'h2o']",James Kelleher,https://stackoverflow.com/users/2712770/james-kelleher,"2,117"
46924980,46924980,2017-10-25T05:59:38,2018-02-28 19:12:58Z,465,"We need to authenticate user using LDAP in sparkling-water. We tried configuring the same using Sparkling-water 1.6.13 and h2O 3.14.0.2. 
Below is the configuration:


*ldaploginmodule {
    org.eclipse.jetty.plus.jaas.spi.LdapLoginModule required
    debug=""true""
    useLdaps=""false""
    contextFactory=""com.sun.jndi.ldap.LdapCtxFactory""
    hostname=""localhost""
    port=""389""
    bindDn=""CN=admin,OU=Users,DC=company,DC=com""
    bindPassword=""password""
    authenticationMethod=""simple""
    forceBindingLogin=""true""
    userBaseDn=""dc=company,dc=com"";
};*



Command used :
 
spark-submit --class water.SparklingWaterDriver --master yarn-client --num-executors 2 --driver-memory 6g --executor-memory 4g --executor-cores 2 --conf 'spark.dynamicAllocation.enabled=false' --conf spark.ext.h2o.log.level=DEBUG --conf spark.ext.h2o.ldap.login=true --conf spark.ext.h2o.login.conf=/home/user/ldap.conf /home/user/sparkling-water-1.6.13/assembly/build/libs/sparkling-water-assembly_2.10-1.6.13-all.jar


But we are facing some issue. Please find below error logs. Would appreciate any help on this.

ERROR:


java.lang.NullPointerException
        at com.sun.jndi.ldap.AbstractLdapNamingEnumeration.getNextBatch(AbstractLdapNamingEnumeration.java:130)
        at com.sun.jndi.ldap.AbstractLdapNamingEnumeration.nextAux(AbstractLdapNamingEnumeration.java:258)
        at com.sun.jndi.ldap.AbstractLdapNamingEnumeration.nextImpl(AbstractLdapNamingEnumeration.java:249)
        at com.sun.jndi.ldap.AbstractLdapNamingEnumeration.next(AbstractLdapNamingEnumeration.java:203)
        at com.sun.jndi.ldap.AbstractLdapNamingEnumeration.nextElement(AbstractLdapNamingEnumeration.java:106)
        at com.sun.jndi.ldap.AbstractLdapNamingEnumeration.nextElement(AbstractLdapNamingEnumeration.java:40)
        at org.eclipse.jetty.plus.jaas.spi.LdapLoginModule.findUser(LdapLoginModule.java:513)
        at org.eclipse.jetty.plus.jaas.spi.LdapLoginModule.bindingLogin(LdapLoginModule.java:468)
        at org.eclipse.jetty.plus.jaas.spi.LdapLoginModule.login(LdapLoginModule.java:399)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195)
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682)
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)
        at javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.eclipse.jetty.plus.jaas.JAASLoginService.login(JAASLoginService.java:217)
        at org.eclipse.jetty.security.authentication.BasicAuthenticator.validateRequest(BasicAuthenticator.java:83)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:456)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)
        at org.eclipse.jetty.server.Server.handle(Server.java:349)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)
        at org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:47)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:910)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:634)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:230)
        at org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:66)
        at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:254)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:599)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:534)
        at java.lang.Thread.run(Thread.java:745)
17/10/17 12:45:47 WARN JAASLoginService:
javax.security.auth.login.LoginException: Error obtaining user info.
        at org.eclipse.jetty.plus.jaas.spi.LdapLoginModule.login(LdapLoginModule.java:438)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195)
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682)
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)
        at javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.eclipse.jetty.plus.jaas.JAASLoginService.login(JAASLoginService.java:217)
        at org.eclipse.jetty.security.authentication.BasicAuthenticator.validateRequest(BasicAuthenticator.java:83)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:456)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)
        at org.eclipse.jetty.server.Server.handle(Server.java:349)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)
        at org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:47)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:910)
        at","['ldap', 'h2o', 'sparkling-water']",Satish Agrawal,https://stackoverflow.com/users/8829480/satish-agrawal,51
46891829,46891829,2017-10-23T14:27:17,2017-10-23 19:36:05Z,769,"i have some problem with read files in h2o.


import h2o
from h2o.estimators.deeplearning import H2ODeepLearningEstimator
h2o.init()
train = h2o.import_file(""(""https://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris_wheader.csv"""")
splits = train.split_frame(ratios=[0.75], seed=1234)
dl = H2ODeepLearningEstimator(distribution=""quantile"",quantile_alpha=0.8)
dl.train(x=range(0,2), y=""petal_len"", training_frame=splits[0])
print(dl.predict(splits[1]))



UPDATE_1, The fourth line has this form(sorry, i copied wrong from IDE):


train = h2o.import_file(""https://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris_wheader.csv"")



I got H2OTypeError: Argument 
x
 should be a None | integer | string | list(string | integer) | set(integer | string), got range range(0, 2). 


This is due to the fact that ""train"" is empty.


In [23]: train

Out[23]: 



I thought that there is a problem with reading from and linking and manually downloading file.


train = h2o.import_file(""iris_wheader.csv"")



But i got same result.


In [26]: train

Out[26]: 



I connected pandas and open this .csv in pandas. It opened, I got a pandas-dataframe, I used


 train = h2o.H2OFrame(train) 



and got an empty train.


In [29]: train

Out[29]:



How to solve this problem?


UPDATE_2 When I went to 127.0.0.1:54321/flow/index.html, and it shows me that the dataframe has been loaded into the cluster. But in Python, I get empty train. I use Spyder IDE with IPython console, can it somehow influence the result?","['python', 'h2o']",Unknown,,N/A
46853720,46853720,2017-10-20T16:47:30,2017-10-20 16:47:30Z,566,"I was trying to detect outliers using the H2OAutoEncoderEstimator.

Basically I load 4 KPIs from a CSV file.

For each KPI I have 1 month of data.

The data in the CSV file has been manually created and are all the same for each KPI


The following picture shows the trend of the KPIs:



The first black vertical line (x=4000) indicates the end of the training data.

All the others light black vertical lines indicate the data that I use to detect the outliers every time.

As you can see data are very regular (I'v copied & pasted first 1000 rows 17 times).

This is what my code does:




Loads the data from a CSV file (1 row represents the value of all kpis in a specific timestamp)


Trains the model using the first 4000 timestamps


Starting from the 4001 timestamp, every 250 Timestamps it calls the function 
model.anomaly
 to detect the outliers in a specific window (250 timestamps)




My questions are:
 




Is it normal that every time that I call the function model.anomaly the errors returned increases every time (from 0.1 to 1.8)?


If I call again model.train, the training phase will be performed from scratch replacing the existing model or it will be updated with the new data provided?




This is my python code:


data = loadDataFromCsv()


nOfTimestampsForTraining = 4000
frTrain = h2o.H2OFrame(data[:nOfTimestampsForTraining])
colsName = frTrain.names

model = H2OAutoEncoderEstimator(activation=""Tanh"", 
                                hidden=[5,4,3], 
                                l1=1e-5, 
                                ignore_const_cols=False, 
                                autoencoder=True,
                                epochs=100)
# Init indexes
nOfTimestampsForWindows = 250
fromIndex = nOfTimestampsForWindows
toIndex = fromWindow + nOfTimestampsForWindows

# Perform the outlier detection every nOfTimestampsForWindows TimeStamps
while toIndex <= len(data) :
    frTest = h2o.H2OFrame(data[fromWindow:toWindow])
    error = model.anomaly(frTest)

    df = error.as_data_frame()
    print(df)
    print(df.describe())

    # Adjust indexes for the next window
    fromIndex = toIndex
    toIndex = fromIndex + nOfTimestampsForWindows",['h2o'],Fabry,https://stackoverflow.com/users/980515/fabry,"1,630"
46853591,46853591,2017-10-20T16:39:40,2017-10-20 17:00:42Z,488,"Using the h2o interface I am not able to figure out how to rename a data frame previously created.




I was trying to find a way via: 
getFrameSummary
 command, but there is no rename option.



Any workaround?, Thanks in advance.",['h2o'],David Leal,https://stackoverflow.com/users/6237093/david-leal,"6,739"
46852111,46852111,2017-10-20T15:11:12,2017-10-20 15:26:55Z,0,"I am receiving this error when using h2o.randomforest.  Please see the function call and associated error below.    


base_line_rf <- h2o.randomForest(x=2:ncol(train),
                                y=1,
                                ntrees = 10000,
                                mtries = ncol(train)-1,
                                training_frame = train,
                                model_id <- model_id,
                                stopping_rounds = 5,
                                stopping_tolerance = 0,
                                stopping_metric = ""AUC"",
                                binomial_double_trees = TRUE
)



The error:


java.lang.AssertionError: I am really confused about the heap usage; MEM_MAX=7624720384 heapUsedGC=7626295912
    at water.MemoryManager.set_goals(MemoryManager.java:97)
    at water.MemoryManager.malloc(MemoryManager.java:265)
    at water.MemoryManager.malloc(MemoryManager.java:222)
    at water.MemoryManager.malloc8d(MemoryManager.java:281)
    at hex.tree.DHistogram.init(DHistogram.java:281)
    at hex.tree.DHistogram.init(DHistogram.java:240)
    at hex.tree.ScoreBuildHistogram2$ComputeHistoThread.computeChunk(ScoreBuildHistogram2.java:326)
    at hex.tree.ScoreBuildHistogram2$ComputeHistoThread.map(ScoreBuildHistogram2.java:306)
    at water.LocalMR.compute2(LocalMR.java:84)
    at water.LocalMR.compute2(LocalMR.java:76)
    at water.LocalMR.compute2(LocalMR.java:76)
    at water.LocalMR.compute2(LocalMR.java:76)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1255)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.popAndExecAll(ForkJoinPool.java:904)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:977)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



What is the reason for this error?  


Thank you","['r', 'h2o']",John Smith,https://stackoverflow.com/users/7611710/john-smith,51
46849368,46849368,2017-10-20T12:45:12,2017-12-20 14:29:09Z,0,"I built an H2O model in R and saved the POJO code. I want to score parquet files in hdfs using the POJO but I'm not sure how to go about it. I plan on reading the parquet files into spark (scala/SparkR/PySpark) and scoring them on there. Below is the excerpt I found on 
H2O's documentation page.




""How do I run a POJO on a Spark Cluster?


The POJO provides just the math logic to do predictions, so you won’t find any  Spark (or even H2O) specific code there. If you want to use the POJO to make  predictions on a dataset in Spark, create a map to call the POJO for each row and save the result to a new column, row-by-row""




Does anyone have some example code of how I can do this? I'd greatly appreciate any assistance. I code primarily in R and SparkR, and I'm not sure how I can ""map"" the POJO to each line.


Thanks in advance.","['scala', 'apache-spark', 'pyspark', 'pojo', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
46815427,46815427,2017-10-18T16:56:09,2019-01-10 19:50:25Z,0,"Using SparkR, I am wondering if it is possible to convert a Spark DataFrame into an H2O frame? 


I have seen examples of converting R 
data.frames
 to h2o frames, but, sadly, this is not a viable option (data size).


I know it is possible to use 
sparklyr
 and 
rsparkling
 to create an h2o frame, but I am not using HIVE, or Hadoop, 
sparklyr
 or 
rsparkling
.


Instead, my goal is to convert the 
sdf
 from this:


set.seed(123)
df<- data.frame(ColA=rep(c(""dog"", ""cat"", ""fish"", ""shark""), 4), ColB=rnorm(16), ColC=rep(seq(1:8),2))
sdf<- SparkR::createDataFrame(df)



into this:


as.h2o(sdf, destination_frame = ""hsdf"") # fails, came from Spark (SparkR)
as.h2o(df, destination_frame = ""hdf"") # succeeds, but this is a regular R data.frame



Hopefully, someone has figured out a way to do this using what SparkR can provide. I think it would be a huge boon to R users.","['r', 'h2o', 'sparkr']",Konrad Rudolph,https://stackoverflow.com/users/1968/konrad-rudolph,544k
46814953,46814953,2017-10-18T16:28:14,2017-10-18 20:26:42Z,320,"I'm just trying to get pysparkling working, but change the port of the web UI. I've looked in the help files and they seem to reference old versions of sparkling water. Currently am running


from pysparkling import *

hc = H2OContext.getOrCreate(spark)



and is starting up on the default 54321 port. I see there is a conf object to pass in, but am unsure of how to set this correctly. Any help would be appreciated.","['h2o', 'sparkling-water']",chib,https://stackoverflow.com/users/8796785/chib,13
46799076,46799076,2017-10-17T20:54:37,2017-10-18 23:08:06Z,0,"I am parsing a file which contains UUID type too.
I cannot parse the file and get this error.


DistributedException from /127.0.0.1:54321: 'NewChunk has type Numeric, but the Vec is of type UUID', caused by java.lang.AssertionError: NewChunk has type Numeric, but the Vec is of type UUID


Anyone know what this means?","['machine-learning', 'artificial-intelligence', 'data-science', 'h2o']",kivk02,https://stackoverflow.com/users/4152164/kivk02,651
46787105,46787105,2017-10-17T09:40:10,2017-10-17 10:09:22Z,526,"I am curious to know how memory is managed in H2O. 
Is it completely 'in-memory' or does it allow swapping in case the memory consumption goes beyond available physical memory? Can I set -mapperXmx parameter to 350GB if I have a total of 384GB of RAM on a node? I do realise that the cluster won't be able to handle anything other than the H2O cluster in this case.
Any pointers are much appreciated, Thanks.",['h2o'],Shean,https://stackoverflow.com/users/8752030/shean,31
46727500,46727500,2017-10-13T10:11:41,2017-10-16 15:21:32Z,417,"I wanted to add this as a comment to this question - 
is multi-cpu supported by h2o-xgboost?
 - but apparently my rep is too low.


I am using the latest stable version of h2o (3.14.06).


In order to try and solve this problem i've made sure that gcc is built within my docker image (using apt-get install gcc)


    dpkg -l | grep gcc
    gcc              4:5.3.1-1ubuntu1       amd64 GNU C compiler
    gcc-5            5.4.0-6ubuntu1~16.04.5 amd64 GNU C compiler
    **output truncated**



Unfortunately when the cluster is spun up its still reporting:


    INFO: Found XGBoost backend with library: xgboost4j
    INFO: Your system supports only minimal version of XGBoost (no GPUs, no multithreading)!



Can anyone provide any insights? Clearly I'm missing a piece of the puzzle.","['h2o', 'xgboost']",gm209,https://stackoverflow.com/users/8183099/gm209,31
46713688,46713688,2017-10-12T15:39:03,2017-10-13 14:03:01Z,367,"I am using H2O.AI h2o.automl function to perform a standard binary classification problem. I am  using the last package version published on CRAN. I ran the following code:


my_automl_model<-h2o.automl(x=predictorsList, y=""Purchase"", training_frame = train.h2o, validation_frame =  test.h2o, stopping_metric = ""logloss"", max_runtime_secs = 60*60*3).



being purchase a two levels factors (""N"", ""S"") and the predictors' list is predictorsList.


The log of the fast call is the following:


                                   model_id auc logloss
1 GLM_grid_0_AutoML_20171012_150410_model_1 NaN     NaN
2 GLM_grid_0_AutoML_20171012_150410_model_0 NaN     NaN
3     DeepLearning_0_AutoML_20171012_145911 NaN     NaN
4  StackedEnsemble_0_AutoML_20171012_145911 NaN     NaN
5 GLM_grid_0_AutoML_20171012_145911_model_1 NaN     NaN
6 GLM_grid_0_AutoML_20171012_145911_model_0 NaN     NaN



I understand that the package ranks the model, but I wonder why no performance metrics is shown...


Also I would like to understand:
1. what XRT_xxx models represent?
2. if there is any way to specify n-folds cross validation.


Thanks in advance for the support",['h2o'],Rob,https://stackoverflow.com/users/162698/rob,15.1k
46713293,46713293,2017-10-12T15:20:17,2018-06-06 05:36:03Z,0,"I'm trying to overfit a GBM with h2o (I know it's weird, but I need this to make a point). So I increased the max_depth of my trees and the shrinkage, and disabled the stopping criterion :


overfit <- h2o.gbm(y=response
                  , training_frame = tapp.hex
                  , ntrees = 100 
                  , max_depth = 30 
                  , learn_rate = 0.1 
                  , distribution = ""gaussian""
                  , stopping_rounds = 0
                  , distribution = ""gaussian""
                  )



The overfitting works great, but I've noticed that the training error does not improve after the 64th tree. Do you know why ? If I understand the concept of boosting well enough, the training error should converge to 0 as the number of trees increase.


Information on my data :
Around 1 million observations
10 variables
Response variable is quantitative.


Have a good day !","['r', 'h2o', 'gbm']",M. DL,https://stackoverflow.com/users/8766133/m-dl,3
46702563,46702563,2017-10-12T06:09:56,2017-10-13 15:03:36Z,273,"I'm trying to start H2O on a Hadoop Cluster. Sadly it doesn't work and gives me the error that the class water.hadoop.h2omapper is not found.


The Hadoop environment is HDP in the version 2.6 and includes 5 nodes, where 1 runs the YARN resource manager and 3 nodes are data nodes with the YARN client. The data nodes each have resources of 32GB RAM and 4 CPU cores each. No other applications are running on them. I configured a maximum of 16GB and 3 cores per YARN application on each node in Ambari.


I start the H2O cluster from the terminal (did try on all nodes, same error everywhere) with the following output:


[root@host3 h2o-3.14.0.6-hdp2.6]# sudo -u hdfs hadoop jar h2odriver.jar -nodes 3 -mapperXmx 6g -output h2o-test
Determining driver host interface for mapper->driver callback...
[Possible callback IP address: 192.168.20.35]
[Possible callback IP address: 127.0.0.1]
Using mapper->driver callback IP address and port: 192.168.20.35:46619
(You can override these with -driverif and -driverport/-driverportrange.)
Memory Settings:
mapreduce.map.java.opts:     -Xms6g -Xmx6g -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Dlog4j.defaultInitOverride=true
Extra memory percent:        10
mapreduce.map.memory.mb:     6758
17/10/13 07:49:14 INFO client.RMProxy: Connecting to ResourceManager at host2/192.168.20.34:8050
17/10/13 07:49:14 INFO client.AHSProxy: Connecting to Application History server at host2/192.168.20.34:10200
17/10/13 07:49:15 WARN mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).

17/10/13 07:49:15 INFO mapreduce.JobSubmitter: number of splits:3
17/10/13 07:49:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1507793796947_0002
17/10/13 07:49:15 INFO mapred.YARNRunner: Job jar is not present. Not adding any jar to the list of resources.
17/10/13 07:49:15 INFO impl.YarnClientImpl: Submitted application application_1507793796947_0002
17/10/13 07:49:15 INFO mapreduce.Job: The url to track the job: http://host2:8088/proxy/application_1507793796947_0002/
Job name 'H2O_86929' submitted
JobTracker job ID is 'job_1507793796947_0002'
For YARN users, logs command is 'yarn logs -applicationId application_1507793796947_0002'
Waiting for H2O cluster to come up...
17/10/13 07:49:29 INFO client.RMProxy: Connecting to ResourceManager at host2/192.168.20.34:8050
17/10/13 07:49:29 INFO client.AHSProxy: Connecting to Application History server at host2/192.168.20.34:10200

----- YARN cluster metrics -----
Number of YARN worker nodes: 3

----- Nodes -----
Node: http://host5:8042 Rack: /default-rack, RUNNING, 1 containers used, 4,0 / 16,0 GB used, 1 / 3 vcores used
Node: http://host4:8042 Rack: /default-rack, RUNNING, 0 containers used, 0,0 / 16,0 GB used, 0 / 3 vcores used
Node: http://host3:8042 Rack: /default-rack, RUNNING, 0 containers used, 0,0 / 16,0 GB used, 0 / 3 vcores used

----- Queues -----
Queue name:            default
Queue state:       RUNNING
Current capacity:  0,11
Capacity:          1,00
Maximum capacity:  1,00
Application count: 1
----- Applications in this queue -----
Application ID:                  application_1507793796947_0002 (H2O_86929)
    Started:                     hdfs (Fri Oct 13 07:49:15 CEST 2017)
    Application state:           FINISHED
    Tracking URL:                http://host2:8088/proxy/application_1507793796947_0002/
    Queue name:                  default
    Used/Reserved containers:    1 / 0
    Needed/Used/Reserved memory: 4,0 GB / 4,0 GB / 0,0 GB
    Needed/Used/Reserved vcores: 1 / 1 / 0

Queue 'default' approximate utilization: 4,0 / 48,0 GB used, 1 / 9 vcores used

----------------------------------------------------------------------

ERROR: Unable to start any H2O nodes; please contact your YARN administrator.

   A common cause for this is the requested container size (6,6 GB)
   exceeds the following YARN settings:

       yarn.nodemanager.resource.memory-mb
       yarn.scheduler.maximum-allocation-mb





The corresponding error entry in the system log for the Yarn application:


2017-10-13 07:49:24,505 FATAL [IPC Server handler 1 on 40503] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1507793796947_0002_m_000002_0 - exited : java.lang.RuntimeException: java.lang.ClassNotFoundException: Class water.hadoop.h2omapper not found
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2241)
at org.apache.hadoop.mapreduce.task.JobContextImpl.getMapperClass(JobContextImpl.java:186)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:745)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)
Caused by: java.lang.ClassNotFoundException: Class water.hadoop.h2omapper not found
at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2147)
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2239)
... 8 more

2017-10-13 07:49:24,506 INFO [IPC Server handler 1 on 40503] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1507793796947_0002_m_000002_0: Error: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class water.hadoop.h2omapper not found
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2241)
at org.apache.hadoop.mapreduce.task.JobContextImpl.getMapperClass(JobContextImpl.java:186)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:745)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)
Caused by: java.lang.ClassNotFoundException: Class water.hadoop.h2omapper not found
at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2147)
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2239)
... 8 more

2017-10-13 07:49:24,507 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1507793796947_0002_m_000002_0: Error: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class water.hadoop.h2omapper not found
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2241)
at org.apache.hadoop.mapreduce.task.JobContextImpl.getMapperClass(JobContextImpl.java:186)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:745)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)
Caused by: java.lang.ClassNotFoundException: Class water.hadoop.h2omapper not found
at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2147)
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2239)
... 8 more



The full log is available 
here
.


Any help would be appreciated.


Best regards,
Markus","['hadoop', 'hadoop-yarn', 'h2o']",Unknown,,N/A
46613651,46613651,2017-10-06T20:42:52,2024-07-29 17:17:48Z,0,"Following the steps of Sparkling Water from the link 
http://h2o-release.s3.amazonaws.com/sparkling-water/rel-2.2/0/index.html
.


Running in terminal :


~/InstallFile/SparklingWater/sparkling-water-2.2.0$ bin/sparkling-shell --conf ""spark.executor.memory=1g""




Please setup SPARK_HOME variable to your Spark installation","['apache-spark', 'h2o', 'sparkling-water']",OneCricketeer,https://stackoverflow.com/users/2308683/onecricketeer,191k
46583393,46583393,2017-10-05T10:27:01,2017-10-05 16:55:52Z,0,"Facing issues in reading file from s3 after upgrading from h2o 3.10 to h2o 3.14


I am using the following format to read a file from s3 to h2o[using AWS RStudio standalone instance]


h2o.importFile(path = ""s3n://<AWS_ACCESS_KEY>:<AWS_SECRET_KEY>@bucket/path/to/file.csv"")



The above works with the old h2o package [3.10] but throws the below error with h2o r package [3.14]




""Error in h2o.importFolder(path, pattern = """", destination_frame = destination_frame,  :all files failed to import""




I have gone through the migration documentation below but can't seem to find any changes regarding this.


Link to 
documentation
.","['r', 'amazon-s3', 'h2o']",parth,https://stackoverflow.com/users/6779509/parth,"1,621"
46569298,46569298,2017-10-04T15:59:58,2017-10-04 23:19:11Z,0,"I'm running h2o from a docker image which has h2o_3.13.0.356 version. I want to command it from R but I have a different version. I can connect with the cluster but many functions don't work. 
For example 
h2o.importFile
 is throwing error:


ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/3/Parse)

water.exceptions.H2OIllegalArgumentException
[1] ""water.exceptions.H2OIllegalArgumentException: Unknown parameter: decrypt_tool""                               
[2] ""    water.api.Schema.fillFromParms(Schema.java:286)""                                                         
[3] ""    water.api.Handler.handle(Handler.java:46)""                                                               
[4] ""    water.api.RequestServer.serve(RequestServer.java:448)""                                                   
[5] ""    water.api.RequestServer.doGeneric(RequestServer.java:297)""                                               
[6] ""    water.api.RequestServer.doPost(RequestServer.java:223)""                                                  
[7] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                            
[8] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                            
[9] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                  
[10] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                              
[11] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                      
[12] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                               
[13] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                       
[14] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                           
[15] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                   
[16] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                         
[17] ""    water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:183)""                                               
[18] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                   
[19] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                         
[20] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                 
[21] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""          
[22] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""           
[23] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)""                
[24] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)""
[25] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)""                                        
[26] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)""                                   
[27] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                  
[28] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""            
[29] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                        
[30] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                         
[31] ""    java.lang.Thread.run(Thread.java:748)""                                                                   

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Unknown parameter: decrypt_tool



Is there any possibility to install R h2o package in the same version as the docker image has?","['r', 'docker', 'h2o']",filius_arator,https://stackoverflow.com/users/7515482/filius-arator,139
46558913,46558913,2017-10-04T07:05:49,2017-10-27 10:03:04Z,0,"When trying to install driverless-ai on AWS EC2, I execute all the steps until the step 6 in the guide:
""6. Start the Driverless AI docker image:""


I try the command described there:


nvidia-docker run --rm -u ‘id -u‘:‘id -g‘ -p 12345:12345 -p 9090:9090 -v ‘pwd‘/data:/data -v ‘pwd‘/log:/log -v ‘pwd‘/license:/license opsh2oai/h2oai-runtime



and get back:
unknown shorthand flag: 'g' in -g‘","['amazon-ec2', 'h2o', 'nvidia-docker']",neilson,https://stackoverflow.com/users/1701614/neilson,411
46553482,46553482,2017-10-03T21:10:44,2017-10-12 14:48:02Z,0,"I'm trying to import Glove to h2o cluster via R with word2vec function.
Regarding to this 
Does or will H2O provide any pretrained vectors for use with h2o word2vec?

I downloaded pretrained glove.840B.300d.txt file and tried to import it to h2o but there was problem with parsing.
Then I read Glove to R, removed one line recognized as a NA and saved it as csv. With the csv file parsing in h2o went well but I couldn't create word2vec model with it hence it threw 
java.lang.NullPointerException


I have 
h2o_3.15.0.99999
 version.


My code:


h2o.init()
glove<-h2o.importFile(""glove.840B.300d.csv"",header = F)
model<-h2o.word2vec(pre_trained = glove,vec_size = 300)



Full output:


|==========================================================================| 100%

java.lang.NullPointerException
java.lang.NullPointerException
at water.AutoBuffer.tcpOpen(AutoBuffer.java:488)
at water.AutoBuffer.sendPartial(AutoBuffer.java:679)
at water.AutoBuffer.putA4f(AutoBuffer.java:1383)
at hex.word2vec.Word2VecModel$Word2VecOutput$Icer.write90(Word2VecModel$Word2VecOutput$Icer.java)
at hex.word2vec.Word2VecModel$Word2VecOutput$Icer.write(Word2VecModel$Word2VecOutput$Icer.java)
at water.Iced.write(Iced.java:61)
at water.AutoBuffer.put(AutoBuffer.java:771)
at hex.Model$Icer.write86(Model$Icer.java)
at hex.word2vec.Word2VecModel$Icer.write85(Word2VecModel$Icer.java)
at hex.word2vec.Word2VecModel$Icer.write(Word2VecModel$Icer.java)
at water.Iced.write(Iced.java:61)
at water.Iced.asBytes(Iced.java:42)
at water.Value.<init>(Value.java:348)
at water.TAtomic.atomic(TAtomic.java:22)
at water.Atomic.compute2(Atomic.java:56)
at water.Atomic.fork(Atomic.java:39)
at water.Atomic.invoke(Atomic.java:31)
at water.Lockable.unlock(Lockable.java:181)
at water.Lockable.unlock(Lockable.java:176)
at hex.word2vec.Word2Vec$Word2VecDriver.computeImpl(Word2Vec.java:72)
at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:205)
at water.H2O$H2OCountedCompleter.compute(H2O.java:1263)
at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)","['r', 'word2vec', 'h2o']",filius_arator,https://stackoverflow.com/users/7515482/filius-arator,139
46537165,46537165,2017-10-03T04:21:20,2017-10-03 12:39:21Z,0,"Hi I have installed H2O for python on my AWS VM (Amazon Linux), python 3.5 by following the instructions here: 
http://h2o-release.s3.amazonaws.com/h2o/rel-ueno/8/index.html


But when I initialize h2o I am getting an error that I have version mismatch as h2o is running clusters on 3.10.4.8. 


I can run this in aws terminal:


cd ~/Downloads
unzip h2o-3.15.0.4049.zip
cd h2o-3.15.0.4049
java -jar h2o.jar



to get the latest h2o clusters, but I still get the same mismatch error. 


I could use an older version of the python h2o, but that wont work for me as I need the newer features in the latest release of h2o.","['python-3.x', 'amazon-ec2', 'h2o']",Woody Pride,https://stackoverflow.com/users/2484720/woody-pride,13.9k
46535110,46535110,2017-10-02T23:30:11,2017-10-02 23:57:42Z,0,"This question already has answers here
:
                                
                            










Error with H2O in R - can't connect to local host



                                (2 answers)
                            




Closed 
7 years ago
.








I'm trying to start H2O from within RStudio. I've got the latest versions of R, RStudio, the R H2O package, Java SE SDK and the RCurl package. But when trying to initialize H2O, I get the following output:


H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:

 C:\Users\fel069\AppData\Local\Temp\RtmpQZJyS1/h2o_fel069_started_from_r.out
 C:\Users\fel069\AppData\Local\Temp\RtmpQZJyS1/h2o_fel069_started_from_r.err

java version ""9""
Java(TM) SE Runtime Environment (build 9+181)
Java HotSpot(TM) 64-Bit Server VM (build 9+181, mixed mode)

    Starting H2O JVM and connecting:     ............................................................ 
[1] ""localhost""
[1] 54321
[1] TRUE
[1] -1
[1] ""Failed to connect to localhost port 54321: Connection refused""
[1] 127
Error in h2o.init(nthreads = -1, max_mem_size = ""8g"") : 
  H2O failed to start, stopping execution.
In addition: Warning message:
running command 'curl 'http://localhost:54321'' had status 127



Any help appreciated. Thanks!","['r', 'rstudio', 'h2o']",Sasquatch Man,https://stackoverflow.com/users/8687359/sasquatch-man,73
46525548,46525548,2017-10-02T12:34:11,2017-10-02 12:34:11Z,0,"I have just started using the h2o package in order to build a supervised NN network wit the deeplearning package.
In order to get a bit on track I started trying to simulate a function like X + Y = Z


My code is the following:
 


data <- read.table(""DeepLearningTest.csv"", header = FALSE, sep = "","", quote = """", stringsAsFactor = F)

test <- read.table(""DeepLearningTestRun.csv"", header = FALSE, sep = "","", quote = """", stringsAsFactor = F)

   df <- data.frame (data)
   tf <- data.frame (test)
   h2o.init ()
   hf <- as.h2o (df)

m2 <- h2o.deeplearning(
  training_frame=hf, 
  x=0:1,
  y='C',
  hidden = c(100),             
  epochs=100000,                    
  stopping_tolerance=0.001
)

h2o.predict (m2, as.h2o(tf))



Where my test samples are the following (for example):
 


1 1 2
2 2 4
3 3 6
4 4 8
. . .
until
2000 2000 4000


In general is X + X = 2X 


The thing I am not understanding and for which I am writing is that, if I use one layer network (for the univeral approximation theorem should be sufficient)
I can traing the network and the predict quite good values in the range of the prediction.


for instance the network is giving me 


100 100 200
101 101 202
7 7 14


but when I put 
4000 4000 


the result is misleading. It gives me something like 6300
It seems that the network is not able to generalized.
What am I doing wrong to make this behavior? 


Thank you for you attention.


Regards,
Nicola","['r', 'h2o']",Nicola,https://stackoverflow.com/users/8708198/nicola,37
46523998,46523998,2017-10-02T10:42:34,2017-10-02 16:43:51Z,533,"Is there a configuration which allows to run 
H2OXGBoostEstimator
 in multithreading and not in the minimal config with one CPU, with h2o version 3.15.0.4035?",['h2o'],nima,https://stackoverflow.com/users/7885172/nima,21
46490290,46490290,2017-09-29T13:52:31,2017-10-02 12:39:28Z,0,"h2o
 was working before on my laptop, but I didn't use it for a while 
(and have installed new packages and updated things in the meantime). Yesterday I tried using it, but it didn't work. I erased the 
R
 
h2o
 packaged and I've reinstalled 
h2o
 from scratch with


install.packages(""h2o"")



I tried running 
h2o
 with 
h2o.init()
 but it gives me this error


java version ""9""
Java(TM) SE Runtime Environment (build 9+181)
Java HotSpot(TM) 64-Bit Server VM (build 9+181, mixed mode)

Starting H2O JVM and connecting: ............................................................ 
[1] ""localhost""
[1] 54321
[1] TRUE
[1] -1
[1] ""Failed to connect to localhost port 54321: Connection refused""
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 54321: Connection refused
[1] 7
Error in h2o.init() : H2O failed to start, stopping execution.



When I look at the error log it shows


Only Java 1.6-1.8 supported, version is 9



I tried to look for more info and I think the error is triggered by 
this code
 on line 1836:


if (version != null && !(version.startsWith(""1.6"") || version.startsWith(""1.7"") || version.startsWith(""1.8""))) {
  System.err.println(""Only Java 1.6-1.8 supported, version is "" + version);
  return true;



So it seems 
h2o
 is checking for a version that starts with 1.6, 1.7 and 1.8 but my version for some reason starts with 9! 
However, on the terminal, 
java -version
 gives


openjdk version ""1.8.0_121""
OpenJDK Runtime Environment (Zulu 8.20.0.5-macosx) (build 1.8.0_121-b15)
OpenJDK 64-Bit Server VM (Zulu 8.20.0.5-macosx) (build 25.121-b15, mixed mode)



and 
which java
 gives


/Users/myusername/anaconda3/bin/java



------------ EDIT -------


More info: 
/usr/libexec/java_home -V
 gives


Matching Java Virtual Machines (4):
    9, x86_64:  ""Java SE 9"" /Library/Java/JavaVirtualMachines/jdk-9.jdk/Contents/Home
    1.8.0_144, x86_64:  ""Java SE 8"" /Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home
    1.8.0_51, x86_64:   ""Java SE 8"" /Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home
    1.8.0_25, x86_64:   ""Java SE 8"" /Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home



Which explains the 
9
 that 
h2o
 is seeing. I find it weird that it expects 
1.x.0
 but the version is just 
9
. In any case, it seems I need to point that 
h2o.init()
 command to the 
1.8
 java, but I couldn't figure out how to do it.


I usually use 
brew
 to install most things, but I never use 
java
 so I don't know anything about the versions.


Thanks and best!","['java', 'r', 'macos', 'h2o']",Unknown,,N/A
46479218,46479218,2017-09-28T22:33:34,2017-09-29 18:46:26Z,117,"Has anyone run benchmarks of Deep Water and Driverless AI on various video cards? Will a TitanX offer real-world benefit on these tools?


Thanks,


Marc",['h2o'],user1553220,https://stackoverflow.com/users/1553220/user1553220,139
46454196,46454196,2017-09-27T17:48:39,2017-10-03 16:51:14Z,285,"Walkthrough of Smile performance as compared to H2O.
Smile - Statistical Machine Intelligence and Learning Engine.
I Want to use Smile library for Constructing Pipeline using word2vec model.","['scala', 'h2o', 'smile']",roshan_ray,https://stackoverflow.com/users/6440286/roshan-ray,307
46426143,46426143,2017-09-26T12:08:57,2019-01-02 03:52:56Z,0,"By running the commands, 


m <- h2o.getModel(""depth_grid_model_4"")
h2o.varimp(m)



I am able to view the model's performance as well as the variable importance.  


How do I view the splits used in each tree of the GBM model?


Thanks","['r', 'h2o', 'gbm']",John Smith,https://stackoverflow.com/users/7611710/john-smith,51
46417508,46417508,2017-09-26T03:45:31,2017-09-28 03:33:46Z,0,"I am running the following code to connect to H2O in R and getting the output below:
    h2o.init(nthreads=-1, max_mem_size = paste(x,""g"", sep = """"))


R is connected to the H2O cluster: 
H2O cluster uptime:         3 minutes 57 seconds 
H2O cluster version:        3.14.0.3 
H2O cluster version age:    3 days  
H2O cluster name:           H2O_started_from_R_rami_krispin 
H2O cluster total nodes:    1 
H2O cluster total memory:   0.88 GB 
H2O cluster total cores:    4 
H2O cluster allowed cores:  4 
H2O cluster healthy:        TRUE 
H2O Connection ip:          localhost 
H2O Connection port:        54321 
H2O Connection proxy:       NA 
H2O Internal Security:      FALSE 
H2O API Extensions:         Algos, AutoML, Core V3, Core V4 
R Version:                  R version 3.4.1 (2017-06-30) 



Is there a way to capture into boolean variable whatever the connection was successful (something like 
is.h2o.connect()
 with response of True/False) and the attribute of the connection into variable? For example:


x$port
>54321



or


x$memory
>0.88 GB



Thank you in advance!
Rami","['r', 'h2o']",Rami Krispin,https://stackoverflow.com/users/5769996/rami-krispin,99
46412745,46412745,2017-09-25T19:26:26,2017-09-26 19:16:48Z,78,"I have a DRF model created in 
h2o flow
 that is supposed to be binomial and 
flow
 indicates that it is binomial




but I am having a problem where, importing it into 
h2o steam
 and deploying it to the prediction service, the model does not seem to be recognized as binomial. The reason I think this is true is shown below. The reason this is a problem is because I think it is what is causing the prediction service to NOT show the confidence value for the prediction (this reasoning is also shown below).


In the prediction service, I can get a prediction label, but no values filled in the index-label-probability table.




Using the browser inspector (google chrome), the prediction output seems to depend on a file called 
predict.js
.




In order to get the prediction probability values to show in the prediction service, it seems like 
this
 block of code needs to run to get to 
this
 line. Opening the 
predict.js
 file within the inspector on the prediction service page and adding some debug output statements at some of the top lines (indicated with DEBUG/ENDDEBUG comments in the code below), my 
showResults
() function then looks like:


function showResult(div, status, data) {
    ////////// DEBUG
    console.log(""showResult entered"")
    ////////// ENDDEBUG

    var result = '<legend>Model Predictions</legend>'

    //////////  DEBUG
    console.log(data)
    console.log(data.classProbabilities)
    console.log(""**showResult: isBinPred="" + isBinaryPrediction)
    ////////// ENDDEBUG

    if (data.classProbabilities) {
      ////////// DEBUG
      console.log(""**showResult: data.classProbabilities not null"")
      ////////// ENDDEBUG

      // binomial and multinomial
      var label = data.label;
      var index = data.labelIndex;
      var probs = data.classProbabilities;
      var prob = probs[index];

      result += '<p>Predicting <span class=""labelHighlight"">' + label + '</span>';
      if (probs.length == 2) {
        result += ' based on max F1 threshold </p>';
      }
      result += ' </p>';
      result += '<table class=""table"" id=""modelPredictions""> \
                  <thead> \
                    <tr> \
                      <th>Index</th> \
                      <th>Labels</th> \
                      <th>Probability</th> \
                    </tr> \
                   </thead> \
                   <tbody> \
                  ';

      if (isBinaryPrediction) {
        var labelProbabilitiesMapping = [];
        outputDomain.map(function(label, i) {
          var labelProbMap = {};
          labelProbMap.label = outputDomain[i];
          labelProbMap.probability = probs[i];
          if (i === index) {
            labelProbMap.predicted = true;
          }
          labelProbMap.originalIndex = i;
          labelProbabilitiesMapping.push(labelProbMap);
        });
        labelProbabilitiesMapping.sort(function(a, b) {
          return b.probability - a.probability;
        });
        var limit = labelProbabilitiesMapping.length > 5 ? 5 : labelProbabilitiesMapping.length;
        for (var i = 0; i < limit; i++) {
          if (labelProbabilitiesMapping[i].predicted === true) {
            result += '<tr class=""rowHighlight"">'
          } else {
            result += '<tr>'
          }
          result += '<td>' + labelProbabilitiesMapping[i].originalIndex + '</td><td>' + labelProbabilitiesMapping[i].label + '</td> <td>' + labelProbabilitiesMapping[i].probability.toFixed(4) + '</td></tr>';
        }
      } else {
        for (var label_i in outputDomain) {
          if (parseInt(label_i) === index ){
            result += '<tr class=""rowHighlight"">'
          } else {
            result += '<tr>'
          }
          result += '<td>' + label_i + '</td><td>' + outputDomain[label_i] + '</td> <td>' + probs[label_i].toFixed(4) + '</td></tr>';
        }
      }

      result += '</tbody></table>';
    }
    else if (""cluster"" in data) {
      // clustering result
      result = ""Cluster <b>"" + data[""cluster""] + ""</b>"";
    }
    else if (""value"" in data) {
      // regression result
      result = ""Value <b>"" + data[""value""] + ""</b>"";
    }
    else if (""dimensions"" in data) {
      // dimensionality reduction result
      result = ""Dimensions <b>"" + data[""dimensions""] + ""</b>"";
    }
    else {
      result = ""Can't parse result: "" + data;
    }

    div.innerHTML = result;
  }



and clicking the ""predict"" in the prediction service now generates the console output:




If I were to add a statement 
isBinaryPrediction = true
 to forcec the global variable to true (around 
here
) and run the prediction again, the console shows:




indicating that the 
variable
 
outputDomain
 is undefined. The variable 
outputDomain
 seems to be set in the 
function
 
showModel
. This function appears to run when the page loads, so I can't edit it in the chrome inspector to see what the variable values are. If anyone knows how to fix this problem (getting the prediction probability values to show up for 
h2o steam
's prediction service for binomial models) it would a big help. Thanks :)",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
46392819,46392819,2017-09-24T17:22:00,2017-09-24 20:29:58Z,0,"I am writing a Spring boot app (java class) which calls the H2o predict method which is inside another java class.


I have tested this java class independently and I can read the MOJO model if it is in the same place from where java is invoked and I can make predictions.


With the Spring boot App, This time my resultant jar of the maven project compilation cannot read that zip.
I use the standard way to read the MOJO zip file using the Wrapper h2o provides.


EasyPredictModelWrapper model = new EasyPredictModelWrapper(MojoModel.load(""DRF_model_python_1504558159175_1.zip""));



My Maven project structure looks like this:




The generated jar of the Spring boot App also contains the zip as shown below:




I don't understand why it gives me the error (it is an IOexception) that it cannot find the MOJO zip file.


File DRF_model_python_1504558159175_1.zip cannot be found.



I think the solutions could be:
1. Adding something in the Maven pom file so that the resultant jar knows where to pick up the model from.
OR
2. If the MojoModel.load method accepts a path to the file rather than just the file name. But I think this doesn't work.


Any thoughts?","['java', 'maven', 'spring-boot', 'machine-learning', 'h2o']",kivk02,https://stackoverflow.com/users/4152164/kivk02,651
46392394,46392394,2017-09-24T16:40:59,2018-01-17 04:39:49Z,0,"I can't get the h2o to work in my R. It shows the following error. Have no clue what it means. Previously it gave me an error because I didn't have Java 64 bit version. I downloaded the 64bit - restarted my pc - and started the process again and now it gives me this error.


Any suggestions? 


library(h2o)

----------------------------------------------------------------------

Your next step is to start H2O:
    > h2o.init()

For H2O package documentation, ask for help:
    > ??h2o

After starting H2O, you can use the Web UI at http://localhost:54321
For more information visit http://docs.h2o.ai

----------------------------------------------------------------------


Attaching package: ‘h2o’

The following objects are masked from ‘package:stats’:

    cor, sd, var

The following objects are masked from ‘package:base’:

    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames, colnames<-, ifelse,
    is.character, is.factor, is.numeric, log, log10, log1p, log2, round, signif, trunc

> h2o.init(nthreads = -1)

H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    C:\Users\ADM_MA~1\AppData\Local\Temp\RtmpygK1EJ/h2o_Adm_Mayur_started_from_r.out
    C:\Users\ADM_MA~1\AppData\Local\Temp\RtmpygK1EJ/h2o_Adm_Mayur_started_from_r.err

java version ""9""
Java(TM) SE Runtime Environment (build 9+181)
Java HotSpot(TM) 64-Bit Server VM (build 9+181, mixed mode)

Starting H2O JVM and connecting: ............................................................ 
[1] ""localhost""
[1] 54321
[1] TRUE
[1] -1
[1] ""Failed to connect to localhost port 54321: Connection refused""
[1] 127
Error in h2o.init(nthreads = -1) : 
  H2O failed to start, stopping execution.
In addition: Warning message:
running command 'curl 'http://localhost:54321'' had status 127 



Screenshot for h2o error in R","['r', 'h2o']",Roman Luštrik,https://stackoverflow.com/users/322912/roman-lu%c5%a1trik,70.5k
46374929,46374929,2017-09-23T00:47:28,2017-09-23 00:52:34Z,620,"Using 
h2o steam
's prediction service for a deployed model, the default threshold that seems to be used by the prediction service is the max f1 threshold. However, in my case I would like the be able to use other thresholds (as displayed by the model when built in 
h2o flow
) (eg. max f2 or max accuracy thresholds) like these. 




Is there a way to set these thresholds in 
steam
?


Looking at the inspector on the prediction service page, seems to shows that the logic for the predictor is from a script called ""predict.js"" (see below):




But I can't find where in the 
steam
 launch directory (running from local host based on 
these
 instructions) these files are (doing a file search in this directory for anything named ""predict.js"" returns nothing).",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
46373561,46373561,2017-09-22T21:36:50,2017-09-22 21:43:34Z,140,"I am importing a model from 
h2o flow
 into 
h2o steam
 and deploying it as a prediction service. A problem that I am having is that the model has a date input feature that was converted to a 
time
 type field when loading the training data .csv for the model in 
h2o flow
.




These time values are converted to (I think) POSIX timestamps (in milliseconds) in the parsed .hex file in 
h2o flow
.




Thus, when I deploy models trained on this data in 
steam
's prediction service, the input fields expect 
Double
 values (the timestamps) rather than any kind of date string (eg. ""2016-12-21"") which human users of this service are expecting to be able to enter. This is the error that the 
steam
 prediction service gives me for input date 2016-12-21. 




Is there any way around this? The service needs to be used by humans and having to have users enter POSIX millisecond timestamps conversions of actual dates makes it unusable. Currently just using a model that does not include date inputs.",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
46372293,46372293,2017-09-22T19:53:59,2017-09-22 21:30:33Z,79,"When trying to use the 
prediction service
 for a model deployed by 
steam
, this is what I see:




Notice that when I click the ""Predict"" button, I get a prediction label response from the model. But there are no input fields being displayed. Why is this happening?


I start my steam session like this:




I launch 
h2o flow


java -Xmx4g -jar h2o.jar


I start the steam jetty server for the prediction service (as instructed 
here
): 


java -Xmx6g -jar var/master/assets/jetty-runner.jar var/master/assets/ROOT.war




I use -Xmx6g because I was getting a 
java.lang.OutOfMemoryError

 from the prediction service earlier.




I launch the steam server:


./steam serve master --prediction-service-host=localhost --prediction-service-port-range=12345:22345




I use a custom port range for the prediction service since I was having problems deploying models from 
steam
 where it could not access port 8080 (if anyone knows a better way around this please let me know). From here, I import model from the localhost 
h2o flow
 server in 
steam
 and deploy it to get the screen show at the top of this post.


I was having problems before where the prediction service 
builder
 server (launched with 
GRADLE_OPTS=-Xmx6g ./gradlew jettyRunWar
 following the instruction 
here
) was not showing input fields for .war files built from mojos (see 
here
), but I am using a model imported directly from 
h2o flow
 into 
steam
 in this case. If anyone knows what is going on here it would be a big help. Thanks :) 


UPDATE

Used a smaller similar model (POJO size of ~200MB) and can now see input fields (after waiting on the prediction service screen for ~10sec.). Can't tell what kind of file the model is currently being transferred as under the hood though, I assume POJO now. One weird thing though is that the input fields also include the models binomial response labels (as if the user could just choose the response as input).",['h2o'],Unknown,,N/A
46369378,46369378,2017-09-22T16:33:41,2019-07-09 16:44:02Z,0,"Using H2O is mandatory, Perform Embedding on the text file(dataset as input) using word2vec  model to produce an output as Vectors(arrays).
The word2Vec model will be using is below-
https://code.google.com/archive/p/word2vec/

which is written in C/C++.
So, how can one implement using H2O?","['scala', 'word2vec', 'h2o']",Unknown,,N/A
46355861,46355861,2017-09-22T02:19:18,2017-09-27 09:32:25Z,107,"I am trying to use 
h2o steam
 (running on localhost) to deploy a model. After importing the model from 
h2o flow
, clicking the 
""deploy model""
 option in the 
""models""
 section of the project, filling out the resulting dialog box, and clicking the ""deploy"" button, the following messages are displayed:


 


At first I thought that it was because maybe I needed to start up the service builder on my own, so I started it up following the docs 
here
, but still got the same error. Any suggestions would be appreciated. Thanks :)",['h2o'],Unknown,,N/A
46355251,46355251,2017-09-22T00:47:55,2017-09-22 21:00:03Z,165,"I trying to build a .war file in 
h2o steam
's prediction service builder using a (~800MB) 
pojo
 file (a similara pojo of size ~200MB also produced these same problems). However, when trying this, an error appears after clicking 'build':


Problem accessing /makewar. Reason:

    Compilation of pojo failed exit value 3  warning: [options] bootstrap class path not set in conjunction with -source 1.6


The system is out of resources.
Consult the following stack trace for details.
java.lang.OutOfMemoryError
    at java.io.FileInputStream.readBytes(Native Method)
    at java.io.FileInputStream.read(FileInputStream.java:255)
    at com.sun.tools.javac.util.BaseFileManager.makeByteBuffer(BaseFileManager.java:302)
    at com.sun.tools.javac.file.RegularFileObject.getCharContent(RegularFileObject.java:114)
    at com.sun.tools.javac.file.RegularFileObject.getCharContent(RegularFileObject.java:53)
    at com.sun.tools.javac.main.JavaCompiler.readSource(JavaCompiler.java:602)
    at com.sun.tools.javac.main.JavaCompiler.parse(JavaCompiler.java:665)
    at com.sun.tools.javac.main.JavaCompiler.parseFiles(JavaCompiler.java:950)
    at com.sun.tools.javac.main.JavaCompiler.compile(JavaCompiler.java:857)
    at com.sun.tools.javac.main.Main.compile(Main.java:523)
    at com.sun.tools.javac.main.Main.compile(Main.java:381)
    at com.sun.tools.javac.main.Main.compile(Main.java:370)
    at com.sun.tools.javac.main.Main.compile(Main.java:361)
    at com.sun.tools.javac.Main.compile(Main.java:56)
    at com.sun.tools.javac.Main.main(Main.java:42)



I am launching the Prediction Service Builder from the command line following the instruction in 
this
 documentation. Is there a way to launch the service builder with more memory?


UPDATE

Using the command:

$ GRADLE_OPTS=-Xmx4g ./gradlew jettyRunWar


Trying to build a .war from the pojo returns the cli error:


SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/reedv/.gradle/wrapper/dists/gradle-2.7-all/2glqtbnmvcq45bfjvhghri39p6/gradle-2.7/lib/gradle-core-2.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/reedv/Documents/h2o_production/h2o-steam/steam/prediction-service-builder/build/tmp/jettyRunWar/webapp/WEB-INF/lib/slf4j-simple-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.SimpleLoggerFactory]
2017-09-21 15:22:48.084 -1000 [1222676357@qtp-1014435252-3] INFO MakeWarServlet - servletPath = /home/reedv/Documents/h2o_production/h2o-steam/steam/prediction-service-builder/build/tmp/jettyRunWar/webapp
2017-09-21 15:22:48.086 -1000 [1222676357@qtp-1014435252-3] INFO MakeWarServlet - tmpDir /tmp/makeWar316567921053262563022859149567148
2017-09-21 15:22:57.175 -1000 [1222676357@qtp-1014435252-3] INFO MakeWarServlet - added pojo model drf_denials_v4_v3-10-5-2.java
2017-09-21 15:22:57.190 -1000 [1222676357@qtp-1014435252-3] INFO MakeWarServlet - prejar null  preclass null
2017-09-21 15:22:58.047 -1000 [1222676357@qtp-1014435252-3] INFO Util - warning: [options] bootstrap class path not set in conjunction with -source 1.6
2017-09-21 15:23:25.017 -1000 [1190941229@qtp-1014435252-0] INFO MakeWarServlet - tmpDir /tmp/makeWar432278342000106527922896081353600
2017-09-21 15:23:39.448 -1000 [1190941229@qtp-1014435252-0] INFO MakeWarServlet - added pojo model drf_denials_v4_v3-10-5-2.java
2017-09-21 15:23:39.569 -1000 [1190941229@qtp-1014435252-0] INFO MakeWarServlet - prejar null  preclass null
2017-09-21 15:23:40.651 -1000 [1190941229@qtp-1014435252-0] INFO Util - warning: [options] bootstrap class path not set in conjunction with -source 1.6
2017-09-21 15:23:57.124 -1000 [1190941229@qtp-1014435252-0] INFO Util - OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000006efd00000, 1592786944, 0) failed; error='Cannot allocate memory' (errno=12)
2017-09-21 15:23:57.604 -1000 [1190941229@qtp-1014435252-0] INFO Util - #
2017-09-21 15:23:57.605 -1000 [1190941229@qtp-1014435252-0] INFO Util - # There is insufficient memory for the Java Runtime Environment to continue.
2017-09-21 15:23:57.616 -1000 [1190941229@qtp-1014435252-0] INFO Util - # Native memory allocation (mmap) failed to map 1592786944 bytes for committing reserved memory.
2017-09-21 15:23:57.619 -1000 [1190941229@qtp-1014435252-0] INFO Util - # An error report file with more information is saved as:
2017-09-21 15:23:57.622 -1000 [1190941229@qtp-1014435252-0] INFO Util - # /tmp/makeWar432278342000106527922896081353600/hs_err_pid32313.log
2017-09-21 15:23:57.747 -1000 [1190941229@qtp-1014435252-0] ERROR MakeWarServlet - doPost failed 
java.lang.Exception: Compilation of pojo failed exit value 1  warning: [options] bootstrap class path not set in conjunction with -source 1.6
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000006efd00000, 1592786944, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 1592786944 bytes for committing reserved memory.
# An error report file with more information is saved as:
# /tmp/makeWar432278342000106527922896081353600/hs_err_pid32313.log

    at ai.h2o.servicebuilder.Util.runCmd(Util.java:162)
    at ai.h2o.servicebuilder.MakeWarServlet.doPost(MakeWarServlet.java:151)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:390)
    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:440)
    at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
    at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)
    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
    at org.mortbay.jetty.Server.handle(Server.java:326)
    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:943)
    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)
    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)
    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
    at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
    at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
2017-09-21 15:23:58.039 -1000 [1190941229@qtp-1014435252-0] ERROR MakeWarServlet - Compilation of pojo failed exit value 1  warning: [options] bootstrap class path not set in conjunction with -source 1.6
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000006efd00000, 1592786944, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 1592786944 bytes for committing reserved memory.
# An error report file with more information is saved as:
# /tmp/makeWar432278342000106527922896081353600/hs_err_pid32313.log



Increasing the memory allocation value to Xmx6g or Xmx7g still gives this same error.


Furthermore, looking for the file 
/tmp/makeWar432278342000106527922896081353600/hs_err_pid32313.log
 that was supposedly created by this error, there seems to not be a directory named ""makeWar432278342000106527922896081353600"" in my root tmp/ directory, so I'm not really sure where to look for it.",['h2o'],Unknown,,N/A
46353578,46353578,2017-09-21T21:42:13,2017-09-21 22:56:47Z,367,"In 
h2o
's documentation for the Steam Prediction Service Builder, 
here
, it says that the service builder can compile both 
h2o
 
pojo
s (.java files) and 
mojo
s (downloaded from 
h2o flow
 in my case as a .zip (version 3.10.5.2), which I have been using in the manner shown 
here
). However, doing something like this:




gives this error:


Problem accessing /makewar. Reason:

    Compilation of pojo failed exit value 1  warning: [options] bootstrap class path not set in conjunction with -source 1.6
error: Class names, 'drf_denials_v4.zip', are only accepted if annotation processing is explicitly requested
1 error
1 warning



So how can I use mojo files in the servicec builder? DO I need to use the ""exported"" model file from 
h2o flow
 rather than the ""downloaded"" zip file? The reason that I need to use 
mojo
s rather than the .java 
pojo
s is that my model is too large to fit in the 
pojo
 downloadable from 
h2o flow
.


UPDATE
:


Trying to use the CLI with the command:


$ curl -X POST --form mojo=@drf_denials_v4.zip --form 
[email protected]
 localhost:55000/makewar > drf_denials_v4.war
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  106M  100 53.6M  100 52.7M  6748k  6632k  0:00:08  0:00:08 --:--:--  229k



in the dir. containing the relevant files, then using the command:


prediction-service-builder git:(master)$ java -jar jetty-runner-9.3.9.M1.jar --port 55001 ~/Documents/h2o_production/mojos/drf_denials_v4/drf_denials_v4.war



gives the output:


2017-09-21 12:33:58.226:INFO::main: Logging initialized @232ms
2017-09-21 12:33:58.234:INFO:oejr.Runner:main: Runner
2017-09-21 12:33:58.558:INFO:oejs.Server:main: jetty-9.3.9.M1
2017-09-21 12:33:59.557:WARN:oeja.AnnotationConfiguration:main: ServletContainerInitializers: detected. Class hierarchy: empty
2017-09-21 12:34:00.068 -1000 [main] INFO ServletUtil - modelNames size 1
2017-09-21 12:34:01.285 -1000 [main] INFO ServletUtil - added model drf_denials_v4  new size 1
2017-09-21 12:34:01.290 -1000 [main] INFO ServletUtil - added 1 models
2017-09-21 12:34:01.291:INFO:oejsh.ContextHandler:main: Started o.e.j.w.WebAppContext@4c75cab9{/,file:///tmp/jetty-0.0.0.0-55001-drf_denials_v4.war-_-any-39945022624149883.dir/webapp/,AVAILABLE}{file:///home/reedv/Documents/h2o_production/mojos/drf_denials_v4/drf_denials_v4.war}
2017-09-21 12:34:01.321:INFO:oejs.AbstractConnector:main: Started ServerConnector@176c9571{HTTP/1.1,[http/1.1]}{0.0.0.0:55001}
2017-09-21 12:34:01.322:INFO:oejs.Server:main: Started @3329ms



Going to localhost:55001, and trying to make a prediction, I see:



Notice that a prediction is given with a label, but there are no parameter input fields present and I get the cli error message:


2017-09-21 12:35:11.270:WARN:oejs.ServletHandler:qtp1531448569-12: Error for /info
java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:3332)
    at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
    at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448)
    at java.lang.StringBuffer.append(StringBuffer.java:270)
    at java.io.StringWriter.write(StringWriter.java:101)
    at java.io.StringWriter.append(StringWriter.java:143)
    at java.io.StringWriter.append(StringWriter.java:41)
    at com.google.gson.stream.JsonWriter.value(JsonWriter.java:519)
    at com.google.gson.internal.bind.TypeAdapters$5.write(TypeAdapters.java:210)
    at com.google.gson.internal.bind.TypeAdapters$5.write(TypeAdapters.java:194)
    at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:68)
    at com.google.gson.internal.bind.ArrayTypeAdapter.write(ArrayTypeAdapter.java:93)
    at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:68)
    at com.google.gson.internal.bind.ArrayTypeAdapter.write(ArrayTypeAdapter.java:93)
    at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:68)
    at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:112)
    at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:239)
    at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:68)
    at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:112)
    at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:239)
    at com.google.gson.Gson.toJson(Gson.java:661)
    at com.google.gson.Gson.toJson(Gson.java:640)
    at com.google.gson.Gson.toJson(Gson.java:595)
    at com.google.gson.Gson.toJson(Gson.java:575)
    at InfoServlet.doGet(InfoServlet.java:59)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:837)
    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)
    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
    at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
    at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)



The cli 
pojo
 
example
 works, but trying to use my 
mojo
 zip does not.",['h2o'],Unknown,,N/A
46333680,46333680,2017-09-21T00:39:07,2017-09-21 02:09:03Z,365,"I would like to let users make single predictions in real time (where they enter the features themselves) using a model saved to a remote 
h2o flow
 instance. Is there any way to do this or something like this?


I have a trained model saved to a remote 
h2o flow
 instance and would like to give other users on my local network the ability to go to the 
h2o
 instance url in their web browsers and make single predictions based on features that they input themselves (ie. not based on a .csv or .hex file uploaded to the flow instance). Basically, the ideal situation would be if h2o flow had some higher level web interface where you could just 'hook up' a model and enter in fill in boxes for different feature values and hit a button to get a prediction on those features. Is there anything remotely like this or a similar workaround?",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
46328579,46328579,2017-09-20T17:42:15,2017-09-22 07:40:08Z,301,"Is there a way to see how the categorical features are encoded when we allow h2o to automatically create categorical data by casting a column to 
enum
 type? 


I am implementing holdout stacking where my underlying training data differs for each model. I have a common feature that I want to make sure is encoded the same way across both sets. The feature contains names (str). It is guaranteed that all names that appear in one data set will be appear in the other.","['python', 'h2o', 'categorical-data']",joceratops,https://stackoverflow.com/users/5360652/joceratops,417
46317073,46317073,2017-09-20T08:30:56,2017-09-22 08:04:32Z,0,"I am working with a huge 
H2OFrame
 (~150gb, ~200 million rows), which I need to manipulate a little. To be more specific: I have to use the frame's 
ip
 column, to find the location/city names for each IP and add this information to each of the frame's rows. 


Converting the frame to a plain python object and manipulating it locally is not an option, due to the huge size of the frame. So what I was hoping I could do is to use my H2O cluster to create a new H2OFrame 
city_names
 using the original frame's 
ip
 column and then merge both frames.


My question is kind of similar to the question posed 
here
, and what I gathered from this question's answer is that there is no way in H2O to do complex manipulations of each of the frame's rows. Is that really the case? 
H2OFrame
's 
apply
 function only accepts a lambda without custom methods after all.


One option I thought of was to use 
Spark/Sparkling Water
 for this kind of data manipulation and then convert the spark frame to an H2OFrame to do the machine learning operations. However, if possible I would prefer to avoid that and only use H2O, not least due to the overhead that such a conversion creates.


So I guess it comes down to this: Is there any way to do this kind of manipulation using only H2O? And if not is there another option to do this without having to change my cluster architecture (i.e. without having to turn my H2O cluster into a sparkling water cluster?)","['python', 'h2o', 'sparkling-water']",Unknown,,N/A
46296441,46296441,2017-09-19T09:14:59,2017-09-19 14:26:13Z,0,"In the R version of H2O, is it possible to specify a blocking factor when splitting data in training/validation/test sets and/or when doing cross-validation?


I'm working on a clinical dataset with multiple observations from the same patient that should be kept together during these operations.


If this is not possible to do within the H2O framework then suggestions on how to achieve this in R and integrate with H2O functions would be great.


Thanks!","['r', 'h2o']",Unknown,,N/A
46253317,46253317,2017-09-16T11:18:52,2017-09-17 00:21:45Z,618,"I'm trying to calculate performance in a different way how it is built in for models right now.


I would like to access raw predictions during cross-validation, so I can calculate performance on my own.


g = h2o.get_grid(grid_id)
for m in g.models:
    print ""Model %s"" % m.model_id
    rrc[m.model_id] = m.cross_validation_holdout_predictions()



I could just run prediction with a model on my dataset, but I think then this test might be biased because the model has seen this data before, or not? Can I take new predictions made on the same data set and use it to calculate performance?",['h2o'],Unknown,,N/A
46243483,46243483,2017-09-15T15:56:06,2017-11-12 16:44:26Z,0,"I'm trying to run H2O's anomaly detection in R (h2o_3.14.0.2).


First, I've tried to use my main deep learning model and got the error:


water.exceptions.H2OIllegalArgumentException
 [1] ""water.exceptions.H2OIllegalArgumentException: Only for AutoEncoder Deep Learning model.""
 ...



OK, my bad. I've set 
autoencoder
 to 
TRUE
:


h2o.deeplearning(y = response, training_frame = training.frame, validation_frame = test.frame, autoencoder = TRUE)



And got new error:


Error in .verify_dataxy(training_frame, x, y, autoencoder): `y` should not be specified for autoencoder=TRUE, remove `y` input
Traceback:

1. h2o.deeplearning(y = response, training_frame = training.frame, 
 .     validation_frame = test.frame, autoencoder = TRUE)
2. .verify_dataxy(training_frame, x, y, autoencoder)
3. stop(""`y` should not be specified for autoencoder=TRUE, remove `y` input"")



OK, so I should've removed 
y
:


h2o.deeplearning(training_frame = training.frame, validation_frame = test.frame, autoencoder = TRUE)



But:


Error in is.numeric(y): argument ""y"" is missing, with no default
Traceback:

1. h2o.deeplearning(training_frame = training.frame, validation_frame = test.frame, 
 .     autoencoder = TRUE)
2. is.numeric(y)



Hm, the last two requirements look mutually exclusive. But OK, I'll try another model:


anomaly.detection.model <- h2o.glrm(training_frame = training.frame, k = 10, seed = common.seed)

h2o.anomaly(anomaly.detection.model, training.frame, per_feature = FALSE)



And get another type of error:


java.lang.AssertionError
 [1] ""java.lang.AssertionError""                                                                                    
 [2] ""    water.api.ModelMetricsHandler.predict(ModelMetricsHandler.java:439)""
 ...



The failed assertion is 
assert s.reconstruct_train;
. Didn't dig into it yet. Maybe I will have luck with GBM or RF?


model = h2o.gbm(y = response,
                training_frame = training.frame,
                validation_frame = validation.frame,
                max_hit_ratio_k = 10,
                seed = common.seed,
                stopping_rounds = 3,
                stopping_tolerance = 1e-2)

h2o.anomaly(model, training.frame, per_feature = FALSE)

water.exceptions.H2OIllegalArgumentException
 [1] ""water.exceptions.H2OIllegalArgumentException: Requires a Deep Learning, GLRM, DRF or GBM model.""



And the same for RF.


So I have two questions:




How to detect anomalies?


Are these are bugs or I did something wrong?","['r', 'h2o']",Igor Melnichenko,https://stackoverflow.com/users/3652569/igor-melnichenko,145
46235467,46235467,2017-09-15T08:49:25,2017-09-15 10:29:07Z,326,"I am using Mojo model for scoring my queries. I am using this statement to use mojo model


EasyPredictModelWrapper model = new EasyPredictModelWrapper(MojoModel.load(""gbm.zip""));



The question is can i use this model object from different threads? Is it thread safe?","['thread-safety', 'h2o', 'mojo']",user3553836,https://stackoverflow.com/users/3553836/user3553836,95
46220893,46220893,2017-09-14T13:46:41,2019-10-25 00:56:24Z,689,"This seems to be related to the issue specified 
here
 and seems resolved now but I am doing a grid search (alpha values for GLM - gamma with log family) using Python API and getting the following error (for all alpha values):


Rollups not possible, because Vec was deleted: $04ff12000000fffffffff6664ae26e403db2c741167b02d01a0f$


`


Hyper-parameter: alpha, [0.0]
failure_details: 1701
failure_stack_traces: java.lang.ArrayIndexOutOfBoundsException: 1701
    at hex.optimization.OptimizationUtils$MoreThuente.evaluate(OptimizationUtils.java:362)
    at hex.glm.GLM$GLMDriver.fitIRLSM(GLM.java:691)
    at hex.glm.GLM$GLMDriver.fitModel(GLM.java:945)
    at hex.glm.GLM$GLMDriver.computeSubmodel(GLM.java:1029)
    at hex.glm.GLM$GLMDriver.computeImpl(GLM.java:1098)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:173)
    at hex.glm.GLM$GLMDriver.compute2(GLM.java:543)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1255)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



`


Changing to L-BFGS solver I get:
`


Hyper-parameter: alpha, [0.0]
failure_details: Rollups not possible, because Vec was deleted: $04ff12000000ffffffff29c907ce45483f9f244f54c4d0$%;K
failure_stack_traces: java.lang.RuntimeException: Rollups not possible, because Vec was deleted: $04ff12000000ffffffff29c907ce45483f9f244f54c4d0$%;K
    at water.fvec.RollupStats.get(RollupStats.java:322)
    at water.fvec.RollupStats.get(RollupStats.java:352)
    at water.fvec.Vec.rollupStats(Vec.java:847)
    at water.fvec.Vec.checksum_impl(Vec.java:866)
    at water.Keyed.checksum(Keyed.java:69)
    at water.fvec.Frame.checksum_impl(Frame.java:558)
    at water.Keyed.checksum(Keyed.java:69)
    at hex.Model$Parameters.checksum_impl(Model.java:394)
    at hex.Model$Parameters.checksum(Model.java:325)
    at hex.grid.GridSearch$2.filter(GridSearch.java:298)
    at water.KeySnapshot.filter(KeySnapshot.java:65)
    at hex.grid.GridSearch.buildModel(GridSearch.java:290)
    at hex.grid.GridSearch.gridSearch(GridSearch.java:213)
    at hex.grid.GridSearch.access$000(GridSearch.java:68)
    at hex.grid.GridSearch$1.compute2(GridSearch.java:135)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1255)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



`","['python', 'h2o']",Unknown,,N/A
46220306,46220306,2017-09-14T13:20:12,2017-09-14 14:34:22Z,0,"I have a model built in R that runs via POJO on an H2O cluster. I need the values spat out by the model to be fed into an R script that does a bunch of calculations/scaling for our purposes.




Can this script be called directly from H2O? 


Is my understanding correct in that 
only
 ML models created in H2O can be exported to a POJO? Or would it be possible to export this R script as a POJO as well?




Edit: Adding more details for clarity.


An existing POJO is used in our production instance of H2O, and is invoked via a service that returns the required data as a JSON object. The JSON object is then passed by the service to the R script, which does a bunch of calculations/scaling and then returns the results. Essentially, the R script is not part of the H2O ecosystem and I'm trying to make it so. Ideally I'd like to export the script as a POJO but I'm not sure if it's possible, given that the script isn't a model per se.","['r', 'pojo', 'h2o']",Unknown,,N/A
46205042,46205042,2017-09-13T19:02:07,2017-09-13 21:05:34Z,0,"I have generated a POJO from an H2O model.


On a separate EC2 instance, I want to deploy this model to a Jetty servelet to be used as an API scoring endpoint.  How can I do this?


1. Example POJO


Here is a minimal example, 
demo_glm
, predicting Titanic passenger survival as a logistic regression on age, fare class, and sex:


/*
  Licensed under the Apache License, Version 2.0
    http://www.apache.org/licenses/LICENSE-2.0.html

  AUTOGENERATED BY H2O at 2017-09-13T17:30:17.931Z
  3.13.0.3908

  Standalone prediction code with sample test data for GLMModel named demo_glm

  How to download, compile and execute:
      mkdir tmpdir
      cd tmpdir
      curl http://XXX.XX.XX.XXX:54321/3/h2o-genmodel.jar > h2o-genmodel.jar
      curl http://XXX.XX.XX.XXX:54321/3/Models.java/demo_glm > demo_glm.java
      javac -cp h2o-genmodel.jar -J-Xmx2g -J-XX:MaxPermSize=128m demo_glm.java

     (Note:  Try java argument -XX:+PrintCompilation to show runtime JIT compiler behavior.)
*/
import java.util.Map;
import hex.genmodel.GenModel;
import hex.genmodel.annotations.ModelPojo;

@ModelPojo(name=""demo_glm"", algorithm=""glm"")
public class demo_glm extends GenModel {
  public hex.ModelCategory getModelCategory() { return hex.ModelCategory.Binomial; }

  public boolean isSupervised() { return true; }
  public int nfeatures() { return 3; }
  public int nclasses() { return 2; }

  // Names of columns used by model.
  public static final String[] NAMES = NamesHolder_demo_glm.VALUES;
  // Number of output classes included in training data response column.
  public static final int NCLASSES = 2;

  // Column domains. The last array contains domain of response column.
  public static final String[][] DOMAINS = new String[][] {
    /* pclass */ demo_glm_ColInfo_0.VALUES,
    /* sex */ demo_glm_ColInfo_1.VALUES,
    /* age */ null,
    /* survived */ demo_glm_ColInfo_3.VALUES
  };
  // Prior class distribution
  public static final double[] PRIOR_CLASS_DISTRIB = null;
  // Class distribution used for model building
  public static final double[] MODEL_CLASS_DISTRIB = null;

  public demo_glm() { super(NAMES,DOMAINS); }
  public String getUUID() { return Long.toString(-1806915013443955212L); }

  // Pass in data in a double[], pre-aligned to the Model's requirements.
  // Jam predictions into the preds[] array; preds[0] is reserved for the
  // main prediction (class for classifiers or value for regression),
  // and remaining columns hold a probability distribution for classifiers.
  public final double[] score0( double[] data, double[] preds ) {
    final double [] b = BETA.VALUES;
    for(int i = 0; i < 2; ++i) if(Double.isNaN(data[i])) data[i] = CAT_MODES.VALUES[i];
    for(int i = 0; i < 1; ++i) if(Double.isNaN(data[i + 2])) data[i+2] = NUM_MEANS.VALUES[i];
    double eta = 0.0;
    for(int i = 0; i < CATOFFS.length-1; ++i) if(data[i] != 0) {
      int ival = (int)data[i] - 1;
      if(ival != data[i] - 1) throw new IllegalArgumentException(""categorical value out of range"");
      ival += CATOFFS[i];
      if(ival < CATOFFS[i + 1])
        eta += b[ival];
    }
    for(int i = 2; i < b.length-1-1; ++i)
    eta += b[1+i]*data[i];
    eta += b[b.length-1]; // reduce intercept
    double mu = hex.genmodel.GenModel.GLM_logitInv(eta);
    preds[0] = (mu >= 0.3701702514726391) ? 1 : 0; // threshold given by ROC
    preds[1] = 1.0 - mu; // class 0
    preds[2] =       mu; // class 1
    return preds;
  }
    public static class BETA implements java.io.Serializable {
      public static final double[] VALUES = new double[5];
      static {
        BETA_0.fill(VALUES);
      }
      static final class BETA_0 implements java.io.Serializable {
        static final void fill(double[] sa) {
          sa[0] = -1.280567936795408;
          sa[1] = -2.2896567020762353;
          sa[2] = -2.4978421167555616;
          sa[3] = -0.034393168117166584;
          sa[4] = 3.5220688949789816;
        }
      }
}
// Imputed numeric values
    static class NUM_MEANS implements java.io.Serializable {
      public static final double[] VALUES = new double[1];
      static {
        NUM_MEANS_0.fill(VALUES);
      }
      static final class NUM_MEANS_0 implements java.io.Serializable {
        static final void fill(double[] sa) {
          sa[0] = 29.881134512434045;
        }
      }
}
// Imputed categorical values.
    static class CAT_MODES implements java.io.Serializable {
      public static final int[] VALUES = new int[2];
      static {
        CAT_MODES_0.fill(VALUES);
      }
      static final class CAT_MODES_0 implements java.io.Serializable {
        static final void fill(int[] sa) {
          sa[0] = 2;
          sa[1] = 1;
        }
      }
}
    // Categorical Offsets
    public static final int[] CATOFFS = {0,2,3};
}
// The class representing training column names
class NamesHolder_demo_glm implements java.io.Serializable {
  public static final String[] VALUES = new String[3];
  static {
    NamesHolder_demo_glm_0.fill(VALUES);
  }
  static final class NamesHolder_demo_glm_0 implements java.io.Serializable {
    static final void fill(String[] sa) {
      sa[0] = ""pclass"";
      sa[1] = ""sex"";
      sa[2] = ""age"";
    }
  }
}
// The class representing column pclass
class demo_glm_ColInfo_0 implements java.io.Serializable {
  public static final String[] VALUES = new String[3];
  static {
    demo_glm_ColInfo_0_0.fill(VALUES);
  }
  static final class demo_glm_ColInfo_0_0 implements java.io.Serializable {
    static final void fill(String[] sa) {
      sa[0] = ""1st"";
      sa[1] = ""2nd"";
      sa[2] = ""3rd"";
    }
  }
}
// The class representing column sex
class demo_glm_ColInfo_1 implements java.io.Serializable {
  public static final String[] VALUES = new String[2];
  static {
    demo_glm_ColInfo_1_0.fill(VALUES);
  }
  static final class demo_glm_ColInfo_1_0 implements java.io.Serializable {
    static final void fill(String[] sa) {
      sa[0] = ""female"";
      sa[1] = ""male"";
    }
  }
}
// The class representing column survived
class demo_glm_ColInfo_3 implements java.io.Serializable {
  public static final String[] VALUES = new String[2];
  static {
    demo_glm_ColInfo_3_0.fill(VALUES);
  }
  static final class demo_glm_ColInfo_3_0 implements java.io.Serializable {
    static final void fill(String[] sa) {
      sa[0] = ""0"";
      sa[1] = ""1"";
    }
  }
}



2. What I've tried so far


After installing Jetty on a separate EC2 instance, I follow the instructions in the comments above:


cd $JETTY_HOME/demo-base/webapps
mkdir model_demo
cd model_demo
curl http://XXX.XX.XX.XXX:54321/3/h2o-genmodel.jar > h2o-genmodel.jar
curl http://XXX.XX.XX.XXX:54321/3/Models.java/demo_glm > demo_glm.java
javac -cp h2o-genmodel.jar -J-Xmx2g -J-XX:MaxPermSize=128m 



I can access the servelet at 
http://XXX.XX.XX.YY:8080/
 and see the ""Welcome to Jetty-9"" screen in browser or via curl.


My problem is that when I try:


curl http://XXX.XX.XX.YY:8080/predict?pclass=1st&age=29&sex=female



or:


curl http://XXX.XX.XX.YY:8080/model_demo/predict?pclass=1st&age=29&s‌​ex=female



I get:




HTTP ERROR 404


Problem accessing /predict. Reason:


  Not Found","['amazon-ec2', 'jetty', 'pojo', 'h2o', 'jetty-9']",Joakim Erdfelt,https://stackoverflow.com/users/775715/joakim-erdfelt,49.3k
46174071,46174071,2017-09-12T10:30:19,2018-04-02 16:14:39Z,611,We are using Prediction Service Builder by using Java POJO for serving our model. But when we pass a new categorical value of a feature which model haven't seen before while training. It gives an exception. How can i handle that ?,"['random-forest', 'h2o']",Shashank Agarwal,https://stackoverflow.com/users/790389/shashank-agarwal,116
46172137,46172137,2017-09-12T08:59:45,2020-07-27 08:52:55Z,0,"I noticed a relatively recend add to the h2o.ai suite, the ability to perform supplementary Platt Scaling to improve the calibration of output probabilities. (See 
calibrate_model
 in h2o manual
.) Nevertheless few guidance is avaiable on the online help docs. In particular I wonder whether when Platt Scaling is enabled:




How it affects the models' leaderboard? That is, is the platt scaling calculated after the ranking metric or before?


How it affects computing performance?


Can the 
calibration_frame
 be the same as 
validation_frame
 or should not (both under a computation or theoretical point of view)?




Thanks in advance","['h2o', 'calibration']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
46167613,46167613,2017-09-12T03:50:40,2017-09-13 18:59:17Z,258,"Are there any plans to implement a leaky ReLU in the Deep Learning module of H2O? I am a beginner to neural nets, but in the limited amount of model building and parameter tuning, I have found the ReLUs to generalize better, and was wondering if even better performance might be obtained by using leaky ReLUs to avoid the dying ReLU problem.","['deep-learning', 'h2o', 'activation-function']",Unknown,,N/A
46160751,46160751,2017-09-11T16:56:01,2017-09-11 18:29:56Z,111,"I try to load data from database into h2o for R modeling analysis. 


I can get a data frame successfully with h2o function 
h2o.import_sql_table()
 like instruction at 
here
. But this data frame can't be applied with h2o functions like: 
h2o.hist()
, 
as.data.frame()
 and many more others which are used in 
this
. That makes me impossible to follow the demo steps to analyze data frames generated from 
h2o.import_sql_table()
. 


I want to know what differences are between data frames created by 
h2o.import_sql_table()
 and 
h2o.parseRaw()
. And ideally, are there any methods that can convert data frames created by 
h2o.import_sql_table()
 into more traditional h2o data frames? Any code samples would be greatly appreciated.",['h2o'],robert,https://stackoverflow.com/users/7193290/robert,417
46153863,46153863,2017-09-11T10:47:43,2017-09-11 16:13:58Z,443,"import h2o

from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o.estimators.deeplearning import H2ODeepLearningEstimator
from h2o.estimators.glm import H2OGeneralizedLinearEstimator
h2o.init()

inputFile = h2o.import_file(""SQLBlocked.csv"")

inputFile['cat'] = inputFile['cat'].asfactor()
inputFile['entityN'] = inputFile['entityN'].asfactor()
inputFile['expectedT'] = inputFile['expectedT'].asfactor()
inputFile['u_play'] = inputFile['u_play'].asfactor()
inputFile['sub'] = inputFile['sub'].asfactor()

predictors = [""attempts"", ""cat"", ""entityN"", ""expectedT"", ""u_play"", ""sub""]
response1 = ['count.value']

inputFile.types
model = H2OGeneralizedLinearEstimator()
model.train(predictors, response1, training_frame = inputFile)



I am getting the following error:




H2OTypeError: Argument 
y
 should be a None | integer | string, got list ['count.value']",['h2o'],Antti29,https://stackoverflow.com/users/954312/antti29,"3,003"
46124347,46124347,2017-09-08T20:51:20,2017-09-08 22:03:49Z,744,"Looking at an H2o MOJO model, is there a way to figure out the datatypes of the training data it was trained on?","['machine-learning', 'deep-learning', 'data-analysis', 'data-science', 'h2o']",kivk02,https://stackoverflow.com/users/4152164/kivk02,651
46120544,46120544,2017-09-08T16:05:50,2017-09-09 00:02:24Z,0,Is there any way to extract the time/memory required for each iteration of a grid search? I am looking to plot an outcome metric (e.g. AUC) vs. processing requirements to examine the cost-benefit of adding complexity to my model.,"['r', 'h2o', 'grid-search']",jhearn,https://stackoverflow.com/users/8581033/jhearn,525
46117348,46117348,2017-09-08T13:12:50,2018-03-14 16:08:09Z,0,"I have an H2O frame R object like this


h2odf

A | B | C | D
--|---|---|---
1 | NA| 2 | 0
2 | 1 | 2 | 0
3 | NA| 2 | 0
4 | 3 | 2 | 0



I want to remove all those rows where B is NA (1st and 3rd row). I have tried


na <- is.na(h2odf[,""b""])
h2odf <- h2odf[!na,]



and


h2odf <- h2odf[!is.na(h2odf$B),]



and 


h2odf <- subset(h2odf, B!=NA)



This works for R Dataframe but not H2O. Giving this error: 


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

DistributedException from localhost/127.0.0.1:54321: 'Cannot set illegal UUID value'



Desired output is 


h2odf

A | B | C | D
--|---|---|---
2 | 1 | 2 | 0
4 | 3 | 2 | 0



One option I have is to convert it into R Dataframe, remove rows and convert it back to H2O frame. But that is taking long time because input file size is close to 4.5 GB. Is it possible to do this in H2O frame hex object itself?


I am running Rstudio on aws cluster.","['r', 'dataframe', 'h2o']",Unknown,,N/A
46102269,46102269,2017-09-07T17:40:11,2022-06-11 09:58:35Z,0,"I'm trying to execute 
this RSparkling example
 on an offline CDH-5.10.2 cluster. My environment is:




Spark 1.6.0;


sparklyr 0.6.2;


h2o 3.10.5.2;


rsparkling 0.2.1.




I use custom Sparkling Water JAR which is basically 1.6.12 with 
this PR
 applied:


options(rsparkling.sparklingwater.location = ""/opt/h2o/sparkling-water-1.6.13-SNAPSHOT/assembly/build/libs/sparkling-water-assembly_2.10-1.6.13-SNAPSHOT-all.jar"")



After successful connection:


config <- spark_config()
config$spark.dynamicAllocation.enabled <- ""false""
config$spark.driver.memory <- ""6g""
config$spark.executor.memory <- ""6g""
config$spark.executor.heartbeatInterval <- ""20s""

sc <- spark_connect(master = ""yarn-client"", config = config)



I create H2O context:


h2o_context(sc)



H2O context creation takes few minutes (it's the first strange thing).


After creation, the application becomes unresponsive for another few minutes (even Spark master UI becomes unreachable). No H2O logs are printed at this time.


After that, H2O logs appear but they contain mostly these messages:


Got IO error when sending batch UDP bytes: java.net.ConnectException: Connection refused



and rare these ones in between:


WARN: Unblock allocations; cache below desired, but also OOM: OOM, (K/V:Zero   + POJO:661.8 MB + FREE:306.7 MB == MEM_MAX:968.5 MB), desiredKV=121.1 MB OOM!



Then the following code that is unrelated to H2O is executed fast:


flights_tbl <- copy_to(sc, nycflights13::flights, ""flights"")
airports_tbl <- copy_to(sc, nycflights13::airports, ""airports"")
airlines_tbl <- copy_to(sc, nycflights13::airlines, ""airlines"")
model_tbl <- flights_tbl %>%
  filter(!is.na(arr_delay) & !is.na(dep_delay) & !is.na(distance)) %>%
  filter(dep_delay > 15 & dep_delay < 240) %>%
  filter(arr_delay > -60 & arr_delay < 360) %>%
  left_join(airlines_tbl, by = c(""carrier"" = ""carrier"")) %>%
  mutate(gain = dep_delay - arr_delay) %>%
  select(origin, dest, carrier, airline = name, distance, dep_delay, arr_delay, gain)



But when H2O must come into play again:


df_hex <- as_h2o_frame(sc,model_tbl,name=""model_hex"",FALSE)



the application hangs again (to the moment, it has been hanging twenty minutes or so).


I tried to rerun this code multiple times and succeeded once but normally it just hangs. How to troubleshoot this?


I checked CPU, RAM, and disk usage, all these seems to be OK. There are no evident network problems as well.


Update 1
. Maybe 
ConnectException
 is just a consequence of 
K/V:Zero   + POJO:661.8 MB + FREE:306.7 MB == MEM_MAX:968.5 MB
. So I will try to find out how to increase H2O's max memory (and why it's below 1 GB in the first place).","['r', 'cloudera-cdh', 'h2o', 'sparklyr', 'sparkling-water']",Carl,https://stackoverflow.com/users/7327058/carl,"7,530"
46095531,46095531,2017-09-07T11:46:04,2017-09-08 09:34:42Z,0,"I was running 
h2o.automl()
 example from: 
http://h2o-release.s3.amazonaws.com/h2o/master/3888/docs-website/h2o-docs/automl.html
 . Everything went fine except 
NaN
 values in 
leaderboard
. Predictions also works fine. Is it a bug or I'm doing something wrong?


library(h2o)

localH2O <- h2o.init(ip = ""localhost"",
                 port = 54321, 
                 nthreads = -1, 
                 min_mem_size = ""20g"")

train <- h2o.importFile(""https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv"")
test <- h2o.importFile(""https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv"")

y <- ""response""
x <- setdiff(names(train), y)

train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])

aml <- h2o.automl(x = x, y = y,
              training_frame = train,
              leaderboard_frame = test,
              max_runtime_secs = 30)

lb <- aml@leaderboard
lb

                                   model_id auc logloss
1  StackedEnsemble_0_AutoML_20170908_094736 NaN     NaN
2  StackedEnsemble_0_AutoML_20170908_094407 NaN     NaN
3 GBM_grid_0_AutoML_20170908_094736_model_1 NaN     NaN
4 GBM_grid_0_AutoML_20170908_094407_model_0 NaN     NaN
5 GBM_grid_0_AutoML_20170908_094407_model_1 NaN     NaN
6 GBM_grid_0_AutoML_20170908_094736_model_0 NaN     NaN



I've checked and there are normal values in H2O Flow on 
localhost:54321
 and also I'm getting normal values using 
h2o.getFrame()
:


h2o.getFrame(""leaderboard"")
                                   model_id      auc  logloss
1  StackedEnsemble_0_AutoML_20170908_094736 0,787145 0,554983
2  StackedEnsemble_0_AutoML_20170908_094407 0,785154 0,556897
3 GBM_grid_0_AutoML_20170908_094736_model_1 0,778587 0,563741
4 GBM_grid_0_AutoML_20170908_094407_model_0 0,776755 0,564247
5 GBM_grid_0_AutoML_20170908_094407_model_1 0,776640 0,564436
6 GBM_grid_0_AutoML_20170908_094736_model_0 0,774611 0,566920



I'm using h2o v. 3.15.0.4018


h2o.clusterInfo()
R is connected to the H2O cluster: 
H2O cluster uptime:         2 hours 8 minutes 
H2O cluster version:        3.15.0.4018 
H2O cluster version age:    15 hours and 47 minutes  
H2O cluster name:           H2O_started_from_R_maju116_ozj558 
H2O cluster total nodes:    1 
H2O cluster total memory:   19.03 GB 
H2O cluster total cores:    8 
H2O cluster allowed cores:  8 
H2O cluster healthy:        TRUE 
H2O Connection ip:          localhost 
H2O Connection port:        54321 
H2O Connection proxy:       NA 
H2O Internal Security:      FALSE 
H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
R Version:                  R version 3.4.1 (2017-06-30) 



Session info:


R version 3.4.1 (2017-06-30)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 16.04.2 LTS

Matrix products: default
BLAS: /usr/lib/openblas-base/libblas.so.3
LAPACK: /usr/lib/libopenblasp-r0.2.18.so

locale:
 [1] LC_CTYPE=pl_PL.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=pl_PL.UTF-8        LC_COLLATE=pl_PL.UTF-8    
 [5] LC_MONETARY=pl_PL.UTF-8    LC_MESSAGES=pl_PL.UTF-8   
 [7] LC_PAPER=pl_PL.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=pl_PL.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] dplyr_0.7.2       purrr_0.2.3       readr_1.1.1       tidyr_0.7.1      

[5] tibble_1.3.4      ggplot2_2.2.1     tidyverse_1.1.1   h2oEnsemble_0.2.1
 [9] h2o_3.15.0.4018  

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.12     cellranger_1.1.0 compiler_3.4.1   plyr_1.8.4      
 [5] bindr_0.1        forcats_0.2.0    bitops_1.0-6     tools_3.4.1     

 [9] lubridate_1.6.0  jsonlite_1.5     nlme_3.1-131     gtable_0.2.0    

[13] lattice_0.20-35  pkgconfig_2.0.1  rlang_0.1.2      psych_1.7.5     

[17] parallel_3.4.1   haven_1.1.0      bindrcpp_0.2     xml2_1.1.1      

[21] httr_1.3.1       stringr_1.2.0    hms_0.3          grid_3.4.1      

[25] glue_1.1.1       R6_2.2.2         readxl_1.0.0     foreign_0.8-69  

[29] modelr_0.1.1     reshape2_1.4.2   magrittr_1.5     scales_0.5.0    

[33] rvest_0.3.2      assertthat_0.2.0 mnormt_1.5-5     colorspace_1.3-2
[37] stringi_1.1.5    lazyeval_0.2.0   munsell_0.4.3    RCurl_1.95-4.8  

[41] broom_0.4.2","['r', 'h2o']",Unknown,,N/A
46093109,46093109,2017-09-07T09:44:36,2018-05-14 13:44:08Z,0,"I'm having issues updating 
rsparkling
 to work with Sparkling Water 2.2 and Spark 2.2. Everything worked with previous versions (<2.1). 


I have installed the rsparkling version R package that comes with the latest Sparkling Water 2.2 binaries (as per 
https://h2o-release.s3.amazonaws.com/sparkling-water/rel-2.2/0/index.html
), and set the sparkling water version to the install location (i.e. options(rsparkling.sparklingwater.location = ""/Users/me/sparkling-water-2.2.0/"")).


I can now connect to my cluster, but get error 


java.lang.ClassNotFoundException: org.apache.spark.h2o.H2OContext
 


I think this may have to do with the h2o version I am using - 
3.14.0.2
 which is the version recommended in the install page. 


Does anyone know which version of h2o sparkling water 2.2 works with? The rsparkling documentation (
https://github.com/h2oai/rsparkling
) is not updated to 2.2. Could this error be the result of something else?


I am connecting to a standalone spark cluster, and my setup is:


Cluster/local Spark version: 2.2
R: 3.4.2
RStudio: 1.0.153
Sparklyr: 0.6.2
h2o: 3.14.0.2
rsparkling: 2.1","['r', 'apache-spark', 'h2o', 'sparkling-water']",renegademonkey,https://stackoverflow.com/users/7469564/renegademonkey,467
46084545,46084545,2017-09-06T21:23:57,2018-12-13 18:37:30Z,0,"I had to install H2O R package on an offline server. I created a local repo via 
miniCRAN
 and tried to install the package from it but installation failed with this error:


Error in download.file(md5_url, destfile = md5_file, mode = ""w"", cacheOK = FALSE,  :
  cannot open URL 'http://s3.amazonaws.com/h2o-release/h2o/rel-vajda/3/Rjar/h2o.jar.md5'
Error : unable to load R code in package ‘h2o’
ERROR: lazy loading failed for package ‘h2o’
* removing ‘/usr/lib64/R/library/h2o’



I circumvented this by manual transferring of installed binaries from an online server but this is obviously just a workaround. So is there a normal way to install H2O R package on an offline server?","['r', 'h2o']",Igor Melnichenko,https://stackoverflow.com/users/3652569/igor-melnichenko,145
46080528,46080528,2017-09-06T16:47:48,2017-09-13 02:00:40Z,0,"**When running, 


h2o.gbm(y = ""Y"", nfolds = 3, ntrees = 100, training_frame = train, verbose = TRUE)



I receive the error:** 


ERROR: Unexpected HTTP Status code: 404 Not Found (url = 
http://localhost:54321/3/Models/GBM_model_R_1504715033543_1
)


water.exceptions.H2OKeyNotFoundArgumentException
 [1] ""water.exceptions.H2OKeyNotFoundArgumentException: Object 'GBM_model_R_1504715033543_1' not found for argument: key""
 [2] ""    water.api.ModelsHandler.getFromDKV(ModelsHandler.java:97)""
 [3] ""    water.api.ModelsHandler.fetch(ModelsHandler.java:116)""
 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""
 [5] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""
 [6] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""
 [7] ""    java.lang.reflect.Method.invoke(Method.java:498)""
 [8] ""    water.api.Handler.handle(Handler.java:63)""
 [9] ""    water.api.RequestServer.serve(RequestServer.java:448)""
[10] ""    water.api.RequestServer.doGeneric(RequestServer.java:297)""
[11] ""    water.api.RequestServer.doGet(RequestServer.java:221)""
[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:735)""
[13] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""
[14] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""
[15] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""
[16] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""
[17] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""
[18] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""
[19] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""
[20] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""
[21] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""
[22] ""    water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:183)""
[23] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""
[24] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""
[25] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""
[26] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""
[27] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)""
[29] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)""
[30] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)""
[31] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)""
[32] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""
[33] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""
[34] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""
[35] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""
[36] ""    java.lang.Thread.run(Thread.java:748)""



Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  :




Other details:


Starting H2O JVM and connecting: . Connection successful!

R is connected to the H2O cluster:
    H2O cluster uptime:         1 seconds 407 milliseconds
    H2O cluster version:        3.14.0.2
    H2O cluster version age:    15 days
    H2O cluster name:           H2O_started_from_R_root_bjy160
    H2O cluster total nodes:    1
    H2O cluster total memory:   6.98 GB
    H2O cluster total cores:    8
    H2O cluster allowed cores:  8
    H2O cluster healthy:        TRUE
    H2O Connection ip:          localhost
    H2O Connection port:        54321
    H2O Connection proxy:       NA
    H2O Internal Security:      FALSE
    H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4
    R Version:                  R version 3.4.1 (2017-06-30)



sessionInfo()


R version 3.4.1 (2017-06-30)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 16.04.2 LTS

Matrix products: default
BLAS: /usr/lib/libblas/libblas.so.3.6.0
LAPACK: /usr/lib/lapack/liblapack.so.3.6.0

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] h2o_3.14.0.2

loaded via a namespace (and not attached):
[1] compiler_3.4.1 tools_3.4.1    RCurl_1.95-4.8 jsonlite_1.5   bitops_1.0-6



Suggestions?  Thank you","['r', 'h2o', 'gbm']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
46056853,46056853,2017-09-05T13:57:55,2017-09-05 14:54:12Z,392,"I set up a cluster with a 4 core (2GHz) and a 16 core (1.8GHz) virtual machine. The creation and connection to the cluster works without problems. But now I want to do some deep learning on the cluster, where I see an uneven distribution for the performance usage of those two virtual machines. The one with 4 cores is always at 100% CPU usage while the 16 core machine is idle most of the time.


Do I have to make additional configuration during the cluster generation? Because it is odd for me that the stronger machine of the two is idle while the weaker one does all the work.


Best regards,
Markus",['h2o'],Markus Wilhelm,https://stackoverflow.com/users/1984860/markus-wilhelm,171
45976338,45976338,2017-08-31T08:04:57,2017-08-31 10:36:46Z,0,"I am trying to train a machine learning model with H2O (3.14). My dataset size is 4Gb and my computer RAM is 2Gb with 2G swap, JDK 1.8. Refer to this 
article
, H2O can process a huge dataset with 2Gb RAM.






A note on Bigger Data and GC: We do a user-mode swap-to-disk when the Java heap gets too full, i.e., you're using more Big Data than
  physical DRAM.  We won't die with a GC death-spiral, but we will
  degrade to out-of-core speeds.  We'll go as fast as the disk will
  allow.  I've personally tested loading a 12Gb dataset into a 2Gb
  (32bit) JVM; it took about 5 minutes to load the data, and another 5
  minutes to run a Logistic Regression.






Some questions around this issues:




Loading data bigger than the memory size in h2o
 . The answer mentioned user-mode swap-to-disk was disabled since performance was so bad. However, he did not explain any alternative method and how can enable the flags 
--cleaner
 in h2o? 




Work around 1:


I configured the java heap with options 
java -Xmx10g -jar h2o.jar
. When I load dataset. The H2O information as follows:


However, JVM consumed all RAM memory and Swap, then operating system halted java h2o program.


Work around 2:


I installed 
H2O spark
. I can load dataset but spark was hanging with the following logs with a full swap memory:


 + FREE:426.8 MB == MEM_MAX:2.67 GB), desiredKV=841.3 MB OOM!
09-01 02:01:12.377 192.168.233.133:54321 6965   Thread-47 WARN: Swapping!  OOM, (K/V:1.75 GB + POJO:513.2 MB + FREE:426.8 MB == MEM_MAX:2.67 GB), desiredKV=841.3 MB OOM!
09-01 02:01:12.377 192.168.233.133:54321 6965   Thread-48 WARN: Swapping!  OOM, (K/V:1.75 GB + POJO:513.2 MB + FREE:426.8 MB == MEM_MAX:2.67 GB), desiredKV=841.3 MB OOM!
09-01 02:01:12.381 192.168.233.133:54321 6965   Thread-45 WARN: Swapping!  OOM, (K/V:1.75 GB + POJO:513.3 MB + FREE:426.7 MB == MEM_MAX:2.67 GB), desiredKV=803.2 MB OOM!
09-01 02:01:12.382 192.168.233.133:54321 6965   Thread-46 WARN: Swapping!  OOM, (K/V:1.75 GB + POJO:513.4 MB + FREE:426.5 MB == MEM_MAX:2.67 GB), desiredKV=840.9 MB OOM!
09-01 02:01:12.384 192.168.233.133:54321 6965   #e Thread WARN: Swapping!  GC CALLBACK, (K/V:1.75 GB + POJO:513.4 MB + FREE:426.5 MB == MEM_MAX:2.67 GB), desiredKV=802.7 MB OOM!
09-01 02:01:12.867 192.168.233.133:54321 6965   FJ-3-1    WARN: Swapping!  OOM, (K/V:1.75 GB + POJO:513.4 MB + FREE:426.5 MB == MEM_MAX:2.67 GB), desiredKV=1.03 GB OOM!
09-01 02:01:13.376 192.168.233.133:54321 6965   Thread-46 WARN: Swapping!  OOM, (K/V:1.75 GB + POJO:513.2 MB + FREE:426.8 MB == MEM_MAX:2.67 GB), desiredKV=803.2 MB OOM!
09-01 02:01:13.934 192.168.233.133:54321 6965   Thread-45 WARN: Swapping!  OOM, (K/V:1.75 GB + POJO:513.2 MB + FREE:426.8 MB == MEM_MAX:2.67 GB), desiredKV=841.3 MB OOM!
09-01 02:01:12.867 192.168.233.133:54321 6965   #e Thread WARN: Swapping!  GC CALLBACK, (K/V:1.75 GB + POJO:513.2 MB + FREE:426.8 MB == MEM_MAX:2.67 GB), desiredKV=803.2 MB OOM!



In this case, I think the 
gc
 collector is waiting for cleaning some unused memory in swap.


How can I process huge dataset with a limited RAM memory ?","['java', 'garbage-collection', 'jvm', 'h2o']",Unknown,,N/A
45953884,45953884,2017-08-30T06:56:07,2019-12-03 11:24:57Z,0,"in traditional gbm, we can use
    predict.gbm(model, newsdata=..., n.tree=...)


So that I can compare result with different number of trees for the test data.


In h2o.gbm, although it has n.tree to set, it seems it doesn't have any effect on the result. It's all the same as the default model:


h2o.test.pred <- as.vector(h2o.predict(h2o.gbm.model, newdata=test.frame, n.tree=100))
R2(h2o.test.pred, test.mat$y)
[1] -0.00714109
h2o.test.pred <- as.vector(h2o.predict(h2o.gbm.model, newdata=test.frame, n.tree=10))
> R2(h2o.test.pred, test.mat$y)
[1] -0.00714109



Does anybod have similar problem? How to solve it? h2o.gbm is much faster than gbm, so if it can get detailed result of each tree that would be great.","['r', 'machine-learning', 'h2o', 'gbm']",Wei Li,https://stackoverflow.com/users/1649878/wei-li,35
45948642,45948642,2017-08-29T21:33:53,2017-08-29 22:57:23Z,287,"Regarding 
h2o.glm lambda search not appearing to iterate over all lambdas
, I read the question as complaining that lambda was too high;  they tried setting 
early_stopping=F
 in the hope that might fix that ""bug"".


Isn't it the case that the original behaviour was a feature, not a bug? And if that is correct, then you should always use 
early_stopping=T
 when using cross-validation with GLM, otherwise the error estimate from cross-validation is useless; you also risk over-fitting.


(My main question is if my understanding of the way GLM and CV work together is correct; but I'd be interested if there are any other things to watch out for when using lambda_search and cross-validation together.)","['glm', 'cross-validation', 'h2o']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
45890985,45890985,2017-08-26T00:53:36,2017-08-28 22:38:50Z,0,"Please consider the following basic reproducible example:


library(h2o)
h2o.init()
data(""iris"")
iris.hex = as.h2o(iris, ""iris.hex"")
mod = h2o.glm(y = ""Sepal.Length"", x = setdiff(colnames(iris), ""Sepal.Length""), 
              training_frame = iris.hex, nfolds = 2, seed = 100,
              lambda_search = T, early_stopping = F, 
              family = ""gamma"", nlambdas = 100)



When I run the above, I expect that 
h2o
 will iterate over 100 different values of lambda. However, running 
length(mod@allparameters$lambda)
 will show that only 79 values of lambda were actually tested. These 79 values are the first 79 values in the sequence:


maxLambda = max(mod@allparameters$lambda)
lambdaMinRatio = mod@allparameters$lambda_min_ratio
exp(seq(log(maxLambda), log(maxLambda*lambdaMinRatio), length.out = 100))



Could you please let me know how I can get the function to iterate over all 100 values of lambda? (I tried setting 
early_stopping = F
 to see if that would fix the issue but it does not.)


Here is my cluster info if it helps:


R is connected to the H2O cluster: 
    H2O cluster uptime:         11 hours 21 minutes 
    H2O cluster version:        3.10.5.3 
    H2O cluster version age:    1 month and 26 days  
    H2O cluster name:           H2O_started_from_R_xaq943 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   6.75 GB 
    H2O cluster total cores:    8 
    H2O cluster allowed cores:  4 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    R Version:                  R version 3.3.3 (2017-03-06) 



Thank you!


EDIT: As requested, here is the output of    h2o.getGLMFullRegularizationPath(mod):


$`__meta`
$`__meta`$schema_version
[1] 3

$`__meta`$schema_name
[1] ""GLMRegularizationPathV3""

$`__meta`$schema_type
[1] ""RegularizationPath""


$model
NULL

$lambdas
 [1] 1.434114617 1.306711827 1.190627150 1.084855115 0.988479577 0.900665776 0.820653111 0.747748550 0.681320630 0.620793983 0.565644356
[12] 0.515394071 0.469607882 0.427889212 0.389876714 0.355241141 0.323682497 0.294927436 0.268726896 0.244853939 0.223101790 0.203282042
[23] 0.185223025 0.168768322 0.153775410 0.140114426 0.127667047 0.116325458 0.105991425 0.096575439 0.087995943 0.080178626 0.073055778
[34] 0.066565704 0.060652190 0.055264017 0.050354514 0.045881158 0.041805202 0.038091343 0.034707413 0.031624102 0.028814704 0.026254885
[45] 0.023922474 0.021797267 0.019860858 0.018096474 0.016488833 0.015024011 0.013689319 0.012473198 0.011365113 0.010355468 0.009435517
[56] 0.008597291 0.007833532 0.007137622 0.006503536 0.005925779 0.005399349 0.004919686 0.004482635 0.004084410 0.003721562 0.003390949
[67] 0.003089706 0.002815225 0.002565128 0.002337249 0.002129615 0.001940426 0.001768044 0.001610975 0.001467861 0.001337460 0.001218644
[78] 0.001110383 0.001011740

$explained_deviance_train
 [1] -3.294962e-08  1.278780e-01  2.352402e-01  3.253159e-01  4.008369e-01  4.641126e-01  5.170944e-01  5.614293e-01  5.985067e-01
[10]  6.294974e-01  6.553869e-01  6.770044e-01  6.950464e-01  7.100979e-01  7.226495e-01  7.331127e-01  7.418320e-01  7.490957e-01
[19]  7.551451e-01  7.687710e-01  7.815713e-01  7.921910e-01  8.010014e-01  8.083105e-01  8.143741e-01  8.194045e-01  8.235584e-01
[28]  8.270239e-01  8.298991e-01  8.322847e-01  8.342640e-01  8.359064e-01  8.372692e-01  8.384000e-01  8.393384e-01  8.401172e-01
[37]  8.407634e-01  8.411713e-01  8.420553e-01  8.434391e-01  8.445680e-01  8.454431e-01  8.462240e-01  8.468835e-01  8.476350e-01
[46]  8.481135e-01  8.497288e-01  8.513965e-01  8.528687e-01  8.541499e-01  8.551259e-01  8.560063e-01  8.566711e-01  8.572853e-01
[55]  8.578407e-01  8.583362e-01  8.586877e-01  8.590151e-01  8.593148e-01  8.595864e-01  8.596849e-01  8.599377e-01  8.600233e-01
[64]  8.602430e-01  8.603153e-01  8.605097e-01  8.605776e-01  8.608212e-01  8.608821e-01  8.610499e-01  8.611065e-01  8.611627e-01
[73]  8.612156e-01  8.616241e-01  8.616940e-01  8.617575e-01  8.617782e-01  8.617988e-01  8.618557e-01

$explained_deviance_valid
NULL

$coefficients
      Species.setosa Species.versicolor Species.virginica  Sepal.Width Petal.Length  Petal.Width Intercept
 [1,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000  0.000000000 0.000000e+00 0.1711352
 [2,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.001046643 0.000000e+00 0.1750882
 [3,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.002009314 0.000000e+00 0.1787588
 [4,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.002894275 0.000000e+00 0.1821621
 [5,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.003707356 0.000000e+00 0.1853133
 [6,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.004453990 0.000000e+00 0.1882274
 [7,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.005139245 0.000000e+00 0.1909189
 [8,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.005767843 0.000000e+00 0.1934021
 [9,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.006344186 0.000000e+00 0.1956907
[10,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.006872371 0.000000e+00 0.1977980
[11,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.007356208 0.000000e+00 0.1997366
[12,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.007799235 0.000000e+00 0.2015187
[13,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.008204738 0.000000e+00 0.2031555
[14,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.008575759 0.000000e+00 0.2046579
[15,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.008915116 0.000000e+00 0.2060361
[16,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.009225414 0.000000e+00 0.2072996
[17,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.009509059 0.000000e+00 0.2084574
[18,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.009768269 0.000000e+00 0.2095177
[19,]   0.000000e+00       0.0000000000      0.000000e+00  0.000000000 -0.010005092 0.000000e+00 0.2104884
[20,]   0.000000e+00       0.0000000000      0.000000e+00 -0.001127417 -0.010319589 0.000000e+00 0.2151915
[21,]   0.000000e+00       0.0000000000      0.000000e+00 -0.002359216 -0.010623694 0.000000e+00 0.2201719
[22,]   0.000000e+00       0.0000000000      0.000000e+00 -0.003480564 -0.010900376 0.000000e+00 0.2247086
[23,]   0.000000e+00       0.0000000000      0.000000e+00 -0.004501465 -0.011152087 0.000000e+00 0.2288412
[24,]   0.000000e+00       0.0000000000      0.000000e+00 -0.005430894 -0.011381098 0.000000e+00 0.2326054
[25,]   0.000000e+00       0.0000000000      0.000000e+00 -0.006277042 -0.011589469 0.000000e+00 0.2360339
[26,]   0.000000e+00       0.0000000000      0.000000e+00 -0.007047377 -0.011779076 0.000000e+00 0.2391565
[27,]   0.000000e+00       0.0000000000      0.000000e+00 -0.007743794 -0.011951139 0.000000e+00 0.2419836
[28,]   0.000000e+00       0.0000000000      0.000000e+00 -0.008382717 -0.012108203 0.000000e+00 0.2445752
[29,]   0.000000e+00       0.0000000000      0.000000e+00 -0.008964450 -0.012251159 0.000000e+00 0.2469356
[30,]   0.000000e+00       0.0000000000      0.000000e+00 -0.009494120 -0.012381280 0.000000e+00 0.2490854
[31,]   0.000000e+00       0.0000000000      0.000000e+00 -0.009976404 -0.012499729 0.000000e+00 0.2510434
[32,]   0.000000e+00       0.0000000000      0.000000e+00 -0.010415558 -0.012607559 0.000000e+00 0.2528268
[33,]   0.000000e+00       0.0000000000      0.000000e+00 -0.010815455 -0.012705728 0.000000e+00 0.2544511
[34,]   0.000000e+00       0.0000000000      0.000000e+00 -0.011179617 -0.012795108 0.000000e+00 0.2559306
[35,]   0.000000e+00       0.0000000000      0.000000e+00 -0.011511250 -0.012876490 0.000000e+00 0.2572783
[36,]   0.000000e+00       0.0000000000      0.000000e+00 -0.011813271 -0.012950594 0.000000e+00 0.2585058
[37,]   0.000000e+00       0.0000000000      0.000000e+00 -0.012088333 -0.013018075 0.000000e+00 0.2596239
[38,]   0.000000e+00       0.0000000000      0.000000e+00 -0.012254270 -0.013069805 0.000000e+00 0.2603445
[39,]   0.000000e+00      -0.0001175922      0.000000e+00 -0.012623025 -0.013136288 0.000000e+00 0.2617830
[40,]   0.000000e+00      -0.0005066170      0.000000e+00 -0.013031762 -0.013198821 0.000000e+00 0.2634171
[41,]   0.000000e+00      -0.0008532154      0.000000e+00 -0.013400532 -0.013255288 0.000000e+00 0.2648907
[42,]   0.000000e+00      -0.0011428955      0.000000e+00 -0.013718316 -0.013304258 0.000000e+00 0.2661590
[43,]   0.000000e+00      -0.0014293556      0.000000e+00 -0.014023516 -0.013351005 0.000000e+00 0.2673789
[44,]   0.000000e+00      -0.0016797073      1.243120e-05 -0.014304179 -0.013396541 0.000000e+00 0.2685020
[45,]   0.000000e+00      -0.0018706468      9.790433e-05 -0.014536882 -0.013478186 8.361933e-05 0.2694643
[46,]   0.000000e+00      -0.0019698629      1.717337e-04 -0.014665554 -0.013530772 1.814935e-04 0.2699431
[47,]   0.000000e+00      -0.0021078477      2.246836e-04 -0.014925921 -0.013849890 8.489923e-04 0.2711751
[48,]   0.000000e+00      -0.0021556371      3.034315e-04 -0.015150706 -0.014237748 1.656453e-03 0.2723528
[49,]   0.000000e+00      -0.0021453273      4.458210e-04 -0.015348300 -0.014616464 2.413512e-03 0.2734328
[50,]   0.000000e+00      -0.0020839569      6.461732e-04 -0.015520020 -0.014980050 3.109852e-03 0.2744131
[51,]   0.000000e+00      -0.0020107174      8.597081e-04 -0.015660412 -0.015278421 3.659515e-03 0.2752178
[52,]   0.000000e+00      -0.0019078474      1.101906e-03 -0.015786052 -0.015572930 4.186424e-03 0.2759708
[53,]   0.000000e+00      -0.0018175109      1.323132e-03 -0.015890456 -0.015809763 4.599076e-03 0.2765883
[54,]   0.000000e+00      -0.0017094991      1.558056e-03 -0.015986195 -0.016047486 5.006251e-03 0.2771791
[55,]   0.000000e+00      -0.0015842081      1.807162e-03 -0.016071634 -0.016281094 5.397220e-03 0.2777320
[56,]   0.000000e+00      -0.0014430021      2.070103e-03 -0.016146458 -0.016507391 5.765349e-03 0.2782422
[57,]   0.000000e+00      -0.0013372850      2.282679e-03 -0.016207766 -0.016676301 6.033973e-03 0.2786413
[58,]   0.000000e+00      -0.0012235170      2.499826e-03 -0.016264638 -0.016845566 6.300372e-03 0.2790268
[59,]   0.000000e+00      -0.0011012638      2.721871e-03 -0.016315538 -0.017012360 6.558645e-03 0.2793901
[60,]   0.000000e+00      -0.0009710435      2.949010e-03 -0.016360197 -0.017174819 6.804753e-03 0.2797282
[61,]   0.000000e+00      -0.0009387214      3.037293e-03 -0.016389637 -0.017231436 6.890089e-03 0.2798895
[62,]   0.000000e+00      -0.0008039241      3.270133e-03 -0.016434978 -0.017400280 7.145560e-03 0.2802397
[63,]   0.000000e+00      -0.0007660395      3.357739e-03 -0.016459753 -0.017456898 7.230221e-03 0.2803861
[64,]   0.000000e+00      -0.0006199118      3.595215e-03 -0.016496474 -0.017622417 7.475811e-03 0.2807040
[65,]   0.000000e+00      -0.0005768718      3.683036e-03 -0.016516321 -0.017676868 7.554952e-03 0.2808322
[66,]  -3.476645e-05      -0.0004234267      3.926329e-03 -0.016530611 -0.017839966 7.778584e-03 0.2811053
[67,]  -6.891206e-05      -0.0003785185      4.015034e-03 -0.016531687 -0.017896456 7.848434e-03 0.2812048
[68,]  -2.509409e-04      -0.0001852860      4.347389e-03 -0.016506532 -0.018118743 8.088206e-03 0.2815679
[69,]  -3.133552e-04      -0.0001451602      4.438172e-03 -0.016500367 -0.018173876 8.139393e-03 0.2816729
[70,]  -5.214018e-04      -0.0000198928      4.695164e-03 -0.016468891 -0.018337882 8.275330e-03 0.2819765
[71,]  -6.024159e-04       0.0000000000      4.785344e-03 -0.016466374 -0.018391875 8.314552e-03 0.2821158
[72,]  -6.921978e-04       0.0000000000      4.869432e-03 -0.016471686 -0.018446669 8.353211e-03 0.2822946
[73,]  -7.920269e-04       0.0000000000      4.942796e-03 -0.016472703 -0.018501428 8.391136e-03 0.2824681
[74,]  -2.055117e-03       0.0000000000      5.491157e-03 -0.016310937 -0.018964797 8.523048e-03 0.2838078
[75,]  -2.353043e-03       0.0000000000      5.606834e-03 -0.016260884 -0.019047344 8.505333e-03 0.2840483
[76,]  -2.644396e-03       0.0000000000      5.720820e-03 -0.016211592 -0.019126493 8.483952e-03 0.2842812
[77,]  -2.743107e-03       0.0000000000      5.760265e-03 -0.016195310 -0.019153151 8.477521e-03 0.2843592
[78,]  -2.843096e-03       0.0000000000      5.800458e-03 -0.016179171 -0.019181275 8.473083e-03 0.2844411
[79,]  -3.135365e-03       0.0000000000      5.915736e-03 -0.016130870 -0.019263792 8.457283e-03 0.2846831

$coefficients_std
      Species.setosa Species.versicolor Species.virginica   Sepal.Width Petal.Length  Petal.Width Intercept
 [1,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000  0.000000000 0.0000000000 0.1711352
 [2,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.001847636 0.0000000000 0.1711550
 [3,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.003547039 0.0000000000 0.1712078
 [4,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.005109259 0.0000000000 0.1712854
 [5,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.006544589 0.0000000000 0.1713811
 [6,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.007862621 0.0000000000 0.1714893
 [7,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.009072299 0.0000000000 0.1716056
 [8,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.010181963 0.0000000000 0.1717265
 [9,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.011199381 0.0000000000 0.1718492
[10,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.012131784 0.0000000000 0.1719716
[11,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.012985900 0.0000000000 0.1720920
[12,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.013767976 0.0000000000 0.1722091
[13,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.014483810 0.0000000000 0.1723221
[14,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.015138773 0.0000000000 0.1724302
[15,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.015737839 0.0000000000 0.1725331
[16,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.016285607 0.0000000000 0.1726305
[17,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.016786324 0.0000000000 0.1727224
[18,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.017243908 0.0000000000 0.1728086
[19,]   0.000000e+00       0.0000000000      0.000000e+00  0.0000000000 -0.017661971 0.0000000000 0.1728892
[20,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0004914029 -0.018217153 0.0000000000 0.1729636
[21,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0010283025 -0.018753989 0.0000000000 0.1730351
[22,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0015170607 -0.019242414 0.0000000000 0.1731038
[23,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0019620369 -0.019686760 0.0000000000 0.1731692
[24,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0023671437 -0.020091032 0.0000000000 0.1732312
[25,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0027359511 -0.020458869 0.0000000000 0.1732897
[26,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0030717141 -0.020793582 0.0000000000 0.1733446
[27,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0033752589 -0.021097325 0.0000000000 0.1733959
[28,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0036537438 -0.021374590 0.0000000000 0.1734438
[29,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0039073015 -0.021626949 0.0000000000 0.1734884
[30,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0041381667 -0.021856652 0.0000000000 0.1735299
[31,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0043483781 -0.022065749 0.0000000000 0.1735682
[32,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0045397906 -0.022256101 0.0000000000 0.1736038
[33,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0047140920 -0.022429399 0.0000000000 0.1736366
[34,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0048728180 -0.022587182 0.0000000000 0.1736668
[35,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0050173658 -0.022730846 0.0000000000 0.1736947
[36,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0051490065 -0.022861661 0.0000000000 0.1737203
[37,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0052688970 -0.022980784 0.0000000000 0.1737439
[38,]   0.000000e+00       0.0000000000      0.000000e+00 -0.0053412234 -0.023072104 0.0000000000 0.1737627
[39,]   0.000000e+00      -0.0001175922      0.000000e+00 -0.0055019511 -0.023189466 0.0000000000 0.1738240
[40,]   0.000000e+00      -0.0005066170      0.000000e+00 -0.0056801058 -0.023299856 0.0000000000 0.1739735
[41,]   0.000000e+00      -0.0008532154      0.000000e+00 -0.0058408402 -0.023399537 0.0000000000 0.1741074
[42,]   0.000000e+00      -0.0011428955      0.000000e+00 -0.0059793516 -0.023485982 0.0000000000 0.1742201
[43,]   0.000000e+00      -0.0014293556      0.000000e+00 -0.0061123779 -0.023568506 0.0000000000 0.1743313
[44,]   0.000000e+00      -0.0016797073      1.243120e-05 -0.0062347093 -0.023648890 0.0000000000 0.1744252
[45,]   0.000000e+00      -0.0018706468      9.790433e-05 -0.0063361369 -0.023793017 0.0000637378 0.1744695
[46,]   0.000000e+00      -0.0019698629      1.717337e-04 -0.0063922205 -0.023885848 0.0001383412 0.1744746
[47,]   0.000000e+00      -0.0021078477      2.246836e-04 -0.0065057056 -0.024449186 0.0006471339 0.1745119
[48,]   0.000000e+00      -0.0021556371      3.034315e-04 -0.0066036820 -0.025133872 0.0012626110 0.1745132
[49,]   0.000000e+00      -0.0021453273      4.458210e-04 -0.0066898067 -0.025802418 0.0018396700 0.1744739
[50,]   0.000000e+00      -0.0020839569      6.461732e-04 -0.0067646533 -0.026444255 0.0023704464 0.1743980
[51,]   0.000000e+00      -0.0020107174      8.597081e-04 -0.0068258458 -0.026970969 0.0027894199 0.1743114
[52,]   0.000000e+00      -0.0019078474      1.101906e-03 -0.0068806078 -0.027490866 0.0031910501 0.1742055
[53,]   0.000000e+00      -0.0018175109      1.323132e-03 -0.0069261140 -0.027908946 0.0035055892 0.1741087
[54,]   0.000000e+00      -0.0017094991      1.558056e-03 -0.0069678433 -0.028328598 0.0038159527 0.1740017
[55,]   0.000000e+00      -0.0015842081      1.807162e-03 -0.0070050834 -0.028740986 0.0041139641 0.1738844
[56,]   0.000000e+00      -0.0014430021      2.070103e-03 -0.0070376966 -0.029140468 0.0043945661 0.1737569
[57,]   0.000000e+00      -0.0013372850      2.282679e-03 -0.0070644186 -0.029438645 0.0045993214 0.1736560
[58,]   0.000000e+00      -0.0012235170      2.499826e-03 -0.0070892073 -0.029737447 0.0048023811 0.1735510
[59,]   0.000000e+00      -0.0011012638      2.721871e-03 -0.0071113928 -0.030031888 0.0049992462 0.1734416
[60,]   0.000000e+00      -0.0009710435      2.949010e-03 -0.0071308582 -0.030318677 0.0051868390 0.1733278
[61,]   0.000000e+00      -0.0009387214      3.037293e-03 -0.0071436900 -0.030418624 0.0052518855 0.1732887
[62,]   0.000000e+00      -0.0008039241      3.270133e-03 -0.0071634528 -0.030716683 0.0054466147 0.1731721
[63,]   0.000000e+00      -0.0007660395      3.357739e-03 -0.0071742512 -0.030816631 0.0055111469 0.1731316
[64,]   0.000000e+00      -0.0006199118      3.595215e-03 -0.0071902568 -0.031108822 0.0056983444 0.1730097
[65,]   0.000000e+00      -0.0005768718      3.683036e-03 -0.0071989073 -0.031204944 0.0057586692 0.1729675
[66,]  -3.476645e-05      -0.0004234267      3.926329e-03 -0.0072051358 -0.031492861 0.0059291294 0.1728522
[67,]  -6.891206e-05      -0.0003785185      4.015034e-03 -0.0072056052 -0.031592582 0.0059823720 0.1728199
[68,]  -2.509409e-04      -0.0001852860      4.347389e-03 -0.0071946407 -0.031984985 0.0061651351 0.1727122
[69,]  -3.133552e-04      -0.0001451602      4.438172e-03 -0.0071919537 -0.032082312 0.0062041522 0.1726902
[70,]  -5.214018e-04      -0.0000198928      4.695164e-03 -0.0071782342 -0.032371830 0.0063077682 0.1726367
[71,]  -6.024159e-04       0.0000000000      4.785344e-03 -0.0071771375 -0.032467144 0.0063376645 0.1726279
[72,]  -6.921978e-04       0.0000000000      4.869432e-03 -0.0071794528 -0.032563872 0.0063671323 0.1726309
[73,]  -7.920269e-04       0.0000000000      4.942796e-03 -0.0071798959 -0.032660538 0.0063960401 0.1726409
[74,]  -2.055117e-03       0.0000000000      5.491157e-03 -0.0071093875 -0.033478523 0.0064965885 0.1728921
[75,]  -2.353043e-03       0.0000000000      5.606834e-03 -0.0070875711 -0.033624243 0.0064830849 0.1729542
[76,]  -2.644396e-03       0.0000000000      5.720820e-03 -0.0070660864 -0.033763964 0.0064667874 0.1730147
[77,]  -2.743107e-03       0.0000000000      5.760265e-03 -0.0070589896 -0.033811024 0.0064618858 0.1730346
[78,]  -2.843096e-03       0.0000000000      5.800458e-03 -0.0070519551 -0.033860671 0.0064585028 0.1730548
[79,]  -3.135365e-03       0.0000000000      5.915736e-03 -0.0070309023 -0.034006338 0.0064464596 0.1731155

$coefficient_names
[1] ""Species.setosa""     ""Species.versicolor"" ""Species.virginica""  ""Sepal.Width""        ""Petal.Length""       ""Petal.Width""       
[7] ""Intercept""



EDIT #2: In response to @Darren's answer. I am now seeing the following on my actual (confidential) dataset. The cross-validated models have selected smaller lambda's, yet the main model stops at a very large lambda.


> tail(mx@allparameters$lambda)
[1] 0.1536665 0.1400152 0.1275767 0.1162431 0.1059164
> mx@model$lambda_best
[1] 0.1059164
> 
> lapply(mx@model$cross_validation_models, function(m_cv){
+     m <- h2o.getModel(m_cv$name)
+     list( tail(m@allparameters$lambda), m@model$lambda_best )
+ })

[[1]]
[[1]][[1]]
[1] 2.446806e-05 2.229438e-05 2.031381e-05 1.850919e-05 1.686488e-05 1.536665e-05

[[1]][[2]]
[1] 0.01135707


[[2]]
[[2]][[1]]
[1] 2.446806e-05 2.229438e-05 2.031381e-05 1.850919e-05 1.686488e-05 1.536665e-05

[[2]][[2]]
[1] 0.01808366


[[3]]
[[3]][[1]]
[1] 2.446806e-05 2.229438e-05 2.031381e-05 1.850919e-05 1.686488e-05 1.536665e-05

[[3]][[2]]
[1] 0.01647716","['r', 'glm', 'h2o']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
45883791,45883791,2017-08-25T14:35:15,2017-08-25 16:30:59Z,0,"How can I skip rows while importing files?? Skip is not an available option on the h2o.importFile comand.


I have used the library h2o and the command:


h2o.importFile()","['r', 'csv', 'h2o', 'skip']",Jesus,https://stackoverflow.com/users/8490853/jesus,462
45848439,45848439,2017-08-23T20:05:40,2017-08-24 00:51:43Z,548,"I am using 
h2o
's 
flow ui
 to uplaod a csv file to train a model on. When I upload the file and edit the column types before parsing, this is what I am setting a date column to:




After parsing, the data summary shows that all of the date column values are 'missing' and viewing the data with the 
view data
 button shows that they are indeed blanks (.).


Looking 
here
 for acceptable date formats, it says that:




""The first format is for dates formatted as yyyy-MM-dd. Year is a four-digit number, the month is a two-digit number ranging from 1 to 12, and the day is a two-digit value ranging from 1 to 31. This format can also be followed by a space and then a time (specified below).""




I have tried this format with and without (see image above) leading zeros for single digit numbers and still have the same result that h2o parses the date fields and blank, like this:




What is happening here and how can I fix it. Thanks :)",['h2o'],lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
45838312,45838312,2017-08-23T11:19:44,2017-08-25 09:42:34Z,635,"I'm an early starter in H2O (python package). My problem is that I can't figure out how I can create a H2OFrame from a Pandas' dataframe succesfully.


My environment is:




Windows 10 Home, build 15063.540, with 16,0 GB memory


Java SE Development Kit 8u144 (64-bit)


Java SE Runtime Environment (build 1.8.0_144-b01)


Anaconda Python 3.5.4




I started the server with h2o.init():


H2O cluster uptime: 19 hours 14 mins
H2O cluster version:    3.14.0.1
H2O cluster version age:    12 days
H2O cluster name:   H2O_from_python_pedro_23i63g
H2O cluster total nodes:    1
H2O cluster free memory:    3.456 Gb
H2O cluster total cores:    4
H2O cluster allowed cores:  4
H2O cluster status: locked, healthy
H2O connection url: http://localhost:54321
H2O connection proxy:   None
H2O internal security:  False
H2O API Extensions: Algos, AutoML, Core V3, Core V4
Python version: 3.5.4 final



I'm trying to create my H2OFrame from the 
train1
 pandas' dataframe through the following command:


hf1 = h2o.H2OFrame(train1)



Crash info:


OSError: Job with key $03017f00000132d4ffffffff$_8ef7ebc5204725b046d7b31ca7194c71 failed with an exception: DistributedException from /127.0.0.1:54321: 'null', caused by java.lang.AssertionError
stacktrace: 
DistributedException from /127.0.0.1:54321: 'null', caused by java.lang.AssertionError
    at water.MRTask.getResult(MRTask.java:478)
    at water.MRTask.getResult(MRTask.java:486)
    at water.MRTask.doAll(MRTask.java:402)
    at water.parser.ParseDataset.parseAllKeys(ParseDataset.java:245)
    at water.parser.ParseDataset.access$000(ParseDataset.java:26)
    at water.parser.ParseDataset$ParserFJTask.compute2(ParseDataset.java:194)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1255)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
Caused by: java.lang.AssertionError
    at water.parser.Categorical.addKey(Categorical.java:41)
    at water.parser.FVecParseWriter.addStrCol(FVecParseWriter.java:133)
    at water.parser.CsvParser.parseChunk(CsvParser.java:126)
    at water.parser.ParseDataset$MultiFileParseTask$DistributedParse.map(ParseDataset.java:888)
    at water.MRTask.compute2(MRTask.java:637)
    at water.MRTask.compute2(MRTask.java:591)
    at water.MRTask.compute2(MRTask.java:591)
    at water.MRTask.compute2(MRTask.java:591)
    at water.MRTask.compute2(MRTask.java:591)
    at water.MRTask.compute2(MRTask.java:591)
    at water.MRTask.compute2(MRTask.java:591)
    at water.H2O$H2OCountedCompleter.compute1(H2O.java:1258)
    at water.parser.ParseDataset$MultiFileParseTask$DistributedParse$Icer.compute1(ParseDataset$MultiFileParseTask$DistributedParse$Icer.java)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1254)
    ... 5 more



However
, I tried to create the H2OFrame by using the first 
6 rows
 of the pandas' dataframe and it went well.


hf1 = h2o.H2OFrame(train1.loc[:6,:])
[out] Parse progress: |█████████████████████████████████████████████████████████| 100%



But when I try more than these 6 rows (e.g. 
7 rows
), it fails again with the previous crash info:


hf1 = h2o.H2OFrame(train1.loc[:7,:])
[out] Parse progress: | (failed)



What can be wrong in this?


Thanks in advance.


Pedro","['python', 'python-3.x', 'pandas', 'dataframe', 'h2o']",Unknown,,N/A
45837493,45837493,2017-08-23T10:43:07,2017-08-23 11:41:02Z,0,"I'm currently trying to extract model result information from H2O in Python.


For context, I have something along the following lines:


model = H2ODeepLearningEstimator(...)
model.train()



After the model is trained, I can call up the various model metrics. However, some metric will only be generated depending on the type of model.


So for example, I could call 
model.mse(valid=True)
 and it will return a value. but if I call 
model.aic(valid=True)
 it will throw and exception if AIC was not generated. This is a problem because I need to generate a function that writes all generated metrics to a HIVE table. If the metric has not been generated and calling it throws an except, we'll simply write NULL to hive.


So, I've tried to do the following:


param=None
try:
    param=tryParam(model.aic())
except:
    pass
print(param)



This will correctly return 
None
 for the AIC value


However, I need to do this for every parameter, so would prefer to wrap a generic function around it. For example, I would like to pass the function 
model.aic()
 as a parameter to a function called 
tryParam
, which then evaluates it and catches any expections that occur. Such as:


def tryParam(getParam):
    try:
        return getParam
    except:   
        return None

tryParam(model.aic())
tryParam(model.mse())
tryParam(model.mae())
etc.



However, this does not work. When I call 
tryParam(model.aic())
 the exception is raised and the program stops. It would seem to me that the parameter is being evaluated before it is passed to 
tryParam
 (hence triggering the exception before the code to handle it is actually called). This is only a guess though.


Does anyone know how I could do this?","['python', 'h2o']",Karl,https://stackoverflow.com/users/141789/karl,"5,782"
45832671,45832671,2017-08-23T06:57:17,2017-08-23 07:19:18Z,0,"how can I subset a h2o frame in python.
if x is a df & Origin is a variable then in pandas we generally can do subsetting by


x[x.Origin == 'AAF']



but with h2o frame it gives the following error:
""H2OResponseError: Server error java.lang.IllegalArgumentException:
  Error: Name lookup of 'x.hex' failed""","['python', 'subset', 'h2o']",Devrath Mohanty,https://stackoverflow.com/users/5829106/devrath-mohanty,140
45819380,45819380,2017-08-22T13:43:48,2017-08-22 23:01:09Z,0,"I have create a DF and want to convert it to H2O Frame. 


To do that, I do:


library(h2o)
h2o.init(nthreads=-1)
df<-data.table(matrix(0,ncol=46,nrow=30000))
df<-as.h2o(df)



When I do htop on the comand line I see that only one processor of the 4 available are working. It is not possible to do in other way?


Thanks!","['r', 'parallel-processing', 'h2o']",Jesus,https://stackoverflow.com/users/8490853/jesus,462
45816896,45816896,2017-08-22T11:52:30,2017-08-23 01:52:01Z,392,"I built a 
H2O Deep Water model with TensorFlow backend
 (following the 
Deep Water Booklet
 MNIST example).  


(First I did the same with a H2O GBM model to verify that it works in general without the Deep Water dependencies in my Java application. This works well.)


I downloaded my Deep Water TensorFlow model (which I built with Python):


modelfile = model.download_mojo(path=""/h2o_testdata/generatedModels"", get_genmodel_jar=True)



I think this step worked fine again. Now, I want to use the model zip file in my Java application:


I added the dependencies (h2o-genmodel.jar via Maven and deepwater-all.jar via copy&paste as I could not find it on Maven) to my project. 
It seems like I need to add some additional dependencies or source code to add / apply the TensorFlow 'backend' (not sure what 'backend' means here as I just want to do inference in Java, not model training)?
 


I instantiate the model:


EasyPredictModelWrapper model = new EasyPredictModelWrapper(MojoModel.load(""src/main/resources/generatedModels/DeepWater_model_python_1503388273301_1.zip""));



Here is the exception:


java.lang.UnsatisfiedLinkError: /private/var/folders/0s/0xdkb9n12yqdb3fs71926z3c0000gp/T/libtensorflow_jni.so147ba98b-e7e3-49bb-8d4e-e5a06efd7a66: dlopen(/private/var/folders/0s/0xdkb9n12yqdb3fs71926z3c0000gp/T/libtensorflow_jni.so147ba98b-e7e3-49bb-8d4e-e5a06efd7a66, 1): no suitable image found.  Did find:
    /private/var/folders/0s/0xdkb9n12yqdb3fs71926z3c0000gp/T/libtensorflow_jni.so147ba98b-e7e3-49bb-8d4e-e5a06efd7a66: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x00
    /private/var/folders/0s/0xdkb9n12yqdb3fs71926z3c0000gp/T/libtensorflow_jni.so147ba98b-e7e3-49bb-8d4e-e5a06efd7a66: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x00
    at java.lang.ClassLoader$NativeLibrary.load(Native Method)
    at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)
    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)
    at java.lang.Runtime.load0(Runtime.java:809)
    at java.lang.System.load(System.java:1086)
    at deepwater.backends.tensorflow.LibraryLoader.loadNativeLib(LibraryLoader.java:87)
    at deepwater.backends.tensorflow.TensorflowBackend.<clinit>(TensorflowBackend.java:30)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at hex.genmodel.algos.deepwater.DeepwaterMojoModel.createDeepWaterBackend(DeepwaterMojoModel.java:96)
    at hex.genmodel.algos.deepwater.DeepwaterMojoReader.readModelData(DeepwaterMojoReader.java:31)
    at hex.genmodel.ModelMojoReader.readAll(ModelMojoReader.java:143)
    at hex.genmodel.ModelMojoReader.readFrom(ModelMojoReader.java:34)
    at hex.genmodel.MojoModel.load(MojoModel.java:35)
    at com.github.megachucky.kafka.streams.machinelearning.Kafka_Streams_Deep_Learning_H2O_MNIST.main(Kafka_Streams_Deep_Learning_H2O_MNIST.java:15)
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: darwin, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/java/README.md for possible solutions (such as building the library from source).
    at org.tensorflow.NativeLibrary.load(NativeLibrary.java:66)
    at org.tensorflow.TensorFlow.init(TensorFlow.java:27)
    at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:31)
    at org.tensorflow.Graph.<clinit>(Graph.java:194)
    at deepwater.backends.tensorflow.models.ModelFactory.readMetaGraph(ModelFactory.java:143)
    at deepwater.backends.tensorflow.models.ModelFactory.LoadModelFromFile(ModelFactory.java:220)
    at deepwater.backends.tensorflow.TensorflowBackend.buildNet(TensorflowBackend.java:74)
    at hex.genmodel.algos.deepwater.DeepwaterMojoReader.readModelData(DeepwaterMojoReader.java:64)
    at hex.genmodel.ModelMojoReader.readAll(ModelMojoReader.java:143)
    at hex.genmodel.ModelMojoReader.readFrom(ModelMojoReader.java:34)
    at hex.genmodel.MojoModel.load(MojoModel.java:35)
    at com.github.megachucky.kafka.streams.machinelearning.Kafka_Streams_Deep_Learning_H2O_MNIST.main(Kafka_Streams_Deep_Learning_H2O_MNIST.java:15)



I cannot find any documentation on how to set this up, so maybe someone can help? 
Or do I need to install TensorFlow on my laptop explicitly available for the Java application, too? I thought this is not necessary with H2O because it ""just generates Java code"", which I can use in an application - similar to other non-DeepWater H2O models?


By the way, side question: How do I get deepwater-all.jar via Maven dependency? I cannot find it.","['java', 'tensorflow', 'deep-learning', 'h2o']",Kai Wähner,https://stackoverflow.com/users/538775/kai-w%c3%a4hner,"5,400"
45814469,45814469,2017-08-22T09:55:58,2017-08-22 22:58:53Z,0,"Supose I have 2 data.frames and I want to calculate the euclidean distance between all of the rows of them. My code is:


set.seed(121)
# Load library
library(h2o)
system.time({
  h2o.init()
  # Create the df and convert to h2o frame format
  df1 <- as.h2o(matrix(rnorm(7500 * 40), ncol = 40))
  df2 <- as.h2o(matrix(rnorm(1250 * 40), ncol = 40))
  # Create a matrix in which I will record the distances
  matrix1 <- as.h2o(matrix(0, nrow = 7500, ncol = 40))
  # Loop to calculate all the distances
  for (i in 1:nrow(df2)){
    matrix1[, i] <- h2o.sqrt(h2o.distance(df1, df2[, i]))
  }
})



I´m sure there is more efficient way to store it into a matrix.","['r', 'optimization', 'parallel-processing', 'h2o']",Uwe,https://stackoverflow.com/users/3817004/uwe,42.4k
45808473,45808473,2017-08-22T03:30:46,2017-08-22 04:12:39Z,0,"I am able to save an H2O model, load it again and then show it...


# save the model
model_path_2 = h2o.save_model(model=my_xgboost_2, path=""tmp/mymodel"", force=True)
print (model_path_2)

>>>/home/dell/Documents/Enigma/tmp/mymodel/XGBoost_model_python_1503367354328_27

# load the model
saved_model_2 = h2o.load_model(model_path_2)



but I cannot use it to predict.


saved_model_2.predict(test)

    >>>xgboost prediction progress: | (failed)
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-100-fa76fd498ee6> in <module>()
----> 1 saved_model_2.predict(test)

/home/dell/anaconda3/lib/python3.6/site-packages/h2o/model/model_base.py in predict(self, test_data)
    130         j = H2OJob(h2o.api(""POST /4/Predictions/models/%s/frames/%s"" % (self.model_id, test_data.frame_id)),
    131                    self._model_json[""algo""] + "" prediction"")
--> 132         j.poll()
    133         return h2o.get_frame(j.dest_key)
    134 

/home/dell/anaconda3/lib/python3.6/site-packages/h2o/job.py in poll(self, verbose_model_scoring_history)
     75             if (isinstance(self.job, dict)) and (""stacktrace"" in list(self.job)):
     76                 raise EnvironmentError(""Job with key {} failed with an exception: {}\nstacktrace: ""
---> 77                                        ""\n{}"".format(self.job_key, self.exception, self.job[""stacktrace""]))
     78             else:
     79                 raise EnvironmentError(""Job with key %s failed with an exception: %s"" % (self.job_key, self.exception))

OSError: Job with key $03017f00000132d4ffffffff$_927b7278904ecf169173d48a23de4c10 failed with an exception: java.lang.NullPointerException
stacktrace: 
java.lang.NullPointerException



I can, however, predict on the model without saving it. I am using Python 3.6.1, and H2O 3.14.0.1 on Ubuntu 16.04.",['h2o'],David Comfort,https://stackoverflow.com/users/6287730/david-comfort,153
45799270,45799270,2017-08-21T14:11:46,2017-08-21 14:11:46Z,0,"When i was doing prediction in h2o,i realized that all predict results are totally the same for different days in test.I imported the train and validation data feed the network, and set the nflods number as 10. According to my understanding, the prediction result should be various depends on days. But the results are totally the same. given input and output in validation and train data set are different, do you know why the predictions are the same, any adjustments i should do for more data ?
Thnaks very much for your reading and helping !!!","['r', 'h2o']",hanting du,https://stackoverflow.com/users/8412595/hanting-du,11
45783640,45783640,2017-08-20T15:13:03,2017-08-23 00:00:51Z,0,"I have Python 2 and 3 on Debian (via 
apt
) and installed H2O following the 
official instructions
 (thereby using 
pip3
 instead of 
pip
 when executing the install commands).


Afterwards, Python seems fine:


$ python
Python 3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:09:58)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux



But when I try to use h2o, it fails:


>>> import h2o
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'h2o'



What I've tried that so far – without solving the issue:




conda install h2o -> install OK (verified with ""conda list"")


conda install h2o-py:




Error:


UnsatisfiableError: The following specifications were found to be in conflict:
  - h2o-py -> python 2.7* -> openssl 1.0.1*
  - python 3.6*





Following this 
""second"" official install instructions
 from h2o I found -> install without errors, but problem not resolved. 


Seems I now have many Python envs installed - not sure if this causes the problem: 




Output of 
whereis python


python: 
/usr/bin/python3.6m 
/usr/bin/python3.5m 
/usr/bin/python3.6 
/usr/bin/python 
/usr/bin/python3.5 
/usr/bin/python2.7
/usr/lib/python3.6 
/usr/lib/python3.5 
/usr/lib/python2.6
/usr/lib/python2.7 
/etc/python3.6 /etc/python /etc/python3.5
/etc/python2.7 
/usr/local/lib/python3.6 
/usr/local/lib/python3.5
/usr/local/lib/python2.7 
/usr/include/python3.6m
/usr/include/python3.5m 
/usr/include/python2.7 
/usr/share/python
/home/mac/Apps/anaconda3/bin/python3.6m
/home/mac/Apps/anaconda3/bin/python3.6m-config
/home/mac/Apps/anaconda3/bin/python3.6
/home/mac/Apps/anaconda3/bin/python
/home/mac/Apps/anaconda3/bin/python3.6-config
/usr/share/man/man1/python.1.gz



How can I let Python find h2o?
 If this fix includes purging Python2, I am happy, just afraid of cutting dependencies.","['python', 'python-3.x', 'machine-learning', 'installation', 'h2o']",Mathias Renner,https://stackoverflow.com/users/4628550/mathias-renner,11
45783048,45783048,2017-08-20T14:12:42,2017-08-20 19:55:38Z,0,"How it is possible that storing data into H2O matrix are slower than in data.table?  


#Packages used ""H2O"" and ""data.table""
library(h2o)
library(data.table)
#create the matrix
matrix1<-data.table(matrix(rnorm(1000*1000),ncol=1000,nrow=1000))
matrix2<-h2o.createFrame(1000,1000)

h2o.init(nthreads=-1)
#Data.table variable store
for(i in 1:1000){
matrix1[i,1]<-3
}
#H2O Matrix Frame store
for(i in 1:1000){
  matrix2[i,1]<-3
}



Thanks!","['r', 'data.table', 'h2o']",Dave2e,https://stackoverflow.com/users/5792244/dave2e,23.8k
45782023,45782023,2017-08-20T12:16:02,2017-08-29 16:06:00Z,0,"I am using H2O with R to calculate the euclidean distance between 2 data.frames:


set.seed(121)

#create the data
df1<-data.frame(matrix(rnorm(1000),ncol=10))
df2<-data.frame(matrix(rnorm(300),ncol=10))
#init h2o
h2o.init()

#transform to h2o
df1.h<-as.h2o(df1)
df2.h<-as.h2o(df2)



if I use normal calculations, i.e. the first row:


distance1<-sqrt(sum((df1[1,]-df2[1,])^2))



And If I use the H2O library:


distance.h2o<-h2o.distance(df1.h[1,],df2.h[1,],""l2"")

print(distance1)
print(distance.h2o)



The distance1 and distance.h2o are not the same. Does anybody knows why? Thanks!!","['r', 'distance', 'h2o', 'euclidean-distance']",Jesus,https://stackoverflow.com/users/8490853/jesus,462
45749290,45749290,2017-08-18T05:43:40,2017-08-18 05:43:40Z,507,"When trying to print out the cluster show status of a H2O cluster on Windows computers, it gives the following error:




h2o.init(nthreads = -1, strict_version_check = False)


File ""c:\python27\lib\site-packages\h2o\h2o.py"", line 267, in init
      h2oconn.cluster.show_status()


File ""c:\python27\lib\site-packages\h2o\backend\cluster.py"", line 237, in show_status


[""Python version:"",            ""%d.%d.%d %s"" % tuple(sys.version_info[:4])],
    File ""c:\python27\lib\site-packages\h2o\display.py"", line 49, in 
init

      self.pprint()


File ""c:\python27\lib\site-packages\h2o\display.py"", line 59, in pprint
      print(r)


File ""c:\python27\lib\site-packages\colorama\ansitowin32.py"", line 40, in write
      self.__convertor.write(text)


File ""c:\python27\lib\site-packages\colorama\ansitowin32.py"", line 141, in write self.write_and_convert(text)


File ""c:\python27\lib\site-packages\colorama\ansitowin32.py"", line 169, in write_and_convert
      self.write_plain_text(text, cursor, len(text))


File ""c:\python27\lib\site-packages\colorama\ansitowin32.py"", line 174, in write_plain_text
      self.wrapped.write(text[start:end])
  File ""C:\Python27\lib\encodings\cp857.py"", line 12, in encode
      return codecs.charmap_encode(input,errors,encoding_map)


UnicodeEncodeError: 'charmap' codec can't encode character u'\u2021' in position 210: character maps to 
  H2O session _sid_a367 closed.




It seems that colorama Python library can not handle different locale/charsets outputs on Windows terminals.","['h2o', 'colorama']",XentneX,https://stackoverflow.com/users/1928229/xentnex,117
45726324,45726324,2017-08-17T03:53:27,2017-08-17 14:08:41Z,394,"H2O process crashed when doing a Grid Search with XGBoost:




terminate called after throwing an instance of 'thrust::system::system_error'
    what():  /tmp/xgboost/plugin/updater_gpu/src/device_helpers.cuh(387): an illegal memory access was encountered




After giving the INFO message below:




08-17 06:44:46.672 10.0.1.89:54321       14426  FJ-1-3    INFO: Checking convergence with logloss metric: 0.04519170911104479 --> 0.02811784326194906 (still improving)
  .
  08-17 06:44:46.672 10.0.1.89:54321       14426  FJ-1-3    INFO: For grid: final_grid built: 90 models.




The Java exception dumps:




08-17 06:44:46.742 10.0.1.89:54321       14426  #12317-18 INFO: GET /99/Grids/final_grid, parms: {}
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR: java.lang.IllegalArgumentException: Field not found: 'col_sample_rate_change_per_level/_col_sample_rate
  _change_per_level' on object hex.tree.xgboost.XGBoostModel$XGBoostParameters@49356589
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at water.util.PojoUtils.getFieldValue(PojoUtils.java:562)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at hex.grid.Grid.createSummaryTable(Grid.java:370)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at hex.schemas.GridSchemaV99.fillFromImpl(GridSchemaV99.java:158)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at water.api.GridsHandler.fetch(GridsHandler.java:41)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at java.lang.reflect.Method.invoke(Method.java:498)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at water.api.Handler.handle(Handler.java:63)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at water.api.RequestServer.serve(RequestServer.java:448)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at water.api.RequestServer.doGeneric(RequestServer.java:297)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at water.api.RequestServer.doGet(RequestServer.java:221)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:183)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:183)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.Server.handle(Server.java:370)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:49
  4)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53
  )
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:9
  71)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpCo
  nnection.java:1033)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:         at java.lang.Thread.run(Thread.java:748)
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR: Caught exception:
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR: ERROR MESSAGE:
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR:
  08-17 06:44:46.747 10.0.1.89:54321       14426  #12317-18 ERRR: Field not found: 'col_sample_rate_change_per_level/_col_sample_rate_change_per_level' on object hex.tre
  e.xgboost.XGBoostModel$XGBoostParameters@49356589","['h2o', 'xgboost']",XentneX,https://stackoverflow.com/users/1928229/xentnex,117
45725614,45725614,2017-08-17T02:17:53,2017-08-17 02:33:13Z,198,"First I would like to thank the H2o team for a great product and rapid development / iteration. 


I was testing h2o autoML on a 4 machine cluster. (40 cores, 256 gigs of ram, gigabite bandwidth) 


For a 20MB dataset I am noticing that the cluster is using up a lot of network and hardly touching the CPU. I was wondering if it makes sense for h2o to train 1 model per computer instead of trying to train every model on the entire cluster.",['h2o'],user3078500,https://stackoverflow.com/users/3078500/user3078500,302
45718219,45718219,2017-08-16T15:59:20,2017-08-21 13:06:38Z,217,"Update 1:


Log File from the H2O Deep Water Cloud: 
https://drive.google.com/file/d/0B_1g718qYsqhcUl4WFQ5S1NKbE0/view?usp=sharing




mxnet backend - now solved (after stopping/starting the VM in Azure)


tensorflow backend - still failure






I want to test H2Os Deep Water using a GPU enabled cloud instance on MS Azure (NC6 - 
https://azure.microsoft.com/en-us/blog/azure-n-series-general-availability-on-december-1/
). 
But running H2O Deep Water I got an error saying:




mxnet backend: 
java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: Could not initialize class deepwater.backends.mxnet.MXNetBackend$MXNetLoader


tensorflow backend: 
java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: null




Config and setup is as follows:


After provisioning a DSVM on a NC6 VM. I checked the prerequisite of deep water - CUDA & CUDANN:


sysadmin@DEVSMTTSYGPU002:~$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Tue_Jan_10_13:22:03_CST_2017
Cuda compilation tools, release 8.0, V8.0.61
sysadmin@DEVSMTTSYGPU002:~$ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
#define CUDNN_MAJOR      5
#define CUDNN_MINOR      1
#define CUDNN_PATCHLEVEL 10



After that I ran the following steps:


Setup env vars: 




export CUDA_PATH=/usr/local/cuda


export LD_LIBRARY_PATH=$CUDA_PATH/lib64:$LD_LIBRARY_PATH




Install pip for python 2.7




sudo apt-get install python-pip




Install Deep Water:




pip2 install http://s3.amazonaws.com/h2o-deepwater/public/nightly/latest/h2o-3.13.0-py2.py3-none-any.whl




Install libatlas-base-dev




sudo apt-get install libatlas-base-dev




To run the example I start python 2.7 and run


import h2o
h2o.init()



After that I used H2O Flow to create some artificial data and learn a simple deep water model




createFrame {""dest"":""MNIST_SIM_60k"",""rows"":""60000"",""cols"":""784"",""seed"":7595850248774472000,""seed_for_column_types"":-1,""randomize"":true,""value"":0,""real_range"":100,""categorical_fraction"":""0"",""factors"":5,""integer_fraction"":""1"",""binary_fraction"":""0"",""binary_ones_fraction"":""0"",""time_fraction"":0,""string_fraction"":0,""integer_range"":""127"",""missing_fraction"":""0"",""response_factors"":2,""has_response"":true}


buildModel 'deepwater', {""model_id"":""deepwater-782cc564-497c-4c39-a22a-b6904fb04188"",""training_frame"":""MNIST_SIM_60k"",""nfolds"":0,""response_column"":""response"",""ignored_columns"":[],""epochs"":""100"",""ignore_const_cols"":true,""network"":""auto"",""activation"":""Rectifier"",""hidden"":[100],""problem_type"":""dataset"",""checkpoint"":"""",""autoencoder"":false,""balance_classes"":false,""score_each_iteration"":false,""categorical_encoding"":""AUTO"",""train_samples_per_iteration"":-2,""standardize"":true,""distribution"":""AUTO"",""score_interval"":5,""score_training_samples"":10000,""score_validation_samples"":0,""score_duty_cycle"":0.1,""stopping_rounds"":5,""stopping_metric"":""AUTO"",""stopping_tolerance"":0,""max_runtime_secs"":0,""backend"":""tensorflow"",""image_shape"":[0,0],""channels"":3,""network_definition_file"":"""",""network_parameters_file"":"""",""mean_image_file"":"""",""export_native_parameters_prefix"":"""",""input_dropout_ratio"":0,""hidden_dropout_ratios"":[],""overwrite_with_best_model"":true,""target_ratio_comm_to_comp"":0.05,""seed"":-1,""learning_rate"":0.001,""learning_rate_annealing"":0.000001,""momentum_start"":0.9,""momentum_ramp"":10000,""momentum_stable"":0.9,""classification_stop"":0,""shuffle_training_data"":true,""mini_batch_size"":32,""clip_gradient"":10,""sparse"":false,""gpu"":true,""device_id"":[0],""cache_data"":true}




For both backends (mxnet and tensorflow) I got the errors named above. For tensorflow the stacktrace is 


java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: null
    at hex.deepwater.DeepWaterModelInfo.setupNativeBackend(DeepWaterModelInfo.java:267)
    at hex.deepwater.DeepWaterModelInfo.<init>(DeepWaterModelInfo.java:214)
    at hex.deepwater.DeepWaterModel.<init>(DeepWaterModel.java:227)
    at hex.deepwater.DeepWater$DeepWaterDriver.buildModel(DeepWater.java:131)
    at hex.deepwater.DeepWater$DeepWaterDriver.computeImpl(DeepWater.java:118)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:173)
    at hex.deepwater.DeepWater$DeepWaterDriver.compute2(DeepWater.java:111)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1255)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



And for mxnet the stacktrace is 


java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: Could not initialize class deepwater.backends.mxnet.MXNetBackend$MXNetLoader
    at hex.deepwater.DeepWaterModelInfo.setupNativeBackend(DeepWaterModelInfo.java:267)
    at hex.deepwater.DeepWaterModelInfo.<init>(DeepWaterModelInfo.java:214)
    at hex.deepwater.DeepWaterModel.<init>(DeepWaterModel.java:227)
    at hex.deepwater.DeepWater$DeepWaterDriver.buildModel(DeepWater.java:131)
    at hex.deepwater.DeepWater$DeepWaterDriver.computeImpl(DeepWater.java:118)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:173)
    at hex.deepwater.DeepWater$DeepWaterDriver.compute2(DeepWater.java:111)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1255)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



How can I get H2O Deep Water running with at least one of the backend?


Sidenote: xgboost with GPU from H2O support works.


Many thanks


Roberto",['h2o'],Unknown,,N/A
45705789,45705789,2017-08-16T05:37:05,2017-08-16 07:12:49Z,0,"I used 
h2o.deeplearning
 function to performe DNN in R.


I want to initialze the weights for the iteration and performance improvement of the Deep neural network.


I know that weighing initialization should be a small value between -1 and +1 rather than a large value.


Then, what is the parameter code in 
h2o.deeplearning
 that initialze the weight???? And how to use it to initialze between -1 and +1??


please help me..!","['r', 'neural-network', 'deep-learning', 'h2o']",신익수,https://stackoverflow.com/users/8058467/%ec%8b%a0%ec%9d%b5%ec%88%98,77
45684592,45684592,2017-08-14T23:43:10,2017-08-14 23:51:35Z,666,"I am curious to know that when using the Standardised feature in a H2O model in R how does it work when scoring out new data.


I know that when it standardises on a training set is sets the mean to 0 and standard deviation to 1 based on the mean and standard deviation of the training data but what does it do with new data?


Does it standardise based on the training data mean and standard deviation or does it standardise based on the new data being scored?","['modeling', 'h2o', 'standardized']",Mark Aitkin,https://stackoverflow.com/users/2318882/mark-aitkin,31
45684402,45684402,2017-08-14T23:19:02,2018-03-29 23:36:15Z,0,"I am using H2O (Python) where I am playing with 
H2OGridSearch
 for alpha values of a GLM (
H2OGeneralizedLinearEstimator
), also using 
lambda_search=True
 using k-fold cross-validation.


How can I get the best model's lambda value?


EDIT
: Fully reproducible example


Data:


34.40 17:1 73:1 127:1 265:1 912:1 1162:1 1512:1 1556:1 1632:1 1738:1
205.10 127:1 138:1 338:1 347:1 883:1 912:1 1120:1 1122:1 1512:1
7.75 66:1 127:1 347:1 602:1 1422:1 1512:1 1535:1 1738:1
8.85 127:1 608:1 906:1 979:1 1077:1 1512:1 1738:1
51.80 127:1 347:1 608:1 766:1 912:1 928:1 952:1 1034:1 1512:1 1610:1 1738:1
110.00 127:1 229:1 347:1 602:1 608:1 1171:1 1512:1 1718:1
8.90 66:1 127:1 205:1 347:1 490:1 589:1 912:1 1016:1 1512:1



Call this file 
h2o_example.svmlight


Then run:


h2o_data = h2o.import_file(""h2o_example.svmlight"")
cols = h2o_data.columns[1:]
hyper_parameters = {""alpha"": [0.0, 0.01, 0.99, 1.0]}
grid = H2OGridSearch(H2OGeneralizedLinearEstimator(family=""gamma"", link=""log"", lambda_search=True, nfolds=2, intercept=True, standardize=False),
hyper_params=hyper_parameters)
grid.train(y=""C1"", x=cols, training_frame=h2o_data)
grid_table = grid.get_grid(sort_by=""r2"", decreasing=True)
best = grid_table.models[0]
best.actual_params[""lambda""]
best.actual_params[""alpha""]



The last two commands fail, giving me an error:


TypeError: 'property' object has no attribute '__getitem__'



Apparently, I am using 
lambda_search
 in a wrong way. How can I get a single alpha and lambda value for the best model according to my criterion?","['python', 'glm', 'h2o']",Unknown,,N/A
45678743,45678743,2017-08-14T16:15:17,2017-08-14 19:12:14Z,861,"After using autoML to generate aml leaderboard, I ran 


h2o.predict(aml@leader, test_df) 



but how can I know which model on the leaderboard it is using? And if I want to access the structure or hyperparameter of any model on leaderboard how can I do so? 


Besides the result on test set is not nearly as good as the one on validation set, is it common - did I use it wrongly or does it has a tendency to overfit? 


Also want to understand its infrastructure better, after h2o.init does the data transmit to a server in h2o.ai's clusters or do everything happen on my local laptop?


Thanks.","['machine-learning', 'h2o']",santoku,https://stackoverflow.com/users/3983454/santoku,"3,417"
45672118,45672118,2017-08-14T10:07:04,2020-11-02 10:39:47Z,0,"When converting a Pandas dataframe to a H2O frame using the h2o.H2OFrame() function an error is occurring.


Additional rows are being created in the H2o Frame. When I looked into this, it appears the new rows are duplicates of other rows. Depending on the data size the number of duplicate rows added varies, but typically around 2-10.


Code:


train_h2o = h2o.H2OFrame(python_obj=train_df_complete)

print(train_df_complete.shape[0])
print(train_h2o.nrow)



Output:


3871998
3872000



As you can see here, 2 additional rows have being added. When studied closer there are now 2 rows per user for 2 of the users. I.e. 2 rows have being duplicated.


This appears to be a major bug, does anyone have experience of this problem and is there a way to fix it?


Thanks","['python', 'python-3.x', 'pandas', 'h2o']",George,https://stackoverflow.com/users/6510824/george,684
45665995,45665995,2017-08-14T00:18:30,2018-09-09 20:07:38Z,953,"I tried to use AutoML for a binary classification task with 100 hours. It appears that it is just building a large number of GBM models and not getting to other types. (So far built 40) 


Is there a way to set the maximum number of GBM models?",['h2o'],user3078500,https://stackoverflow.com/users/3078500/user3078500,302
45664403,45664403,2017-08-13T20:01:08,2017-08-14 07:08:34Z,0,"I am training a model in h2o like so:


import h2o

h2o.init()
trainFrame = h2o.import_file(path = ""C:/train.csv"")

train, test = trainFrame.split_frame([0.8])

x = [""A"", ""B"", ""C""]
y = ""Target""

m = h2o.estimators.H2ORandomForestEstimator(model_id=""RF_defaults"")
m.train(x, y, train)



now I want to save these results to a pandas dataframe.


I am trying to do that like so:


m.as_data_frame(pandas = True)


but that returns:


AttributeError: type object 'H2ORandomForestEstimator' has no attribute 'as_data_frame'","['python', 'python-2.7', 'pandas', 'h2o']",Steve Piercy,https://stackoverflow.com/users/2214933/steve-piercy,14.9k
45655163,45655163,2017-08-12T21:39:54,2017-08-14 20:28:54Z,536,"When trying to acquire the recall score using e.g.


rf_model.recall()



I get the error:


h2o ValueError: No metric tpr



I can get other metrics, such as the accuracy, AUC, precision and F1 but no recall...
This is presumably a bug.


If I run:


from h2o.model.metrics_base import H2OBinomialModelMetrics as bmm
reporter = bmm(rf_model.metric)
rf_model.metric('recall')



I get:


Could not find exact threshold 0.0; using closest threshold found 0.0.



What is going on?


I am running the h2o version 'h2o-3.15.0.3990'.


I followed the h2o tutorial:


https://github.com/h2oai/h2o-tutorials/blob/master/training/h2o_algos/src/py/decision_tree_ensembles.ipynb


and with using my own dataset, I get the error described above.


Any help?


Also, how does one plot a precision / recall curve using h2o?


Thanks","['python', 'machine-learning', 'h2o', 'precision-recall']",Unknown,,N/A
45649375,45649375,2017-08-12T10:51:04,2017-08-14 18:33:06Z,0,"We followed the text categorization process with iterating the items below:




Create a Word2Vec word embedding model with text documents.


Do a Grid Search and tree depth parameters.


Select the best performed Final GBM model.




As we iterate through the list CPU cores are working at %100 load. Are there any procedure or solution iterating the above process with H2O Deep Water GPU capabilities ?","['deep-learning', 'word2vec', 'h2o', 'grid-search', 'gbm']",Unknown,,N/A
45634616,45634616,2017-08-11T11:54:40,2017-08-17 17:48:31Z,0,"I have a rather simple question, but have not been able to find a documented solution anywhere.


I'm currently building a pipeline with H2O models and as part of the process I need to write some basic information about each trained model into a table.


Let's say I have something like:


model = H2ODeepLearningEstimator(...)
model.train(...)



After doing this, I want to pull the type of model from the 
model
 object. I.e, I am looking for something like:


model.getType()



which then returns a string 
""H2ODeepLearningEstimator""
 or equivalently 
""deeplearning""
 which H2O appears to use internally as the model type identifier. I would also like to get other details, such as whether it was a regression or classification model. I don't see a parameter where this information is exposed.


if I run 
model.save_model_details
 for example, I get:


H2ODeepLearningEstimator :  Deep Learning
Model Key:  Grid_DeepLearning_py_4_sid_a02a_model_python_1502450758585_2_model_0


ModelMetricsRegression: deeplearning
** Reported on train data. **

MSE: 19.5334650304
RMSE: 4.4196679774
MAE: 1.44489752843
RMSLE: NaN
Mean Residual Deviance: 19.5334650304

ModelMetricsRegression: deeplearning
** Reported on validation data. **
...
...



Presumably 
model.save_model_details
 builds up this summary from individual parameters. I would like to access these (and similar) parameters directly via the 
model
 object (for performance metrics this is possible via 
model.mse()
, 
model.mae()
 etc.)","['python', 'h2o']",Karl,https://stackoverflow.com/users/141789/karl,"5,782"
45631045,45631045,2017-08-11T08:59:24,2017-08-11 16:51:25Z,589,"sorry, yet another real beginner question:


I downloaded the H2O Deep Water Docker container as explained in the Deep Water Booklet (
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/DeepWaterBooklet.pdf
):


docker run -it --rm -p 54321:54321 -p 8080:8080 -v $PWD:/host opsh2oai/h2o-deepwater-cpu

java -jar /opt/h2o.jar



I started H2O successfully and can access it with Flow UI in my local browser via localhost:54321.


I want to do the MNIST Python example from the above booklet. I use Anaconda and Spyder IDE with Python 2.7.


I think instead of using init(), I assume I have to use the connect() method. Either way does not work, I get TimeOut, Connection Error or HTTP Exception depending on what I try... I tried several IPs (including localhost), e.g.:


from h2o.estimators.deepwater import H2ODeepWaterEstimator
# Start or connect to H2O
#h2o.init()
h2o.connect(ip=""127.0.0.1"", port=54321)



My startup logs show the following. 


root@d54b0501b7c1:/# java -jar /opt/h2o.jar
08-11 08:46:32.904 172.17.0.2:54321      9      main      INFO: Found XGBoost backend with library: xgboost4j
08-11 08:46:32.926 172.17.0.2:54321      9      main      INFO: ----- H2O started  -----
08-11 08:46:32.928 172.17.0.2:54321      9      main      INFO: Build git branch: (HEAD detached at 0c917c766)
08-11 08:46:32.928 172.17.0.2:54321      9      main      INFO: Build git hash: 0c917c766f28860a19ff3f110e1f405eb9f41623
08-11 08:46:32.928 172.17.0.2:54321      9      main      INFO: Build git describe: jenkins-master-3971
08-11 08:46:32.928 172.17.0.2:54321      9      main      INFO: Build project version: 3.13.0.356 (latest version: 3.14.0.1)
08-11 08:46:32.928 172.17.0.2:54321      9      main      INFO: Build age: 12 days
08-11 08:46:32.929 172.17.0.2:54321      9      main      INFO: Built by: 'jenkins'
08-11 08:46:32.929 172.17.0.2:54321      9      main      INFO: Built on: '2017-07-29 13:31:58'
08-11 08:46:32.929 172.17.0.2:54321      9      main      INFO: Watchdog Build git branch: (unknown)
08-11 08:46:32.929 172.17.0.2:54321      9      main      INFO: Watchdog Build git hash: (unknown)
08-11 08:46:32.929 172.17.0.2:54321      9      main      INFO: Watchdog Build git describe: (unknown)
08-11 08:46:32.929 172.17.0.2:54321      9      main      INFO: Watchdog Build project version: (unknown)
08-11 08:46:32.930 172.17.0.2:54321      9      main      INFO: Watchdog Built by: (unknown)
08-11 08:46:32.930 172.17.0.2:54321      9      main      INFO: Watchdog Built on: (unknown)
08-11 08:46:32.930 172.17.0.2:54321      9      main      INFO: XGBoost Build git branch: (unknown)
08-11 08:46:32.930 172.17.0.2:54321      9      main      INFO: XGBoost Build git hash: (unknown)
08-11 08:46:32.930 172.17.0.2:54321      9      main      INFO: XGBoost Build git describe: (unknown)
08-11 08:46:32.930 172.17.0.2:54321      9      main      INFO: XGBoost Build project version: (unknown)
08-11 08:46:32.931 172.17.0.2:54321      9      main      INFO: XGBoost Built by: (unknown)
08-11 08:46:32.931 172.17.0.2:54321      9      main      INFO: XGBoost Built on: (unknown)
08-11 08:46:32.931 172.17.0.2:54321      9      main      INFO: Processed H2O arguments: []
08-11 08:46:32.931 172.17.0.2:54321      9      main      INFO: Java availableProcessors: 2
08-11 08:46:32.931 172.17.0.2:54321      9      main      INFO: Java heap totalMemory: 31.0 MB
08-11 08:46:32.932 172.17.0.2:54321      9      main      INFO: Java heap maxMemory: 444.5 MB
08-11 08:46:32.932 172.17.0.2:54321      9      main      INFO: Java version: Java 1.8.0_131 (from Oracle Corporation)
08-11 08:46:32.932 172.17.0.2:54321      9      main      INFO: JVM launch parameters: []
08-11 08:46:32.932 172.17.0.2:54321      9      main      INFO: OS version: Linux 4.9.36-moby (amd64)
08-11 08:46:32.932 172.17.0.2:54321      9      main      INFO: Machine physical memory: 1.95 GB
08-11 08:46:32.932 172.17.0.2:54321      9      main      INFO: X-h2o-cluster-id: 1502441188713
08-11 08:46:32.933 172.17.0.2:54321      9      main      INFO: User name: 'root'
08-11 08:46:32.933 172.17.0.2:54321      9      main      INFO: IPv6 stack selected: false
08-11 08:46:32.933 172.17.0.2:54321      9      main      INFO: Possible IP Address: eth0 (eth0), 172.17.0.2
08-11 08:46:32.933 172.17.0.2:54321      9      main      INFO: Possible IP Address: lo (lo), 127.0.0.1
08-11 08:46:32.933 172.17.0.2:54321      9      main      INFO: H2O node running in unencrypted mode.
08-11 08:46:32.943 172.17.0.2:54321      9      main      INFO: Internal communication uses port: 54322
08-11 08:46:32.943 172.17.0.2:54321      9      main      INFO: Listening for HTTP and REST traffic on http://172.17.0.2:54321/
08-11 08:46:32.945 172.17.0.2:54321      9      main      INFO: H2O cloud name: 'root' on /172.17.0.2:54321, discovery address /225.53.128.226:57653
08-11 08:46:32.946 172.17.0.2:54321      9      main      INFO: If you have trouble connecting, try SSH tunneling from your local machine (e.g., via port 55555):
08-11 08:46:32.946 172.17.0.2:54321      9      main      INFO:   1. Open a terminal and run 'ssh -L 55555:localhost:54321 
[email protected]
'
08-11 08:46:32.946 172.17.0.2:54321      9      main      INFO:   2. Point your browser to http://localhost:55555
08-11 08:46:32.955 172.17.0.2:54321      9      main      INFO: Log dir: '/tmp/h2o-root/h2ologs'
08-11 08:46:32.957 172.17.0.2:54321      9      main      INFO: Cur dir: '/'
08-11 08:46:33.072 172.17.0.2:54321      9      main      INFO: HDFS subsystem successfully initialized
08-11 08:46:33.085 172.17.0.2:54321      9      main      INFO: S3 subsystem successfully initialized
08-11 08:46:33.088 172.17.0.2:54321      9      main      INFO: Flow dir: '/root/h2oflows'
08-11 08:46:33.142 172.17.0.2:54321      9      main      INFO: Cloud of size 1 formed [/172.17.0.2:54321]
08-11 08:46:33.166 172.17.0.2:54321      9      main      INFO: Registered parsers: [GUESS, ARFF, XLS, SVMLight, AVRO, PARQUET, CSV]
08-11 08:46:33.167 172.17.0.2:54321      9      main      INFO: Watchdog extension initialized
08-11 08:46:33.168 172.17.0.2:54321      9      main      INFO: XGBoost extension initialized
08-11 08:46:33.171 172.17.0.2:54321      9      main      INFO: Registered 2 core extensions in: 341ms
08-11 08:46:33.172 172.17.0.2:54321      9      main      INFO: Registered H2O core extensions: [Watchdog, XGBoost]
08-11 08:46:33.184 172.17.0.2:54321      9      main      INFO: Found XGBoost backend with library: xgboost4j
08-11 08:46:33.702 172.17.0.2:54321      9      main      INFO: Registered: 160 REST APIs in: 529ms
08-11 08:46:33.711 172.17.0.2:54321      9      main      INFO: Registered REST API extensions: [XGBoost, Algos, AutoML, Core V3, Core V4]
08-11 08:46:34.073 172.17.0.2:54321      9      main      INFO: Registered: 230 schemas in 354ms
08-11 08:46:34.073 172.17.0.2:54321      9      main      INFO: H2O started in 5343ms
08-11 08:46:34.073 172.17.0.2:54321      9      main      INFO: 
08-11 08:46:34.075 172.17.0.2:54321      9      main      INFO: Open H2O Flow in your web browser: http://172.17.0.2:54321



What is the right Python code or missing configuration on my Mac / in the Docker container to connect to this H2O Deep Water Docker container?


Seems to be a security / Docker config issue???


h2o.connect(ip=""127.0.0.1"", port=54321)
Connecting to H2O server at http://127.0.0.1:54321...Traceback (most recent call last):

  File ""<ipython-input-12-e78edc3b0d61>"", line 1, in <module>
    h2o.connect(ip=""127.0.0.1"", port=54321)

  File ""/Users/kai.waehner/anaconda/lib/python2.7/site-packages/h2o/h2o.py"", line 74, in connect
    cluster_id=cluster_id, cookies=cookies, verbose=verbose)

  File ""/Users/kai.waehner/anaconda/lib/python2.7/site-packages/h2o/backend/connection.py"", line 175, in open
    conn._cluster = conn._test_connection(retries, messages=_msgs)

  File ""/Users/kai.waehner/anaconda/lib/python2.7/site-packages/h2o/backend/connection.py"", line 414, in _test_connection
    cld = self.request(""GET /3/Cloud"")

  File ""/Users/kai.waehner/anaconda/lib/python2.7/site-packages/h2o/backend/connection.py"", line 259, in request
    return self._process_response(resp, save_to)

  File ""/Users/kai.waehner/anaconda/lib/python2.7/site-packages/h2o/backend/connection.py"", line 574, in _process_response
    data = response.json(object_pairs_hook=H2OResponse)

  File ""/Users/kai.waehner/anaconda/lib/python2.7/site-packages/requests/models.py"", line 877, in json
    self.content.decode(encoding), **kwargs

  File ""/Users/kai.waehner/anaconda/lib/python2.7/json/__init__.py"", line 352, in loads
    return cls(encoding=encoding, **kw).decode(s)

  File ""/Users/kai.waehner/anaconda/lib/python2.7/json/decoder.py"", line 364, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())

  File ""/Users/kai.waehner/anaconda/lib/python2.7/json/decoder.py"", line 380, in raw_decode
    obj, end = self.scan_once(s, idx)

  File ""/Users/kai.waehner/anaconda/lib/python2.7/site-packages/h2o/backend/connection.py"", line 691, in __new__
    if schema == ""CloudV3"": return H2OCluster.from_kvs(keyvals)

  File ""/Users/kai.waehner/anaconda/lib/python2.7/site-packages/h2o/backend/cluster.py"", line 47, in from_kvs
    raise AttributeError(""Attribute %s cannot be set on H2OCluster (= %r)"" % (k, v))

AttributeError: Attribute internal_security_enabled cannot be set on H2OCluster (= False)



Thanks for help.","['python', 'deep-learning', 'h2o']",Unknown,,N/A
45626764,45626764,2017-08-11T04:01:25,2017-08-11 19:37:05Z,447,"Using h2o, I have used a .csv data frame that includes a column of dates, some of which are NULL, to train a model. Looking at the .hex dataframe that was output by h2o Flow UI after parsing the input .csv file, the null values are represented by 
.
s and the remaining dates are represented as timestamp doubles (ie. milliseconds since epoch time).


When trying to use the model's MOJO file in a java program to make predictions, on a dataset, I am getting the error


Exception in thread ""main"" java.lang.NullPointerException
        at hex.genmodel.easy.EasyPredictModelWrapper.fillRawData(EasyPredictModelWrapper.java:554)
        at hex.genmodel.easy.EasyPredictModelWrapper.predict(EasyPredictModelWrapper.java:615)
        at hex.genmodel.easy.EasyPredictModelWrapper.preamble(EasyPredictModelWrapper.java:489)
        at hex.genmodel.easy.EasyPredictModelWrapper.predictBinomial(EasyPredictModelWrapper.java:303)
        at SimpleCsvPredictor.predictCsv(SimpleCsvPredictor.java:287)
        at SimpleCsvPredictor.main(SimpleCsvPredictor.java:210)



since I am handling NULL values in the dataset's date column by setting them t null in the 
RowData
 object that h2o's model EasyPredictionModelWrapper can make predictions on.


The problem is that, for this column, the model is expecting a Double value. But there is no Double value to pass in because the value is null. Note that I cannot just set these null values to 0.0 because of how the model is trained (since not all the dates are null, so setting some to zero would be misrepresenting the particular sample the the model). So how can I fix this or what can I put in the place of a null where a Double is expected?


Thanks for the advice :)","['java', 'h2o']",lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
45623506,45623506,2017-08-10T21:10:28,2017-08-10 21:10:28Z,529,"I am trying to run h2o for python (version 2.7.13) and am getting this error when trying to use 
h2o.init()
 (or 
h2o.h2o.init()
).


Checking whether there is an H2O instance running at http://localhost:54321. connected.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-37-a1412a2642a8> in <module>()
      2 
      3 # http://h2o-release.s3.amazonaws.com/h2o/master/3135/docs-website/h2o-py/docs/h2o.html#h2o.h2o.init
----> 4 h2o.h2o.init(start_h2o=True)

/cygdrive/c/Users/rvillanueva/python-virtualenvs-base/h2o-model-retraining/lib/python2.7/site-packages/h2o/h2o.pyc in init(url, ip, port, https, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, **kwargs)
    256         hs = H2OLocalServer.start(nthreads=nthreads, enable_assertions=enable_assertions, max_mem_size=mmax,
    257                                   min_mem_size=mmin, ice_root=ice_root, port=port)
--> 258         h2oconn = H2OConnection.open(server=hs, https=https, verify_ssl_certificates=not insecure,
    259                                      auth=auth, proxy=proxy,cookies=cookies, verbose=True)
    260     if check_version:

/cygdrive/c/Users/rvillanueva/python-virtualenvs-base/h2o-model-retraining/lib/python2.7/site-packages/h2o/h2o.pyc in version_check()
    108     return h2oconn
    109 
--> 110 
    111 def version_check():
    112     """"""Used to verify that h2o-python module and the H2O server are compatible with each other.""""""

/cygdrive/c/Users/rvillanueva/python-virtualenvs-base/h2o-model-retraining/lib/python2.7/site-packages/h2o/__init__.py in <module>()
      9 from __future__ import absolute_import, division, print_function, unicode_literals
     10 
---> 11 from h2o.h2o import (connect, init, api, connection,
     12                      lazy_import, upload_file, import_file, import_sql_table, import_sql_select,
     13                      parse_setup, parse_raw, assign, deep_copy, get_model, get_grid, get_frame,

ImportError: cannot import name flow



I am doing this in jupyter notebook, but I get this same error when I run


>>import h2o
>>h2o.init()



from the console. Using 
h2o.h2o.H2OConnection.jar_paths()
 shows that my h2o.jar file is in the path and I have made sure that the h2o.jar version and python h2o package version are the same (and latest 3.10.5.4)), so I am not sure what could be going wrong here. 


Running 
h2o.cluster().show_status()
 after trying to init() h2o gives the output:


H2O cluster uptime: 16 mins 56 secs
H2O cluster version:    3.10.5.4
H2O cluster version age:    23 days
H2O cluster name:   ******
H2O cluster total nodes:    2
H2O cluster free memory:    7.028 Gb
H2O cluster total cores:    16
H2O cluster allowed cores:  16
H2O cluster status: accepting new members, 1 nodes are not healthy
H2O connection url: http://localhost:54321
H2O connection proxy:   None
H2O internal security:  False
Python version: 2.7.13 final



Which makes me think that the init() worked (at least partially), but the error makes me wonder what problems are going to sneak up down the road. 


Help about what this means and how to resolve it would be appreciated.


Thanks :)","['python', 'h2o']",lampShadesDrifter,https://stackoverflow.com/users/8236733/lampshadesdrifter,"4,139"
45565029,45565029,2017-08-08T09:49:13,2017-08-09 18:58:10Z,77,"How does H2O handle multi-attribute categorical features (i.e. columns with comma-separated values like ('1,2,3', '1,4', '1,2')? Do I need to split those in multiple columns manually (one-hot encoding)?


More specifically assume the data being a tab-separated file as follows:


col1    col2
1    1,2,3
2    1,4
1    1,2",['h2o'],Unknown,,N/A
45547698,45547698,2017-08-07T13:08:04,2018-08-10 23:47:30Z,0,"I'm using H2O's 
Random Forest Regression
 model in python. While evaluating its performance in terms of speed, I've compared it to scikit-learn's 
RandomForestRegressor
.

The dataset consists of ~20,000 rows and 20 columns of data.

My machine runs windows with python 3.6 on it.



import time
import h2o
import pandas as pd
from h2o.estimators.random_forest import H2ORandomForestEstimator
from sklearn.ensemble import RandomForestRegressor

def timing(f):
    def wrap(*args):
        time1 = time.time()
        ret = f(*args)
        time2 = time.time()
        print('%s function took %0.3f ms' % (f.__name__ , (time2-time1)*1000.0))
        return ret
    return wrap


@timing
def predict_row(model, row):
    return model.predict(row)


# read data
h2o.init(max_mem_size = ""8G"")
h2o.remove_all() 
h2o_df = h2o.import_file(csv_path)
train, valid, test = h2o_df.split_frame([0.6, 0.2], seed=1234)
X = h2o_df.col_names[:-1]
y = h2o_df.col_names[-1]  


# random forest at H2O
rf_h2o = H2ORandomForestEstimator(model_id=""rf_h2o_v1"", ntrees=100, stopping_rounds=2, score_each_iteration=True, seed=1000000,
                             col_sample_rate_per_tree=0.3)
rf_h2o.train(X, y, training_frame=train, validation_frame=valid)


# random forest at sklearn
train_df = train[:-1].as_data_frame(use_pandas=True)
y_df = h2o_df[:,-1].as_data_frame(use_pandas=True)
y_df = y_df[y_df.index.isin(train_df.index)]
rf_sklearn = RandomForestRegressor(n_estimators=100, min_samples_leaf=3, oob_score=True, max_features=0.25)
rf_sklearn.fit(train_df, y_df)


# prediction comparison, let's take row #5 from the test set for example
row_h20 = test[5,:-1]
row_array = test[5,:-1].as_data_frame(use_pandas=True).values
rf_sklearn.predict(row_array)

# run 20 times and test performance on all iterations
for i in range(20):
    predict_row(rf_sklearn, row_array)
    predict_row(rf_h2o, row_h20)



When I ran this code, I get that the Random Forest by scikit-learn in 
X4-5 times faster
 than H2O's model (6-7 milliseconds vs. ~30 milliseconds on the fastest iterations in both cases).

 
My questions are:
 




is that really the case? Can anyone verify that I'm not doing anything wrong? 

 I know H2O is built upon JVM, which is not supposed to be lightning fast, but I thought they would do at least as good as scikit-learn.


I need a super fast implementation of a predict in a regression algorithm. Does anyone have any suggestions? 
 scikit-learn / H2O are not mandatory for me, but I need something reliable.","['python', 'performance', 'python-3.x', 'machine-learning', 'h2o']",TomP,https://stackoverflow.com/users/7141941/tomp,21
45538140,45538140,2017-08-07T01:10:49,2020-06-19 06:09:25Z,0,"data1.dl.r2 = vector()
for (i in 1:100) {
if (i==1) {
  data1.hex = as.h2o(data1)
} else {
  data1.hex = nextdata 
}
  data1.dl = h2o.deeplearning    (x=2:1000,y=1,training_frame=data1.hex,nfolds=5,activation=""Tanh"",hidden=30,seed=5,reproducible=TRUE)
  data1.dl.pred = h2o.predict(data1.dl,data1.hex)
  data1.dl.r2[i] = sum((as.matrix(data1.dl.pred)-mean(as.matrix(data1.hex[,1])))^2)/
sum((as.matrix(data1.hex[,1])-mean(as.matrix(data1.hex[,1])))^2) # R-squared

  prevdata = as.matrix(data1.hex)
  nextpred = as.matrix(h2o.predict(data1.dl,as.h2o(data0[i,])))
  colnames(nextpred) = ""response""
  nextdata = as.h2o(rbind(prevdata,cbind(nextpred,data0[i,-1])))

  print(i)
}



This is my code with a dataset (data1) of 100 observations and 1000 features.
When I run this, it gave me an error message at 50~60th iteration
""


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Total input file size of 87.5 KB is much larger than total cluster memory of Zero  , please use either a larger cluster or smaller data.



When I run 'h20.init()', it tells me that the total cluster memory is zero.


    H2O cluster total nodes:    1 
    H2O cluster total memory:   0.00 GB 
    H2O cluster total cores:    8 
    H2O cluster allowed cores:  8 
    H2O cluster healthy:        TRUE 



So I wonder why cluster total memory is zero,
and why it didn't go wrong at earlier iterations.","['memory', 'memory-management', 'deep-learning', 'cluster-computing', 'h2o']",Unknown,,N/A
45523997,45523997,2017-08-05T16:15:56,2017-08-07 19:30:45Z,0,"I have trained and stored a random forest binary classification model. Now I'm trying to simulate processing new (out-of-sample) data with this model. My Python (Anaconda 3.6) code is:


import h2o
import pandas as pd
import sys

localH2O = h2o.init(ip = ""localhost"", port = 54321, max_mem_size = ""8G"", nthreads = -1)
h2o.remove_all()

model_path = ""C:/sm/BottleRockets/rf_model/DRF_model_python_1501621766843_28117"";
model = h2o.load_model(model_path)

new_data = h2o.import_file(path=""C:/sm/BottleRockets/new_data.csv"")
print(new_data.head(10))

predict = model.predict(new_data)  # predict returns a data frame
print(predict.describe())
predicted = predict[0,0]
probability = predict[0,2]  # probability the prediction is a ""1""

print('prediction: ', predicted, ', probability: ', probability)



When I run this code I get:


>>> import h2o
>>> import pandas as pd
>>> import sys
>>> localH2O = h2o.init(ip = ""localhost"", port = 54321, max_mem_size = ""8G"", nthreads = -1)
Checking whether there is an H2O instance running at http://localhost:54321. connected.
--------------------------  ------------------------------
H2O cluster uptime:         22 hours 22 mins
H2O cluster version:        3.10.5.4
H2O cluster version age:    18 days
H2O cluster name:           H2O_from_python_Charles_0fqq0c
H2O cluster total nodes:    1
H2O cluster free memory:    6.790 Gb
H2O cluster total cores:    8
H2O cluster allowed cores:  8
H2O cluster status:         locked, healthy
H2O connection url:         http://localhost:54321
H2O connection proxy:
H2O internal security:      False
Python version:             3.6.1 final
--------------------------  ------------------------------
>>> h2o.remove_all()
>>> model_path = ""C:/sm/BottleRockets/rf_model/DRF_model_python_1501621766843_28117"";
>>> model = h2o.load_model(model_path)
>>> new_data = h2o.import_file(path=""C:/sm/BottleRockets/new_data.csv"")

Parse progress: |█████████████████████████████████████████████████████████| 100%
>>> print(new_data.head(10))
  BoxRatio    Thrust    Velocity    OnBalRun    vwapGain
----------  --------  ----------  ----------  ----------
     1.502    55.044        0.38          37       0.845

[1 row x 5 columns]

>>> predict = model.predict(new_data)  # predict returns a data frame

drf prediction progress: |████████████████████████████████████████████████| 100%
>>> print(predict.describe())
Rows:1
Cols:3


         predict    p0                  p1
-------  ---------  ------------------  -------------------
type     enum       real                real
mins                0.8849431818181818  0.11505681818181818
mean                0.8849431818181818  0.11505681818181818
maxs                0.8849431818181818  0.11505681818181818
sigma               0.0                 0.0
zeros               0                   0
missing  0          0                   0
0        1          0.8849431818181818  0.11505681818181818
None
>>> predicted = predict[0,0]
>>> probability = predict[0,2]  # probability the prediction is a ""1""
>>> print('prediction: ', predicted, ', probability: ', probability)
prediction:  1 , probability:  0.11505681818181818
>>> 



I am confused by the contents of the ""predict"" data frame.  Please tell me what the numbers in the columns labeled ""p0"" and ""p1"" mean.  I hope they are probabilities, and as you can see by my code, I am trying to get the predicted classification (0 or 1) and a probability that this classification is correct.  Does my code correctly do that?


Any comments will be greatly appreciated.
Charles","['python-3.x', 'h2o']",CBrauer,https://stackoverflow.com/users/334757/cbrauer,"1,079"
45516147,45516147,2017-08-04T22:09:05,2017-08-08 22:17:18Z,0,"How do I see the coefficients or importance of the various base models in an stacked ensemble in h2o?  For example if I have a GBM, GLM, and RF how do I know how important each one is in the stacking?  Is this possible?


For example using the python code... here....


http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html","['python', 'h2o']",runningbirds,https://stackoverflow.com/users/3788557/runningbirds,"6,565"
45512346,45512346,2017-08-04T17:19:12,2017-08-07 21:47:25Z,200,Does anyone know if it is possible to add the time remaining when you use  model.train() in a Jupiter notebook?  The progress bar is nice but it would be even more helpful if this updated with the estimated time remaining.,"['python', 'jupyter-notebook', 'h2o']",jack,https://stackoverflow.com/users/8157418/jack,102
45510759,45510759,2017-08-04T15:39:14,2017-08-04 16:41:35Z,0,"I used install package to install h2o. While I could do h2o.init(), the h2o.autoML function isn't found:


could not find function ""h2o.automl""



After some searching I installed the 'nightly bleeding edge' 
version
 in tar.gz. but after install that even h2o.init() no longer works and shows this error:


Error: package or namespace load failed for ‘h2o’ in get(method, envir = home):
 lazy-load database '/Library/Frameworks/R.framework/Versions/3.4/Resources/library/h2o/R/h2o.rdb' is corrupt
In addition: Warning messages:
1: In .registerS3method(fin[i, 1], fin[i, 2], fin[i, 3], fin[i, 4],  :
restarting interrupted promise evaluation
2: In get(method, envir = home) :
restarting interrupted promise evaluation
3: In get(method, envir = home) : internal error -3 in R_decompress1","['r', 'installation', 'h2o']",santoku,https://stackoverflow.com/users/3983454/santoku,"3,417"
45493208,45493208,2017-08-03T19:40:10,2017-08-03 21:29:33Z,0,"I have an error executing the following code R:


hsc = h2o.init(ip=""127.0.0.1"",port=54321,nthreads=-1,max_mem_size=""8G"")

model_tf <- h2o.deepwater(
  x = col_start:col_end,
  y = col_class,
  backend = ""tensorflow"",
  training_frame = train)



Error from console h2o:




A fatal error has been detected by the Java Runtime Environment:


SIGILL (0x4) at pc=0x00007f49f117892d, pid=4616, tid=0x00007f4a7d88a700


JRE version: Java(TM) SE Runtime Environment (8.0_144-b01) (build 1.8.0_144-b01)
Java VM: Java HotSpot(TM) 64-Bit Server VM (25.144-b01 mixed mode linux-amd64 compressed oops)


Problematic frame:
C  [libtensorflow_jni.so00358a4a-1301-4222-a4f6-273b7a1baf4c+0x211992d]","['r', 'tensorflow', 'gpu', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
45487655,45487655,2017-08-03T14:48:46,2018-08-06 04:42:19Z,769,"any one else unable to load the H2OAutoML library in python2.7 and/or 3.6? 


I've tried these (alternatives) as per the example on 
http://h2o-release.s3.amazonaws.com/h2o/master/3888/docs-website/h2o-docs/automl.html
, however neither imports successfully load the AutoML Library? That is, I've tried from h2o.AutoML import H2OAutoML (or from h2o.automl import H2OAutoML). 


Any help is appreciated.","['python-2.7', 'h2o', 'automl']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
45486923,45486923,2017-08-03T14:19:38,2017-08-10 10:19:36Z,819,"I am trying to maximize precision in a binary classification problem (there is a high cost to false positives). The data set is really unbalanced as well. Would it make sense to run a DRF or XGBOOST model twice, using the weights column the second time in order to counter-act false positives? 


Are there other methods within these H2O algorithms to maximize precision (rather than log-loss) besides this potential method? I am also going to use an ensemble (which does seems to increase precision). Cross-validation does not appear to help.","['random-forest', 'h2o', 'xgboost']",David Comfort,https://stackoverflow.com/users/6287730/david-comfort,153
45485680,45485680,2017-08-03T13:29:12,2017-08-10 10:29:23Z,876,"I am currently running 
h2o
's DRF algorithm an a 3-node EC2 cluster (the h2o server spans across all 3 nodes).
My data set has 1m rows and 41 columns (40 predictors and 1 response).


I use the 
R
 bindings to control the cluster and the RF call is as follows


model=h2o.randomForest(x=x,
                       y=y,
                       ignore_const_cols=TRUE,
                       training_frame=train_data,
                       seed=1234,
                       mtries=7,
                       ntrees=2000,
                       max_depth=15,
                       min_rows=50,
                       stopping_rounds=3,
                       stopping_metric=""MSE"",
                       stopping_tolerance=2e-5)



For the 3-node cluster (c4.8xlarge, enhanced networking turned on), this takes about 240sec; the CPU utilization is between 10-20%; RAM utilization is between 20-30%; network transfer is between 10-50MByte/sec (in and out). 300 trees are built until early stopping kicks in.


On a 
single-node
 cluster, I can get the same results in about 80sec. So, instead of an expected 3-fold speed up, I get a 3-fold slow down for the 3-node cluster.


I did some research and found a few resources that were reporting the same issue (not as extreme as mine though). See, for instance:

https://groups.google.com/forum/#!topic/h2ostream/bnyhPyxftX8


Specifically, the author of 
http://datascience.la/benchmarking-random-forest-implementations/
 notes that




While not the focus of this study, there are signs that running the
  distributed random forests implementations (e.g. H2O) on multiple
  nodes does not provide the speed benefit one would hope for (because
  of the high cost of shipping the histograms at each split over the
  network).




Also 
https://www.slideshare.net/0xdata/rf-brighttalk
 points at 2 different DRF implementations, where one has a larger network overhead.


I think that I am running into the same problems as described in the links above.
How can I improve 
h2o
's DRF performance on a multi-node cluster?
Are there any settings that might improve runtime?
Any help highly appreciated!","['parallel-processing', 'random-forest', 'h2o']",cryo111,https://stackoverflow.com/users/983028/cryo111,"4,474"
45483359,45483359,2017-08-03T11:50:37,2017-08-07 06:07:49Z,217,"I started the Deep Water Docker container (CPU mode) on my Mac as described in the docs (
https://github.com/h2oai/deepwater/blob/master/README.md
):


docker run -it --rm -p 54321:54321 -p 8080:8080 -v $PWD:/host opsh2oai/h2o-deepwater-cpu


It starts correctly and without errors, but I cannot access the H2O UI at 
http://172.17.0.2:54321
 ...


There is also a hint in the logs:
If you have trouble connecting, try SSH tunneling from your local machine 1. Open a terminal and run 'ssh -L 55555:localhost:54321 
[email protected]
'
2. Point your browser to 
http://localhost:55555


But this is also not working... 


I use Docker CE Version 17.06.0-ce-mac19. 


Any ideas what to do?


Here are the complete logs of starting H2O:","['docker', 'deep-learning', 'h2o']",Unknown,,N/A
45476088,45476088,2017-08-03T06:06:27,2017-08-03 18:06:08Z,0,"I have installed 
h2o_3.13.0.tar.gz
 on 
R 3.4.1
 on a 
ppc64le
 box with 4 Tesla P100s running 
Ubuntu 16.04.2 LTS
.  It has Cuda v8.0.61 (I am not sure how to check for cuDNN).


When I run the example in the deep-water booklet, in this step


model <- h2o.deepwater(x=features, y=target, ... nfolds=5, gpu=TRUE, seed=1234)



I get the error


java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: /tmp/libmxnet.so: /tmp/libmxnet.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)



While the file is there, when I run ldd on it, I get 
not a dynamic executable
.


Does this mean that I need to compile mxnet for this architecture or am I missing something else?  Then I set 
gpu=FALSE
 in the same function.  I now get the following error


java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: Could not initialize class deepwater.backends.mxnet.MXNetBackend$MXNetLoader



Any ideas?","['r', 'h2o']",ironv,https://stackoverflow.com/users/2789334/ironv,"1,058"
45470674,45470674,2017-08-02T20:44:39,2017-08-10 09:57:59Z,284,"I have an svmlight-formatted file with values of the form:


92.91 18256731:1 71729421:1 72329637:1 83328561:1 118265976:1 134892759:1 198163358:1 352348616:1 526943048:1
5.30 102156934:1 134892759:1 198163358:1 254112843:1 262373758:1 512748316:1 526943048:1
22.00 32172600:1 72329637:1 118265976:1 134892759:1 198163358:1 411824213:1 443226486:1 445371412:1 526943048:1


I am trying to import this in h2o using 
h2o.import_file(fname.svmlight)


Does h2o support high dimensional sparse binary features?


Do I need to convert the hashed values in some indexes for this to work?","['python', 'h2o', 'svmlight']",Unknown,,N/A
45464691,45464691,2017-08-02T15:13:08,2017-08-02 16:46:32Z,103,"We have created a word2vec model over Python H2O API on version 3.13.0.3975.
When we issued a h2o.get_model on the previously created word2vec model Python API
gives an error.




File ""C:\Python27\lib\site-packages\h2o\h2o.py"", line 714, in get_model
  raise ValueError(""Unknown algo type: "" + algo)
  ValueError: Unknown algo type: word2vec




As we look at the h2o.py, there is no algo definition about word2vec.","['python', 'h2o']",XentneX,https://stackoverflow.com/users/1928229/xentnex,117
45463738,45463738,2017-08-02T14:32:44,2017-08-03 07:05:20Z,0,"Error in h2o.ensemble(x = x, y = y, training_frame = train, family = family, : family = gamma requires a positive respone
Traceback:




h2o.ensemble(x = x, y = y, training_frame = train, family = family, 
.     learner = learner, metalearner = metalearner, cvControl = list(V = 5, 
.         shuffle = TRUE))


stop(""family = gamma requires a positive respone"")




reponse ""y"" is with both negative and positive values.`


code
:


## Load required packages
library(h2o)
library(h2oEnsemble)

h2o.init(nthreads = -1, max_mem_size = ""8G"")

data <- h2o.importFile('./input/df_train.csv')

# Partition the data into train and test sets
splits <- h2o.splitFrame(data, seed = 1)
train <- splits[[1]]
test <- splits[[2]]

# Identify response and predictor variables
y <- ""logerror""
x <- setdiff(colnames(data), c(y, ""parcelid"", ""transactiondate""))
print(x)


# Specify the base learner library & the metalearner
learner <- c(""h2o.glm.wrapper"", ""h2o.randomForest.wrapper"", 
            ""h2o.xgboost.wrapper"",
            ""h2o.gbm.wrapper"", ""h2o.deeplearning.wrapper"")
metalearner <- ""h2o.glm.wrapper""
family <- ""gaussian""

# Train the ensemble using 5-fold CV to generate level-one data

fit <- h2o.ensemble(x = x, y = y,
                training_frame = train,
                family = family,
                learner = learner,
                metalearner = metalearner,
                cvControl = list(V = 5, shuffle = TRUE))

# Evaluate performance on a test set
perf <- h2o.ensemble_performance(fit, newdata = test)
perf","['r', 'gaussian', 'h2o', 'ensemble-learning']",Unknown,,N/A
45450284,45450284,2017-08-02T01:59:03,2017-08-02 16:14:44Z,0,"DNN was performed using the 
h2o.deeplearning
 function. 


Finally, I used the 
h2o.predict
 function to perform test data prediction. 


But when I try to display the actual values ​​and the predicted values ​​in a visual way, I got an error. Here is my code:


library(""h2o"")
h2o.init(nthreads = -1, max_mem_size = ""5G"")

credit<-read.csv(""http://freakonometrics.free.fr/german_credit.csv"", header=TRUE)
F=c(1,2,4,5,7,8,9,10,11,12,13,15,16,17,18,19,20,21)
for(i in F) credit[,i]=as.factor(credit[,i])
str(credit)

library(caret)
set.seed(1000)
intrain<-createDataPartition(y=credit$Creditability, p=0.7, list=FALSE) 
train<-credit[intrain, ]
test<-credit[-intrain, ]


deep_train<-as.h2o(train,destination_frame = ""deep_train"")
deep_test<-as.h2o(test,destination_frame = ""deep_test"")


h2o.str(deep_train)
h2o.str(deep_test)

x<-names(train[,-1])
y<-""Creditability""

deep_model<-h2o.deeplearning(x=x, y=y,
                             training_frame = deep_train,
                             activation = ""RectifierWithDropout"",
                             hidden=c(30,40,50),
                             epochs = 10,
                             input_dropout_ratio = 0.2,
                             hidden_dropout_ratios = c(0.5,0.5,0.5),
                             l1=1e-5 ,l2= 0,
                             rho = 0.99, epsilon = 1e-08,
                             loss = ""CrossEntropy"",
                             variable_importances = TRUE)



pred<-h2o.predict(deep_model, newdata=deep_test)

confusionMatrix(pred$predict, test$Creditability)
Error in unique.default(x, nmax = nmax) : 
  invalid type/length (environment/0) in vector allocation



How to visualize the predict table??","['r', 'neural-network', 'deep-learning', 'h2o']",신익수,https://stackoverflow.com/users/8058467/%ec%8b%a0%ec%9d%b5%ec%88%98,77
45448483,45448483,2017-08-01T22:13:06,2017-08-02 03:22:14Z,0,"I am trying to predict the taxi out times with a h2o deeplearning model:


  deep<-h2o.deeplearning(
        training_frame = train,
        validation_frame = valid,
        x=predictors,
        y=target,
        #distribution = ""gaussian"",
        #loss = ""Automatic"",
        #hidden=c(30,30),
        epochs = 50,
        #activation=""Rectifier"",
        stopping_metric=""deviance"",
        stopping_tolerance=1e-5,      # stops when deviance does 
                                not improve by >=0.0001 for 5 scoring events
        stopping_rounds=5

        )



This is how the input variables look like, TAXI_OUT is the target and it is in minutes, of course always >0:


   DAY_OF_WEEK CARRIER ORIGIN DEST TAXI_OUT congestion sin_deptime cos_deptime dep_blk_sin dep_blk_cos Temp Dew_point
18           1      DL    ATL  PHL       32         53 -0.80644460   0.5913096  -0.3246995   0.9458172   11        12
24           1      DL    ATL  EWR       23         75 -0.40673664   0.9135455   0.8371665   0.5469482   11        12
25           1      DL    ATL  EWR       24         55  0.68199836  -0.7313537   0.4759474  -0.8794738   11        12
30           1      DL    ATL  FLL       35         52 -0.04361939  -0.9990482  -0.7357239  -0.6772816   11        12
32           1      DL    ATL  PBI       30         68 -0.78260816  -0.6225146  -0.9694003   0.2454855   11        12
36           1      DL    ATL  DTW       13         50 -0.68835458   0.7253744   0.6142127   0.7891405   11        12
   Humidity Sea_Level_Press Visibility Wind Event_1 Event_2      Event_3
18       99            1019          2   11     Fog    Rain Thunderstorm
24       99            1019          2   11     Fog    Rain Thunderstorm
25       99            1019          2   11     Fog    Rain Thunderstorm
30       99            1019          2   11     Fog    Rain Thunderstorm
32       99            1019          2   11     Fog    Rain Thunderstorm
36       99            1019          2   11     Fog    Rain Thunderstorm



Do I need to rescale the numeric input variables in a range, say, [0, 1] or [-1, 1] or do I let h2o deal with them?","['r', 'h2o']",user8400607,https://stackoverflow.com/users/8400607/user8400607,55
45446315,45446315,2017-08-01T19:36:35,2017-08-01 21:50:12Z,654,"I exited R and found h2o.jar remains in the background. Confirmed by checking H2OFLOW in localhost:54321, where I found the data frames from my last session. Is this expected? How to completely shutdown the java server?","['java', 'h2o']",horaceT,https://stackoverflow.com/users/2434201/horacet,651
45442608,45442608,2017-08-01T16:02:21,2017-08-01 16:19:28Z,569,"I am trying to predict taxi-out times at US airports with a h2o deeplearning model:


#Deep learning neural network

  deep<-h2o.deeplearning(
    training_frame = train,
    validation_frame = valid,
    x=predictors,
    y=target,
    #distribution = ""gaussian"",
    #loss = ""Automatic"",
    hidden=c(200,200,200),
    epochs = 50,
    #activation=""Rectifier"",
    stopping_metric=""deviance"",
    stopping_tolerance=1e-4,      # stops when deviance does not improve by 
                                     >=0.0001 for 5 scoring events
  )

  summary(deep)



This is the truncated variable importance list:


Variable Importances: 


         variable relative_importance scaled_importance percentage
1     Event_1.Fog            1.000000          1.000000   0.024205
2    Event_2.Rain            0.983211          0.983211   0.023799
3      CARRIER.NK            0.946493          0.946493   0.022910
4 Event_1.noevent            0.936131          0.936131   0.022659
5     cos_deptime            0.934558          0.934558   0.022621



I understand that the ""importance"" is calculated as the relative impact of the variable but how do I know if that variable contributes to increase or decrease the taxi-out times? Does h2o shows the coefficient of each variable with a sign?
I have read this doc 
http://h2o-release.s3.amazonaws.com/h2o/latest_stable_doc.html
 but it doesn't explain whether, say, the variable fog or rain increases or decreases taxi-out times and how much.","['h2o', 'coefficients']",user8400607,https://stackoverflow.com/users/8400607/user8400607,55
45435739,45435739,2017-08-01T10:47:03,2017-08-10 09:23:17Z,0,"Similar to this question in R 
here
, I get out of memory issues when running loops with grid search in H2O. In R, doing gc() during each loop did help. What is the proposed solution here?","['python', 'memory-management', 'h2o']",user90772,https://stackoverflow.com/users/2912888/user90772,377
45427595,45427595,2017-08-01T01:28:18,2017-08-01 01:52:17Z,0,"I want to perform a DNN using the 
h2o.deeplearning
 function. To do this, we loaded the data using the 
as.h2o
 function.


I entered the following code to verify that the data was entered correctly, and there were only ten observations. The data has a total of 1,000 observations. 


However, when I loaded the data as using 
as.h2o
 function, only ten data were entered. Which part is wrong?


This is my R code.


h2o.init(nthreads = -1, max_mem_size = ""5G"")
credit<-read.csv(""http://freakonometrics.free.fr/german_credit.csv"", header=TRUE)
deep_credit<-as.h2o(credit,destination_frame = ""deep_credit"")

h2o.str(deep_credit)
Class 'H2OFrame' <environment: 0x0000000035bb4ad8> 
 - attr(*, ""op"")= chr ""Parse""
 - attr(*, ""id"")= chr ""deep_credit""
 - attr(*, ""eval"")= logi FALSE
 - attr(*, ""nrow"")= int 1000
 - attr(*, ""ncol"")= int 21
 - attr(*, ""types"")=List of 21
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
  ..$ : chr ""int""
 - attr(*, ""data"")='data.frame':    10 obs. of  21 variables:
  ..$ Creditability                    : num  1 1 1 1 1 1 1 1 1 1
  ..$ Account.Balance                  : num  1 1 2 1 1 1 1 1 4 2
  ..$ Duration.of.Credit..month.       : num  18 9 12 12 12 10 8 6 18 24
  ..$ Payment.Status.of.Previous.Credit: num  4 4 2 4 4 4 4 4 4 2
  ..$ Purpose                          : num  2 0 9 0 0 0 0 0 3 3
  ..$ Credit.Amount                    : num  1049 2799 841 2122 2171 ...
  ..$ Value.Savings.Stocks             : num  1 1 2 1 1 1 1 1 1 3
  ..$ Length.of.current.employment     : num  2 3 4 3 3 2 4 2 1 1
  ..$ Instalment.per.cent              : num  4 2 2 3 4 1 1 2 4 1
  ..$ Sex...Marital.Status             : num  2 3 2 3 3 3 3 3 2 2
  ..$ Guarantors                       : num  1 1 1 1 1 1 1 1 1 1
  ..$ Duration.in.Current.address      : num  4 2 4 2 4 3 4 4 4 4
  ..$ Most.valuable.available.asset    : num  2 1 1 1 2 1 1 1 3 4
  ..$ Age..years.                      : num  21 36 23 39 38 48 39 40 65 23
  ..$ Concurrent.Credits               : num  3 3 3 3 1 3 3 3 3 3
  ..$ Type.of.apartment                : num  1 1 1 1 2 1 2 2 2 1
  ..$ No.of.Credits.at.this.Bank       : num  1 2 1 2 2 2 2 1 2 1
  ..$ Occupation                       : num  3 3 2 2 2 2 2 2 1 1
  ..$ No.of.dependents                 : num  1 2 1 2 1 2 1 2 1 1
  ..$ Telephone                        : num  1 1 1 1 1 1 1 1 1 1
  ..$ Foreign.Worker                   : num  1 1 1 2 2 2 2 2 1 1","['r', 'deep-learning', 'h2o']",신익수,https://stackoverflow.com/users/8058467/%ec%8b%a0%ec%9d%b5%ec%88%98,77
45426642,45426642,2017-07-31T23:14:22,2017-08-11 17:54:08Z,0,"I'm interested in creating interaction terms in h2o.glm().  But I do not want to generate all pairwise interactions.  For example, in the mtcars dataset...I want to interact 'mpg' with all the other factors such as 'cyl','hp', and 'disp' but I don't want the other factors to interact with each other (so I don't want disp_hp or disp_cyl).  


How should I best approach this problem using the (interactions = interactions_list) parameter in h2o.glm() ?


Thank you","['r', 'glm', 'h2o', 'one-hot-encoding']",Raag Agrawal,https://stackoverflow.com/users/7782613/raag-agrawal,146
45404116,45404116,2017-07-30T19:57:50,2017-07-31 16:12:02Z,536,"My Random Forest model code concludes with:


print('\nModel performance:')
performance = best_nn.model_performance(test_data = test)
accuracy  = performance.accuracy()
precision = performance.precision()
F1        = performance.F1()
auc       = performance.auc()
print('  accuracy.................', accuracy)
print('  precision................', precision)
print('  F1.......................', F1)
print('  auc......................', auc)



and this code produces the following output:


Model performance:
  accuracy................. [[0.6622929108639558, 0.9078947368421053]]
  precision................ [[0.6622929108639558, 1.0]]
  F1....................... [[0.304835115538703, 0.5853658536585366]]
  auc...................... 0.9103448275862068



Why am I getting two numbers for accuracy, precision and F1, and what do they mean?


Charles


PS: My environment is:


H2O cluster uptime:         6 mins 02 secs
H2O cluster version:        3.10.4.8
H2O cluster version age:    2 months and 9 days
H2O cluster name:           H2O_from_python_Charles_wdmhb7
H2O cluster total nodes:    1
H2O cluster free memory:    21.31 Gb
H2O cluster total cores:    8
H2O cluster allowed cores:  4
H2O cluster status:         locked, healthy
H2O connection url:         http://localhost:54321
H2O connection proxy:
H2O internal security:      False
Python version:             3.6.2 final","['python-3.x', 'h2o']",Unknown,,N/A
45360398,45360398,2017-07-27T20:25:19,2017-07-30 07:53:17Z,0,"At first I thought it was a random issue, but re-running the script it happens again. 


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  : 
Unexpected CURL error: Recv failure: Connection reset by peer



I'm doing a grid search on a medium-size dataset (roughly 40000 x 30) with a Gradient Boosting Machine model. The largest tree in the grid is 1000. This usually happens after running for a couple of hours. I set 
max_mem_size
 to 30Gb. 


for ( k in 1:nrow(par.grid)) {
    hg = h2o.gbm(training_frame = Xtr.hf, 
                 validation_frame = Xt.hf,
                 distribution=""huber"",
                 huber_alpha = HuberAlpha,
                 x=2:ncol(Xtr.hf),        
                 y=1,                     
                 ntrees = par.grid[k,""ntree""],
                 max_depth = depth,
                 learn_rate = par.grid[k,""shrink""],
                 min_rows = par.grid[k,""min_leaf""],
                 sample_rate = samp_rate,
                 col_sample_rate = c_samp_rate,
                 nfolds = 5,
                 model_id = p(iname, ""_gbm_CV"")
                 )
    cv_result[k,1] = h2o.mse(hg, train=TRUE)
    cv_result[k,2] = h2o.mse(hg, valid=TRUE)
  }","['r', 'h2o']",horaceT,https://stackoverflow.com/users/2434201/horacet,651
45358671,45358671,2017-07-27T18:39:45,2017-11-09 13:17:05Z,0,"I'm trying to set up a H2O cloud on a 4 data nodes hadoop spark cluster using R in a Zeppelin notebook. I found that I have to give each executor at least 20Gb of memory before my R paragraph stops complaining about running out of memory (java error message of GC out of memory).


Is it expected that I need 20Gb of memory per executor for running an H2O cloud? Or are there any configuration entries that I can change to reduce the memory requirement?","['r', 'h2o']",C8H10N4O2,https://stackoverflow.com/users/2573061/c8h10n4o2,18.9k
45357217,45357217,2017-07-27T17:18:44,2017-07-27 18:05:17Z,0,"I am trying to write a simple ifelse function in h2o R.


my_cl$Seg<-ifelse((my_cl$predict==0), '1', ifelse(my_cl$predict==1), '2','0'))



Single ifelse works with no issues.
But when I try to to even write a code with just two ifelse it gives me an error:


Error: unexpected ')' in ""my_cl$Seg<-ifelse((my_cl$predict==0), '1', ifelse(my_cl$predict==1), '2','0'))""



In regular R it would be correct.
If I remove a ) 


Error in ifelse((my_cl$predict == 0), ""1"", ifelse(my_cl$predict == 1),  : unused arguments (""2"", ""0"")



Is it because in h2o the multiple ifelse are not supported? 
I tried to replace 
ifelse
 with 
h2o.ifelse
   - same errors.
Thank you","['r', 'if-statement', 'h2o']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
45340158,45340158,2017-07-27T02:06:12,2018-05-16 01:44:40Z,657,"I dont know how to create h2o multi node cluster?
I tried to use flatfile but It still dose not work.


Please check the log file:


[root@ptdl-167 home]# java -Xmx10G -ea -jar h2o.jar -name VIPCLUS -flatfile flatfile.txt -ip 10.61.74. 202 -port 54231
07-27 09:03:42.476 10.61.74.202:54231    13206  main      INFO: ----- H2O started  -----
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Build git branch: rel-vapnik
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Build git hash: 47dceae5c504ed6a2fad5de29292509d0b8024cc
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Build git describe: jenkins-master-3905-6-g47dceae
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Build project version: 3.12.0.1 (latest version: unknown)
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Build age: 1 month and 20 days
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Built by: 'jenkins'
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Built on: '2017-06-06 23:26:13'
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Watchdog Build git branch: (unknown)
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Watchdog Build git hash: (unknown)
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Watchdog Build git describe: (unknown)
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Watchdog Build project version: (unknown)
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Watchdog Built by: (unknown)
07-27 09:03:42.490 10.61.74.202:54231    13206  main      INFO: Watchdog Built on: (unknown)
07-27 09:03:42.491 10.61.74.202:54231    13206  main      INFO: Processed H2O arguments: [-name, VIPCLUS, -flatfile, flatfile.txt, -ip, 10.61.74.202, -port, 54231]
07-27 09:03:42.491 10.61.74.202:54231    13206  main      INFO: Java availableProcessors: 8
07-27 09:03:42.491 10.61.74.202:54231    13206  main      INFO: Java heap totalMemory: 238.0 MB
07-27 09:03:42.491 10.61.74.202:54231    13206  main      INFO: Java heap maxMemory: 8.89 GB
07-27 09:03:42.491 10.61.74.202:54231    13206  main      INFO: Java version: Java 1.8.0_121 (from Oracle Corporation)
07-27 09:03:42.491 10.61.74.202:54231    13206  main      INFO: JVM launch parameters: [-Xmx10G, -ea]
07-27 09:03:42.491 10.61.74.202:54231    13206  main      INFO: OS version: Linux 3.10.0-514.el7.x86_64 (amd64)
07-27 09:03:42.491 10.61.74.202:54231    13206  main      INFO: Machine physical memory: 15.44 GB
07-27 09:03:42.491 10.61.74.202:54231    13206  main      INFO: X-h2o-cluster-id: 1501121020922
07-27 09:03:42.491 10.61.74.202:54231    13206  main      INFO: User name: 'root'
07-27 09:03:42.491 10.61.74.202:54231    13206  main      INFO: IPv6 stack selected: false
07-27 09:03:42.492 10.61.74.202:54231    13206  main      INFO: Possible IP Address: enp0s31f6 (enp0s31f6), fe80:0:0:0:f71d:d7d1:c1e9:a164%enp0s31f6
07-27 09:03:42.492 10.61.74.202:54231    13206  main      INFO: Possible IP Address: enp0s31f6 (enp0s31f6), 10.61.74.202
07-27 09:03:42.492 10.61.74.202:54231    13206  main      INFO: Possible IP Address: lo (lo), 0:0:0:0:0:0:0:1%lo
07-27 09:03:42.492 10.61.74.202:54231    13206  main      INFO: Possible IP Address: lo (lo), 127.0.0.1
07-27 09:03:42.492 10.61.74.202:54231    13206  main      INFO: H2O node running in unencrypted mode.
07-27 09:03:42.493 10.61.74.202:54231    13206  main      INFO: Internal communication uses port: 54232
07-27 09:03:42.493 10.61.74.202:54231    13206  main      INFO: Listening for HTTP and REST traffic on 
http://10.61.74
. 202 :54231/
07-27 09:03:42.494 10.61.74. 202:54231    13206  main      WARN: Flatfile configuration does not include self: /10.61.74. 202:54231 but contains [/10.60.74.201:54231, /10.60.74.203:54231, /10.60.74.202:54231]
07-27 09:03:42.494 10.61.74.202:54231    13206  main      INFO: H2O cloud name: 'VIPCLUS' on /10.61.74. 202:54231, static configuration based on -flatfile flatfile.txt
07-27 09:03:42.494 10.61.74.202:54231    13206  main      INFO: If you have trouble connecting, try SSH tunneling from your local machine (e.g., via port 55555):
07-27 09:03:42.494 10.61.74.202:54231    13206  main      INFO:   1. Open a terminal and run 'ssh -L 55555:localhost:54231 
[email protected]
'
07-27 09:03:42.494 10.61.74.202:54231    13206  main      INFO:   2. Point your browser to http:// localhost :55555
07-27 09:03:42.495 10.61.74.202:54231    13206  main      INFO: Log dir: '/tmp/h2o-root/h2ologs'
07-27 09:03:42.495 10.61.74.202:54231    13206  main      INFO: Cur dir: '/home'
07-27 09:03:42.504 10.61.74.202:54231    13206  main      INFO: HDFS subsystem successfully initialized
07-27 09:03:42.506 10.61.74.202:54231    13206  main      INFO: S3 subsystem successfully initialized
07-27 09:03:42.506 10.61.74.202:54231    13206  main      INFO: Flow dir: '/root/h2oflows'
07-27 09:03:42.513 10.61.74.202:54231    13206  main      INFO: Cloud of size 1 formed [ptdl-167.viettel.com.vn/10.61.74.202:54231]
07-27 09:03:42.519 10.61.74.202:54231    13206  main      INFO: Registered parsers: [GUESS, ARFF, XLS, SVMLight, AVRO, PARQUET, CSV]
07-27 09:03:42.519 10.61.74.202:54231    13206  main      INFO: Watchdog extension initialized
07-27 09:03:42.519 10.61.74.202:54231    13206  main      INFO: Registered 1 core extensions in: 1ms
07-27 09:03:42.519 10.61.74.202:54231    13206  main      INFO: Registered H2O core extensions: [Watchdog]
07-27 09:03:42.659 10.61.74.202:54231    13206  main      INFO: Registered: 160 REST APIs in: 140ms
07-27 09:03:42.660 10.61.74.202:54231    13206  main      INFO: Registered REST API extensions: [XGBoost, water.api.RegisterResourceRoots, Core V3, Core V4, Algos, AutoML]
07-27 09:03:42.730 10.61.74.202:54231    13206  main      INFO: Registered: 230 schemas in 70ms
07-27 09:03:42.731 10.61.74.202:54231    13206  main      INFO: H2O started in 1806ms
07-27 09:03:42.731 10.61.74.202:54231    13206  main      INFO: 
07-27 09:03:42.731 10.61.74.202:54231    13206  main      INFO: Open H2O Flow in your web browser: 
http://10.61.74.202:54231

07-27 09:03:42.731 10.61.74.202:54231    13206  main      INFO: 
07-27 09:04:30.291 10.61.74.202:54231    13206  #02:54231 ERRR: Got IO error when sending batch UDP bytes: java.net.NoRouteToHostException: No route to host
07-27 09:04:30.298 10.61.74.202:54231    13206  #01:54231 ERRR: Got IO error when sending batch UDP bytes: java.net.NoRouteToHostException: No route to host
07-27 09:04:30.298 10.61.74.202:54231    13206  #03:54231 ERRR: Got IO error when sending batch UDP bytes: java.net.NoRouteToHostException: No route to host","['cluster-computing', 'h2o']",Tuong Nguyen,https://stackoverflow.com/users/7700946/tuong-nguyen,143
45335697,45335697,2017-07-26T19:19:13,2021-01-07 10:37:40Z,0,"I'm catching up on 
h2o
's MOJO and POJO model format. I'm able to save a model in MOJO/POJO with 


h2o.download_mojo(model, path = ""/media/somewhere/tmp"") # ok
h2o.download_pojo(model, path = ""/media/somewhere/tmp"") # ok



which writes an object with name like 
mymodel.zip
 or 
mymodel.java
 to the directory.


However, it's not clear to me how to read it back into the server in R. I tried,


saved_model2 <- h2o.loadModel(""/media/somewhere/tmp/mymodel.java"") # not work
saved_model3 <- h2o.loadModel(""/media/somewhere/tmp/mymodel.zip"") # not work



but got error msg like this,


ERROR: Unexpected HTTP Status code: 400 Bad Request (url = http://localhost:54321/99/Models.bin/)

java.lang.IllegalArgumentException
 [1] ""java.lang.IllegalArgumentException: Missing magic number 0x1CED at stream start""  
....
Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 

ERROR MESSAGE:

Missing magic number 0x1CED at stream start","['r', 'io', 'h2o']",horaceT,https://stackoverflow.com/users/2434201/horacet,651
45333883,45333883,2017-07-26T17:35:19,2017-07-28 01:46:35Z,971,"I've been working with H2O for the last year, and I am getting very tired of server crashes. I have given up on ""nightly releases"", as they are easily crashed by my data sets. Please tell me where I can download a release that is stable.


Charles


My environment is:




Windows 10 enterprise, build 1607, with 64 GB memory.   


Java SE Development Kit 8 Update 77 (64-bit).


Anaconda Python 3.6.2-0.




I started the server with:


localH2O = h2o.init(ip = ""localhost"",
                    port = 54321,
                    max_mem_size=""12G"",
                    nthreads = 4)



The h2o init information is:


H2O cluster uptime:         12 hours 12 mins
H2O cluster version:        3.10.5.2
H2O cluster version age:    1 month and 6 days
H2O cluster name:           H2O_from_python_Charles_ji1ndk
H2O cluster total nodes:    1
H2O cluster free memory:    6.994 Gb
H2O cluster total cores:    8
H2O cluster allowed cores:  4
H2O cluster status:         locked, healthy
H2O connection url:         http://localhost:54321
H2O connection proxy:
H2O internal security:      False
Python version:             3.6.2 final



The crash information is:


OSError: Job with key $03017f00000132d4ffffffff$_a0ce9b2c855ea5cff1aa58d65c2a4e7c failed with an exception: java.lang.AssertionError: I am really confused about the heap usage; MEM_MAX=11453595648 heapUsedGC=11482667352
stacktrace: 
java.lang.AssertionError: I am really confused about the heap usage; MEM_MAX=11453595648 heapUsedGC=11482667352
    at water.MemoryManager.set_goals(MemoryManager.java:97)
    at water.MemoryManager.malloc(MemoryManager.java:265)
    at water.MemoryManager.malloc(MemoryManager.java:222)
    at water.MemoryManager.arrayCopyOfRange(MemoryManager.java:291)
    at water.AutoBuffer.expandByteBuffer(AutoBuffer.java:719)
    at water.AutoBuffer.putA4f(AutoBuffer.java:1355)
    at hex.deeplearning.Storage$DenseRowMatrix$Icer.write129(Storage$DenseRowMatrix$Icer.java)
    at hex.deeplearning.Storage$DenseRowMatrix$Icer.write(Storage$DenseRowMatrix$Icer.java)
    at water.Iced.write(Iced.java:61)
    at water.AutoBuffer.put(AutoBuffer.java:771)
    at water.AutoBuffer.putA(AutoBuffer.java:883)
    at hex.deeplearning.DeepLearningModelInfo$Icer.write128(DeepLearningModelInfo$Icer.java)
    at hex.deeplearning.DeepLearningModelInfo$Icer.write(DeepLearningModelInfo$Icer.java)
    at water.Iced.write(Iced.java:61)
    at water.AutoBuffer.put(AutoBuffer.java:771)
    at hex.deeplearning.DeepLearningModel$Icer.write105(DeepLearningModel$Icer.java)
    at hex.deeplearning.DeepLearningModel$Icer.write(DeepLearningModel$Icer.java)
    at water.Iced.write(Iced.java:61)
    at water.Iced.asBytes(Iced.java:42)
    at water.Value.<init>(Value.java:348)
    at water.TAtomic.atomic(TAtomic.java:22)
    at water.Atomic.compute2(Atomic.java:56)
    at water.Atomic.fork(Atomic.java:39)
    at water.Atomic.invoke(Atomic.java:31)
    at water.Lockable.unlock(Lockable.java:181)
    at water.Lockable.unlock(Lockable.java:176)
    at hex.deeplearning.DeepLearning$DeepLearningDriver.trainModel(DeepLearning.java:491)
    at hex.deeplearning.DeepLearning$DeepLearningDriver.buildModel(DeepLearning.java:311)
    at hex.deeplearning.DeepLearning$DeepLearningDriver.computeImpl(DeepLearning.java:216)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:173)
    at hex.deeplearning.DeepLearning$DeepLearningDriver.compute2(DeepLearning.java:209)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1349)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)","['python-3.x', 'h2o']",CBrauer,https://stackoverflow.com/users/334757/cbrauer,"1,079"
45330693,45330693,2017-07-26T14:59:57,2017-07-26 16:28:43Z,0,"I was using h2o.randomforest in R to build a classifier on 2 groups, says the group ""A"" & group ""B"". As an example, I generated a sample dataset randomly as shown below and converted it into a h2oframe:


    a <- sample(0:1,10000,replace=T)
    b <- sample(0:1,10000,replace=T)
    c <- sample(1:10,10000,replace=T)
    d <- sample(0:1,10000,replace=T)
    e <- sample(0:1,10000,replace=T)
    f <- sample(0:1,10000,replace=T)



Basically, they would be factorized and all had 2 levels, except c, which had 10 levels.The first 5000 rows were assigned label as ""A"" and the rest were assigned label ""B"". Also, I had another column called nlabel which was ""B"" for the first 5000 rows and ""A"" for the rest.


Here is the first 10 rows and the last 10 rows of my dataset:


          a b  c d e f label nlabel
    1     0 0  5 0 1 0     A      B
    2     0 1  5 1 1 1     A      B
    3     0 0  6 0 0 1     A      B
    4     0 0  8 0 0 1     A      B
    5     1 1  1 1 1 1     A      B
    6     1 1  6 1 0 1     A      B
    7     1 0  3 1 1 1     A      B
    8     1 1  9 1 0 1     A      B
    9     1 0  8 1 0 1     A      B
    10    0 0  1 0 1 1     A      B
    .............
    9991  1 1  3 0 0 1     B      A
    9992  0 0  7 1 0 0     B      A
    9993  1 0  9 0 1 1     B      A
    9994  0 1  3 0 0 0     B      A
    9995  1 1  8 0 1 0     B      A
    9996  0 1  8 0 1 0     B      A
    9997  1 1  9 0 1 0     B      A
    9998  0 0  5 1 0 1     B      A
    9999  0 1  9 1 1 0     B      A
    10000 0 1 10 1 0 1     B      A



Since I generate the dataset randomly, I didn't except I could get a good classifier at all (or I could be the luckiest guy in the world). I excepted something more like a random guess. Here is the one result I got by using ""randomForest"" package in R:


    > rf <- randomForest(label ~ a + b + c + e + f, 
    +                            data = test, 
                                 ntree = 100)
    > rf

        Call:
         randomForest(formula = label ~ a + b + c + e + f, data = test,      ntree = 100) 
                       Type of random forest: classification
                             Number of trees: 100
        No. of variables tried at each split: 2

                OOB estimate of  error rate: 50.17%
        Confusion matrix:
             A    B class.error
        A 2507 2493      0.4986
        B 2524 2476      0.5048



However, by using h2o.randomforest with the same dataset, I got a different result. Here are the code I used and the result I got:


        > TEST <- as.h2o(test)
        > rfh2o <- h2o.randomForest(y = ""label"",
                                  x = c(""a"",""b"",
                                        ""c"",""d"",
                                        ""e"",""f""),
                                  training_frame = TEST,
                                  ntrees = 100) 
    > rfh2o
    Model Details:
    ==============

    H2OBinomialModel: drf
    Model ID:  DRF_model_R_1501015614001_1029 
    Model Summary: 
      number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves
    1             100                      100              366582         7        14   11.33000          1
      max_leaves mean_leaves
    1        319   286.52000


    H2OBinomialMetrics: drf
    ** Reported on training data. **
    ** Metrics reported on Out-Of-Bag training samples **

    MSE:  0.2574374
    RMSE:  0.5073829
    LogLoss:  0.7086906
    Mean Per-Class Error:  0.5
    AUC:  0.4943865
    Gini:  -0.01122696

    Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
           A     B    Error         Rate
    A      0  5000 1.000000   =5000/5000
    B      0  5000 0.000000      =0/5000
    Totals 0 10000 0.500000  =5000/10000

    Maximum Metrics: Maximum metrics at their respective thresholds
                            metric threshold    value idx
    1                       max f1  0.231771 0.666667 399
    2                       max f2  0.231771 0.833333 399
    3                 max f0point5  0.231771 0.555556 399
    4                 max accuracy  0.459704 0.506800 251
    5                max precision  0.723654 0.593750  10
    6                   max recall  0.231771 1.000000 399
    7              max specificity  0.785389 0.999800   0
    8             max absolute_mcc  0.288276 0.051057 389
    9   max min_per_class_accuracy  0.500860 0.488000 200
    10 max mean_per_class_accuracy  0.459704 0.506800 251

Based on the result above, the confusion matrix is different from what I got from ""randomForest"" package. 



In addition, if I used the 
""nlabel""
 instead of 
""label""
 with h2o.randomforest, I still got a high error rate on predicting A's. But in the current model, the A's were the same as the B's in the last model. Here is the code and the result I got:


> rfh2o_n <- h2o.randomForest(y = ""nlabel"",
+                           x = c(""a"",""b"",
+                                 ""c"",""d"",
+                                 ""e"",""f""),
+                           training_frame = TEST,
+                           ntrees = 100)

> rfh2o_n
Model Details:
==============

H2OBinomialModel: drf
Model ID:  DRF_model_R_1501015614001_1113 
Model Summary: 
  number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves
1             100                      100              365232        11        14   11.18000          1
  max_leaves mean_leaves
1        319   285.42000


H2OBinomialMetrics: drf
** Reported on training data. **
** Metrics reported on Out-Of-Bag training samples **

MSE:  0.2575674
RMSE:  0.507511
LogLoss:  0.7089465
Mean Per-Class Error:  0.5
AUC:  0.4923496
Gini:  -0.01530088

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
       A     B    Error         Rate
A      0  5000 1.000000   =5000/5000
B      0  5000 0.000000      =0/5000
Totals 0 10000 0.500000  =5000/10000

Maximum Metrics: Maximum metrics at their respective thresholds
                        metric threshold    value idx
1                       max f1  0.214495 0.666667 399
2                       max f2  0.214495 0.833333 399
3                 max f0point5  0.214495 0.555556 399
4                 max accuracy  0.617230 0.506600  74
5                max precision  0.621806 0.541833  70
6                   max recall  0.214495 1.000000 399
7              max specificity  0.749866 0.999800   0
8             max absolute_mcc  0.733630 0.042465   6
9   max min_per_class_accuracy  0.499186 0.486400 201
10 max mean_per_class_accuracy  0.617230 0.506600  74



Such kinds of results made me wonder if label plays a role in h2o.randomforest. 
I don't use h2o often, but the results above really confused me. Is that just due to the probability, or I just made some silly mistakes, or something else?","['r', 'classification', 'random-forest', 'h2o']",Scarabee,https://stackoverflow.com/users/6656269/scarabee,"5,694"
45319194,45319194,2017-07-26T06:35:54,2017-07-27 22:53:44Z,0,"I tried to transform from R data to h2o data using 
as.h2o
 function.  


However, the number of observations ​​transformed by the 
as.h2o
 function is reduced to 10. 


There are a total of 1,000 observations in my data. There are 700 train data and 300 test data. However, using the 
as.h2o
 function only has 10 observations.


Following is my full and open source code. You can use it. please help me.


Why the data that transformed to h2o data has 10 observation?


install.packages(""h2o"")
library(h2o)
h2o.init(max_mem_size = ""10G"", nthreads = -1)

df<-read.csv(""http://freakonometrics.free.fr/german_credit.csv"", header=TRUE)
F=c(1,2,4,5,7,8,9,10,11,12,13,15,16,17,18,19,20,21)
for(i in F) df[,i]=as.factor(df[,i])

library(caret)

set.seed(1000)
intrain<-createDataPartition(y=df$Creditability, p=0.7, list=FALSE)
train<-df[intrain, ]
test<-df[-intrain, ]
str(train)
str(test)

h2o_train<-as.h2o(train, destination_frame = ""h2o_train"")
h2o_test<-as.h2o(test, destination_frame = ""h2o_test"")
str(h2o_train)
str(h2o_test)","['r', 'deep-learning', 'h2o']",이순우,https://stackoverflow.com/users/7408823/%ec%9d%b4%ec%88%9c%ec%9a%b0,89
45290052,45290052,2017-07-24T21:06:54,2019-10-23 06:00:39Z,0,"For trainining speed, it would be nice to be able to train a H2O model with GPUs, take the model file, and then predict on a machine without GPUs.


It seems like that should be possible in theory, but with the H2O release 3.13.0.341, that doesn't seem to happen, except for XGBoost model.


When I run 
gpustat -cup
 I can see the GPUs kick in when I train H2O's XGBoost model.  This doesn't happen with DL, DRF, GLM, or GBM.


I wouldn't be surprised if a difference in float point size (16, 32, 64) could cause some inconsistency, not to mention the vagaries due to multiprocessor modeling, but I think I could live with that.


(This is related to my question here, but now that I understand the environment better I can see that the GPUs aren't used all the time.)


How can I tell if H2O 3.11.0.266 is running with GPUs?","['gpu', 'cpu', 'h2o']",Clem Wang,https://stackoverflow.com/users/2263303/clem-wang,739
45258952,45258952,2017-07-22T20:34:35,2017-07-23 00:39:26Z,0,"Is p-value deprecated in elasticnet? Below reference shows the p-value in coefficients

http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html


But I don't see p-values when I execute below command


 head(elastic_net_best_model@model$coefficients_table)
 Coefficients: glm coefficients

              names  coefficients standardized_coefficients","['r', 'h2o']",Richard,https://stackoverflow.com/users/6718853/richard,"1,121"
45255460,45255460,2017-07-22T14:07:09,2019-03-13 22:14:41Z,0,"This is a totally newbie question to see if I am missing something key (like there is more to install?).


After installing H20 (python 2.7) on a 9 node Hadoop / Spark cluster
using pip install of the whl file (h2o-3.10.4.8-py2.py3-none-any.whl) (which says it installed correctly).....


I can import h2o successfully. 


But, when I run h2o.init() then I get:


""Checking whether there is an H20 instance running at 
http://localhost:54321
. connected.""


But then an error is thrown:


H2oServerError: HTTP 500 Server Error: u'Error: 500'


Should I be able to run H20 by simply pip installing that whl or is there more? The documentation seems outdated and there are lots of different versions found online. Anyone have any experience with this?","['python-2.7', 'h2o']",B_Miner,https://stackoverflow.com/users/569313/b-miner,"1,820"
45219053,45219053,2017-07-20T15:25:19,2023-09-14 18:17:41Z,0,"I wanted to ask if anyone has come across a h2o machine learning python cheatsheet or comparison between h2o using python and scikit-learn


Would be very helpful since I am a scikit-learn guy.","['machine-learning', 'scikit-learn', 'deep-learning', 'artificial-intelligence', 'h2o']",Unknown,,N/A
45217753,45217753,2009-09-08T11:34:28,2023-12-08 17:50:27Z,0,"I get this error message as I execute my 
JUnit
 tests:


java.lang.OutOfMemoryError: GC overhead limit exceeded



I know what an 
OutOfMemoryError
 is, but what does GC overhead limit mean? How can I solve this?","['java', 'garbage-collection', 'out-of-memory', 'heap-memory']",İsmail Y.,https://stackoverflow.com/users/2039546/%c4%b0smail-y,"3,927"
45206076,45206076,2017-07-20T05:42:33,2017-08-14 14:32:21Z,0,"In 
h2o flow
, is there a way to ensure that my data frame splits have a controlled ratio of response classes.


For example, say I plan to train a binary classifier on a data frame X where 
0_class_ratio
% of the samples are in class 0 and 
1_class_ratio
% are in class 1. I want to split X into frame splits X_train and X_test by ratios 0.75 and 0.25, respectively. How would I be able to ensure that both X_train and X_test are comprised 
0_class_ratio
% of samples in category 0 and 
1_class_ratio
% of samples in category 1?


In python's scikit-learn package I would do something like:


from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=rng_seed_)
# go thru all (split and shuffled) indices of my_data dataframe stratified by response_class values
for train_index, test_index in split.split(my_data, my_data[""response_class""]):
    strat_train_set = my_data.loc[train_index]
    strat_test_set = my_data.loc[test_index]



I am aware of the 
h2o
 hyper-parameters 
sample_rate
 and 
sample_rate_per_class
, but I'm not fully sure how to use them in this situation.","['machine-learning', 'h2o']",Cœur,https://stackoverflow.com/users/1033581/c%c5%93ur,38.5k
45204330,45204330,2017-07-20T02:56:39,2018-06-30 01:46:08Z,69,"Is there a way to automate hyper-parameter optimization for models in 
H2O
's 
Flow UI
, such as how python's scikit-learn package includes 
GridGridSearchCV
 and 
RandomizedSearchCV
? Thanks",['h2o'],Unknown,,N/A
45201792,45201792,2017-07-19T22:02:28,2017-07-20 02:35:07Z,916,"I discovered that I can get a collection of EigenVectors from glrm_model (H2O Generalized Low Rank Model Estimateor glrm (Sorry I can't put this in the tags)) this way:


EV = glrm_model._model_json[""output""]['eigenvectors'])


However the type of EV is H2OTwoDimTable which is not very capable.


If I try to do (where M is an H2O Data Frame):


M.mult(EV)



I get the error


AttributeError: 'H2OTwoDimTable' object has no attribute 'nrows'



If I try to convert EV to a numpy matrix:


EV.as_matrix()



I get the error:


AttributeError: 'H2OTwoDimTable' object has no attribute 'as_matrix'



I can convert EV to a panda data frame and then convert it to a numpy matrix, which is an extra step and do the matrix multiplication


IMHO, it would be better if the eigenvector reference return an H2O Data Frame.


Also, it would be good if H2OTwoDimTable could better support matrix multiplication either as a left or right operand.


And EV.as_data_frame() has no use_pandas=False option.


Here's the python code which could be modified to better support matrix type things:


https://github.com/h2oai/h2o-3/blob/master/h2o-py/h2o/two_dim_table.py","['numpy', 'matrix', 'matrix-multiplication', 'h2o']",Unknown,,N/A
45194421,45194421,2017-07-19T15:03:26,2017-07-22 20:00:58Z,652,"It looks like H2OGeneralizedLowRankEstimator (GLRM) is exactly what I want.  I used to transform my training set data.  My idea was then to build a classifier based on the new features (archetypes)


What's not clear (after looking at the documentation and examples), is how one would apply the trained GLRM model to new data to get a new data frame to pass onto the classifier model trained with archetype data.  Trying to use GLRM predict in an obvious way doesn't seem to work.


My hunch is this must be possible.  Based on the Linear Algebra equation (where A is the original data, X is the matrix of new archetypes and Y is the transforming matrix):


A=XY


So that


A*inverse(Y) = X


[Sorry I don't have enough points to add 
GLRM
 as a Tag]


glrm = h2o.estimators.glrm.H2OGeneralizedLowRankEstimator(
    k = 100,  # Reduce to top 100 features
                                           transform = ""STANDARDIZE"",
                                           loss = ""Quadratic"",
                                           regularization_x = ""Quadratic"",
                                           regularization_y = ""L1"",
                                           gamma_x = 0.25,
                                           gamma_y = 0.5,
                                           max_iterations = 100


)

glrm.train(
    training_frame = df
)

# Try to transform the original data with the trained GLRM
pg = glrm.predict(df)



Results in this error:


EnvironmentError: Job with key $03017f00000132d4ffffffff$_96037b3ddc5e3e48d5273428d2fa43ee failed with an exception: java.lang.IllegalArgumentException: Can not make vectors of different length compatible!
stacktrace: 
java.lang.IllegalArgumentException: Can not make vectors of different length compatible!
at water.fvec.Frame.makeCompatible(Frame.java:1390)
at water.fvec.Frame.makeCompatible(Frame.java:1378)
at water.fvec.Frame.bulkAdd(Frame.java:591)
at water.fvec.Frame.add(Frame.java:576)
at water.fvec.Frame.add(Frame.java:630)
at hex.glrm.GLRMModel.reconstruct(GLRMModel.java:225)
at hex.glrm.GLRMModel.predictScoreImpl(GLRMModel.java:247)
at hex.Model.score(Model.java:1118)
at water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:352)
at water.H2O$H2OCountedCompleter.compute(H2O.java:1256)
at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)",['h2o'],Clem Wang,https://stackoverflow.com/users/2263303/clem-wang,739
45190787,45190787,2017-07-19T12:34:51,2017-07-19 14:25:33Z,0,"Is there a performance difference between the implementation of Random Forest in H2O and standard Random Forest library?


Has anybody performed or done some analysis for these two implementations.","['machine-learning', 'classification', 'random-forest', 'data-science', 'h2o']",chintan s,https://stackoverflow.com/users/1479974/chintan-s,"6,468"
45182882,45182882,2017-07-19T06:49:26,2017-07-19 14:22:58Z,0,"We have learned some models in H2O and want to export the models into PMML.
This is needed to feed the learned model into another running platform.


Has anyone a suggestion in how to do this? 


Thnx


Jan","['export', 'h2o', 'pmml']",Unknown,,N/A
45174782,45174782,2017-07-18T18:46:53,2017-08-19 18:02:01Z,150,"I want to study Deep Learning by using H2O Deep Water. I prefer using Docker images. 


So I followed the instructions @ 
H2O Deep Water installation
. I installed Docker, the nvidia driver, nvidia-docker and the H2O w/GPU Docker Image and in the H2O Deep Water container I executed the .jar app. I'm able to use the Flow from a web browser. No errors. 


The DIY instructions to build H2O Deep Water include a section to integrate either TensorFlow, MXnet or Caffe. I don't see this integration in the Docker installation section. Is this integration of a DL framework/software library not necessary for the Docker H2O Deep Water image?


I understand from the H2O slide presentations that with using TensorFlow, MXnet or Caffe the H2O DL models can have more hidden layers, using larger datasets with more features. 


If this high-end functionality can only be obtained via mentioned DL frameworks / software libraries: How can I integrate a Docker H2O Deep Water container with for example a Docker TensorFlow container to obtain the same high-end DL functionality and capacity? 
Or is this type of Docker integration of H2O Deep Water with a DL software library not possible and do I have to build H2O Deep Water and the DL software library manually as described in the DIY section?",['h2o'],Luc,https://stackoverflow.com/users/3208709/luc,223
45153751,45153751,2017-07-17T21:12:27,2017-07-18 00:59:18Z,590,"Does h2o-3 have node.js bindings? I found 
https://github.com/h2oai/h2o.js
 but when I saw no updates in two years I realized it was for h2o v2.


I'm specifically asking about deploying a POJO/MOJO jar file, in the context of an Electron app. I.e. offline, not using the REST API to communicate back to a server.  (Maybe my question is more: could I use the h2o.jar, and a pojo/mojo file, with something like 
https://github.com/joeferner/node-java
 and expect everything to work together, across each of Linux/Mac/Windows?)


FWIW, TensorFlow integration seems vapourware at the moment: 
https://github.com/node-tensorflow/node-tensorflow
, but MxNet seems to have something working: 
https://github.com/dmlc/mxnet.js/
   So, if H2O pojo/mojo can be used from within node.js apps, could Deep Water models also work?","['node.js', 'electron', 'h2o', 'node-java']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
45153176,45153176,2017-07-17T20:31:15,2022-09-11 00:35:02Z,0,"This is my situation.  I have over 400 features, many of which are probably useless and often zero.  I would like to be able to:




train an model with a subset of those features


query that model for the features actually used to build that model


build a H2OFrame containing just those features (I get a sparse list of non-zero values for each row I want to predict.)


pass this newly constructed frame to H2OModel.predict() to get a prediction




I am pretty sure what found is unsupported but works for now (v 3.13.0.341).  Is there a more robust/supported way of doing this?


model._model_json['output']['names']



The response variable appears to be the last item in this list.


In a similar vein, it would be nice to have a supported way of finding out which 
H2O version
 that the model was built under.  I cannot find the version number in the json.",['h2o'],Clem Wang,https://stackoverflow.com/users/2263303/clem-wang,739
45150001,45150001,2017-07-17T17:11:18,2017-07-18 21:26:49Z,516,"I am trying to make use of the max_runtime_seconds but either I am having a hard time understanding how exactly this should be work, or what I feel is more likely - there is some sort of bug.


I have been testing with random forests and it never seems to cut down on the runtime.


import h2o
h2o.init()
from h2o.estimators import H2ORandomForestEstimator

df=h2o.import_file('covtype.csv') #### https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/
for i in df.names:
    df[i]=df[i].asfactor()
df.types  ## just showing everything is categorical


train,test = df.split_frame(ratios=[0.75], seed = 2017)

response  = 'C55'
xvars  = train.drop([""C55""]).col_names


mymodel = H2ORandomForestEstimator(
nfolds = 10,
max_runtime_secs = 30,
    stopping_rounds = 5,
    ntrees = 500   
)

mymodel.train(
x = xvars,
y = response,
validation_frame = test,
training_frame = train)
## does not finish remotely close to <30 seconds
mymodel.actual_params()



Note that the max run time parameter doesn't seem to be saved and stays at 0.
I'm using the 'bleeding edge' version of h2o right now ~3.13 and python.","['python', 'h2o']",Unknown,,N/A
45147066,45147066,2017-07-17T14:36:37,2017-07-17 15:10:52Z,67,"My colleague and I installed slightly different versions of H2O.  It doesn't seem right because you can't even tell from the download page ( 
https://www.h2o.ai/download/
 ) which exact minor version you are downloading:


 pip install http://s3.amazonaws.com/h2o-deepwater/public/nightly/latest/h2o-3.13.0-py2.py3-none-any.whl



He got the error:


Server error java.lang.IllegalArgumentException:
  Error: Found version 3.13.0.337, but running version 3.13.0.341
  Request: POST /99/Models.bin/



I humbly suggest that 3.13.0.X be compatible with 3.13.0.Y","['version', 'h2o']",Clem Wang,https://stackoverflow.com/users/2263303/clem-wang,739
45137541,45137541,2017-07-17T06:42:43,2017-07-26 06:12:31Z,0,"I am planning to use H2O algorithms in my product.


The product is developed in python and uses majorly pandas, sklearn and H2O. 


Can I deploy my python code with H2O algorithms in python directly? Or do I need to use MOJO or POJO ?","['python', 'machine-learning', 'licensing', 'h2o']",Mike Doe,https://stackoverflow.com/users/997162/mike-doe,17.5k
45107028,45107028,2017-07-14T15:47:45,2017-07-14 16:58:23Z,0,"What is the maximum dataset size that I am allowed to use on h2o. 


Specifically can the dataset size be larger than the ram / diskspace on each node. 


I have nodes with around 25 gb disk space and 40 gb of ram, I want to use a dataset that around 70 gb. 


Thank you


Getting errors of: 


Exception in thread ""qtp1392425346-39505"" java.lang.OutOfMemoryError: GC overhead limit exceeded",['h2o'],Unknown,,N/A
45094045,45094045,2017-07-14T03:22:22,2018-06-30 01:45:34Z,237,"I have a distributed random forest POJO model using the default model setting except for:


ntrees
 = 150


max_depth
 = 50


min_rows
 = 5


Here are the full settings:




buildModel 'drf', {""model_id"":""drf-335270ee-8970-4855-b521-c4fb4ca184f5"",""training_frame"":""frame_0.750"",""validation_frame"":""frame_0.250"",""nfolds"":0,""response_column"":""DENIAL"",""ignored_columns"":[""tx_match_date""],""ignore_const_cols"":true,""ntrees"":""150"",""max_depth"":""50"",""min_rows"":""5"",""nbins"":20,""seed"":-1,""mtries"":-1,""sample_rate"":0.6320000290870667,""score_each_iteration"":true,""score_tree_interval"":0,""balance_classes"":false,""nbins_top_level"":1024,""nbins_cats"":1024,""r2_stopping"":1.7976931348623157e+308,""stopping_rounds"":0,""stopping_metric"":""AUTO"",""stopping_tolerance"":0.001,""max_runtime_secs"":0,""checkpoint"":"""",""col_sample_rate_per_tree"":1,""min_split_improvement"":0.00001,""histogram_type"":""AUTO"",""categorical_encoding"":""AUTO"",""build_tree_one_node"":false,""sample_rate_per_class"":[],""binomial_double_trees"":true,""col_sample_rate_change_per_level"":1,""calibrate_model"":false}




When I try to compile the pojo with:


$javac -cp ""h2o-genmodel.jar"" -J-Xmx2g -J-XX:MaxPermSize=128m drf_335270ee_8970_4855_b521_c4fb4ca184f5.java



I get the following error.




An exception has occurred in the compiler (1.8.0_131). Please file a bug against the Java compiler via the Java bug reporting page (
http://bugreport.java.com
) after checking the Bug Database (
http://bugs.java.com
) for duplicates. Include your program and the following diagnostic in your report. Thank you.
  java.lang.IllegalArgumentException
          at java.nio.ByteBuffer.allocate(ByteBuffer.java:334)
          at com.sun.tools.javac.util.BaseFileManager$ByteBufferCache.get(BaseFileManager.java:325)
          at com.sun.tools.javac.util.BaseFileManager.makeByteBuffer(BaseFileManager.java:294)
          at com.sun.tools.javac.file.RegularFileObject.getCharContent(RegularFileObject.java:114)
          at com.sun.tools.javac.file.RegularFileObject.getCharContent(RegularFileObject.java:53)
          at com.sun.tools.javac.main.JavaCompiler.readSource(JavaCompiler.java:602)
          at com.sun.tools.javac.main.JavaCompiler.parse(JavaCompiler.java:665)
          at com.sun.tools.javac.main.JavaCompiler.parseFiles(JavaCompiler.java:950)
          at com.sun.tools.javac.main.JavaCompiler.compile(JavaCompiler.java:857)
          at com.sun.tools.javac.main.Main.compile(Main.java:523)
          at com.sun.tools.javac.main.Main.compile(Main.java:381)
          at com.sun.tools.javac.main.Main.compile(Main.java:370)
          at com.sun.tools.javac.main.Main.compile(Main.java:361)
          at com.sun.tools.javac.Main.compile(Main.java:56)
          at com.sun.tools.javac.Main.main(Main.java:42)




I don't get this error when replacing the DRF model with a deep learning pojo that I have also downloaded from h2o's Flow UI, so I'm thinking it is likely related to the 
drf_335270ee_8970_4855_b521_c4fb4ca184f5.java
 file (note that the POJO was too big to preview in H2O's Flow UI). What could be going on here?
Thanks","['javac', 'h2o']",Unknown,,N/A
45093030,45093030,2017-07-14T01:00:07,2018-06-30 01:45:13Z,536,"For example, say I am trying to train a binary classifier that takes sample inputs of the form


x = {d=(type of desk), p1=(type of pen on desk), p2=(type of *another* pen on desk)}



Say I then train a model on the samples:


x1 = {wood, ballpoint, gel},      y1 = {0}

x2 = {wood, ballpoint, ink-well}, y2 = {1}.



and try to predict on the new sample: 
x3 = {wood, gel, ballpoint}
. The response that I am hoping for in this case is 
y3 = {0}
, since conceptually it should not matter (ie. I don't want it to matter) which pen is designated as p1 or p2.


When trying to run this model (in my case, using an h2o.ai generated model), I get the error that the category enum for 
p2
 is not valid (since the model has never seen 'ballpoint' in 
p2
's category during training) (in h2o: 
hex.genmodel.easy.exception.PredictUnknownCategoricalLevelException
)


My first idea was to generate permutations of the 'pens' features for each sample to train the model on. Is there a better way to handle this situation? Specifically, in h2o.ai Flow UI solution, since that is what I am using to build the model. Thanks.","['machine-learning', 'h2o']",Unknown,,N/A
45088729,45088729,2017-07-13T18:45:04,2017-07-14 08:49:04Z,294,"How does h2o implementation of deep learning differs from tensorflow or theano. 
This tutorial
 shows the tensorflow implementation in h2o.


And what is the use of tensorflow in h2o, when h2o itself can find minimum for the NN's gradient descent problem.",['h2o'],Chandra,https://stackoverflow.com/users/2289986/chandra,524
45087624,45087624,2017-07-13T17:41:40,2017-07-13 17:41:40Z,0,"I am trying to create a function where I need to pass h2o data frame and variable and then use in built H2o function to create partial dependence plots for the model. My function is below:


pdp<-function(data,var, bin){
data1 <<- data
var1 <<- var
data1<<-h2o.arrange(data1,var1)
h2o.partialPlot(object = gbm26, data = data1[1:floor(nrow(data1)*0.95),], 
cols = var1 , nbins = 20)
}
pdp(data = dev_final, var = ""trans_amt_AVG_0_6m"", bin = 20)



This function throws an error ""Error in checkMatch(by, names(x)) : Column 'var1' in by not found
"". is there another way to pass variable name to this function.","['r', 'h2o']",Abhi,https://stackoverflow.com/users/6767970/abhi,25
45087168,45087168,2017-07-13T17:15:09,2017-07-14 03:51:33Z,0,"I'm running out of memory when I try to fit a random forest model on my dataset (5888 bytes) using the 
rsparkling
 random forest function with the following:


 h2o.randomForest(x = x, 
                  y = y,
                  training_frame = trainDatasetTopTen_tbl,
                  nfolds = 5).



My configuration setting:


config <- spark_config()
config$spark.driver.cores <- 3 
config$spark.driver.memory <- ""3.4G"" 
config$spark.driver.extraJavaOptions <- ""append -XX:MaxPermSize= 3.8G""

sc <- spark_connect(master = 'local', config = config,
                version = '2.1.0')



The memory available in my machine is 4 GB.


H2O cluster info is:


R is connected to the H2O cluster: 
H2O cluster uptime:         30 minutes 376 milliseconds 
H2O cluster version:        3.10.5.2 
H2O cluster version age:    24 days  
H2O cluster name:           sparkling-water-mubarak_local-1499963226139 
H2O cluster total nodes:    1 
H2O cluster total memory:   0.7 GB 
H2O cluster total cores:    4 
H2O cluster allowed cores:  4 
H2O cluster healthy:        TRUE 
H2O Connection ip:          127.0.0.1 
H2O Connection port:        54321 
H2O Connection proxy:       NA 
H2O Internal Security:      FALSE 
R Version:                  R version 3.4.0 (2017-04-21) 





H2O started  log info for Java (under 
http://localhost:4040/sparkling-water/
):


thread INFO: Java heap totalMemory: 461.0 MB
Java heap maxMemory: 910.5 MB
thread INFO: Java version: Java 1.8.0_65 (from Oracle Corporation)
thread INFO: JVM launch parameters: [-Xmx1g]




Therefore my question is: 
how to increase JVM parameter from 1GB to 3 GB?


My devtools information is:


Session info --------------------------------------
setting  value                       
version  R version 3.4.0 (2017-04-21)
system   x86_64, darwin15.6.0        
ui       RStudio (1.0.143)           
language (EN)                        
collate  en_GB.UTF-8                 
tz       Europe/London               
date     2017-07-13`  

package      * version    date      
base            * 3.4.0      2017-04-21
caret          * 6.0-76     2017-04-18
datasets     * 3.4.0      2017-04-21
dplyr         * 0.7.1      2017-06-22
ggplot2       * 2.2.1      2016-12-30
graphics      * 3.4.0      2017-04-21
grDevices    * 3.4.0      2017-04-21
h2o            * 3.10.5.2   2017-07-01
lattice      * 0.20-35    2017-03-25
methods      * 3.4.0      2017-04-21
rsparkling   * 0.2.1      2017-06-30
sparklyr     * 0.5.6-9011 2017-07-05
stats        * 3.4.0      2017-04-21
utils        * 3.4.0      2017-04-21`



Thank you,
MJ","['r', 'apache-spark', 'h2o', 'sparklyr']",tsn,https://stackoverflow.com/users/783711/tsn,838
45085994,45085994,2017-07-13T16:08:13,2020-02-27 16:51:33Z,0,"I am trying to specify the column types going into my H2o.Frame. I have tried this several different ways and in every case. Unit tests are below. They all fail except the last two but those last two only work because I have changed 99.0 to 99.9. Why can't I tell it that 99.0 is still a float and not an int?


import unittest
from unittest import TestCase
import h2o

class TestInputtingTypes(TestCase):
    def setUp(self):
        h2o.init()

    def test_h2o_1(self):
        data =[(1,'one', 9),(9,'two',3), (8,'three', 99.0)]
        given_types = {'C1': 'int', 'C2': 'string', 'C3': 'real'}
        frame = h2o.H2OFrame(data, column_types=given_types)
        actual_types = frame.types

        self.assertDictEqual(given_types, actual_types)

    def test_h2o_2(self):
        data =[(1,'one', 9),(9,'two',3), (8,'three', 99.0)]
        given_types = {'C1': 'int', 'C2': 'string', 'C3': 'real'}
        names = ['C1', 'C2', 'C3']
        frame = h2o.H2OFrame(data, column_types=given_types, column_names=names)
        actual_types = frame.types

        self.assertDictEqual(given_types, actual_types)

    def test_h2o_3(self):
        data =[{'C1': 1, 'C2': 'one',   'C3': 9},
               {'C1': 9, 'C2': 'two',   'C3': 3},
               {'C1': 8, 'C2': 'three', 'C3': 99.0}]

        given_types = {'C1': 'int', 'C2': 'string', 'C3': 'real'}
        names = ['C1', 'C2', 'C3']
        frame = h2o.H2OFrame(data, column_types=given_types, column_names=names)
        actual_types = frame.types

        self.assertDictEqual(given_types, actual_types)

    def test_h2o_4(self):
        data =[{'C1': 1, 'C2': 'one',   'C3': 9},
               {'C1': 9, 'C2': 'two',   'C3': 3},
               {'C1': 8, 'C2': 'three', 'C3': 99.0}]

        given_types = {'C1': 'int', 'C2': 'string', 'C3': 'real'}
        given_types_input = {'C1': 'numeric', 'C2': 'string', 'C3': 'float'}
        names = ['C1', 'C2', 'C3']
        frame = h2o.H2OFrame(data, column_types=given_types_input, column_names=names)
        actual_types = frame.types

        self.assertDictEqual(given_types, actual_types)

    def test_h2o_5(self):
        data =[(1,'one', 9),(9,'two',3), (8,'three', 99.0)]
        given_types = ['int', 'string', 'real']
        names = ['C1', 'C2', 'C3']
        frame = h2o.H2OFrame(data, column_types=given_types, column_names=names)
        actual_types = frame.types

        self.assertDictEqual(given_types, actual_types)

    def test_h2o_6_this_one_passes_because_has_nonzero_decimals(self):
        data =[(1,'one', 9),(9,'two',3), (8,'three', 99.9)]
        given_types = {'C1': 'int', 'C2': 'string', 'C3': 'real'}
        given_types_input = ['int', 'string', 'real']
        names = ['C1', 'C2', 'C3']
        frame = h2o.H2OFrame(data, column_types=given_types_input, column_names=names)
        actual_types = frame.types

        self.assertDictEqual(given_types, actual_types)

    def test_h2o_7_this_one_passes_because_has_nonzero_decimals(self):
        data =[(1,'one', 9),(9,'two',3), (8,'three', 99.9)]
        given_types = {'C1': 'int', 'C2': 'string', 'C3': 'real'}
        names = ['C1', 'C2', 'C3']
        frame = h2o.H2OFrame(data)
        actual_types = frame.types

        self.assertDictEqual(given_types, actual_types)

if __name__ == ""__main__"" :
    unittest.main()","['python', 'h2o']",Dave31415,https://stackoverflow.com/users/1024495/dave31415,"2,936"
45082454,45082454,2017-07-13T13:42:48,2017-07-13 13:42:48Z,248,"I am trying to use a H2o model to score some data which is in a python process. The data is not available to me in a file but rather a stream. I want to do something like this in python.


scores = [model.predict(line) for line in data]



where data is some generic iterable; for example a generator. 


This isn't possible as written because the data is in python and first must be moved to the H2o JVM. I don't want to read the whole stream into memory and pass it as a H2oFrame. In fact, it could have infinite length. 


I could do 


scores = [model.predict(h2o.H2oFrame([line]) for line in data]



but this is very slow because every call to H2oFrame has to generate a new parser. The creation of the parser (I assume the creation, not the actual parsing but I don't know for sure) takes 25.2 ms which is far too slow to iterate over streams with millions of items. 


Is there any way to make this fast? For example, can you pass in the parser rather than have it create a new one each time?


I realize there are other ways to do this such as scoring with the POJO in Java code but I'd like to do it in Python. Also don't want any heavy-weight solutions like Spark streaming. I already tried creating a FIFO and reading from that as if it was a real file but that didn't work.","['python', 'h2o']",Dave31415,https://stackoverflow.com/users/1024495/dave31415,"2,936"
45061890,45061890,2017-07-12T15:26:25,2017-07-12 18:48:54Z,0,"I am working with stacked learners. According to the docs for 
H2OStackedEnsembleEstimator
 h2o's python implementation allows you to easily build ensemble models. However this is limited to building base classifiers with the same underlying training data. I have time based features whose minimum date varies depending on the data source. Each sample of data is a point in time. To take advantage of as much data as I can, I split the features up until two groups (depending on relevance and minimum date) and train two separate models. I would like to combine these models, but H2OStackedEnsembleEstimator requires the features to be the same. 


According to 
this post
 about R's stacked ensemble implementation there is an option to only perform the metalearning step which should require only the k-fold cross-validation predicitons for each base model and the true target value. 


In case it crosses anyone's mind...for my particular problem, I realize I am going to run into an issue with the metalearning step with this mismatch in minimum date, and I have ideas to circumvent this.","['python', 'r', 'h2o', 'ensemble-learning']",joceratops,https://stackoverflow.com/users/5360652/joceratops,417
45059748,45059748,2017-07-12T13:56:50,2017-09-17 13:07:43Z,0,"I want to implement 
LIME
 on a model created using h2o(deep learning) in R. For using the data in the model, I created h2oFrames and converted it back to dataframe before using it in LIME (lime function, because LIME's explain function can't recognize a h2oFrame). Here I am able to run the function


Next step is to use the explain function on test data to generate explanations. Here R throws an error for using a dataframe as well as a h2oFrame.


This is the error generated when using a dataframe:




Error in chk.H2OFrame(x) : must be an H2OFrame





This is the error generated when using a h2oframe:




Error in UseMethod(""permute_cases"") : 
  no applicable method for 'permute_cases' applied to an object of class ""H2OFrame""





if(!require(pacman))  install.packages(""pacman"")
pacman::p_load(h2o, lime, data.table, e1071)

data(iris)
h2o.init( nthreads = -1 )
h2o.no_progress()

# Split up the data set
iris <- as.h2o(iris)

split <- h2o.splitFrame( iris, c(0.6, 0.2), seed = 1234 )
iris_train <- h2o.assign( split[[1]], ""train"" ) # 60%
iris_valid <- h2o.assign( split[[2]], ""valid"" ) # 20%
iris_test  <- h2o.assign( split[[3]], ""test"" )  # 20%


output <- 'Species'
input <- setdiff(names(iris),output)


model_dl_1 <- h2o.deeplearning(
  model_id = ""dl_1"", 
  training_frame = iris_train, 
  validation_frame = iris_valid,
  x = input,
  y = output,
  hidden = c(32, 32, 32),
  epochs = 10, # hopefully converges earlier...
  score_validation_samples = 10000, 
  stopping_rounds = 5,
  stopping_tolerance = 0.01
)

pred1 <- h2o.predict(model_dl_1, iris_test)
list(dimension = dim(pred1), pred1$predict)

#convert to df from h2ofdataframe

train_org<-as.data.frame(iris_train) 
#converting train h2oframe to dataframe
sapply(train_org,class) #checking the class of train_org
test_df <- as.data.frame(iris_test) 
#converting test data h2oFrame to dataframe
test_sample <- test_df[1:1,] 

#works
#lime is used to get explain on the train data
explain <- lime(train_org, model_dl_1, bin_continuous = FALSE, n_bins = 
                  5, n_permutations = 1000)


# Explain new observation
explanation <- explain(test_sample, n_labels = 1, n_features = 1)
h2o.shutdown(prompt=F)



Can anyone please help me with finding a solution or a way to use the explain function of LIME with the appropriate dataFrame","['r', 'dataframe', 'h2o']",Hack-R,https://stackoverflow.com/users/3604745/hack-r,23.1k
45047445,45047445,2017-07-12T02:41:19,2020-07-18 03:32:13Z,0,"I am running H2O 3.10.5.3 (on  hadoop cdh5.8) on a remote server, connected to my local machine using R (3.4.1) with 'h2o.init(startH2O=F, port=54321)'.


The code below works fine:


Train.hex <- as.h2o(iris)
h2o.gbm(x=""Sepal.Length"", y=""Petal.Width"", training_frame = Train.hex) 



but xgboost does not work and generates the following error message:


h2o.xgboost(x=""Sepal.Length"", y=""Petal.Width"", training_frame = Train.hex) 


ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://localhost:54321/3/ModelBuilders/xgboost)

java.lang.ArrayIndexOutOfBoundsException
 [1] ""java.lang.ArrayIndexOutOfBoundsException: -1""                                                                       
 [2] ""    hex.ModelBuilder.make(ModelBuilder.java:117)""                                                                   
 [3] ""    water.api.ModelBuildersHandler.fetch(ModelBuildersHandler.java:35)""                                             
 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                    
 [5] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                                  
 [6] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                          
 [7] ""    java.lang.reflect.Method.invoke(Method.java:498)""                                                               
 [8] ""    water.api.Handler.handle(Handler.java:63)""                                                                      
 [9] ""    water.api.RequestServer.serve(RequestServer.java:448)""                                                          
[10] ""    water.api.RequestServer.doGeneric(RequestServer.java:297)""                                                      
[11] ""    water.api.RequestServer.doGet(RequestServer.java:221)""                                                          
[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:707)""                                                   
[13] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:820)""                                                   
[14] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                         
[15] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                                     
[16] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                             
[17] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                                      
[18] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                              
[19] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                  
[20] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                          
[21] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                
[22] ""    water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:183)""                                                      
[23] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                          
[24] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                
[25] ""    org.eclipse.jetty.server.Server.handle(Server.java:366)""                                                        
[26] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                 
[27] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                  
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)""                
[29] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)""
[30] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)""                                               
[31] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)""                                          
[32] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                         
[33] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                   
[34] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                               
[35] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                
[36] ""    java.lang.Thread.run(Thread.java:745)""                                                                          

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

-1



Can anyone help me to understand what is going on?


Many thanks in advance,
Kere","['xgboost', 'h2o']",K Klein,https://stackoverflow.com/users/8134049/k-klein,21
45044891,45044891,2017-07-11T21:37:28,2017-07-12 13:42:41Z,150,"In H2O's gbm() function, is there any way to specify a particular link function for the offset? For example, if the distribution is set to ""poisson"", then I'd like to set the link to ""log"". This is certainly possible in h2o.glm(), but I don't see such an option in h2o.gbm().","['machine-learning', 'h2o', 'gbm']",Unknown,,N/A
45029437,45029437,2017-07-11T08:37:21,2017-07-11 16:30:41Z,0,"Is it possible to set a seed for h2o models via mlr? I could only find how to do it in h2o directly, e.g.


gbm_w_seed_2 <- h2o.gbm(x = predictors, y = response, training_frame = train,
                        validation_frame = valid, col_sample_rate =.7 ,
                        seed = 1234)","['r', 'h2o', 'mlr']",tover,https://stackoverflow.com/users/4647519/tover,545
45025667,45025667,2017-07-11T04:51:26,2018-06-30 01:44:58Z,0,"Specifically, what is the difference in how 
H2O
 treats 
enum
 and 
string
 data types in contrast to '
int's
 and '
numerical
' types?


For example, say I have a binary classifier that takes input samples that have features 


x1=(1 of 10 possible favorite ice cream flavors (enum))

x2=(some random phrase (string))

x3=(some number (int))



What would be the difference in how the classifier treats these types during training?


When uploading data into 
h2o
 Flow 
UI
, I get the option to convert certain data types (like 
enum
) to 'numerical.' This makes me think that there is more than just string-to-number mapping going on when I just leave the '
enum
' as an '
enum
' (not converting to '
numerical
' type), but I can't find information on what that difference is.


Clarification would be appreciated, thanks.","['machine-learning', 'h2o']",Unknown,,N/A
45024544,45024544,2017-07-11T02:47:34,2017-07-11 03:35:02Z,81,"I am trying to build a new application on Nimbix so I can use the latest H2O releases (the H2O community versions on the Nimbix servers are outdated). 


I have tried building a new application using the instructions provided here: 
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/cloud-integration/nimbix.html


And using the Docker Repository: opsh2oai/h2oai_nae
and the Git Source URL: 
http://github.com/h2oai/h2o3-nae


System architecture is set to Intel x86.


I pull the application and logout and log back in.


I can start a Jupyter notebook. However, I cannot import H2O (No module named 'h2o')


Also, it is not clear what the differences between H2o3, H2o3 for POWER8, and H2oAI?


In addition, which version has the GPU-enabled algos (H2O with GPU-Enabled Machine Learning)?",['h2o'],David Comfort,https://stackoverflow.com/users/6287730/david-comfort,153
45021925,45021925,2017-07-10T21:29:54,2018-05-05 13:52:09Z,0,"Is there a way to combine multiple predictions from different models in mlr into a single average prediction so that it can be used to calculate performance measures etc.?


library(mlr)
data(iris)
iris2 <- iris
iris2$Species <- ifelse(iris$Species==""setosa"", ""ja"", ""nein"")
task = makeClassifTask(data = iris2, target = ""Species"")
lrn = makeLearner(""classif.h2o.deeplearning"", predict.type=""prob"")
model1 = train(lrn, task)
model2 = train(lrn, task)
pred1 = predict(model1, newdata=iris2)
pred2 = predict(model2, newdata=iris2)
performance(pred1, measures = auc)
g = generateThreshVsPerfData(pred1)
plotThreshVsPerf(g)



A workaround to show what I mean could be maybe


pred_avg = pred1
pred_avg$data[,c(""prob.ja"",""prob.nein"")] = (pred1$data[,c(""prob.ja"",""prob.nein"")] + 
                                              pred2$data[,c(""prob.ja"",""prob.nein"")])/2
performance(pred_avg, measures = auc)
g_avg = generateThreshVsPerfData(pred_avg)
plotThreshVsPerf(g_avg)



Is there a way to do this without a workaround and could this workaround have any unwanted side effects?","['r', 'h2o', 'mlr']",tover,https://stackoverflow.com/users/4647519/tover,545
45012473,45012473,2017-07-10T12:46:00,2017-07-10 20:44:23Z,0,"How can I save an h2o model trained with the mlr package and load it in a new session to predict the target variable for a new data set? In the following example I tried it with save and h2o.saveModel, but it throws an error.


library(mlr)
a <- data.frame(y=factor(c(1,1,1,1,1,1,1,1,0,0,1,0)), 
                x1=rep(c(""a"",""b"", ""c""), times=c(6,3,3)))
aTask <- makeClassifTask(data = a, target = ""y"", positive=""1"")
h2oLearner <- makeLearner(""classif.h2o.deeplearning"")
model <- train(h2oLearner, aTask)
# save mlr and h2o model separately:
save(file=""saveh2omodel.rdata"", list=c(""model""))
h2o.saveModel(getLearnerModel(model), path=""h2o_model"")

# shutdown h2o and close R and open new session
h2o.shutdown()

library(mlr)
library(h2o)
h2o.init()
h2o.loadModel(""h2o_model"")
load(file=""saveh2omodel.rdata"")
#ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/99/Models.bin/)
# Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  :                             
#  ERROR MESSAGE:
#  Illegal argument: dir of function: importModel: h2o_model

b <- data.frame(x1=rep(c(""a"",""b"", ""c""), times=c(3,5,4)))
pred <- predict(model, newdata=b) 
# only works if h2o wasn't shut down!","['r', 'h2o', 'mlr']",tover,https://stackoverflow.com/users/4647519/tover,545
45009855,45009855,2017-07-10T10:37:02,2017-07-10 20:40:03Z,0,"I tried to train a h2o model using the following code and make a prediction for new data, but it leads to an error. How can I avoid this error?


library(mlr)
a <- data.frame(y=factor(c(1,1,1,1,1,1,1,1,0,0,1,0)), 
                x1=rep(c(""a"",""b"",""c""), times=c(6,3,3)))
aTask <- makeClassifTask(data = a, target = ""y"", positive = ""1"")
h2oLearner <- makeLearner(""classif.h2o.deeplearning"", 
                          predict.type = ""prob"")
model <- train(h2oLearner, aTask)

b <- data.frame(x1=rep(c(""a"",""b"", ""c""), times=c(3,5,4)))
pred <- predict(model, newdata=b)



leads to the following error:




Error in checkPredictLearnerOutput(.learner, .model, p) :

  predictLearner for classif.h2o.deeplearning has returned not the class
  levels as column names: p0,p1




If I change predict.type to ""response"" it works. So how to predict probabilities?","['r', 'h2o', 'mlr']",Marco Sandri,https://stackoverflow.com/users/1166684/marco-sandri,24.2k
44986674,44986674,2017-07-08T13:19:30,2017-07-09 19:29:50Z,309,"In the h2o models that have the ""ignore_const_cols"" parameter, how is ""constant"" determined with respect to missing values? 


i.e if a column has only one unique non-missing value and also some missing values, and ""ignore_const_cols"" is selected, will no splits occur on this column to separate the missing values from the one constant value?","['machine-learning', 'statistics', 'h2o']",ctlaltdefeat,https://stackoverflow.com/users/2362948/ctlaltdefeat,878
44968297,44968297,2017-07-07T10:16:07,2017-07-07 18:06:16Z,0,"I am trying to predict test times in a Kaggle comp using the H2OGeneralizedLinearEstimator function. The model trains normally in line 3 and the metrics are all reasonable. However when I come to the predict step I get an error despite the test data frame matching the train data frame. 


Has anyone seen this error before?


 h2o_glm = H2OGeneralizedLinearEstimator()

 h2o_glm.train(training_frame=train_h2o,y='y')

 h2o_glm_predictions = h2o_glm.predict(test_data=test_h2o).as_data_frame()

 test_pred = pd.read_csv('test.csv')[['ID']]
 test_pred['y'] = h2o_glm_predictions
 test_pred.to_csv('h2o_glm_predictions.csv',index=False)



glm Model Build progress: |███████████████████████████████████████████████| 100%


glm prediction progress: | (failed)

OSError Traceback (most recent call last) in () 3 h2o_glm.train(training_frame=train_h2o,y='y') 4 ----> 5 h2o_glm_predictions = h2o_glm.predict(test_data=test_h2o).as_data_frame() 6 7 test_pred = pd.read_csv('test.csv')[['ID']]

/Applications/anaconda/lib/python3.6/site-packages/h2o/model/model_base.py in predict(self, test_data) 130 j = H2OJob(h2o.api(""POST /4/Predictions/models/%s/frames/%s"" % (self.model_id, test_data.frame_id)), 131 self._model_json[""algo""] + "" prediction"") --> 132 j.poll() 133 return h2o.get_frame(j.dest_key) 134

/Applications/anaconda/lib/python3.6/site-packages/h2o/job.py in poll(self) 71 if (isinstance(self.job, dict)) and (""stacktrace"" in list(self.job)): 72 raise EnvironmentError(""Job with key {} failed with an exception: {}\nstacktrace: "" ---> 73 ""\n{}"".format(self.job_key, self.exception, self.job[""stacktrace""])) 74 else: 75 raise EnvironmentError(""Job with key %s failed with an exception: %s"" % (self.job_key, self.exception))





OSError: Job with key
  $03017f00000132d4ffffffff$_868312f4c32f683871930a1145c1476a failed
  with an exception: DistributedException from /127.0.0.1:54321: 'null',
  caused by java.lang.ArrayIndexOutOfBoundsException stacktrace:
  DistributedException from /127.0.0.1:54321: 'null', caused by
  java.lang.ArrayIndexOutOfBoundsException at
  water.MRTask.getResult(MRTask.java:478) at
  water.MRTask.getResult(MRTask.java:486) at
  water.MRTask.doAll(MRTask.java:390) at
  water.MRTask.doAll(MRTask.java:396) at
  hex.glm.GLMModel.predictScoreImpl(GLMModel.java:1215) at
  hex.Model.score(Model.java:1077) at
  water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:351)
  at water.H2O$H2OCountedCompleter.compute(H2O.java:1349) at
  jsr166y.CountedCompleter.exec(CountedCompleter.java:468) at
  jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263) at
  jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974) at
  jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477) at
  jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) Caused
  by: java.lang.ArrayIndexOutOfBoundsException","['java', 'python', 'glm', 'h2o']",Akshay,https://stackoverflow.com/users/6886968/akshay,"1,161"
44966134,44966134,2017-07-07T08:31:25,2018-06-30 01:44:36Z,420,"For example, I want to export a model (not the POJO) from an 
h2o flow
 running on a remote hadoop cluster, so that I can upload it to my local machine h2o session and and view the output metrics without having to train the model again with the same data. However, when I try to export the model from 
flow
 (under some name ""export_test""), I cannot find it. To start the h2o session, I am doing:


$cd cd h2o-3.14.0.6-mapr5.2
$hadoop jar h2odriver.jar -nodes 5 -mapperXmx 6g -output hdfsOutputDirName



but I don't see any file called ""export_test"" in the hdfsOutputDirName nor in the 
h2o-3.14.0.6-mapr5.2
 directory (which is where models are stored whenever I export them when using 
flow
 on my local machine) so I'm not sure where it could be going. Is there a default base directory that h2o flow may be exporting to somewhere on the node I am remoted into (like baseDir/export_test)? Thanks.","['hadoop', 'h2o']",Unknown,,N/A
44947850,44947850,2017-07-06T11:39:51,2017-08-13 20:10:14Z,0,"ERROR MESSAGE:


Illegal argument(s) for DeepLearning model: dl_model_faster.

Details: ERRR on field: _stopping_metric: Stopping metric cannot be misclassification for regression.




I am getting this error but actually I am using h2o.deeplearning for a classification problem, I don't want to run regression model. How can I specify that?","['r', 'deep-learning', 'classification', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
44940219,44940219,2017-07-06T05:08:22,2017-07-07 03:37:09Z,0,"Trying to perform kmeans using h2o package. this is info about my h2o cluster:


java version ""1.8.0_131""
Java(TM) SE Runtime Environment (build 1.8.0_131-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)

Starting H2O JVM and connecting: . Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         5 seconds 382 milliseconds 
    H2O cluster version:        3.10.5.3 
    H2O cluster version age:    5 days  
    H2O cluster name:           H2O_started_from_R_rgb505 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   14.22 GB 
    H2O cluster total cores:    4 
    H2O cluster allowed cores:  4 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    R Version:                  R version 3.4.1 (2017-06-30) 



My data is [32000, 14]. So, very small.
I get the following error when trying to perform a h2o.kmeans


h2o_kmeans <- h2o.kmeans(training_frame = spmx_train.h2o, 
                         nfolds = 10,
                         k = 20,
                         estimate_k = TRUE,
                         max_iterations = 10,
                         standardize = FALSE
)



Error:




java.lang.ArrayIndexOutOfBoundsException: 6

java.lang.ArrayIndexOutOfBoundsException: 6
  at water.util.ArrayUtils.add(ArrayUtils.java:163)
  at hex.ModelMetricsClustering$MetricBuilderClustering.reduce(ModelMetricsClustering.java:131)
  at hex.ModelMetricsClustering$MetricBuilderClustering.reduce(ModelMetricsClustering.java:80)
  at hex.ModelBuilder.cv_mainModelScores(ModelBuilder.java:512)
  at hex.ModelBuilder.computeCrossValidation(ModelBuilder.java:292)
  at hex.ModelBuilder$1.compute2(ModelBuilder.java:207)
  at water.H2O$H2OCountedCompleter.compute(H2O.java:1256)
  at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
  at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
  at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
  at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
  at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

Error: java.lang.ArrayIndexOutOfBoundsException: 6





by when I change nfolds to 5, then it runs OK.
So, there is some sort of a memory problem.
it's just very hard to believe that h2o cannot handle 1o-folds kmeans on such a small data. 
Sometimes randomly it runs the code. I've closed all my other applications and only run R. Is there anything i can do to improve this?","['r', 'out-of-memory', 'k-means', 'h2o']",Unknown,,N/A
44930886,44930886,2017-07-05T15:51:17,2017-07-10 13:54:12Z,0,"I am trying to convert larger data.frames consisting of round about 70 numerical columns and one character column to h2o objects.
There is no error message but it only converts a fraction of the data frame and skips the rest (i.e. the number of rows of the resulting h2o object is much lower than the one of the original data frame)
Has anybody an advice?


Here an example (I build an example data frame similar to the one I am trying to analyze in my study.)


nameDF <- c(paste(""O"",letters, sep=""_""),  paste(""T"",letters, sep=""_""),
   paste(""TR"",letters, sep=""_""))
DF <- matrix( data=numeric(length(nameDF)*1000000), nrow=1000000)
colnames(DF) <- nameDF
DF <- as.data.frame(DF)
DF$char <- rep(""bla"", 1000000)
DFh2o <- as.h2o(DF)
dim(DFh2o)
dim(DF)","['r', 'h2o']",Unknown,,N/A
44910513,44910513,2017-07-04T16:19:09,2017-07-11 02:53:33Z,497,"I'm having trouble with H2O's apply function in Python 3.6.1 under anaconda 4.3.22.  I'm running version H2O 3.10.4.4 on Windows 10.  I suspect this may be a bug (or else there's a bug in the documentation.)


I took this example snippet from H2O booklet, page 14:

http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/PythonBooklet.pdf


df5 = h2o.H2OFrame.from_python(np.random.randn(100,4).tolist(), column_names=list(""ABCD""))
df5.apply(lambda x: x.mean(na_rm=True))



And I got the error:


IndexError                                Traceback (most recent call last)
<ipython-input-138-e45989298b6f> in <module>()
      1 df5 = h2o.H2OFrame.from_python(np.random.randn(100,4).tolist(), column_names=
      2 list(""ABCD""))
----> 3 df5.apply(lambda x: x.mean(na_rm=True))

C:\cygwin64\usr\local\anaconda3\lib\site-packages\h2o\frame.py in apply(self, fun, axis)
   2756         assert_is_type(fun, FunctionType)
   2757         assert_satisfies(fun, fun.__name__ == ""<lambda>"")
-> 2758         res = _bytecode_decompile_lambda(fun.__code__)
   2759         return H2OFrame._expr(expr=ExprNode(""apply"", self, 1 + (axis == 0), *res))
   2760 

C:\cygwin64\usr\local\anaconda3\lib\site-packages\h2o\astfun.py in _bytecode_decompile_lambda(co)
     86                 raise ValueError(""unimpl: op in hasjrel"")
     87             elif op in haslocal:
---> 88                 args.append(co.co_varnames[oparg])  # LOAD_FAST
     89             elif op in hascompare:
     90                 args.append(cmp_op[oparg])  # COMPARE_OP

IndexError: tuple index out of range","['python', 'apply', 'h2o']",Unknown,,N/A
44901421,44901421,2017-07-04T08:55:17,2017-07-04 14:25:56Z,0,"When predicting on a test set where the response variable is not present, h2o fails in various different ways if one hot encoding was used for a factor variable in the training, either when specified implicitly when training a GLM or when specifying it explicitly in other methods. 


This error is present in R 3.4.0 and h2o 3.12.0.1. We have also tested with h2o 3.10.3.3


 library(h2o)
localH2O = h2o.init()

prostatePath = system.file(""extdata"", ""prostate.csv"", package = ""h2o"")
prostate.hex = read.csv(prostatePath)
prostate.hex$our_factor<-as.factor(paste0(""Q"",c(rep(c(1:380),1))))

prostate.hex<-as.h2o(prostate.hex)
prostate.hex$weight<-1

prostate_train<-prostate.hex[1:300,]
prostate_test<-prostate.hex[301:380,]
prostate_test<-prostate_test[,-3] #delete response variable from test data

model<-h2o.glm(y = ""AGE"", x = c(""our_factor""), 
               training_frame = prostate_train,offset_column=""weight"")
predict(model,newdata=prostate_train)
predict(model,newdata=prostate_test)

model<-h2o.glm(y = ""AGE"", x = c(""our_factor""), 
               training_frame = prostate_train)
predict(model,newdata=prostate_train)
predict(model,newdata=prostate_test)

model<-h2o.gbm(y = ""AGE"", x = c(""our_factor""), 
               training_frame = prostate_train,categorical_encoding = ""OneHotExplicit"")
predict(model,newdata=prostate_train)
predict(model,newdata=prostate_test)



The first GLM example that was trained with an offset column produces all NaNs when predicting on the test data.
The second GLM example produces this error:


DistributedException from localhost/127.0.0.1:54321: '0', caused by java.lang.ArrayIndexOutOfBoundsException: 0

DistributedException from localhost/127.0.0.1:54321: '0', caused by java.lang.ArrayIndexOutOfBoundsException: 0
    at water.MRTask.getResult(MRTask.java:478)
    at water.MRTask.getResult(MRTask.java:486)
    at water.MRTask.doAll(MRTask.java:390)
    at water.MRTask.doAll(MRTask.java:396)
    at hex.glm.GLMModel.predictScoreImpl(GLMModel.java:1215)
    at hex.Model.score(Model.java:1077)
    at water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:351)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1349)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
    at hex.DataInfo.extractDenseRow(DataInfo.java:1025)
    at hex.glm.GLMScore.map(GLMScore.java:148)
    at water.MRTask.compute2(MRTask.java:657)
    at water.H2O$H2OCountedCompleter.compute1(H2O.java:1352)
    at hex.glm.GLMScore$Icer.compute1(GLMScore$Icer.java)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1348)
    ... 5 more

Error: DistributedException from localhost/127.0.0.1:54321: '0', caused by java.lang.ArrayIndexOutOfBoundsException: 0



The GBM example produces this error (even though the only column missing from the test data is the response variable):


java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set
java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set
    at hex.Model.adaptTestForTrain(Model.java:1028)
    at hex.Model.adaptTestForTrain(Model.java:854)
    at hex.Model.score(Model.java:1072)
    at water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:351)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1349)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

Error: java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set



The error seems to be specific to factor variables and using one hot encoding explicitly. It can be worked around by adding a 'fake' response column to the test dataset (we've tested this, and the value of this column makes no difference to the predictions, as we'd expect), but that's obviously not ideal.


The errors remain even if all the factor levels are present in both the train and test set, if there are 5 or more factor levels:


prostate.hex$our_factor<-as.factor(paste0(""Q"",c(rep(c(1:5),76))))



If there are 4 or less, there are no problems with the GLM, but the error message from the GBM remains","['r', 'h2o']",Unknown,,N/A
44883513,44883513,2017-07-03T10:39:07,2017-07-03 10:43:33Z,172,"Using the following code in 
python v2.7
 on 
UBUNTU 16.04
 machine


model1=h2o.load_model('home/administrator/DeepLearning_model_python_1497342069792_59')
b=model1.download_pojo('.')



I can successfully create a 
POJO
 with name: 
DeepLearning_model_python_1497342069792_59.java


I want to change the name of the 
POJO
,simply renaming the 
POJO
 will break the code.

How can I give a custom name while downloading the 
POJO
 itself?","['java', 'python', 'machine-learning', 'pojo', 'h2o']",Unknown,,N/A
44877161,44877161,2017-07-03T02:55:26,2019-05-27 06:00:59Z,0,"I built a simple deep learning regression model with 
h2o
 (below). The model predicts sepal length in the R iris dataset. I notice that as I increase the epochs, the model accuracy (r^2) increases (Figure 1). 


By increasing the number of epochs, am I overfitting the model in a detrimental way or am I increasing the model accuracy in a beneficial way?


library(datasets)
library(h2o)

df <- iris

df.hex <- as.h2o(df)

model <- h2o.deeplearning(x = 2:5, y = 1, df.hex,
                          hidden = c(200, 200, 200, 200, 200),
                          epochs = 5,
                          variable_importances=T)

perf_dl <- h2o.performance(model)
rsq_dl <- h2o.r2(perf_dl)







Figure 1




# Note this code plots the data from the deep learning runs in the previous code
library(ggplot2)

df <- data.frame(epochs = c(5, 10, 100, 300, 500, 1000, 2000, 3000, 5000), rsq = c(0.77, 0.70, 0.57, 0.75, 0.87, 0.92, 0.97, 0.96, 0.98))

p <- ggplot(df, aes(epochs, rsq))
p + geom_point(aes(size = 7)) + stat_smooth(method = ""lm"", formula = y ~ x + I(x^2), size = 1)","['r', 'machine-learning', 'deep-learning', 'h2o']",Borealis,https://stackoverflow.com/users/1446289/borealis,"8,444"
44874870,44874870,2017-07-02T20:19:11,2017-07-02 23:23:00Z,0,"This is an attempt to understand what are the deep features in h2o. The function is 
h2o.deepfeatures
 which is designed for deep neural net. 


My understanding is they are just the activation of the hidden cells in each layer. So, by forward-propagating a row of observation, I should be able to reproduce the deep features in the first layer. And forward-propagate again, I get the deep feature in 2nd layer, and so on. But the result is always a little bit off.


There's a few complications but I think I got them under control. First, I normalize my data before feeding it into 
deeplearning
, so I set 
standardize=FALSE
. Second, I do not use dropout, just to make it simple. Third, I use 
tanh
, again just to get a clean nonlinearity.


dp <- h2o.deeplearning(
  training_frame = as.h2o(X_train),       
  validation_frame = as.h2o(X_test), 
  x = IV.nm,                    
  y = ""y"",                        
  standardize=FALSE,
  activation = ""Tanh"",
  hidden = c(1000, 1000, 750, 250, 5),
  epochs = 1,
  adaptive_rate=TRUE,
  epsilon = 1e-9,       
  loss = ""Huber"", 
  distribution = ""huber"",
  nfolds = 0,
  export_weights_and_biases = TRUE,  
  variable_importances = TRUE,       
  shuffle_training_data = TRUE,      
  model_id = ""MyModel""   
)

w1 = as.matrix(h2o.weights(dp, 1))
b1 = as.matrix(h2o.biases(dp, 1))
ff1 = as.data.frame(h2o.deepfeatures(dp, as.h2o(X_test), layer=1))

nc = ncol(X_test) # same as Xt.hf
tmp3 = rep(b1, each=nrow(X_test))
B1 = matrix(tmp3, ncol=nrow(X_test))
a1 = as.matrix(w1) %*% t(X_test[,3:nc])
A1 = t(a1 + B1)
A1 = tanh(A1)



Now, 
A1
 is the hand-build activation of the first layer, which should match 
ff1
, however they don't quite equal, as seen below,


del = A1 - ff1
summary(apply(del, 2, min))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-0.1844 -0.1835 -0.1828 -0.1824 -0.1819  0.0000
summary(apply(del, 2, max))
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
0.000000 0.001628 0.001692 0.001881 0.001760 0.185080 



How come?


An observation


While the min/max of the differences could be off by ~0.18 as shown above, most of the deltas are actually very small. I took random snapshots and found they're very close. So, this might be in the implementation of 
tanh
 function, or just java floating point vs R?  


New Test


When I use 
RectifierWithDropout
 as my activation, the deltas are significantly larger. 


> summary(apply(del, 2, min))
      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
-0.3731920 -0.0008961 -0.0001467 -0.0038102 -0.0000472  0.0000000 
> summary(apply(del, 2, max))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.7179  0.7422  0.7472  0.9930  0.7505 40.6782 
> summary(apply(del, 2, mean))
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
0.005563 0.011769 0.015453 0.046342 0.020242 4.569975 



This is a bit more puzzling as I would think dropout only affects training. Once the model is built, the deepfeatures should be a straightforward function of the inputs to the layer.


Did I miss anything?","['r', 'neural-network', 'deep-learning', 'h2o']",Unknown,,N/A
44859033,44859033,2017-07-01T08:46:31,2017-07-01 15:59:14Z,185,"I'm trying to extend the hamorspam example(
https://github.com/h2oai/sparkling-water/blob/master/examples/scripts/hamOrSpam.script.scala

) to make parallel predictions for large dataset using spark's parallel computation power(during the inference stage, not the training phase). 


Below is the code I have written for the same. Moreover, it perfectly works fine in single node localmode (for  
export MASTER=""local[*]
 ``), but fails when I run with 
export MASTER=""local-cluster[2,2,1024]
 when 2 worker nodes are spawn.(to check the prediction parallelisation)


val data_test = load(""smsData.txt"") // Should be a large(in GBs) test dataset - using same training data for testing purposes just to test the workflow
val message_test = data.map( r => r(1))
message.take(1000).map(x => isSpam(x, dlModel, hashingTF, idfModel, h2oContext))



So the code fails when executing 
scala> val table:H2OFrame = resultRDD
 (

https://github.com/h2oai/sparkling-water/blob/master/examples/scripts/hamOrSpam.script.scala#L110
)


I have attached the error from the console below:


 17/06/26 20:25:49 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 43, 144.27.27.98, executor 1): java.lang.NoClassDefFoundError: Could not ini
    tialize class $line32.$read$
            at $line41.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:57)
            at $line41.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:57)
            at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
            at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
            at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
            at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
            at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
            at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
            at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
            at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
            at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1010)
            at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)
            at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)
            at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)
            at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
            at org.apache.spark.scheduler.Task.run(Task.scala:99)
            at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
            at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
            at java.lang.Thread.run(Thread.java:748)

17/06/26 20:25:49 ERROR TaskSetManager: Task 0 in stage 6.0 failed 4 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 
6.0 (TID 49, 144.27.27.98, executor 0): java.lang.NoClassDefFoundError: Could not initialize class 
        at $anonfun$1.apply(<console>:57)
        at $anonfun$1.apply(<console>:57)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1010)
        at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)
        at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)
        at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)
  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
  at org.apache.spark.h2o.utils.H2OSchemaUtils$.collectMaxArrays(H2OSchemaUtils.scala:229)
  at org.apache.spark.h2o.utils.H2OSchemaUtils$.expandedSchema(H2OSchemaUtils.scala:107)
  at org.apache.spark.h2o.converters.SparkDataFrameConverter$.toH2OFrame(SparkDataFrameConverter.scala:59)
  at org.apache.spark.h2o.H2OContext.asH2OFrame(H2OContext.scala:167)
  at org.apache.spark.h2o.H2OContextImplicits.asH2OFrameFromDataFrame(H2OContextImplicits.scala:54)
  ... 58 elided


Caused by: java.lang.NoClassDefFoundError: Could not initialize class 
  at $anonfun$1.apply(<console>:57)
  at $anonfun$1.apply(<console>:57)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
  at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1010)
  at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)
  at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)
  at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
  at org.apache.spark.scheduler.Task.run(Task.scala:99)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:748)



Any ideas?. Thanks in advance.","['scala', 'apache-spark', 'h2o', 'sparkling-water']",Unknown,,N/A
44854821,44854821,2017-06-30T21:07:01,2017-11-10 16:21:43Z,0,"When I try to run XGboost in R on Windows 7 and on Windows Server 2008R2 via 
h2o.xgboost()
 with H2O 
3.12.01
 I get the following error:




Error: java.lang.UnsatisfiedLinkError:
  ml.dmlc.xgboost4j.java.XGBoostJNI.XGDMatrixCreateFromCSREx([J[I[FI[J)I




Here's a reproducible example:


library(h2o)
h2o.init(nthreads = -1)
h2o.no_progress() # Don't show progress bars in RMarkdown output

# Import a sample binary outcome train/test set into H2O
train <- h2o.importFile(""https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv"")
test <- h2o.importFile(""https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv"")

# Identify predictors and response
y <- ""response""
x <- setdiff(names(train), y)

# For binary classification, response should be a factor
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])

# Number of CV folds (to generate level-one data for stacking)
nfolds <- 5

# Train & Cross-validate a (shallow) XGB-GBM
my_xgb1 <- h2o.xgboost(x = x,
                       y = y,
                       training_frame = train,
                       distribution = ""bernoulli"",
                       ntrees = 50,
                       max_depth = 3,
                       min_rows = 2,
                       learn_rate = 0.2,
                       nfolds = nfolds,
                       fold_assignment = ""Modulo"",
                       keep_cross_validation_predictions = TRUE,
                       seed = 1)





R version 3.4.0 Patched (2017-05-19 r72713)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows Server 2008 R2 x64 (build 7601) Service Pack 1

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252    LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                           LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] h2o_3.12.0.1

loaded via a namespace (and not attached):
[1] compiler_3.4.0 tools_3.4.0    RCurl_1.95-4.8 jsonlite_1.5   bitops_1.0-6





3.12.01 was the newest development version linked on the h2o.ai homepage, which I upgraded to after not finding this function in 3.10. However, a comment by @MarcoSandri indicated there's a newer development version (3.13) on their Amazon AWS, so downloaded it and upgraded the cluster and R package accordingly.


The upgrade from 3.12 to 3.13 seemed to go smoothly, until I attempted to use the 
h2o.xgboost()
 function. Then it threw a different error:


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 





ERROR MESSAGE:

-1

Error in fetch(key) : 
  lazy-load database 'E:/Program Files/R/R-3.4.0patched/library/h2o/help/h2o.rdb' is corrupt","['r', 'windows', 'xgboost', 'h2o']",Unknown,,N/A
44848861,44848861,2017-06-30T14:30:08,2018-01-07 17:09:24Z,0,"First of all thanks for implementing XGBoost in h2o!


Unfortunately I am unable to predict from an h2o xgboost model that's loaded from disk (which I'm sure you can appreciate is really frustrating).


I am using the latest stable release of h2o i.e. 3.10.5.2 & I am using an R client.


I have included an example below that should enable you to reproduce the issue, 


Thanks in advance


### Start h2o
    require(h2o)
    local_h2o = h2o.init()

### Source the base data set
    data(mtcars)
    h2o_mtcars = as.h2o(x = mtcars,destination_frame = 'h2o_mtcars')   

### Fit a model to be saved
    mdl_to_save = h2o.xgboost(model_id = 'mdl_to_save',y = 1,x = 2:11,training_frame = h2o_mtcars) ##This class doesnt work
    #mdl_to_save = h2o.glm(model_id = 'mdl_to_save',y = 1,x = 2:11,training_frame = h2o_mtcars) ##This class works

### Take some reference predictions
    ref_preds = h2o.predict(object = mdl_to_save,newdata = h2o_mtcars)

### Save the model to disk
    silent = h2o.saveModel(object = mdl_to_save,path = 'INSERT_PATH',force = TRUE)

### Delete the model to make sure there cant be any strange locking issues
    h2o.rm(ids = 'mdl_to_save')

### Load it back up 
    loaded_mdl = h2o.loadModel(path = 'INSERT_PATH/mdl_to_save')

### Score the model
### The h2o.predict statement below is what causes the error: java.lang.NullPointerException
    lod_preds = h2o.predict(object = loaded_mdl,newdata = h2o_mtcars)
    all.equal(ref_preds,lod_preds)","['r', 'xgboost', 'h2o']",gm209,https://stackoverflow.com/users/8183099/gm209,31
44843175,44843175,2017-06-30T09:30:56,2017-06-30 18:01:46Z,263,"I have an h2o random forest in Python. How to extract for each tree the threshold of each features ?


My aim is to implement this random forest in c++


Thanks !","['python', 'random-forest', 'h2o']",user8236495,https://stackoverflow.com/users/8236495/user8236495,1
44832983,44832983,2017-06-29T18:58:22,2017-06-29 18:58:22Z,0,"I have a rare event I'm trying to predict (less than .01%) and wanted to implement Monte Carlo cross-validation (LGOCV in the caret package). How would I go about doing this in H2O? If this isn't available, where is this on the H2O roadmap? Thank you!","['r', 'cross-validation', 'h2o']",tdoyon,https://stackoverflow.com/users/3100502/tdoyon,11
44829663,44829663,2017-06-29T15:42:59,2017-07-06 13:41:45Z,0,"The 
h2oEnsemble
 library's author, Dr. LeDell, suggested checking out v1.9 from the h2o-3 GitHub repo to work around a problem I and others encountered, which is described in 
this
 thread.


This led to a different error in my case, where only the unmodified wrappers can be used with 
h2oEnsemble::h2o.ensemble()
.


For instance, learners like this will work:


learner <- c(""h2o.deeplearning.wrapper"", ""h2o.randomForest.wrapper"", ""h2o.glm.wrapper"") 



But anything custom like this:


h2o.glm.2 <- function(..., alpha = 0.5) h2o.glm.wrapper(..., alpha = alpha)
learner <- c(""h2o.glm.2"") 

ens2 <- h2o.ensemble(x             = setdiff(colnames(train), ""y""),
                    y              = ""y"",
                    training_frame = trainHex,
                    metalearner    = ""h2o.deeplearning.wrapper"",
                    learner        = learner)



throws this error:




Error in h2o.ensemble(x = setdiff(colnames(train), ""y""), y = ""y"", training_frame = trainHex, :
The Naive Bayes function does not support regression, please remove this function from your set of base learners





Dr. LeDell suggested to confirm compatibility through her compatibility table on GitHub and to move further questions to StackOverflow (because they are shutting down their forum).


It seems that v1.9 is not on that table though and my H2O cluster version is also newer than what was listed there. Does anyone have anecdotal knowledge of which H2O clusters will work with v1.9 of the R package?


> h2o.getVersion()





""3.10.0.2""




Update: I tried upgrading to 3.12 and 3.13, but encountered the same error with this. Though I did notice that 
h2o::h2o.stackedEnsemble()
 works in 3.13.


Update:


The package author suggested version 3.10.5.2 in the comments/updated question. I tried updating to that, but sadly the problem still occurred. Here's my cluster info after the update:


> h2o.init(nthreads = -1) 





 Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         2 minutes 28 seconds 
    H2O cluster version:        3.10.5.2 
    H2O cluster version age:    15 days  
    H2O cluster name:           hackr 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   16.98 GB 
    H2O cluster total cores:    24 
    H2O cluster allowed cores:  24 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    R Version:                  R version 3.4.0 Patched (2017-05-19 r72713)





Doesn't work with custom learner:


h2o.glm.2 <- function(..., alpha = 0.5) h2o.glm.wrapper(..., alpha = alpha)
learner <- c(""h2o.glm.2"", ""h2o.deeplearning.wrapper"", ""h2o.randomForest.wrapper"", ""h2o.glm.wrapper"")

ens <- h2o.ensemble(x              = setdiff(colnames(train), ""y""),
                    y              = ""y"",
                    training_frame = trainHex,
                    metalearner    =""h2o.randomForest.wrapper"",
                    learner        = learner)
summary(ens)





Error in h2o.ensemble(x = setdiff(colnames(train), ""y""), y = ""y"",
  training_frame = trainHex,  :    The Naive Bayes function does not
  support regression, please remove this function from your set of base
  learners.




but works with only default wrapper learners:


learner <- c(""h2o.deeplearning.wrapper"", ""h2o.randomForest.wrapper"", ""h2o.glm.wrapper"")

ens <- h2o.ensemble(x              = setdiff(colnames(train), ""y""),
                    y              = ""y"",
                    training_frame = trainHex,
                    metalearner    =""h2o.randomForest.wrapper"",
                    learner        = learner)
summary(ens)



R package version is 
h2o_3.10.5.2
.","['r', 'h2o']",Unknown,,N/A
44829493,44829493,2017-06-29T15:35:51,2017-06-29 16:34:31Z,0,"Please find attached Error while building model and checking accuracy. I am using H2o package.
I have create model as h2o model. I want to apply the model in test and validation data . 


My R code is:-


library(mlbench)
library(h2o)
h2o.init(nthreads = -1)
data(""BreastCancer"")

#Adjusting data types
data<-BreastCancer[,-1] #remove the ID column
#converting all columns to numeric type
data[,c(1:ncol(data))]<-sapply(data[,c(1:ncol(data))],as.numeric)
#convert class column to factor type
data[,'Class']<-as.factor(data[,'Class'])

#converting in the h2o format
splitsample<-sample(1:3,size=nrow(data),prob=c(0.6,0.2,0.2),replace=TRUE)
train_h2o<-as.h2o(data[splitsample==1,])
val_h2o<- as.h2o(data[splitsample==2,])
test_h2o<-as.h2o(data[splitsample==3,])
model<- h2o.deeplearning(x=1:9,# column number for predictors
                         y=10, #column number for label
                         #data in H2o format
                         training_frame = train_h2o,
                         #or 'Tanh'
                         # TanhWithDropout means Tanh function with regularization 
                         activation = ""TanhWithDropout"",
                         #% of inputs dropout
                         # It is used to drop bad or curropted or noise data
                         input_dropout_ratio = 0.2,
                         #balanced the two class 
                         balance_classes = TRUE,
                         #two hidden layers of 10 units
                         hidden = c(10,10),
                         #% for nodes dropout
                         # dropout probability for hidden layers
                         hidden_dropout_ratios = c(0.3,0.3),
                         #max no. of epochs
                         # Times of iterate data
                         epochs = 10,
                         seed=0)
h2o.confusionMatrix(model)


#validation confusion matrix

h2o.confusionMatrix(model,newdata=val_h2o)



My error is :




Error in res$model_metrics[[1L]] : subscript out of bounds




Please anyone help me in deep learning. I am very grateful of you.


I have error in this code :-


h2o.confusionMatrix(model,newdata=val_h2o)


Error in res$model_metrics[[1L]] : subscript out of bounds","['r', 'deep-learning', 'h2o', 'confusion-matrix']",Unknown,,N/A
44803064,44803064,2017-06-28T12:52:31,2017-07-07 14:28:31Z,0,"I am working with a data set in R which contains 42,457 rows and 785 columns.  The first column is a binary response variable (called 
label
) and the remaining columns are binary features.


I tried to fit a logistic regression model using 
rsparkling
  (described 
here
) but got errors.  I traced this to a step that converts a Spark version of this table to an H2O data frame.  


Here's the function I'm using for testing.  The Spark instance 
sc
 is up and running and 
df
 is the raw data set as a standard R data frame:


load_h2o <- function(df, rows = nrow(df), cols = ncol(df)) {
        df <- df[1:rows, 1:cols]
        copy_to(sc, df, ""df"", overwrite = TRUE)
        df_tbl <- tbl(sc, ""df"")
        h2o_tbl <- as_h2o_frame(sc, df_tbl, strict_version_check=FALSE)
        return(h2o_tbl)
}



The head of 
df$label
 is 
1 1 1 0 0 0
.  However if I load the full data set, i.e. 
load_h2o(raw_data)
 the head of the resulting 
label
 column is 
0 0 0 0 0 0
; in fact all values in the H2O data frame are 0.  If I restrict the number of columns to 200 (i.e. 
load_h2o(raw_data, cols = 200)
), then the resulting H2O data frame contains all the data as expected.  If 
cols = 201
, we're back to all zeros.


Finally, if I load the data set directly from disk with 
h2o::h2o.importFile
, then the full data set comes through with no problem and I'm able to fit the logistic regression.  However I'd like to be able to load it from an R data frame in order to distribute the object within a package.


Is there a limit on the maximum number of columns for this workflow?","['r', 'apache-spark', 'h2o']",jkeirstead,https://stackoverflow.com/users/694488/jkeirstead,"2,951"
44796704,44796704,2017-06-28T07:55:31,2017-06-28 07:55:31Z,120,I want to compute the Proximity between the observations from the leaf node assignment after predictions of random forest model. Is there a function for the same. Another R package randomForest has this feature (proximity=T option in predict),['h2o'],usct01,https://stackoverflow.com/users/2160357/usct01,898
44791101,44791101,2017-06-27T22:47:02,2017-07-06 15:48:51Z,848,"How do I prefix an existing column in an h2o data frame with a string value in python?  The column is numerical to begin with.  I have been able to do this in the R H2O but I seem to struggle or can't get this right in the python version of h2o.


In R this seems to work.


h2o.init()
df = as.h2o(mtcars)
df['mpg']=h2o.ascharacter(df['mpg'])
df['mpg']=h2o.sub('','hey--------',df['mpg'])
df



However, when I try to do this in python I get a variety of errors. Sometimes I'm able to adjust the numerical column to a string without an error but then when I go and look at the data frame I receive an error.  I'll post the code if needed.  Given that they are the same functions I imagine it should be relatively easy but I must be missing something.","['python', 'string', 'h2o']",jack,https://stackoverflow.com/users/8157418/jack,102
44753601,44753601,2017-06-26T05:34:13,2017-07-04 03:48:42Z,0,"I got a [210,000 x 500] sparse matrix in R which i'm trying to cluster using h2o.
I imagined that a 210,000 row matrix is not that large for h2o, but when I try to import it to h2o instance it takes a very long time (let it run over 10 minutes and stopped it before completion)
when I subset the first 10,000 rows in a sparse matrix and import it, it takes only a few seconds. and i've tried doing it incrementally and it takes a long time. (by 60,000 I stopped)
Is this normal or I'm doing something wrong? 


here's what i'm using


library(h2o)
localH2O <- h2o.init(nthreads = -1, max_mem_size = ""16g"")     
spmx.h2o <- as.h2o(sparse_mx)



Below is more info about the h2o instance when it's generated:


java version ""1.8.0_131""
Java(TM) SE Runtime Environment (build 1.8.0_131-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)

Starting H2O JVM and connecting: . Connection successful!

    R is connected to the H2O cluster: 
        H2O cluster uptime:         6 seconds 779 milliseconds 
        H2O cluster version:        3.10.4.6 
        H2O cluster version age:    1 month and 30 days  
        H2O cluster name:           H2O_started_from_R_M_vto433 
        H2O cluster total nodes:    1 
        H2O cluster total memory:   14.22 GB 
        H2O cluster total cores:    4 
        H2O cluster allowed cores:  4 
        H2O cluster healthy:        TRUE 
        H2O Connection ip:          localhost 
        H2O Connection port:        54321 
        H2O Connection proxy:       NA 
        H2O Internal Security:      FALSE 
        R Version:                  R version 3.4.0 (2017-04-21) 



I'm trying to avoid writing the matrix to file and import again, simply because I think 210,000 rows and 500 columns should not be a problem for h2o to handle","['r', 'performance', 'import', 'sparse-matrix', 'h2o']",Ankhnesmerira,https://stackoverflow.com/users/6851715/ankhnesmerira,"1,420"
44752584,44752584,2017-06-26T02:58:56,2017-06-29 09:53:10Z,0,"I load csv file of utf-8 encoding with cyrillic strings. After parsing in Flow interface - i see not cyrillic, but not readable symbols like ""пїўпѕЂпѕ™пїђпѕ"" How can i use utf-8 cyrillic strings in H2O?","['character-encoding', 'h2o']",Александр Щеглов,https://stackoverflow.com/users/8213710/%d0%90%d0%bb%d0%b5%d0%ba%d1%81%d0%b0%d0%bd%d0%b4%d1%80-%d0%a9%d0%b5%d0%b3%d0%bb%d0%be%d0%b2,21
44751866,44751866,2017-06-26T00:46:16,2017-12-07 20:12:58Z,0,"I am trying to build a stacked ensemble model to predict merchant churn using R (version 3.3.3) and deep learning in h2o (version 3.10.5.1). The response variable is binary. At the moment I am trying run the code to build a stacked ensemble model using the top 5 models developed by the grid search. However, when the code is run, I get the java.lang.NullPointerException error with the following output:


java.lang.NullPointerException
    at hex.StackedEnsembleModel.checkAndInheritModelProperties(StackedEnsembleModel.java:265)
    at hex.ensemble.StackedEnsemble$StackedEnsembleDriver.computeImpl(StackedEnsemble.java:115)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:173)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1349)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)



Below is the code that I've used to do the hyper-parameter grid search and build the ensemble model:


hyper_params <- list(
                  activation=c(""Rectifier"",""Tanh"",""Maxout"",""RectifierWithDropout"",""TanhWithDropout"",""MaxoutWithDropout""),
                  hidden=list(c(50,50),c(30,30,30),c(32,32,32,32,32),c(64,64,64,64,64),c(100,100,100,100,100)),
                  input_dropout_ratio=seq(0,0.2,0.05),
                  l1=seq(0,1e-4,1e-6),
                  l2=seq(0,1e-4,1e-6),
                  rho = c(0.9,0.95,0.99,0.999),
                  epsilon=c(1e-10,1e-09,1e-08,1e-07,1e-06,1e-05,1e-04)
                )

search_criteria <- list(
                      strategy = ""RandomDiscrete"",
                      max_runtime_secs = 3600,
                      max_models = 100,
                      seed=1234,
                      stopping_metric=""misclassification"",      
                      stopping_tolerance=0.01,                  
                      stopping_rounds=5
                    )

dl_ensemble_grid <- h2o.grid(
                          hyper_params = hyper_params,
                          search_criteria = search_criteria,
                          algorithm=""deeplearning"",
                          grid_id = ""final_grid_ensemble_dl"",
                          x=predictors,
                          y=response,
                          training_frame = h2o.rbind(train, valid, test),
                          nfolds=5,
                          fold_assignment=""Modulo"",
                          keep_cross_validation_predictions = TRUE,
                          keep_cross_validation_fold_assignment = TRUE,
                          epochs=12,
                          max_runtime_secs = 3600,
                          stopping_metric=""misclassification"",
                          stopping_tolerance=0.01,        
                          stopping_rounds=5,
                          seed = 1234,
                          max_w2=10
                        )           

DLsortedGridEnsemble_logloss <- h2o.getGrid(""final_grid_ensemble_dl"",sort_by=""logloss"",decreasing=FALSE)

ensemble <- h2o.stackedEnsemble(x = predictors, 
                            y = response, 
                            training_frame = h2o.rbind(train,valid,test), 
                            base_models = list(                                                   
                                                DLsortedGridEnsemble_logloss@model_ids[[1]],
                                                DLsortedGridEnsemble_logloss@model_ids[[2]],
                                                DLsortedGridEnsemble_logloss@model_ids[[3]],
                                                DLsortedGridEnsemble_logloss@model_ids[[4]],
                                                DLsortedGridEnsemble_logloss@model_ids[[5]],
                                              )



Note: what I have realised so far is that h2o.stackedEnsemble function works when there's only one base model and it gives the Java error as soon as there's two or more base models.


I would really appreciate if I could get some feedback as to how this could be resolved.","['java', 'r', 'deep-learning', 'h2o']",Vivek Kumar,https://stackoverflow.com/users/3374996/vivek-kumar,36.5k
44739293,44739293,2017-06-24T17:50:52,2017-06-27 19:33:56Z,704,"I am trying to build a randomforest on a data set with 120k rows and 518 columns. 
I have two questions:
1. I want to see the progress and logs of building the forest. Is verbose option deprecated in randomForest function?
2. How to increase the speed? Right now it takes more than 6 hours to build a random forest with 1000 trees. 


H2O cluster is initialized with below settings:




hadoop jar h2odriver.jar -Dmapreduce.job.queuename=devclinical 
  -output temp3p -nodes 20 -nthreads -1 -mapperXmx 32g


h2o.init(ip = h2o_ip, port = h2o_port, startH2O = FALSE,
  nthreads=-1,max_mem_size = ""64G"", min_mem_size=""4G"" )","['random-forest', 'h2o']",Chandra,https://stackoverflow.com/users/2289986/chandra,524
44738743,44738743,2017-06-24T16:50:24,2017-06-28 16:18:31Z,0,"UPDATED 28Jun2017, below, in response to @Michal Kurka.


UPDATED 26Jun2017, below.




I am unable to load a large GBM model that I saved in native H2O format (ie, hex).




H2O v3.10.5.1


R v3.3.2


Linux 3.10.0-327.el7.x86_64 GNU/Linux




My goal is to eventually save this model as MOJO.


This model was so large that I had to initialize H2O with min/max memory 100G/200G before H2O's model training would run successfully.


This is how I trained the GBM model:


localH2O <- h2o.init(ip = 'localhost', port = port, nthreads = -1,
                     min_mem_size = '100G', max_mem_size = '200G')

iret <- h2o.gbm(x = predictors, y = response, training_frame = train.hex,
                validation_frame = holdout.hex, distribution=""multinomial"",
                ntrees = 3000, learn_rate = 0.01, max_depth = 5, nbins = numCats,
                model_id = basename_model)

gbm <- h2o.getModel(basename_model)
oPath <- h2o.saveModel(gbm, path = './', force = TRUE)



The training data contains 81,886 records with 1413 columns. Of these columns, 19 are factors.  The vast majority of these columns are 0/1.


$ wc -l training/*.txt
     81887 training/train.txt
     27294 training/holdout.txt



This is the saved model as written to disk:


$ ls -l


total 37G
-rw-rw-r-- 1 bfo7328 37G Jun 22 19:57 my_model.hex



This is how I tried to read the model from disk using the same large memory allocation values 100G/200G:


$ R


R version 3.3.2 (2016-10-31) -- ""Sincere Pumpkin Patch""
Copyright (C) 2016 The R Foundation for Statistical Computing
Platform: x86_64-redhat-linux-gnu (64-bit)

> library(h2o)
> localH2O=h2o.init(ip='localhost', port=65432, nthreads=-1,
                  min_mem_size='100G', max_mem_size='200G')

H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    /tmp/RtmpVSwxXR/h2o_bfo7328_started_from_r.out
    /tmp/RtmpVSwxXR/h2o_bfo7328_started_from_r.err

openjdk version ""1.8.0_121""
OpenJDK Runtime Environment (build 1.8.0_121-b13)
OpenJDK 64-Bit Server VM (build 25.121-b13, mixed mode)

Starting H2O JVM and connecting: .. Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         3 seconds 550 milliseconds 
    H2O cluster version:        3.10.5.1 
    H2O cluster version age:    13 days  
    H2O cluster name:           H2O_started_from_R_bfo7328_kmt050 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   177.78 GB 
    H2O cluster total cores:    64 
    H2O cluster allowed cores:  64 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        65432 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    R Version:                  R version 3.3.2 (2016-10-31) 



From 
/tmp/RtmpVSwxXR/h2o_bfo7328_started_from_r.out
:


INFO: Processed H2O arguments: [-name, H2O_started_from_R_bfo7328_kmt050, -ip, localhost, -port, 65432, -ice_root, /tmp/RtmpVSwxXR]
INFO: Java availableProcessors: 64
INFO: Java heap totalMemory: 95.83 GB
INFO: Java heap maxMemory: 177.78 GB
INFO: Java version: Java 1.8.0_121 (from Oracle Corporation)
INFO: JVM launch parameters: [-Xms100G, -Xmx200G, -ea]
INFO: OS version: Linux 3.10.0-327.el7.x86_64 (amd64)
INFO: Machine physical memory: 1.476 TB



My call to 
h2o.loadModel
:


if ( TRUE ) {
  now <- format(Sys.time(), ""%a %b %d %Y %X"")
  cat( sprintf( 'Begin %s\n', now ))

  model_filename <- './my_model.hex'
  in_model.hex <- h2o.loadModel( model_filename )

  now <- format(Sys.time(), ""%a %b %d %Y %X"")
  cat( sprintf( 'End   %s\n', now ))
}



From 
/tmp/RtmpVSwxXR/h2o_bfo7328_started_from_r.out
:


INFO: GET /, parms: {}
INFO: GET /, parms: {}
INFO: GET /, parms: {}
INFO: GET /3/InitID, parms: {}
INFO: Locking cloud to new members, because water.api.schemas3.InitIDV3
INFO: POST /99/Models.bin/, parms: {dir=./my_model.hex}



After waiting an hour, I see these ""out of memory"" (OOM) error messages:


INFO: POST /99/Models.bin/, parms: {dir=./my_model.hex}
#e Thread WARN: Swapping!  GC CALLBACK, (K/V:24.86 GB + POJO:112.01 GB + FREE:40.90 GB == MEM_MAX:177.78 GB), desiredKV=22.22 GB OOM!
#e Thread WARN: Swapping!  GC CALLBACK, (K/V:26.31 GB + POJO:118.41 GB + FREE:33.06 GB == MEM_MAX:177.78 GB), desiredKV=22.22 GB OOM!
#e Thread WARN: Swapping!  GC CALLBACK, (K/V:27.36 GB + POJO:123.03 GB + FREE:27.39 GB == MEM_MAX:177.78 GB), desiredKV=22.22 GB OOM!
#e Thread WARN: Swapping!  GC CALLBACK, (K/V:28.21 GB + POJO:126.73 GB + FREE:22.83 GB == MEM_MAX:177.78 GB), desiredKV=22.22 GB OOM!



I would not expect to need so much memory to read the model from disk.


How can I read this model from disk into memory.  And once I do, can I save it as a MOJO?




UPDATE 1: 26Jun2017


I just noticed that the disk size of a GBM model increased dramatically between versions of H2O:


H2O v3.10.2.1:
    -rw-rw-r-- 1 169M Jun 19 07:23 my_model.hex

H2O v3.10.5.1:
    -rw-rw-r-- 1  37G Jun 22 19:57 my_model.hex



Any ideas why?  Could this be the root of the problem?




UPDATE 2: 28Jun2017 in response to comments by @Michal Kurka.


When I load the training data via 
fread
, the class (type) of each column is correct:
*   24 columns are ‘character’;
* 1389 columns are ‘integer’ (all but one column are 0/1);
* 1413 total columns.


I then convert the R-native data frame to an H2O data frame and manually factor-ize 20 columns:


train.hex <- as.h2o(df.train, destination_frame = ""train.hex”)
length(factorThese)
[1] 20
train.hex[factorThese] <- as.factor(train.hex[factorThese])
str(train.hex)



A condensed version of the output from 
str(train.hex)
, showing only those 19 columns that are factors (1 factor is the response column):


 - attr(*, ""nrow"")= int 81886
 - attr(*, ""ncol"")= int 1413
 - attr(*, ""types"")=List of 1413
  ..$ : chr ""enum"" : Factor w/ 72 levels
  ..$ : chr ""enum"" : Factor w/ 77 levels
  ..$ : chr ""enum"" : Factor w/ 51 levels
  ..$ : chr ""enum"" : Factor w/ 4226 levels
  ..$ : chr ""enum"" : Factor w/ 4183 levels
  ..$ : chr ""enum"" : Factor w/ 3854 levels
  ..$ : chr ""enum"" : Factor w/ 3194 levels
  ..$ : chr ""enum"" : Factor w/ 735 levels
  ..$ : chr ""enum"" : Factor w/ 133 levels
  ..$ : chr ""enum"" : Factor w/ 16 levels
  ..$ : chr ""enum"" : Factor w/ 25 levels
  ..$ : chr ""enum"" : Factor w/ 647 levels
  ..$ : chr ""enum"" : Factor w/ 715 levels
  ..$ : chr ""enum"" : Factor w/ 679 levels
  ..$ : chr ""enum"" : Factor w/ 477 levels
  ..$ : chr ""enum"" : Factor w/ 645 levels
  ..$ : chr ""enum"" : Factor w/ 719 levels
  ..$ : chr ""enum"" : Factor w/ 678 levels
  ..$ : chr ""enum"" : Factor w/ 478 levels



The above results are exactly the same between v3.10.2.1 (smaller model written to disk: 169M) and v3.10.5.1 (larger model written to disk: 37G).


The actual GBM training uses 
nbins <- 37
:


numCats <- n_distinct(as.matrix(dplyr::select_(df.train,response)))
numCats
[1] 37

iret <- h2o.gbm(x = predictors, y = response, training_frame = train.hex,
          validation_frame = holdout.hex, distribution=""multinomial"",
          ntrees = 3000, learn_rate = 0.01, max_depth = 5, nbins = numCats,
          model_id = basename_model)","['r', 'linux', 'h2o']",Unknown,,N/A
44738316,44738316,2017-06-24T16:05:08,2017-06-26 20:26:17Z,0,"So this may be odd, but H2O doesn't seem to be accessing the memory available to it in my cluster.  It is supposed to be using 24 cores and 512GB total RAM but only sees 26.67 Gb of RAM (but all 24 cores).  I am running H2o in R.


h2o.init(nthreads=-1,max_mem_size = ""500g"")

H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
/tmp/Rtmpx9ndSU/h2o_ra2816_started_from_r.out
/tmp/Rtmpx9ndSU/h2o_ra2816_started_from_r.err

openjdk version ""1.8.0_121""
OpenJDK Runtime Environment (build 1.8.0_121-b13)
OpenJDK 64-Bit Server VM (build 25.121-b13, mixed mode)

Starting H2O JVM and connecting: . Connection successful!

R is connected to the H2O cluster: 
H2O cluster uptime:         1 seconds 304 milliseconds 
H2O cluster version:        3.10.5.2 
H2O cluster version age:    4 days  
H2O cluster name:           H2O_started_from_R_ra2816_qpf255 
H2O cluster total nodes:    1 
H2O cluster total memory:   26.67 GB 
H2O cluster total cores:    24 
H2O cluster allowed cores:  24 
H2O cluster healthy:        TRUE 
H2O Connection ip:          localhost 
H2O Connection port:        54321 
H2O Connection proxy:       NA 
H2O Internal Security:      FALSE 
R Version:                  R version 3.3.2 (2016-10-31) 



I would appreciate any help or ideas about the issue.","['r', 'cluster-computing', 'h2o']",Raag Agrawal,https://stackoverflow.com/users/7782613/raag-agrawal,146
44735518,44735518,2017-06-24T10:43:06,2018-08-23 12:05:58Z,0,"I've been using h2o.gbm for a classification problem, and wanted to understand a bit more about how it calculates the class probabilities. As a starting point, I tried to recalculate the class probability of a gbm with only 1 tree (by looking at the observations in the leafs), but the results are very confusing.


Let's assume my positive class variable is ""buy"" and negative class variable ""not_buy"" and I have a training set called ""dt.train"" and a separate test-set called ""dt.test"".


In a normal decision tree, the class probability for ""buy"" P(has_bought=""buy"") for a new data row (test-data) is calculated by dividing all observations in the leaf with class ""buy"" by the total number of observations in the leaf (based on the training data used to grow the tree). 


However, the h2o.gbm seems to do something differently, even when I simulate a 'normal' decision tree (setting n.trees to 1, and alle sample.rates to 1). I think the best way to illustrate this confusion is by telling what I did in a step-wise fashion.


Step 1: Training the model


I do not care about overfitting or model performance. I want to make my life as easy as possible, so I've set the n.trees to 1, and make sure all training-data (rows and columns) are used for each tree and split, by setting all sample.rate parameters to 1. Below is the code to train the model.


    base.gbm.model <- h2o.gbm(
      x = predictors,
      y = ""has_bought"",
      training_frame = dt.train,
      model_id = ""2"",
      nfolds = 0,
      ntrees = 1,
      learn_rate = 0.001,
      max_depth = 15,
      sample_rate = 1,
      col_sample_rate = 1,
      col_sample_rate_per_tree = 1,
      seed = 123456,
      keep_cross_validation_predictions = TRUE,
      stopping_rounds = 10,
      stopping_tolerance = 0,
      stopping_metric = ""AUC"",
      score_tree_interval = 0
    )



Step 2: Getting the leaf assignments of the training set


What I want to do, is use the same data that is used to train the model, and understand in which leaf they ended up in. H2o offers a function for this, which is shown below.


    train.leafs <- h2o.predict_leaf_node_assignment(base.gbm.model, dt.train)



This will return the leaf node assignment (e.g. ""LLRRLL"") for each row in the training data. As we only have 1 tree, this column is called ""T1.C1"" which I renamed to ""leaf_node"", which I cbind with the target variable ""has_bought"" of the training data. This results in the output below (from here on referred to as ""train.leafs"").




Step 3: Making predictions on the test set


For the test set, I want to predict two things:




The prediction of the model itself P(has_bought=""buy"")


The leaf node assignment according to the model.


test.leafs <- h2o.predict_leaf_node_assignment(base.gbm.model, dt.test)
test.pred <- h2o.predict(base.gbm.model, dt.test)





After finding this, I've used cbind to combine these two predictions with the target variable of the test-set.


    test.total <- h2o.cbind(dt.test[, c(""has_bought"")], test.pred, test.leafs)



The result of this, is the table below, from here on referred to as ""test.total""




Unfortunately, I do not have enough rep point to post more than 2 links. But if you click on ""table ""test.total"" combined with manual
  probability calculation"" in step 5, it's basically the same table
  without the column ""manual_prob_buy"".




Step 4: Manually predicting probabilities


Theoretically, I should be able to predict the probabilities now myself. I did this by writing a loop, that loops over each row in ""test.total"". For each row, I take the leaf node assignment.


I then use that leaf-node assignment to filter the table ""train.leafs"", and check how many observations have a positive class (has_bought == 1) (posN) and how many observations are there in total (totalN) within the leaf associated with the test-row.


I perform the (standard) calculation posN / totalN, and store this in the test-row as a new column called ""manual_prob_buy"", which should be the probability of P(has_bought=""buy"") for that leaf. Thus, each test-row that falls in this leaf should get this probability. 
This for-loop is shown below.


    for(i in 1:nrow(dt.test)){
      leaf <-  test.total[i, leaf_node] 
      totalN <- nrow(train.leafs[train.leafs$leaf_node == leaf])
      posN <- nrow(train.leafs[train.leafs$leaf_node == leaf & train.leafs$has_bought == ""buy"",])
      test.total[i, manual_prob_buy :=  posN / totalN]
    }



Step 5: Comparing the probabilities


This is where I get confused. Below is the the updated ""test.total"" table, in which ""buy"" represents the probability P(has_bought=""buy"") according to the model and ""manual_prob_buy"" represents the manually calculated probability from step 4. As for as I know, these probabilities should be identical, knowing I only used 1 tree and I've set the sample.rates to 1.


Table ""test.total"" combined with manual probability calculation



The Question


I just don't understand why these two probabilities are not the same. As far as I know, I've set the parameters in such a way that it should just be like a 'normal' classification tree. 


So the question: does anyone know why I find differences in these probabilities?


I hope someone could point me to where I might have made wrong assumptions. I just really hope I did something stupid, as this is driving me crazy.


Thanks!","['r', 'classification', 'probability', 'h2o', 'gbm']",Unknown,,N/A
44729632,44729632,2017-06-23T20:41:54,2017-06-23 21:06:30Z,0,"Is there a way to keep the directory 
ice_root
 (eg, 
/tmp/RtmpabcDEFGH
) when H2O exits?




H2O v3.10.5.1


R v3.3.2


Linux 3.10.0-327.el7.x86_64 GNU/Linux




I am not running yarn.","['r', 'linux', 'h2o']",BA88,https://stackoverflow.com/users/7733787/ba88,79
44725587,44725587,2017-06-23T15:55:38,2017-06-23 18:20:39Z,179,"I want to launch a jar on spark that use h20 libraries. I created a scala project on intellij and added a package com.hw inside src/scala. Then I wrote a simple Scala program.


Here is the file sbt


name := ""h20prova""
version := ""1.0""
scalaVersion := ""2.12.1""
libraryDependencies ++= Seq(
""org.apache.spark"" % ""spark-core_2.10"" % ""1.5.1"",
""ai.h2o""%""sparkling-water-core_2.11""%""2.1.9"")



The project can see its library without any problem.


I created an artifact specifying the main class com.hw.h20try and builded the artifact succesfully.


Then i launched the script on spark using this command


spark-submit --class ""com.hw.h20try"" h20prova.jar



and got the following error


java.lang.ClassNotFoundException: com.hw.h20try
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at org.apache.spark.util.Utils$.classForName(Utils.scala:229)
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:695)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)



I am pretty sure it's a dependency problem because I had a similar problem using Java and i solved it adding the compiled jar to the artifact build. But I cannot seem to recreate what i did with Java on Scala.


There is another way to setup a Scala project and run on Spark in a few minutes?","['scala', 'intellij-idea', 'h2o']",Andrea Salvoni,https://stackoverflow.com/users/7774772/andrea-salvoni,21
44720647,44720647,2017-06-23T11:43:47,2017-09-05 18:16:37Z,0,"So, 


I already installed Nvidia drivers on my ubuntu 16.4 and CUDA and CUDNN. I fixed all problems on this, now it's fine.


It's already setup up and the installation went well.


I want to use h2o.deepwater in R. My code:


dlmodel <-  h2o.deepwater(x=predictors, y=response,
                         training_frame=train, 
                         validation_frame=valid, 

                         hidden=c(120,80), epochs=200, 
                         activation=""Tanh"",

                         seed=1234567,
                         stopping_metric='MSE',
                         classification_stop=0.001,
                         stopping_rounds = 5,
                         backend=""tensorflow"")



But I am getting this error 
java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: null


Can anyone help me fix this? I already re-installed R h2o and my gpu drivers.","['r', 'machine-learning', 'deep-learning', 'nvidia', 'h2o']",Unknown,,N/A
44720139,44720139,2017-06-23T11:16:49,2017-06-23 12:47:40Z,0,"This is my first time with deep learning using H2O Package in R. When i try to build the model, it shows error which i am unable to correct.


Here is my data


> head(d_1)
               x vibration_x    Speed
21892 1497340740       0.260 1224.601
21893 1497340800       0.100 1214.440
21894 1497340860       0.840 1218.984
21895 1497340920       0.125 1207.892
21896 1497340980       0.475 1206.744
21897 1497341040       0.025 1241.057



I am converting this dataframe to H2O type.


d_Hex_new <- as.h2o(d_1, destination_frame = ""path_train"")
> d_Hex_new
           x vibration_x    Speed
1 1497340740       0.260 1224.601
2 1497340800       0.100 1214.440
3 1497340860       0.840 1218.984
4 1497340920       0.125 1207.892
5 1497340980       0.475 1206.744
6 1497341040       0.025 1241.057

[8500 rows x 3 columns] 



When i try to fit the model, it shows error


model <- h2o.deeplearning(x = Speed, y = vibration_x, data =  d_Hex_new,
                                                      validation = v_Hex_new,
                                                      activation = ""Rectifier"",
                                                      hidden = c(50, 50, 50),
                                                      epochs = 100,
                                                      classification = FALSE,
                                                      balance_classes = FALSE)





Error in h2o.deeplearning(x = Speed, y = vibration_x, data =
  d_Hex_new,  :    unused argument (data = d_Hex_new)




May i know Why it is not taking the H2Oframe data? 
Please help me to correct it. Thanks,D


Edited:


model <- h2o.deeplearning(x = Speed, y = vibration_x, training_frame =  d_Hex_new,
                          validation_frame = v_Hex_new,
                                                      activation = ""Rectifier"",
                                                      hidden = c(50, 50, 50),
                                                      epochs = 100,
                                                      balance_classes = FALSE)





Error in is.H2OFrame(x) : object 'Speed' not found","['r', 'time-series', 'deep-learning', 'h2o']",Unknown,,N/A
44717507,44717507,2017-06-23T09:10:31,2017-06-23 16:35:46Z,753,I see that recently xgboost has been integrated within the H2O ecosystem. Nevertheless lack of documentation appears in the H2O. In particular I wonder whether It is possible to run a grid search on all xgboost params using the h2o.grid.,"['xgboost', 'h2o']",Giorgio Spedicato,https://stackoverflow.com/users/1259856/giorgio-spedicato,"2,483"
44700570,44700570,2017-06-22T13:19:23,2017-06-22 13:19:23Z,0,"First I have data (
hex_glrm
) that contains 4938 columns (features) to fit the model. After fitting, I download and save the model in MOJO format to the folder 
tmp
.


script.R


...
model_glrm <- tryCatch({
  h2o.glrm(hex_glrm,
           cols = names(hex_glrm),
           model_id = ""model_glrm"",
           transform = ""STANDARDIZE"",
           loss = ""Huber"",
           regularization_x = ""NonNegative"",
           regularization_y = ""UnitOneSparse"",
           k = 100,
           seed = 2017,
           gamma_x = 0.2,
           gamma_y = 0.2,
           init = ""Random"",
           svd_method = ""GramSVD"",
           impute_original = TRUE,
           max_runtime_secs = 1200)
}, error = function(err) {
  flog.error(err)
  NULL
})

if (!is.null(model_glrm)) {
  flog.info("" - GLRM model is fitted!"")

  # Download generated MOJO
  h2o.download_mojo(model_glrm, path = ""/tmp"", get_genmodel_jar = TRUE)
}
...



Then I create 
EasyPredictModelWrapper
 and load 
model_glrm.zip
. After that I get new user data for prediction and fill it within 
RowData
. Then I get the prediction (
drmp
) and create the json for response.


PredictionServlet.java


...
// Get the MOJO model and the prediction
EasyPredictModelWrapper model = new EasyPredictModelWrapper(MojoModel.load(""tmp/model_glrm.zip""));
RowData row = getRowDataFromJson(json); // New data for prediction (4938 features)
DimReductionModelPrediction drmp = model.predictDimReduction(row);

// Create the json response 
JsonObject json = new JsonObject();       
StringBuilder builder = new StringBuilder();
for (double i : drmp.dimensions) {
      builder.append(i);
      builder.append("", "");
}
json.addProperty(""recommendations"", builder.toString());
json.addProperty(""size"", drmp.dimensions.length);    

// Emit the prediction to the servlet response
response.getWriter().write(json.toString());
response.setContentType(""application/json; charset=UTF-8"");
response.setHeader(""Access-Control-Allow-Origin"", ""*"");
response.setStatus(HttpServletResponse.SC_OK);
...



response.json


{
""recommendations"":""1.2323, 2.3423, ..."",
""size"":100
}


But when I get the prediction 
drmp.dimensions
 contains only 100 values. How to get the prediction with all reconstructed features (4938)?","['java', 'r', 'h2o', 'mojo']",Alexander,https://stackoverflow.com/users/8199118/alexander,21
44690236,44690236,2017-06-22T04:36:43,2019-06-18 08:41:53Z,0,"I have two h2o frames and I want to join them based on one identical column exist in both, I am using Java API and get the h2o frames from spark dataframes.


    H2OFrame trainDataFrame = h2oContext.asH2OFrame(train_validation); 
    H2OFrame validationDataFrame = h2oContext.asH2OFrame(train_validation);
    H2OFrame testDataFrame = h2oContext.asH2OFrame(testSparkDataFrame); 



I can use spark dataframes to join data as my data is really big and RDD can work out here so I need to work with h2o frames as an in-memory object.",['h2o'],Luckylukee,https://stackoverflow.com/users/7886897/luckylukee,585
44683514,44683514,2017-06-21T18:22:05,2017-06-22 20:06:18Z,0,"Lets say I have a function 'getData()' which returns data (see of it as a data stream). Now I need to form a h2o data frame with these data. I need to insert them as a new row only if it is not present in the data frame before.


One obvious way is to do :




There is a global h2o data frame


Create a h2o data frame (of 1 row) from the arrived data. (I am using as.h2o())


Check if it is already present in the global data frame (using h2o.which() or any other function)


If it is not present then add it to the data frame (using h2o.rbind())




The above solution is too slow. Creation of h2o data frame every time the data arrives (2nd step) is taking too much time. (Only tested on small dataset)


I was also thinking of storing them in a R data frame and then using h2o.rbind() after some intervals. 


What is the best (time is the priority) way to do it?","['r', 'h2o']",Unknown,,N/A
44668298,44668298,2017-06-21T06:26:18,2017-06-21 17:09:55Z,0,"I have a dataset having a large missing values (more than 40% missing). Genrated a model in xgboost and H2o gradient boosting - got a decent model in both cases. However, the xgboost shows this variable as one of the key contributors to the model but as per H2o Gradient Boosting the variable is not important. Does xgboost handle variables with missing values differently. All the configuration to both the models are exactly the same.","['xgboost', 'h2o']",Arun Lakhotia,https://stackoverflow.com/users/7984336/arun-lakhotia,51
44658942,44658942,2017-06-20T16:53:11,2017-11-18 00:41:25Z,0,"My dataset looks like this:


rownum  a        b      y  x
1     |  A   |   a    |1 | a
2     |  B   |   a    |1 | a
3     |  C   |   a    |1 | a
4     |  D   |   a    |0 | b
5     |  E   |   a    |0 | a
6     |  F   |   a    |0 | b



I want to create many h2o.frames that are based on tissue identity. Like this:


a: 


rownum  a        b     y    x
1     |  A   |   a    |1 | a
2     |  D   |   a    |0 | a
3     |  F   |   a    |0 | a



b:


rownum  a      b       y  x
1     |  B   |   a    |1 | b
2     |  C   |   a    |1 | b
3     |  E   |   a    |0 | b



While I am currently doing it manually, that becomes difficult when I add more tissues to the dataset.


I also want to then push those h2o.frames to h2o.glm and iteratively save the model.


""INSERT x NAME HERE"" = h2o.glm(y = ""y"", x = 
c(""a"",""b""), 
training_frame = ITERATE H2O FRAMES HERE, family = 'poisson')



and then save the model


INSERT x NAME HERE <- h2o.saveModel(object= INSERT x NAME 
HERE, force=TRUE)



I would appreciate any help or advice you might have. I do know about interaction terms in GLM, but would like to do this for now.","['r', 'regression', 'glm', 'h2o']",Unknown,,N/A
44641158,44641158,2017-06-19T22:54:38,2017-11-09 11:14:31Z,0,"When I call the following reproducible doce:


install.packages(""h2o"", type = ""source"", repos =
           ""http://h2o-release.s3.amazonaws.com/h2o/rel-ueno/8/R"")
library(rsparkling)
library(h2o)
library(sparklyr)
library(dplyr)
library(DBI)
library(readr)
library(stringr)

conf <- spark_config()
conf$'sparklyr.shell.executor-memory' <- ""460G""
conf$'sparklyr.shell.driver-memory' <- ""460G""
conf$spark.executor.cores <- 32
conf$spark.executor.memory <- ""460G""
conf$spark.executor.instances <- 8
conf$spark.dynamicAllocation.enabled <- ""false""
conf$maximizeResourceAllocation <- ""true""
conf$spark.default.parallelism <- 32

sc <- spark_connect(master = ""local"", config = conf, version = ""2.0.2"")
h2o_context(sc ,strict_version_check = T)
h2o.init(nthreads = -1)

NFLPlayers <- c('Von Miller','Christian McCaffrey','Joe Thomas')
salary <- c(21000, 23400, 26800)
Valuable = c(1,1,1)
Rdata <- data.frame(NFLPlayers, salary, Valuable)
system.time(Test <- copy_to(sc, Rdata, ""Rdata""))
Test1 = as_h2o_frame(sc, Test, strict_version_check = FALSE)
Test1$Valuable<-as.factor(Test1$Valuable)



On an H2O dataframe I get the following error.


 ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://127.0.0.1:54321/99/Rapids)

 Error: lexical error: invalid char in json text.
                                   <html> <head> <meta http-equiv=
                 (right here) ------^



This is brand new and it is not only happening in my computer but also on that of a colleague. I have restarted the cluster. Reinstalled sparklyr and Rsparkling. I have also changed versions and am constantly getting the same error. Any ideas?


Some of the logs:


06-19 22:46:24.991 127.0.0.1:54321       2158   #2868-244 INFO: POST /99/Rapids, parms: {ast=(tmp= RTMP_sid_8db5_1 (:= frame_rdd_58_b5fd354222706e76381948c109884d05 (as.factor (cols frame_rdd_58_b5fd354222706e76381948c109884d05 [8])) [8] [])), session_id=_sid_8db5}
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR: java.lang.NoClassDefFoundError: no/priv/garshol/duke/Comparator
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at water.rapids.Env.<clinit>(Env.java:278)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at water.rapids.Session.exec(Session.java:86)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at water.rapids.Rapids.exec(Rapids.java:93)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at water.api.RapidsHandler.exec(RapidsHandler.java:41)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at java.lang.reflect.Method.invoke(Method.java:498)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at water.api.Handler.handle(Handler.java:63)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at water.api.RequestServer.serve(RequestServer.java:448)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at water.api.RequestServer.doGeneric(RequestServer.java:297)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at water.api.RequestServer.doPost(RequestServer.java:223)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:183)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.Server.handle(Server.java:370)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at java.lang.Thread.run(Thread.java:748)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR: Caused by: java.lang.ClassNotFoundException: no.priv.garshol.duke.Comparator
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
06-19 22:46:25.081 127.0.0.1:54321       2158   #2868-244 ERRR:     ... 37 more","['r', 'h2o', 'sparklyr', 'sparkling-water']",Sébastien Rochette,https://stackoverflow.com/users/7405597/s%c3%a9bastien-rochette,"6,661"
44628702,44628702,2017-06-19T10:55:52,2017-06-21 09:00:32Z,0,"for example i have this code:


import h2o
h2o.init()
from h2o.estimators.naive_bayes import H2ONaiveBayesEstimator
data = h2o.import_file(""myfile.csv"")
train,valid = data.split_frame([0.8])
predictors = ['col1','col2','col3','col4']
result = 'col_result'
model = H2ONaiveBayesEstimator()
model.train(predictors,result,training_frame=train,validation_frame=valid)
model.model_performance()



After the 
model.model_performance()
, i will able to see our model performance, but now i want to send over API as JSON data, is there any way i can do the converting in easy way? Thank you.",['h2o'],Hans Yulian,https://stackoverflow.com/users/2743726/hans-yulian,"1,170"
44619151,44619151,2017-06-18T20:16:57,2017-06-19 16:31:41Z,0,"I am using h2o package for modelling in R. For this I want to read a dataset which has a size of about 1.5 GB using h2o.importfile(). I start the h2o server using the lines


library(h2oEnsemble)
h2o.init(max_mem_size = '1499m',nthreads=-1)



This produces a log


H2O is not running yet, starting it now...
java version ""1.8.0_121""
Java(TM) SE Runtime Environment (build 1.8.0_121-b13)
Java HotSpot(TM) Client VM (build 25.121-b13, mixed mode)

Starting H2O JVM and connecting: . Connection successful!

R is connected to the H2O cluster: 
H2O cluster uptime:         3 seconds 665 milliseconds 
H2O cluster version:        3.10.4.8 
H2O cluster version age:    28 days, 14 hours and 36 minutes  
H2O cluster name:           H2O_started_from_R_Lucifer_jvn970 
H2O cluster total nodes:    1 
H2O cluster total memory:   1.41 GB 
H2O cluster total cores:    4 
H2O cluster allowed cores:  4 
H2O cluster healthy:        TRUE 
H2O Connection ip:          localhost 
H2O Connection port:        54321 
H2O Connection proxy:       NA 
H2O Internal Security:      FALSE 
R Version:                  R version 3.3.2 (2016-10-31)` 



The following line gives me an error

train=h2o.importFile(path=normalizePath(""C:\\Users\\All data\\traindt.rds""))


DistributedException from localhost/127.0.0.1:54321, caused by java.lang.AssertionError

DistributedException from localhost/127.0.0.1:54321, caused by java.lang.AssertionError
at water.MRTask.getResult(MRTask.java:478)
at water.MRTask.getResult(MRTask.java:486)
at water.MRTask.doAll(MRTask.java:402)
at water.parser.ParseDataset.parseAllKeys(ParseDataset.java:246)
at water.parser.ParseDataset.access$000(ParseDataset.java:27)
at water.parser.ParseDataset$ParserFJTask.compute2(ParseDataset.java:195)
at water.H2O$H2OCountedCompleter.compute(H2O.java:1315)
at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
Caused by: java.lang.AssertionError
at water.parser.Categorical.addKey(Categorical.java:41)
at water.parser.FVecParseWriter.addStrCol(FVecParseWriter.java:127)
at water.parser.CsvParser.parseChunk(CsvParser.java:133)
at water.parser.Parser.readOneFile(Parser.java:187)
at water.parser.Parser.streamParseZip(Parser.java:217)
at water.parser.ParseDataset$MultiFileParseTask.streamParse(ParseDataset.java:907)
at water.parser.ParseDataset$MultiFileParseTask.map(ParseDataset.java:856)
at water.MRTask.compute2(MRTask.java:601)
at water.H2O$H2OCountedCompleter.compute1(H2O.java:1318)
at water.parser.ParseDataset$MultiFileParseTask$Icer.compute1(ParseDataset$MultiFileParseTask$Icer.java)
at water.H2O$H2OCountedCompleter.compute(H2O.java:1314)
... 5 more

Error: DistributedException from localhost/127.0.0.1:54321, caused by java.lang.AssertionError



Any help on how to fix this problem? 
Note: Assigning memory larger than 1499mb also gives me an error (cannot allocate memory). I am using a 16GB ram environment


Edit: I download the 64-bit version of Java and changed my file to a csv file. I was then able to assign max_mem_size to 5G and the problem was solved.


For others who face the problem:
1. Download the latest version of 64 bit jdk 
2. Execute the following line line 


h2o.init(max_mem_size = '5g',nthreads=-1)","['r', 'h2o']",Unknown,,N/A
44577923,44577923,2017-06-15T22:20:52,2020-09-26 02:44:35Z,0,"When I try to import h2o I am told that the package does not exist. When I try to install it, it tells me it already exists. I have tried wiping it out of my computer and reinstalling to no avail. At this point all I can think is some environment variable.


(C:\Users\Lanier\Anaconda2) C:\Users\Lanier>python
Python 2.7.12 |Anaconda custom (64-bit)| (default, Jun 29 2016, 11:07:13) [MSC v.1500 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
>>> import h2o
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named h2o
>>> quit()

(C:\Users\Lanier\Anaconda2) C:\Users\Lanier>conda install h2o
Fetching package metadata ...........
Solving package specifications: .

# All requested packages already installed.
# packages in environment at C:\Users\Lanier\Anaconda2:
#
h2o                       3.10.0.9                      0

(C:\Users\Lanier\Anaconda2) C:\Users\Lanier>","['python', 'python-2.7', 'cmd', 'anaconda', 'h2o']",mlanier,https://stackoverflow.com/users/6014437/mlanier,197
44558411,44558411,2017-06-15T04:10:01,2017-09-16 00:44:52Z,0,"I've got a rather small dataset (162,000 observations with 13 attributes)
that I'm trying to use for modelling, using h2o.GBM. The response variable is  categorical with large number of levels (~ 20,000 levels)
The model doesn't run out of memory or give any errors, but it's been going for nearly 24 hours without any progress (says 0% on H2o.GBM reporting)
I finally gave in and stopped it.
I'm wondering if there's anything wrong with my hyperparameters, as data is not particularly large.


here's my code:


library(h2o)
localH2O <- h2o.init(nthreads = -1, max_mem_size = ""12g"") 
train.h20 <- as.h2o(analdata_train) 

  gbm1 <- h2o.gbm(
                    y = response_var
                  , x = independ_vars
                  , training_frame = train.h20
                  , ntrees = 3    
                  , max_depth = 5  
                  , min_rows = 10  
                  , stopping_tolerance = 0.001    
                  , learn_rate = 0.1  
                  , distribution = ""multinomial"" 
  )","['r', 'h2o', 'categorical-data', 'gbm', 'bigdata']",Community,https://stackoverflow.com/users/-1/community,1
44555015,44555015,2017-06-14T21:28:08,2017-06-19 10:20:43Z,0,"I am trying to implement a very simple ML learning problem, where I use text to predict some outcome. In R, some basic example would be:


import some fake but funny text data


library(caret)
library(dplyr)
library(text2vec)

dataframe <- data_frame(id = c(1,2,3,4),
                        text = c(""this is a this"", ""this is 
                        another"",'hello','what???'),
                        value = c(200,400,120,300),
                        output = c('win', 'lose','win','lose'))

> dataframe
# A tibble: 4 x 4
     id            text value output
  <dbl>           <chr> <dbl>  <chr>
1     1  this is a this   200    win
2     2 this is another   400   lose
3     3           hello   120    win
4     4         what???   300   lose



Use 
text2vec
 to get a 
sparse
 matrix representation of my text (see also 
https://github.com/dselivanov/text2vec/blob/master/vignettes/text-vectorization.Rmd
)


#these are text2vec functions to tokenize and lowercase the text
prep_fun = tolower
tok_fun = word_tokenizer 

#create the tokens
train_tokens = dataframe$text %>% 
  prep_fun %>% 
  tok_fun

it_train = itoken(train_tokens)     
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, vectorizer)

> dtm_train
4 x 6 sparse Matrix of class ""dgCMatrix""
  what hello another a is this
1    .     .       . 1  1    2
2    .     .       1 .  1    1
3    .     1       . .  .    .
4    1     .       . .  .    .



Finally, train the algo (for instance, using 
caret
) to predict 
output
 using my sparse matrix.


mymodel <- train(x=dtm_train, y =dataframe$output, method=""xgbTree"")

> confusionMatrix(mymodel)
Bootstrapped (25 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)

          Reference
Prediction lose  win
      lose 17.6 44.1
      win  29.4  8.8

 Accuracy (average) : 0.264



My problem is:
 


I see how to import data into 
h20
 using 
spark_read_csv
, 
rsparkling
  and 
as_h2o_frame
. 
However, for points 2. and 3. above I am completely lost. 


Can someone please give me some hints or tell me if this approach is even possible with 
h2o
?


Many thanks!!","['r', 'apache-spark', 'h2o', 'sparklyr', 'text2vec']",Dmitriy Selivanov,https://stackoverflow.com/users/1069256/dmitriy-selivanov,"4,595"
44551431,44551431,2017-06-14T17:47:19,2017-08-08 18:35:56Z,466,"Do you guys already worked with serialized models in Sparkling Models ou export models like the Spark to put in production? How can I do that?!


Thanks in advance.


Flavio","['h2o', 'sparkling-water']",Flavio,https://stackoverflow.com/users/7024760/flavio,829
44551002,44551002,2017-06-14T17:19:48,2017-10-20 04:48:24Z,138,"This is a continuation of 


https://community.h2o.ai/questions/2165/error-logging-in.html

and 

https://community.h2o.ai/questions/1665/can-not-login-to-steam-osx.html


from my original post on community.h2o:
""""""
Hi there, I am currently trying to set up an an API on my local machine using steam.
The documentation is out of date on 
http://docs.h2o.ai/steam/latest-stable/steam-docs/Installation.html#steam-start-flags
 which says to start the steam service for the first time, execute ./steam serve master --admin-name=admin --admin-password=admin the first time around to set the password. The current command is ./steam serve master --superuser-name=admin --superuser-password=admin but when I try to log in to 0.0.0.0:9000 (which is what it asks me to point the browser to) I get 2017/06/14 13:03:48 User admin does not exist I tried using single, double and no quotes.
Thank you for your time!
""""""",['h2o'],Kyb3r,https://stackoverflow.com/users/7740703/kyb3r,49
44550808,44550808,2017-06-14T17:07:21,2020-03-06 11:02:45Z,0,"I'm making an R notebook with H2O and I don't want the H2O ""Connection Successful"" message and accompanying info (see below) to show. 


Connection successful!


R is connected to the H2O cluster:


H2O cluster uptime:


H2O cluster version:


H2O cluster version age:


H2O cluster name:


H2O cluster total nodes:


H2O cluster total memory:


H2O cluster total cores:


H2O cluster allowed cores:  4


H2O cluster healthy:


H2O Connection ip:


H2O Connection port:


H2O Connection proxy:


H2O Internal Security:


R Version:


I would appreciate any help!","['r', 'latex', 'r-markdown', 'h2o']",Raag Agrawal,https://stackoverflow.com/users/7782613/raag-agrawal,146
44540331,44540331,2017-06-14T09:09:02,2017-06-14 17:11:08Z,106,"I am trying to run H2O cluster on 4 nodes. I ran the following command: 


hadoop jar h2odriver.jar -nodes 4 -network 192.168.56.0/24 -mapperXmx 2g -output hdfsOutputDirName



But, it contacts an ip not listed on my flatfile and fails to come up,


H2O node 192.168.56.124:54321 requested flatfile
H2O node 192.168.56.101:54321 requested flatfile
H2O node 192.168.56.125:54321 requested flatfile
H2O node 192.168.56.123:54321 requested flatfile



The ip of my 4th node is 192.168.56.122 not 192.168.56.101. Is there an issue with using the 
-network
 flag?","['hadoop', 'h2o']",KrugerXO,https://stackoverflow.com/users/7832820/krugerxo,1
44533297,44533297,2017-06-13T23:55:37,2017-06-14 00:57:27Z,0,"I'm looking for an easy and efficient way to replace all of a certain value in a H2O Python data frame.   In this case this value is NULL.  My dataset contains a very substantial amount of NULLs.


My current way of doing it is extraordinarily slow when I have hundreds of columns in a very large dataset.  I assume there can be substantial improvements by doing this in a better way...


I just can't figure out the syntax.  Thanks, this will save me an enormous amount of time!


My current approach:


for each_col in table_names_list:
    h2o_df[h2o_df[each_col].isna(), each_col]=0","['python', 'if-statement', 'null', 'h2o']",jack,https://stackoverflow.com/users/8157418/jack,102
44527904,44527904,2017-06-13T17:24:33,2018-11-10 00:58:50Z,725,"When I try to convert from spark dataframe to H2O data frame I get the error below. This seems to have to do with the size of the dataframe because when I make it smaller the converter between spark and H2O works well. 


Are there any configurations that need to be changed in order to convert large spark dataframes to H2O using sparkling water? In my configuration I am allowing max memory to the driver and executor so this is not a memory issue.


I am using R here the code is:


training<-as_h2o_frame(sc, final1, strict_version_check = FALSE)



Error:


Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 95.1 failed 4 times, most recent failure: Lost task 4.3 in stage 95.1 (TID 4050, 10.0.0.9): java.lang.ArrayIndexOutOfBoundsException: 65535
                at water.DKV.get(DKV.java:202)
                at water.DKV.get(DKV.java:175)
                at water.Key.get(Key.java:83)
                at water.fvec.Frame.createNewChunks(Frame.java:896)
                at water.fvec.FrameUtils$class.createNewChunks(FrameUtils.scala:43)
                at water.fvec.FrameUtils$.createNewChunks(FrameUtils.scala:70)
                at org.apache.spark.h2o.backends.internal.InternalWriteConverterCtx.createChunks(InternalWriteConverterCtx.scala:29)
                at org.apache.spark.h2o.converters.SparkDataFrameConverter$.org$apache$spark$h2o$converters$SparkDataFrameConverter$$perSQLPartition(SparkDataFrameConverter.scala:95)
                at org.apache.spark.h2o.converters.SparkDataFrameConverter$$anonfun$toH2OFrame$1$$anonfun$apply$2.apply(SparkDataFrameConverter.scala:74)
                at org.apache.spark.h2o.converters.SparkDataFrameConverter$$anonfun$toH2OFrame$1$$anonfun$apply$2.apply(SparkDataFrameConverter.scala:74)
                at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
                at org.apache.spark.scheduler.Task.run(Task.scala:86)
                at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
                at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
                at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)
                at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)
                at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)
                at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
                at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
                at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)
                at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
                at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
                at scala.Option.foreach(Option.scala:257)
                at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
                at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)
                at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)
                at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)
                at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
                at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
                at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)
                at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)
                at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906)
                at org.apache.spark.h2o.converters.WriteConverterCtxUtils$.convert(WriteConverterCtxUtils.scala:83)
                at org.apache.spark.h2o.converters.SparkDataFrameConverter$.toH2OFrame(SparkDataFrameConverter.scala:74)
                at org.apache.spark.h2o.H2OContext.asH2OFrame(H2OContext.scala:145)
                at org.apache.spark.h2o.H2OContext.asH2OFrame(H2OContext.scala:143)
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:498)
                at sparklyr.Invoke$.invoke(invoke.scala:102)
                at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
                at sparklyr.StreamHandler$.read(stream.scala:54)
                at sparklyr.BackendHandler.channelRead0(handler.scala:49)
                at sparklyr.BackendHandler.channelRead0(handler.scala:14)
                at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
                at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
                at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
                at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
                at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
                at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
                at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
                at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 65535
                at water.DKV.get(DKV.java:202)
                at water.DKV.get(DKV.java:175)
                at water.Key.get(Key.java:83)
                at water.fvec.Frame.createNewChunks(Frame.java:896)
                at water.fvec.FrameUtils$class.createNewChunks(FrameUtils.scala:43)
                at water.fvec.FrameUtils$.createNewChunks(FrameUtils.scala:70)
                at org.apache.spark.h2o.backends.internal.InternalWriteConverterCtx.createChunks(InternalWriteConverterCtx.scala:29)
                at org.apache.spark.h2o.converters.SparkDataFrameConverter$.org$apache$spark$h2o$converters$SparkDataFrameConverter$$perSQLPartition(SparkDataFrameConverter.scala:95)
                at org.apache.spark.h2o.converters.SparkDataFrameConverter$$anonfun$toH2OFrame$1$$anonfun$apply$2.apply(SparkDataFrameConverter.scala:74)
                at org.apache.spark.h2o.converters.SparkDataFrameConverter$$anonfun$toH2OFrame$1$$anonfun$apply$2.apply(SparkDataFrameConverter.scala:74)
                at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
                at org.apache.spark.scheduler.Task.run(Task.scala:86)
                at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
                ... 1 more","['apache-spark', 'h2o', 'sparklyr', 'sparkling-water']",Levi Brackman,https://stackoverflow.com/users/5464587/levi-brackman,355
44521080,44521080,2017-06-13T12:07:33,2019-08-19 13:13:46Z,0,I am very beginner in h2o and I want to know if there is any attribute selection capabilities in h2o framework so to be applied in h2oframes?,['h2o'],Unknown,,N/A
44495091,44495091,2017-06-12T08:48:56,2017-06-14 17:15:56Z,0,"I'm a newbie in H2O and also AWS.
I have completed couples of GLM model in H2O and would want to deploy it on AWS lambda. Those model consume more or less same source of data which will be injecting in JSON from API Gateway.


What is the most optimal way to score these model in parallel and sum the result from each GLM within one Lambda function?
- Putting each POJO class sequentially in the Lambda is OK?
- there would be some data transformation script needed to transform data into a shape that required by POJO model with some table lookup in Redis
- After 5 POJO scoring is done, need to sum the score and return this out to S3


Is this setup is possible? Or there are other way round is more suitable for this use case. The scoring of GLM(s) model and the result will be deploy for web application and serving user in real-time.","['amazon-web-services', 'aws-lambda', 'h2o']",jedakyu,https://stackoverflow.com/users/8147769/jedakyu,13
44490925,44490925,2017-06-12T03:14:44,2017-06-13 11:33:08Z,386,"My goal is to create a DRF model in H2O with the TRAIN, VALIDATION and TEST datasets I have and predict the RMSE, R2, MSE etc on the TEST model.


Below is the piece of code:


DRFParameters rfParms = (DRFParameters) algParameter;
rfParms._response_column = trainDataFrame._names[responseColumn(trainDataFrame)]; //The response column 

rfParms._train = trainDataFrame._key;
//rfParms._valid = testDataFrame._key;
rfParms._nfolds = 5;
DRF job = new DRF(rfParms);
DRFModel drf = job.trainModel().get(); // Train the model
Frame pred = drf.score(testDataFrame); //Score the test



Here I don't know how to proceed with in finding the predictions (R2, RMSE, MSE, MAE etc) after scoring.


Could you please help in H2O DRF modeling and predictions calculation using JAVA?","['algorithm', 'apache-spark', 'h2o']",Dat Nguyen,https://stackoverflow.com/users/751145/dat-nguyen,"1,646"
44461925,44461925,2017-06-09T15:46:36,2017-06-09 15:46:36Z,189,"I'm trying to start a H2O context in pyspark using 
H2OContext.getOrCreate(sc)
. With the python packages h2o=3.10.4.8 and h2o-pysparkling-1.6=1.6.8 this works as expected (packages installed using pip), however with h2o-pysparkling-1.6.11 I get the following error:


Py4JJavaError: An error occurred while calling o100.invoke.
: java.util.ServiceConfigurationError: org.apache.spark.h2o.AnnouncementProvider: Provider org.apache.spark.h2o.RestAnnouncementProvider could not be instantiated
    at java.util.ServiceLoader.fail(ServiceLoader.java:232)
    at java.util.ServiceLoader.access$100(ServiceLoader.java:185)
    at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)
    at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
    at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
    at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:42)
    at scala.collection.Iterator$class.toStream(Iterator.scala:1143)
    at scala.collection.AbstractIterator.toStream(Iterator.scala:1157)
    at scala.collection.TraversableOnce$class.toSeq(TraversableOnce.scala:261)
    at scala.collection.AbstractIterator.toSeq(Iterator.scala:1157)
    at org.apache.spark.h2o.AnnouncementServiceFactory$.create(AnnouncementService.scala:73)
    at org.apache.spark.h2o.H2OContext.<init>(H2OContext.scala:66)
    at org.apache.spark.h2o.H2OContext$.getOrCreate(H2OContext.scala:294)
    at org.apache.spark.h2o.H2OContext.getOrCreate(H2OContext.scala)
    at org.apache.spark.h2o.JavaH2OContext.getOrCreate(JavaH2OContext.java:191)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)
    at py4j.Gateway.invoke(Gateway.java:259)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:209)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoClassDefFoundError: org/apache/http/impl/client/HttpClientBuilder
    at org.apache.spark.h2o.RestAnnouncementProvider.<init>(AnnouncementService.scala:119)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at java.lang.Class.newInstance(Class.java:442)
    at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)
    ... 27 more
Caused by: java.lang.ClassNotFoundException: org.apache.http.impl.client.HttpClientBuilder
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 34 more



I'm using Python2.7, Spark 1.6.0 on YARN, CDH5.7.0","['python-2.7', 'apache-spark', 'h2o', 'sparkling-water']",Heuvel,https://stackoverflow.com/users/7011714/heuvel,1
44441556,44441556,2017-06-08T16:54:06,2017-11-30 08:25:28Z,0,"Here is some reproducible code. I want to know what the calculation is for the SE on a per feature basis when the features are one-hot encoded. If I'm to give it my own attempt:


It looks like some of the SEs are 1, which I guess would mean that the reconstruction was 100% sure it was one thing but it was actually the other. For the fractional errors, do they represent various degrees of wrongness with respect to the probability assigned to the category from the softmax classifier?


library(h2o)
art <- data.frame(a = c(""a"",""b"",""a"",""c"",""d"",""e"",""g"",""f"",""a""),
              b = c(""b"",""c"",""d"",""e"",""b"",""c"",""d"",""e"",""b""),
              c = c(4,3,2,5,6,1,2,3,5))

dl = h2o.deeplearning(x = c(""a"",""b"",""c""), training_frame = as.h2o(art),
                      autoencoder = TRUE,
                      reproducible = T,
                      seed = 1234,
                      hidden = c(1), epochs = 1)
sus.anon = h2o.anomaly(dl, as.h2o(art), per_feature=TRUE)","['r', 'h2o', 'autoencoder']",user2997345,https://stackoverflow.com/users/2997345/user2997345,131
44434050,44434050,2017-06-08T11:08:51,2017-06-08 11:32:55Z,679,"I really like H2O especially because you can deploy the built models easily into any Java / JVM application... This is also my goal for TensorFlow: Build models and then run them in Java applications.


H2O uses Spark (Sparking Water) ""in the middle"" when using TensorFlow by running TensorFlow on the distributed Spark nodes. I learned this (hopefully correctly) in a 
H2O demo video
.


Why do you not integrate TensorFlow (and others like MXNet) directly with H2O, but instead go through Apache Spark?
 


Frameworks like TensorFlow itself allow 
distributed training
, so why use Spark ""in the middle""? Doesn't this increase complexity a lot (and no need for it in many scenarios)?


For example, Google built 
Scikit Flow
 (Scikit-learn + TensorFlow)to allow easy usage of TensorFlow to build models. 


Especially for smaller data sets and / or simpler use cases, this seems to be the easier option that using Spark in the middle? If I understand correctly, you could also use this model in Java then via 
TensorFlow4Java
.


I want to leverage H2O much more in future projects and scenarios (like in the past, see e.g. here where I 
applied a H2O model to real time applications using Apache Kafka and its Streams API
). 
Though, 
I am not sure why I need the ""overhead"" of Spark for building models with H2O and TensorFlow
 (especially for smaller data sets and / or simple scenarios where a ""small neural network"" might be good enough?","['java', 'apache-spark', 'tensorflow', 'h2o', 'sparkling-water']",Kai Wähner,https://stackoverflow.com/users/538775/kai-w%c3%a4hner,"5,400"
44425647,44425647,2017-06-08T02:23:15,2017-07-13 15:51:44Z,0,"I was using H2O to build classification models like GBM, DRF and DL. 
The dataset I have contains a few categorical columns, and if I want to use them as features for building models do I need to manually convert them into dummy variables? I read that GBM can dummify the categorical variables internally?","['random-forest', 'categorical-data', 'h2o', 'gbm']",Selena,https://stackoverflow.com/users/8085473/selena,243
44424083,44424083,2017-06-07T23:00:50,2019-09-11 23:21:06Z,0,"I am having trouble loading in a data set to h2o. I keep getting the same H2o server error when I was able to run this same code without issue yesterday.


import h2o
h2o.init()

h2o.import_file('train_csv')



The train_csv is in the same directory and the is saved in the lines above from a spark dataframe. I continue to get the same error:




H2OServerError                            Traceback (most recent call last)
<ipython-input-3-d9a5a8533622> in <module>()
----> 1 h2o.import_file('train_csv')

/home/.local/lib/python3.5/site-packages/h2o/h2o.py in import_file(path, 
destination_frame, parse, header, sep, col_names, col_types, na_strings, 
pattern)
    405         return lazy_import(path, pattern)
    406     else:
--> 407         return H2OFrame()._import_parse(path, pattern, 
destination_frame, header, sep, col_names, col_types, na_strings)
    408 
    409 

/home/.local/lib/python3.5/site-packages/h2o/frame.py in _import_parse(self, 
path, pattern, destination_frame, header, separator, column_names, 
column_types, na_strings)
    306             path = os.path.abspath(path)
    307         rawkey = h2o.lazy_import(path, pattern)
--> 308         self._parse(rawkey, destination_frame, header, separator, 
column_names, column_types, na_strings)
    309         return self
    310 

/home/.local/lib/python3.5/site-packages/h2o/frame.py in _parse(self, 
rawkey, destination_frame, header, separator, column_names, column_types, 
na_strings)
    319     def _parse(self, rawkey, destination_frame="""", header=None, 
separator=None, column_names=None, column_types=None,
    320                na_strings=None):
--> 321         setup = h2o.parse_setup(rawkey, destination_frame, header, 
separator, column_names, column_types, na_strings)
    322         return self._parse_raw(setup)
    323 

/home/.local/lib/python3.5/site-packages/h2o/h2o.py in 
parse_setup(raw_frames, destination_frame, header, separator, column_names, 
column_types, na_strings)
    550         kwargs[""separator""] = ord(separator)
    551 
--> 552     j = api(""POST /3/ParseSetup"", data=kwargs)
    553     if ""warnings"" in j and j[""warnings""]:
    554         for w in j[""warnings""]:

/home/.local/lib/python3.5/site-packages/h2o/h2o.py in api(endpoint, data, 
json, filename, save_to)
     97     # type checks are performed in H2OConnection class
     98     _check_connection()
---> 99     return h2oconn.request(endpoint, data=data, json=json, 
filename=filename, save_to=save_to)
    100 
    101 

/home/.local/lib/python3.5/site-packages/h2o/backend/connection.py in 
request(self, endpoint, data, json, filename, save_to)
   400                                     auth=self._auth, 
verify=self._verify_ssl_cert, proxies=self._proxies)
    401             self._log_end_transaction(start_time, resp)
--> 402             return self._process_response(resp, save_to)
    403 
    404         except (requests.exceptions.ConnectionError, 
requests.exceptions.HTTPError) as e:

/home/.local/lib/python3.5/site-packages/h2o/backend/connection.py in 
_process_response(response, save_to)
    728         # Note that it is possible to receive valid H2OErrorV3 
object in this case, however it merely means the server
    729         # did not provide the correct status code.
--> 730         raise H2OServerError(""HTTP %d %s:\n%r"" % (status_code, 
response.reason, data))
    731 
    732 

H2OServerError: HTTP 500 Server Error:
Server error water.util.DistributedException:
  Error: DistributedException from /127.0.0.1:54321
  Request: None","['python-3.x', 'h2o']",K.Thomas,https://stackoverflow.com/users/7763070/k-thomas,21
44400110,44400110,2017-06-06T21:28:02,2017-06-19 15:56:44Z,0,"I can connect from R to h2o cluster which is running from docker image but there is error appearing:


Version mismatch! H2O is running version 3.11.0.235 but h2o-R package is version 3.11.0.99999 .


I can run many tasks from R but for example i can not run XGB training which I can run with Flow.
I want to create more comprehensive algorithm but I also need R functions to it.


I could not find h2o R package in version 
3.11.0.235.
.
Is there option to use full potential of cluster h2o from R if cluster h2o is running from docker image?","['r', 'h2o']",Unknown,,N/A
44380318,44380318,2017-06-06T02:09:44,2017-06-06 02:09:44Z,208,"I am trying to get Python + deepwater + tensorflow to run on RHEL 6.7.  Using 
conda
, I have installed 
python 3.6.0
, 
tensorflow 1.1.0
 and also 
gcc 4.8.5
.  TF is working fine.


I have installed the following libraries using 
pip install
: 
h2o-3.11.0.3904-py2.py3-none-any.whl
 and 
h2o-3.11.0-py2.py3-none-any.whl
.


I tried to run the following example from the h2o tutorial


import h2o
from h2o.estimators.deepwater import H2ODeepWaterEstimator

h2o.init()

train = h2o.import_file(""https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz"")

features = list(range(0,784))
target = 784

train[target] = train[target].asfactor()

model = H2ODeepWaterEstimator(epochs=100, activation=""Rectifier"", hidden=[200,200], ignore_const_cols=False, 
    mini_batch_size=256, input_dropout_ratio=0.1, hidden_dropout_ratios=[0.5,0.5], stopping_rounds=3, 
    stopping_tolerance=0.05, stopping_metric=""misclassification"", score_interval=2, score_duty_cycle=0.5,
    score_training_samples=1000, score_validation_samples=1000, nfolds=5, gpu=False, seed=1234, backend=""tensorflow"")

model.train(x=features, y=target, training_frame=train)



The following exception is thrown


Exception: Unable to initialize the native Deep Learning backend: Cannot find TensorFlow native library for OS: linux, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/java/README.md for possible solutions (such as building the library from source).



Is there anything else that I am missing?  Would I need to build the bits from scratch for this platform?","['python', 'tensorflow', 'h2o']",ironv,https://stackoverflow.com/users/2789334/ironv,"1,058"
44378128,44378128,2017-06-05T21:41:03,2017-06-05 23:26:06Z,111,"I am trying to run to get R + deepwater + tensorflow to work on a MBP.


The following have been installed.


Python 3.6.1
TensorFlow 1.1



The 
Hello, TensorFlow
 example on the TensorFlow website is working fine.


R version 3.4.0

curl -O http://h2o-release.s3.amazonaws.com/h2o/master/3904/R/src/contrib/h2o_3.11.0.3904.tar.gz
R CMD INSTALL h2o_3.11.0.3904.tar.gz

curl -O http://s3.amazonaws.com/h2o-deepwater/public/nightly/latest/h2o_3.11.0.tar.gz
R CMD INSTALL h2o_3.11.0.tar.gz



I am trying run the following example provided on the h2o website.


require(h2o)

h2o.init()

train <- h2o.importFile(""https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz"")

target <- ""C785""
features <- setdiff(names(train), target)

train[target] <- as.factor(train[target])

model <- h2o.deepwater(x=features, y=target, training_frame=train, epochs=100, activation=""Rectifier"",
    hidden=c(200,200), ignore_const_cols=FALSE, mini_batch_size=256, input_dropout_ratio=0.1, 
    hidden_dropout_ratios=c(0.5,0.5), stopping_rounds=3, stopping_tolerance=0.05, 
    stopping_metric=""misclassification"", score_interval=2, score_duty_cycle=0.5, score_training_samples=1000, 
    score_validation_samples=1000, nfolds=5, gpu=FALSE, seed=1234, backend=""tensorflow"")



The error I get is 
Error: java.lang.RuntimeException: Unable to initialize backend: Cannot find TensorFlow native library for OS: darwin, architecture: x86_64.
  Based on what I read on SO and the git page, I was under the impression that one does not need to build for the Mac platform.  


One other thing that I tried was to use the info from 
https://github.com/rstudio/tensorflow
.  When I run 
install_tensorflow()
 I get 
Error: Prerequisites for installing TensorFlow not available.
  Please help!","['tensorflow', 'h2o']",ironv,https://stackoverflow.com/users/2789334/ironv,"1,058"
44377597,44377597,2017-06-05T20:58:30,2017-06-05 20:58:30Z,71,"In order to run the h2o 
RandomDiscreteValueWalker[DRFParameters]
 with deterministic results, is it sufficient to set the seed on the 
DRFParameters
 and the 
RandomDiscreteValueSearchCriteria
 ? I get non-deterministic results even when I have the seed fixed for these parameters?","['machine-learning', 'h2o', 'sparkling-water']",x89a10,https://stackoverflow.com/users/1367070/x89a10,691
44374389,44374389,2017-06-05T17:27:16,2017-06-06 01:04:24Z,119,"I am very new to H2O and to running models on hives. The reason I am considering H2O at this moment is that my understanding is that H2O helps to optimize data parsing during various modelling processes (such as k-means or logistic regression). My question is: is there a way for me to write my python (or R) k-means code and run it in H2O or the only way would be using the H2O pre-built process? If it's the later, then can I extract the final scoring code in order to schedule it for automated run for a regular scoring?
And if the first option is also possible ( I noticed the option 'import the code""), how would the parsing happen during the process (for instance, during data preparation, variable standardization, actual k-means scoring code, assigning the final segment rules)?


Thank you


Natalie","['k-means', 'modeling', 'h2o']",Nat,https://stackoverflow.com/users/6758949/nat,19
44369393,44369393,2017-06-05T12:52:49,2017-06-05 14:57:55Z,0,"I'm uploading a 
.csv
 file in R-H2o using 
h2o.importFile
. However, the date values are parsed incorrectly.


For example, with date time format 
YYYY-MM-DD hh:mm:ss
, (e.g. 
2016-06-16 12:30:00
), the result is always 
1466073000000
, which is incorrect.","['r', 'date', 'h2o']",gkubed,https://stackoverflow.com/users/975624/gkubed,"1,951"
44361273,44361273,2017-06-05T03:27:50,2017-06-20 03:21:25Z,907,"I'm doing a grid search on a classification model.  When the h20 server starts I get:


R is connected to the H2O cluster: 
    H2O cluster uptime:         9 minutes 35 seconds 
    H2O cluster version:        3.10.4.8 
    H2O cluster version age:    14 days, 4 hours and 1 minute  
    H2O cluster name:           H2O_started_from_R_Charles_huu844 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   21.31 GB 
    H2O cluster total cores:    8 
    H2O cluster allowed cores:  8 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    R Version:                  R version 3.2.2 (2015-08-14) 



When my model enters the grid search, I get the following error message:


ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/99/Grids/mygrid?sort_by=auc&decreasing=TRUE)

water.exceptions.H2OIllegalArgumentException
 [1] ""water.exceptions.H2OIllegalArgumentException: Invalid argument for sort_by specified. Must be one of: [r2, mean_per_class_accuracy, max_per_class_error, err, total_rows, rmse, accuracy, err_count, logloss, mse, mean_per_class_error]""
 [2] ""    hex.schemas.GridSchemaV99.fillFromImpl(GridSchemaV99.java:114)"" 



My R script is as follows:


rm(list=ls())
options(scipen=999) # remove E notation
ptm <- proc.time()

if (Sys.info()[""sysname""] == ""Windows"") {
  filePath = ""//bigsur/sm/Trending/model.csv""
  homedir = ""c:/sm/Trending/""
  setwd(""c:/sm/Trending/R"")
  sink(""C:/Users/Charles/Desktop/log.txt"")
} else {
  filePath = ""/Volumes/sm/Trending/model.csv""
  homedir = ""/Volumes/sm/Trending/""
  setwd(""/Volumes/sm/Trending/R"")
  sink(""~/Desktop/log.txt"")
}

#install.packages(""ggplot2"")
#install.packages(""dplyr"")

sink.reset <- function(){
  for(i in seq_len(sink.number())){
    sink(NULL)
  }
}

printf <- function(...) print(sprintf(...))

results = function(title, best_model) {
  best_params = best_model@parameters
  best_activation            <<- best_model@parameters$activation
  best_hidden                <<- best_model@parameters$hidden
  best_l1                    <<- best_model@parameters$l1
  best_l2                    <<- best_model@parameters$l2
  best_input_dropout_ratio   <<- best_model@parameters$input_dropout_ratio
  best_hidden_dropout_ratios <<- best_model@parameters$hidden_dropout_ratios

  printf("" "")
  printf(""%s"", title)
  best_model
  plot(best_model)
  h2o.performance(best_model)
  h2o.performance(best_model, valid = TRUE)
  h2o.mse(best_model, valid = TRUE)

  printf(""mse: %f"", best_model@model$validation_metrics@metrics$MSE)
  printf(""best activation: %s"", best_activation)
  cat(""best hidden layers: "", best_hidden, ""\n"")
  printf(""Best l1: %f"", best_l1)
  printf(""Best l2: %f"", best_l2)
  #printf(""best_input_dropout_ratio: %f"", best_input_dropout_ratio)
  #cat(""Best best_hidden_dropout_ratios: "", best_hidden_dropout_ratios, ""\n"")

  predictions = h2o.predict(best_model, test)
  summary(predictions, exact_quantiles=TRUE)
  predicted = predictions[,1]
  test_targets = test[, 5]
  correct = predicted == test_targets
  numCorrect = as.integer(sum(correct))
  ntotal = as.integer(nrow(correct))
  percent = round(numCorrect/ntotal*100, 2)
  printf(""Correct classifications on all data: %d/%d (%f)"", numCorrect, ntotal, percent)

  #predicted.h2o = h2o.assign(predicted, key = ""predicted.h2o"")
  #correct.h2o = h2o.assign(correct, key = ""correct.h2o"")

  perf_test = h2o.performance(model = best_model, newdata = test)
  cat(""\nPerformance on test dataset\n"")
  print(perf_test)
  cat(""\nConfusion matrix on test dataset\n"")
  h2o.confusionMatrix(perf_test)

  # Plot Receiver Operating Characteristic (ROC) curve and find its Area Under the Curve (AUC)
  # A ROC curve is a graph of the true positive rate (recall) against the false positive
  # rate for a binary classifier.
  #plot(f1_best_model, type = ""cutoffs"", col = ""blue"")

  cm = h2o.confusionMatrix(best_model, train)
  print(""Confusion Matrix: "")
  print(cm)
  true_negative = cm[1,1]
  true_positive = cm[2,2]
  false_negative = cm[2,1]
  false_positive = cm[1,2]
  total = true_negative + true_positive + false_negative + false_positive
  accuracy = (true_positive + true_negative)/total
  printf(""accuracy: %f"", accuracy)

  misclassification_rate = (false_positive + false_negative)/total
  printf(""misclassification_rate: %f"", misclassification_rate)

  cat(""\nVariable importance\n"")
  print(best_model@model$variable_importance)

  return_list = c(best_activation,
                  best_hidden,
                  best_l1,
                  best_l2,
                  best_input_dropout_ratio,
                  best_hidden_dropout_ratios)

  return (return_list)
}

library(h2o)
library(dplyr)
library(data.table)
library(ggplot2)

localH2O = h2o.init(ip = ""localhost"",
                    port = 54321,
                    startH2O = TRUE,
                    max_mem_size=""24G"",
                    nthreads = -1)
h2o.no_progress()
h2o.removeAll() ## clean slate - just in case the cluster was already running


print(filePath)
model.full <- read.csv(filePath, header = TRUE, sep = "","")
head(model.full)
remove = !colnames(model.full) %in% c(""Date"",
                                      ""Symbol"",
                                      ""BuyIndex"",
                                      ""SellIndex"",
                                      ""BoxRatio"",
                                      ""Acceleration"",
                                      ""nPosVelo"",
                                      ""Gain"")
model_orig = model.full[, remove]
head(model_orig)
model = model_orig[sample(nrow(model_orig)),] # shuffle the rows
head(model)

df <- as.h2o(model, destination_frame = ""df"")

splits <- h2o.splitFrame(df, c(0.6,0.2), seed=1234)
train  <- h2o.assign(splits[[1]], ""train.hex"") # 60%
valid  <- h2o.assign(splits[[2]], ""valid.hex"") # 20%
test   <- h2o.assign(splits[[3]], ""test.hex"")  # 20%

printf(""train----------------------------------------"")
head(train)
train
printf(""valid----------------------------------------"")
#head(valid)
valid
printf(""test-----------------------------------------"")
#head(test)
test

p1 = train$Thrust
p2 = train$Velocity
p3 = train$OnBalRun
p4 = train$vwapGain
p1d = rbind(lapply(p1, as.double))  # p1 is an environment variable, we need doubles
p2d = rbind(lapply(p2, as.double))
p3d = rbind(lapply(p3, as.double))
p4d = rbind(lapply(p4, as.double))
a = unlist(p1d)
b = unlist(p2d)
c = unlist(p3d)
d = unlist(p4d)

pairs(train[1:4], main = ""Scatterplot of predictors"", pch = 21, cex = 0.8, bg = c(""green3"", ""red"")[unclass(model$Altitude)])

cat(""\n\n1. Summary of train dataset------------------------------------------------------\n"")
summary(train, exact_quantiles=TRUE)

cat(""\n\n2. Grid Search on valid data ----------------------------------------------------\n"")

activation_opts = c(""RectifierWithDropout"",""TanhWithDropout"",""MaxoutWithDropout"")
hidden_opts = list(c(80),c(100),c(200),c(300),c(400),c(500),
                   c(80,80),c(100,100),c(200,200),c(300,300),c(400,400),c(500,500),
                   c(80,80,80,80),c(100,100,100,100),c(200,200,200,200),c(300,300,300,300),
                   c(80,80,80,80,80),c(100,100,100,100,100),c(200,200,200,200,200),
                   c(300,300,300,300,300),c(400,400,400,400,400),c(500,500,500,500,500),
                   c(80,80,80,80,80,80),c(100,100,100,100,100,100),c(200,200,200,200,200,200),
                   c(300,300,300,300,300,300),c(400,400,400,400,400,400),
                   c(500,500,500,500,500,500)
                   )
l1_opts = runif(1, 0, 0.0001)
l2_opts = runif(1, 0, 0.0001)

hyperparams = list(
  activation = activation_opts,
  hidden = hidden_opts,
  l1 = l1_opts,
  l2 = l2_opts,
  max_w2 = 10
)

search_criteria = list(strategy = ""RandomDiscrete"",
                       stopping_metric = ""misclassification"",
                       max_models = 10000,
                       max_runtime_secs = 72000,
                       stopping_tolerance = 0.00001,
                       stopping_rounds = 10)

grid_model = h2o.grid(algorithm = ""deeplearning"",
                      grid_id = ""mygrid"",
                      hyper_params = hyperparams,
                      search_criteria = search_criteria,
                      x = 1:4,
                      y = 5,
                      training_frame = train,
                      validation_frame = valid,
                      variable_importances = TRUE,
                      balance_classes=TRUE,
                      score_training_samples=1000,
                      score_validation_samples=1000,
                      score_validation_sampling=""Stratified"",
                      epochs = 1000000,
                      seed = 7)

cat(""\n\n3. Summary of grid_model---------------------------------------------------------\n"")
grid = h2o.getGrid(""mygrid"", sort_by=""auc"", decreasing=TRUE)
summary(grid)

cat(""\n\n4. Performance of best_model-----------------------------------------------------\n"")
best_model = h2o.getModel(grid@model_ids[[1]])

results(""5.best_model"", best_model)

cat(""\n\n6. Performance valid dataset-----------------------------------------------------\n"")
perf_valid = h2o.performance(model = best_model, newdata = valid)
perf_valid

cat(""\n\n7. Performance on test dataset---------------------------------------------------\n"")
perf_test = h2o.performance(model = best_model, newdata = test)
perf_test

plot(perf_test, type=""roc"")     # Plot the roc curve

predicted <- h2o.predict( best_model, test )
actual = test[,5]
cat(""\n\n8. Mean prediction on the test set: "", 100*mean( predicted$predict == actual ), ""%\n"")
correct = predicted == actual
numCorrect = as.integer(sum(correct))
ntotal = as.integer(nrow(correct))
percent = round(numCorrect/ntotal*100, 2)
printf(""9. Test of Mean prediction on the test set: %d/%d (%f)"", numCorrect, ntotal, percent)
cat(""\n\n10. The 'test' set auc is: "", h2o.auc(perf_test), ""\n"")

p = h2o.saveModel(best_model, path = ""C:\\sm\\Trending\\h2o_model"", force = TRUE)
p

minutes = (proc.time() - ptm)[1]
printf(""Elapsed time: %.2f minutes"", minutes)

unlink(""log.txt"")
sink.reset()



My log is:


 Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         9 minutes 35 seconds 
    H2O cluster version:        3.10.4.8 
    H2O cluster version age:    14 days, 4 hours and 1 minute  
    H2O cluster name:           H2O_started_from_R_Charles_huu844 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   21.31 GB 
    H2O cluster total cores:    8 
    H2O cluster allowed cores:  8 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    R Version:                  R version 3.2.2 (2015-08-14) 

[1] 0
[1] ""//bigsur/sm/Trending/model.csv""
        Date Symbol BuyIndex SellIndex BoxRatio  Thrust Acceleration Velocity nPosVelo
1   4/5/2017 GBX         132       199   77.724 49.7190     107.5811   2.9236       59
2   1/5/2017 RH          216       259    4.838  4.8380     137.0574   0.3381       14
3  9/28/2016 DDS         149       444    0.150  0.4860      99.1360   0.1081       59
4 11/25/2016 JKS         132       186    0.932  0.8686      38.9931   0.6057       59
5 12/12/2016 JNUG        132       177    0.582  0.3242      87.1144   1.1274       18
6   3/7/2017 LPL         134       180    8.954  8.9540      34.1691   0.4778       59
  OnBalRun vwapGain    Gain Altitude
1  12.6378  90.0656 -0.1659       no
2   1.5157   0.9495 -0.6496       no
3   0.5476   2.2887  4.1439      yes
4   3.2719   3.8235  0.5051       no
5   2.5006  12.0472 -0.8942       no
6   2.9935   1.1234 -0.1617       no
   Thrust Velocity OnBalRun vwapGain Altitude
1 49.7190   2.9236  12.6378  90.0656       no
2  4.8380   0.3381   1.5157   0.9495       no
3  0.4860   0.1081   0.5476   2.2887      yes
4  0.8686   0.6057   3.2719   3.8235       no
5  0.3242   1.1274   2.5006  12.0472       no
6  8.9540   0.4778   2.9935   1.1234       no
     Thrust Velocity OnBalRun vwapGain Altitude
4427 0.9370   0.3176   1.2786   2.3151       no
3079 2.2060   0.9261   1.1257   1.2506       no
3952 0.0702   0.4430   1.1485   0.9928       no
7765 1.1596   1.1067   6.2563   2.1164      yes
1682 0.6708   0.4519   1.3848   2.1808       no
5145 4.5600   0.3462   1.7386   0.7722       no
[1] ""train----------------------------------------""
  Thrust Velocity OnBalRun vwapGain Altitude
1 2.2060   0.9261   1.1257   1.2506       no
2 0.6708   0.4519   1.3848   2.1808       no
3 4.5600   0.3462   1.7386   0.7722       no
4 3.6930   3.2778  11.4092  49.3335       no
5 0.9980   0.4035   1.6667   1.1264       no
6 0.2016   0.5627   2.4101   1.2642       no
  Thrust Velocity OnBalRun vwapGain Altitude
1 2.2060   0.9261   1.1257   1.2506       no
2 0.6708   0.4519   1.3848   2.1808       no
3 4.5600   0.3462   1.7386   0.7722       no
4 3.6930   3.2778  11.4092  49.3335       no
5 0.9980   0.4035   1.6667   1.1264       no
6 0.2016   0.5627   2.4101   1.2642       no

[5548 rows x 5 columns] 
[1] ""valid----------------------------------------""
  Thrust Velocity OnBalRun vwapGain Altitude
1 0.9370   0.3176   1.2786   2.3151       no
2 0.0702   0.4430   1.1485   0.9928       no
3 1.0230   0.3119   3.0922   0.8788       no
4 6.4100   0.9966   5.3490   2.9436      yes
5 6.9620   0.7004   3.5810   4.8905       no
6 1.6800   1.4518   5.1933   1.7955       no

[1875 rows x 5 columns] 
[1] ""test-----------------------------------------""
  Thrust Velocity OnBalRun vwapGain Altitude
1 1.1596   1.1067   6.2563   2.1164      yes
2 4.7010   0.5369   1.1266   7.5566       no
3 1.7110   0.9247   3.5819   3.0598       no
4 1.4620   0.3315   4.3097   0.4129       no
5 0.5610   0.4494   1.8738   1.3942       no
6 6.7255   1.7309   5.6268   4.4937      yes

[1823 rows x 5 columns] 


1. Summary of train dataset------------------------------------------------------
 Thrust               Velocity          OnBalRun           vwapGain           Altitude  
 Min.   :   -1.4845   Min.   :-0.1241   Min.   : -0.5299   Min.   : -4.7648    no :4875 
 1st Qu.:    0.3984   1st Qu.: 0.3281   1st Qu.:  1.1468   1st Qu.:  0.8684    yes: 671 
 Median :    1.0425   Median : 0.3815   Median :  1.6439   Median :  1.9954       :   2 
 Mean   :   12.1332   Mean   : 0.5913   Mean   :  2.5911   Mean   :  4.8876             
 3rd Qu.:    1.9723   3rd Qu.: 0.5716   3rd Qu.:  2.6507   3rd Qu.:  4.7500             
 Max.   :41279.8960   Max.   :29.4449   Max.   :154.5988   Max.   :314.7143             
 NA's   :2            NA's   :2         NA's   :2          NA's   :2                    


2. Grid Search on valid data ----------------------------------------------------


3. Summary of grid_model---------------------------------------------------------

ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http://localhost:54321/99/Grids/mygrid?sort_by=auc&decreasing=TRUE)

water.exceptions.H2OIllegalArgumentException
 [1] ""water.exceptions.H2OIllegalArgumentException: Invalid argument for sort_by specified. Must be one of: [r2, mean_per_class_accuracy, max_per_class_error, err, total_rows, rmse, accuracy, err_count, logloss, mse, mean_per_class_error]""
 [2] ""    hex.schemas.GridSchemaV99.fillFromImpl(GridSchemaV99.java:114)""                                                                                                                                                                      
 [3] ""    water.api.GridsHandler.fetch(GridsHandler.java:41)""                                                                                                                                                                                  
 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                                                                                                                                         
 [5] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                                                                                                                                                       
 [6] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                                                                                                                                               
 [7] ""    java.lang.reflect.Method.invoke(Method.java:498)""                                                                                                                                                                                    
 [8] ""    water.api.Handler.handle(Handler.java:61)""                                                                                                                                                                                           
 [9] ""    water.api.RequestServer.serve(RequestServer.java:436)""                                                                                                                                                                               
[10] ""    water.api.RequestServer.doGeneric(RequestServer.java:285)""                                                                                                                                                                           
[11] ""    water.api.RequestServer.doGet(RequestServer.java:220)""                                                                                                                                                                               
[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:735)""                                                                                                                                                                        
[13] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                                                                                                                                        
[14] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                                                                                                                                              
[15] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                                                                                                                                                          
[16] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                                                                                                                                                  
[17] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                                                                                                                                                           
[18] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                                                                                                                                                   
[19] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                                                                                                                                       
[20] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                                                                                                                               
[21] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                                                                                                                                     
[22] ""    water.JettyHTTPD$LoginHandler.handle(JettyHTTPD.java:417)""                                                                                                                                                                           
[23] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                                                                                                                               
[24] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                                                                                                                                     
[25] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                                                                                                                                             
[26] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                                                                                                                                      
[27] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                                                                                                                                       
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)""                                                                                                                                     
[29] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)""                                                                                                                     
[30] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)""                                                                                                                                                                    
[31] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)""                                                                                                                                                               
[32] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                                                                                                                                              
[33] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                                                                                                                                        
[34] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                                                                                                                                                    
[35] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                                                                                                                                     
[36] ""    java.lang.Thread.run(Thread.java:745)""                                                                                                                                                                                               



4. Performance of best_model-----------------------------------------------------


6. Performance valid dataset-----------------------------------------------------


7. Performance on test dataset---------------------------------------------------
Called from: sprintf(...)
[1] ""Elapsed time: 85.98 minutes""



Please help.


Charles",['h2o'],Unknown,,N/A
44349062,44349062,2017-06-03T22:23:03,2017-06-05 03:11:50Z,0,"I'm getting this error when trying to run a Pysparkling script on an AWS EMR cluster. I can get everything to work when downloading Sparkling water 2.1.8 and running it from a pysparkling shell. However, spark-submit does not seem to work. 


Error:


NameError: name 'H2OContext' is not defined



My spark-submit:


spark-submit --packages ai.h2o:sparkling-water-core_2.11:2.1.7,ai.h2o:sparkling-water-examples_2.11:2.1.7 --conf spark.dynamicAllocation.enabled=false spark.py



Python file


from pysparkling import *

hc = H2OContext.getOrCreate(sc)



Also, I've tried actually making a spark context but that just results in the same error, but takes longer.


Bootstrap file:


#!/usr/bin/env bash


# install conda (conda 4.2 defaults to pythone35)
wget --quiet http://repo.continuum.io/archive/Anaconda3-4.2.0-Linux-x86_64.sh -O ~/anaconda.sh \
    && /bin/bash ~/anaconda.sh -b -p $HOME/conda

echo -e '\nexport PATH=$HOME/conda/bin:$PATH' >> $HOME/.bashrc && source $HOME/.bashrc

# install packages
conda install -y ipython jupyter

# needed for PySparkling
conda install requests
conda install six
conda install future
conda install tabulate

# install pysparkling
pip install h2o
# pip install pysparkling
pip install h2o_pysparkling_2.1



More detailed output:


[hadoop@ip-172-31-32-30 test]$ spark-submit --packages ai.h2o:sparkling-water-core_2.11:2.1.7,ai.h2o:sparkling-water-examples_2.11:2.1.7 --conf spark.dynamicAllocation.enabled=false spark.py
Ivy Default Cache set to: /home/hadoop/.ivy2/cache
The jars for the packages stored in: /home/hadoop/.ivy2/jars
:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
ai.h2o#sparkling-water-core_2.11 added as a dependency
ai.h2o#sparkling-water-examples_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
        confs: [default]
        found ai.h2o#sparkling-water-core_2.11;2.1.7 in central
        found ai.h2o#h2o-genmodel;3.10.4.7 in central
        found net.sf.opencsv#opencsv;2.3 in central
        found ai.h2o#deepwater-backend-api;1.0.2 in central
        found com.google.guava#guava;19.0 in central
        found ai.h2o#h2o-core;3.10.4.7 in central
        found joda-time#joda-time;2.3 in central
        found gov.nist.math#jama;1.0.3 in central
        found org.javassist#javassist;3.18.2-GA in central
        found org.apache.commons#commons-math3;3.3 in central
        found commons-io#commons-io;2.4 in central
        found ai.h2o#google-analytics-java;1.1.2-H2O-CUSTOM in central
        found org.apache.httpcomponents#httpclient;4.1 in central
        found org.apache.httpcomponents#httpcore;4.1 in central
        found commons-logging#commons-logging;1.1.1 in central
        found commons-codec#commons-codec;1.4 in central
        found org.eclipse.jetty.aggregate#jetty-servlet;8.1.17.v20150415 in central
        found org.eclipse.jetty#jetty-plus;8.1.17.v20150415 in central
        found org.eclipse.jetty.orbit#javax.transaction;1.1.1.v201105210645 in central
        found org.eclipse.jetty#jetty-webapp;8.1.17.v20150415 in central
        found org.eclipse.jetty#jetty-xml;8.1.17.v20150415 in central
        found org.eclipse.jetty#jetty-util;8.1.17.v20150415 in central
        found org.eclipse.jetty#jetty-servlet;8.1.17.v20150415 in central
        found org.eclipse.jetty#jetty-security;8.1.17.v20150415 in central
        found org.eclipse.jetty#jetty-server;8.1.17.v20150415 in central
        found org.eclipse.jetty.orbit#javax.servlet;3.0.0.v201112011016 in central
        found org.eclipse.jetty#jetty-continuation;8.1.17.v20150415 in central
        found org.eclipse.jetty#jetty-http;8.1.17.v20150415 in central
        found org.eclipse.jetty#jetty-io;8.1.17.v20150415 in central
        found org.eclipse.jetty#jetty-jndi;8.1.17.v20150415 in central
        found org.eclipse.jetty.orbit#javax.mail.glassfish;1.4.1.v201005082020 in central
        found org.eclipse.jetty.orbit#javax.activation;1.1.0.v201105071233 in central
        found com.github.rwl#jtransforms;2.4.0 in central
        found ai.h2o#h2o-jaas-pam;3.10.4.7 in central
        found org.kohsuke#libpam4j;1.8 in central
        found net.java.dev.jna#jna;4.0.0 in central
        found log4j#log4j;1.2.15 in central
        found com.google.code.gson#gson;2.3.1 in central
        found commons-lang#commons-lang;2.6 in central
        found ai.h2o#reflections;0.9.11-h2o-custom in central
        found com.google.code.findbugs#jsr305;3.0.0 in central
        found ai.h2o#h2o-algos;3.10.4.7 in central
        found ai.h2o#h2o-web;3.10.4.7 in central
        found ai.h2o#h2o-avro-parser;3.10.4.7 in central
        found ai.h2o#h2o-parquet-parser;3.10.4.7 in central
        found ai.h2o#h2o-orc-parser;3.10.4.7 in central
        found ai.h2o#h2o-scala_2.11;3.10.4.7 in central
        found ai.h2o#h2o-persist-hdfs;3.10.4.7 in central
        found net.java.dev.jets3t#jets3t;0.6.1 in central
        found commons-httpclient#commons-httpclient;3.1 in central
        found ai.h2o#h2o-persist-s3;3.10.4.7 in central
        found com.amazonaws#aws-java-sdk-s3;1.10.47 in central
        found com.amazonaws#aws-java-sdk-kms;1.10.47 in central
        found com.amazonaws#aws-java-sdk-core;1.10.47 in central
        found commons-logging#commons-logging;1.1.3 in central
        found org.apache.httpcomponents#httpclient;4.3.6 in central
        found org.apache.httpcomponents#httpcore;4.3.3 in central
        found commons-codec#commons-codec;1.6 in central
        found joda-time#joda-time;2.8.1 in central
        found ai.h2o#sparkling-water-repl_2.11;2.1.7 in central
        found org.joda#joda-convert;1.7 in central
        found ai.h2o#sparkling-water-examples_2.11;2.1.7 in central
        found ai.h2o#sparkling-water-ml_2.11;2.1.7 in central
downloading https://repo1.maven.org/maven2/ai/h2o/sparkling-water-core_2.11/2.1.7/sparkling-water-core_2.11-2.1.7.jar ...
        [SUCCESSFUL ] ai.h2o#sparkling-water-core_2.11;2.1.7!sparkling-water-core_2.11.jar (56ms)
downloading https://repo1.maven.org/maven2/ai/h2o/sparkling-water-examples_2.11/2.1.7/sparkling-water-examples_2.11-2.1.7.jar ...
        [SUCCESSFUL ] ai.h2o#sparkling-water-examples_2.11;2.1.7!sparkling-water-examples_2.11.jar (15ms)
downloading https://repo1.maven.org/maven2/ai/h2o/h2o-genmodel/3.10.4.7/h2o-genmodel-3.10.4.7.jar ...
        [SUCCESSFUL ] ai.h2o#h2o-genmodel;3.10.4.7!h2o-genmodel.jar (7ms)
downloading https://repo1.maven.org/maven2/ai/h2o/h2o-core/3.10.4.7/h2o-core-3.10.4.7.jar ...
        [SUCCESSFUL ] ai.h2o#h2o-core;3.10.4.7!h2o-core.jar (129ms)
downloading https://repo1.maven.org/maven2/ai/h2o/h2o-algos/3.10.4.7/h2o-algos-3.10.4.7.jar ...
        [SUCCESSFUL ] ai.h2o#h2o-algos;3.10.4.7!h2o-algos.jar (35ms)
downloading https://repo1.maven.org/maven2/ai/h2o/h2o-web/3.10.4.7/h2o-web-3.10.4.7.jar ...
        [SUCCESSFUL ] ai.h2o#h2o-web;3.10.4.7!h2o-web.jar (512ms)
downloading https://repo1.maven.org/maven2/ai/h2o/h2o-scala_2.11/3.10.4.7/h2o-scala_2.11-3.10.4.7.jar ...
        [SUCCESSFUL ] ai.h2o#h2o-scala_2.11;3.10.4.7!h2o-scala_2.11.jar (4ms)
downloading https://repo1.maven.org/maven2/ai/h2o/h2o-persist-hdfs/3.10.4.7/h2o-persist-hdfs-3.10.4.7.jar ...
        [SUCCESSFUL ] ai.h2o#h2o-persist-hdfs;3.10.4.7!h2o-persist-hdfs.jar (2ms)
downloading https://repo1.maven.org/maven2/ai/h2o/h2o-persist-s3/3.10.4.7/h2o-persist-s3-3.10.4.7.jar ...
        [SUCCESSFUL ] ai.h2o#h2o-persist-s3;3.10.4.7!h2o-persist-s3.jar (2ms)
downloading https://repo1.maven.org/maven2/ai/h2o/sparkling-water-repl_2.11/2.1.7/sparkling-water-repl_2.11-2.1.7.jar ...
        [SUCCESSFUL ] ai.h2o#sparkling-water-repl_2.11;2.1.7!sparkling-water-repl_2.11.jar (4ms)
downloading https://repo1.maven.org/maven2/ai/h2o/h2o-jaas-pam/3.10.4.7/h2o-jaas-pam-3.10.4.7.jar ...
        [SUCCESSFUL ] ai.h2o#h2o-jaas-pam;3.10.4.7!h2o-jaas-pam.jar (2ms)
downloading https://repo1.maven.org/maven2/ai/h2o/sparkling-water-ml_2.11/2.1.7/sparkling-water-ml_2.11-2.1.7.jar ...
        [SUCCESSFUL ] ai.h2o#sparkling-water-ml_2.11;2.1.7!sparkling-water-ml_2.11.jar (10ms)
:: resolution report :: resolve 6024ms :: artifacts dl 802ms
        :: modules in use:
        ai.h2o#deepwater-backend-api;1.0.2 from central in [default]
        ai.h2o#google-analytics-java;1.1.2-H2O-CUSTOM from central in [default]
        ai.h2o#h2o-algos;3.10.4.7 from central in [default]
        ai.h2o#h2o-avro-parser;3.10.4.7 from central in [default]
        ai.h2o#h2o-core;3.10.4.7 from central in [default]
        ai.h2o#h2o-genmodel;3.10.4.7 from central in [default]
        ai.h2o#h2o-jaas-pam;3.10.4.7 from central in [default]
        ai.h2o#h2o-orc-parser;3.10.4.7 from central in [default]
        ai.h2o#h2o-parquet-parser;3.10.4.7 from central in [default]
        ai.h2o#h2o-persist-hdfs;3.10.4.7 from central in [default]
        ai.h2o#h2o-persist-s3;3.10.4.7 from central in [default]
        ai.h2o#h2o-scala_2.11;3.10.4.7 from central in [default]
        ai.h2o#h2o-web;3.10.4.7 from central in [default]
        ai.h2o#reflections;0.9.11-h2o-custom from central in [default]
        ai.h2o#sparkling-water-core_2.11;2.1.7 from central in [default]
        ai.h2o#sparkling-water-examples_2.11;2.1.7 from central in [default]
        ai.h2o#sparkling-water-ml_2.11;2.1.7 from central in [default]
        ai.h2o#sparkling-water-repl_2.11;2.1.7 from central in [default]
        com.amazonaws#aws-java-sdk-core;1.10.47 from central in [default]
        com.amazonaws#aws-java-sdk-kms;1.10.47 from central in [default]
        com.amazonaws#aws-java-sdk-s3;1.10.47 from central in [default]
        com.github.rwl#jtransforms;2.4.0 from central in [default]
        com.google.code.findbugs#jsr305;3.0.0 from central in [default]
        com.google.code.gson#gson;2.3.1 from central in [default]
        com.google.guava#guava;19.0 from central in [default]
        commons-codec#commons-codec;1.6 from central in [default]
        commons-httpclient#commons-httpclient;3.1 from central in [default]
        commons-io#commons-io;2.4 from central in [default]
        commons-lang#commons-lang;2.6 from central in [default]
        commons-logging#commons-logging;1.1.3 from central in [default]
        gov.nist.math#jama;1.0.3 from central in [default]
        joda-time#joda-time;2.8.1 from central in [default]
        log4j#log4j;1.2.15 from central in [default]
        net.java.dev.jets3t#jets3t;0.6.1 from central in [default]
        net.java.dev.jna#jna;4.0.0 from central in [default]
        net.sf.opencsv#opencsv;2.3 from central in [default]
        org.apache.commons#commons-math3;3.3 from central in [default]
        org.apache.httpcomponents#httpclient;4.3.6 from central in [default]
        org.apache.httpcomponents#httpcore;4.3.3 from central in [default]
        org.eclipse.jetty#jetty-continuation;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty#jetty-http;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty#jetty-io;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty#jetty-jndi;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty#jetty-plus;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty#jetty-security;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty#jetty-server;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty#jetty-servlet;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty#jetty-util;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty#jetty-webapp;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty#jetty-xml;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty.aggregate#jetty-servlet;8.1.17.v20150415 from central in [default]
        org.eclipse.jetty.orbit#javax.activation;1.1.0.v201105071233 from central in [default]
        org.eclipse.jetty.orbit#javax.mail.glassfish;1.4.1.v201005082020 from central in [default]
        org.eclipse.jetty.orbit#javax.servlet;3.0.0.v201112011016 from central in [default]
        org.eclipse.jetty.orbit#javax.transaction;1.1.1.v201105210645 from central in [default]
        org.javassist#javassist;3.18.2-GA from central in [default]
        org.joda#joda-convert;1.7 from central in [default]
        org.kohsuke#libpam4j;1.8 from central in [default]
        :: evicted modules:
        joda-time#joda-time;2.3 by [joda-time#joda-time;2.8.1] in [default]
        org.apache.httpcomponents#httpclient;4.1 by [org.apache.httpcomponents#httpclient;4.3.6] in [default]
        org.apache.httpcomponents#httpcore;4.1 by [org.apache.httpcomponents#httpcore;4.3.3] in [default]
        commons-logging#commons-logging;1.1.1 by [commons-logging#commons-logging;1.1.3] in [default]
        commons-codec#commons-codec;1.4 by [commons-codec#commons-codec;1.6] in [default]
        com.google.guava#guava;16.0.1 by [com.google.guava#guava;19.0] in [default]
        com.google.guava#guava;18.0 by [com.google.guava#guava;19.0] in [default]
        commons-codec#commons-codec;1.3 by [commons-codec#commons-codec;1.4] in [default]
        commons-logging#commons-logging;1.0.4 by [commons-logging#commons-logging;1.1.1] in [default]
        commons-codec#commons-codec;1.2 by [commons-codec#commons-codec;1.4] in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   68  |   15  |   15  |   10  ||   55  |   12  |
        ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
        confs: [default]
        12 artifacts copied, 43 already retrieved (23416kB/63ms)
Traceback (most recent call last):
  File ""/home/hadoop/scripts/test/spark.py"", line 3, in <module>
    hc = H2OContext.getOrCreate(sc)
NameError: name 'H2OContext' is not defined","['python', 'python-3.x', 'h2o', 'sparkling-water']",Keston,https://stackoverflow.com/users/8108623/keston,119
44348850,44348850,2017-06-03T21:50:16,2017-06-03 23:03:37Z,431,"I have an h2o deeplearning model, ""model1"", that generalizes very well. Unfortunately, I forgot to set export weights and biases = 
TRUE
 when building the model.


I've tried to retrain numerous models with all the exact parameters, seed, and dataset as in the original model1 plus added set export weights and biases to true.


Unfortunately 
none
 of these new models generalize well at all. In fact they all fail miserably to generalize - although all models train, validate, cross-validate and test very well. I've even tried checkpointing the original model1 so I can add the export weights and biases argument = 
TRUE
. However, because I did not use Modulo CV, I'm unable to checkpoint.


Irreproducibility is giving me a huge headache. In order for me to productionalize, I need to somehow extract the weights and biases of this original, working model1 - despite export weights and biases being originally set to 
FALSE
.


I've looked at the 
mean
 weights and biases of model1 and they simply do not match any of mean weights and biases of my retrained models with same parameters, seed, dataset, etc. I'm uncertain if mean weights and biases can be used somehow to force reproducibility.


I've read that downloading model1 to POJO 
may
 allow access to the weights and biases, but I'm uncertain about this, I don't know java and I don't see any example java code to help me along.


Any suggestions or other possible solutions/workarounds?


Thank you in advance for any help.","['deep-learning', 'pojo', 'h2o', 'checkpoint']",ogukku,https://stackoverflow.com/users/6088414/ogukku,53
44341557,44341557,2017-06-03T07:33:00,2017-06-03 14:39:41Z,0,"I am trying to export the weights and biases of a ""model"" in which I did not originally train the model with ""export_weights_and_biases = TRUE""


Therefore, I'd like to try to checkpoint the model and try to export_weights_and_biases = TRUE in a new ""model2"".


However, despite not changing any of the parameters - and ensuring that 
nfolds=10
 just as in the original ""model"", the checkpoint model continues to return a parameter change error almost immediately (h2o version 3.10.4.6):


water.exceptions.H2OIllegalArgumentException: Cannot change parameter: '_nfolds': 10 -> 0

water.exceptions.H2OIllegalArgumentException: Cannot change parameter: '_nfolds': 10 -> 0
    at hex.deeplearning.DeepLearningModel$DeepLearningParameters$Sanity.checkIfParameterChangeAllowed(DeepLearningModel.java:2078)
    at hex.deeplearning.DeepLearning$DeepLearningDriver.buildModel(DeepLearning.java:249)
    at hex.deeplearning.DeepLearning$DeepLearningDriver.computeImpl(DeepLearning.java:211)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:169)
    at hex.deeplearning.DeepLearning$DeepLearningDriver.compute2(DeepLearning.java:204)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1241)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

Error: water.exceptions.H2OIllegalArgumentException: Cannot change parameter: '_nfolds': 10 -> 0



Any ideas how to resolve this issue? All I'm trying to do is obtain all the weights and biases of my original ""model"".


Thank you!","['r', 'h2o', 'checkpointing']",ogukku,https://stackoverflow.com/users/6088414/ogukku,53
44339309,44339309,2017-06-03T01:09:23,2019-01-04 15:29:50Z,0,"I am using 
h2o
 version 
3.10.4.8
.


library(h2o)
h2o.init(nthreads = -1)

df <- as.h2o(data.frame(x = 1:5, y = 11:15))



I'm trying to understand how to use the 
apply()
 function in 
H2O
.


The following works as expected:


h2o::apply(df, 2, mean)
h2o::apply(df, 2, sum)
h2o::apply(df, 2, function(x) {2*x + 1})



But this does not:


h2o::apply(df, 2, sd)



The error returned is:




[1] ""Lookup failed to find is.H2OFrame"" Error in .process.stmnt(stmnt,
  formalz, envs) :    Don't know what to do with statement: is.H2OFrame
  x




I also thought that 
H2O
 is actually using its own functions to do the computation, so the following should work:


h2o::apply(df, 2, h2o.mean)
h2o::apply(df, 2, h2o.sum)
h2o::apply(df, 2, h2o.sd) 



But it does not. The first two lines give the following error:




[1] ""Lookup failed to find .newExpr"" Error in .process.stmnt(stmnt,
  formalz, envs) :    Don't know what to do with statement: .newExpr sd
  x na.rm




While the third line gives the following error:




[1] ""Lookup failed to find .newExpr"" Error in .process.stmnt(stmnt,
  formalz, envs) :    Don't know what to do with statement: .newExpr sd
  x na.rm




What's going on and what are the things I should be aware of when passing stuff into 
FUN
 parameter in the 
apply()
 function? The documentation simply describes 
FUN
 as ""the function to be applied"".","['r', 'apply', 'h2o']",massisenergy,https://stackoverflow.com/users/9592557/massisenergy,"1,820"
44338639,44338639,2017-06-02T23:17:10,2017-06-06 13:34:29Z,505,"Most of the algorithms in h2o have a constraint in the 
response_column
. For some of them, it must be an 
Enum
 type variable only and for other ones an  
int
. I have uploaded a data set where my 
response_column
 has the values: 
0,1
, therefore it can be converted to 
Enum
 easily. Then due to this constraint for some algorithms, I can use my original type for the 
response_column
 but for testing other algorithms I need to convert it as 
Enum
.


I was trying to create an additional column let's say 
outputAsEnum
, but I didn't find a way how to create an additional column based on an existing one. I was trying to create a copy of my original data frame, but I didn't find the option for doing it.


Is there any simple way to have a Flow that allows considering several algorithms for the same output variable, but converting it properly before? (to 
Enum
 or to 
int
)


I have noticed too, that in order balance the classes for the 
response_column
, the column type has to be an 
Enum
. Is there any way to circumvent this?
 Based on that, if I have imbalance data, I am forced to use only the algorithms that can work with 
Enum
 data type. This is just an example, there are other configuration parameters that would depend on the data type of the 
response_column
.",['h2o'],Unknown,,N/A
44315642,44315642,2017-06-01T19:29:54,2017-06-01 21:41:58Z,0,"I am uing h2o version 3.10.4.8.


library(h2o)

h2o.init(nthreads = -1, max_mem_size = ""6g"")

data.url <- ""https://raw.githubusercontent.com/DarrenCook/h2o/bk/datasets/""

iris.hex <- paste0(data.url, ""iris_wheader.csv"") %>%
  h2o.importFile(destination_frame = ""iris.hex"")

y <- ""class""
x <- setdiff(names(iris.hex), y)



Now, I want to count the number of rows in 
iris.hex
 by the column 
class
 just to get the hang of using 
h2o.group_by
. This is what I tried after reading the documentation:


h2o.group_by(iris.hex, by = list(""class""), h2o.nrow)



This results in the following error:


Error in is.H2OFrame(x) : object 'group.cols' not found



The documentation did not provide an example usage so I am not sure if I am even calling this function right.","['r', 'h2o']",mauna,https://stackoverflow.com/users/3775778/mauna,"1,118"
44315497,44315497,2017-06-01T19:21:39,2018-05-31 08:39:29Z,0,"I am using h2o version 3.10.4.8.


library(magrittr)
library(h2o)

h2o.init(nthreads = -1, max_mem_size = ""6g"")

data.url <- ""https://raw.githubusercontent.com/DarrenCook/h2o/bk/datasets/""

iris.hex <- paste0(data.url, ""iris_wheader.csv"") %>%
  h2o.importFile(destination_frame = ""iris.hex"")

y <- ""class""
x <- setdiff(names(iris.hex), y)


model.glm <- h2o.glm(x, y, iris.hex, family = ""multinomial"")

preds <- h2o.predict(model.glm, iris.hex)

h2o.confusionMatrix(model.glm)
h2o.table(preds[""predict""])



This is the output of 
h2o.confusionMatrix(model.glm)
:


Confusion Matrix: vertical: actual; across: predicted
                Iris-setosa Iris-versicolor Iris-virginica  Error      Rate
Iris-setosa              50               0              0 0.0000 =  0 / 50
Iris-versicolor           0              48              2 0.0400 =  2 / 50
Iris-virginica            0               1             49 0.0200 =  1 / 50
Totals                   50              49             51 0.0200 = 3 / 150



Since it says across:predicted, I interpret this to mean that the model made 50 (0 + 48 + 2) predictions that are Iris-versicolor.


This is the output of 
h2o.table(preds[""predict""])
:


          predict Count
1     Iris-setosa    50
2 Iris-versicolor    49
3  Iris-virginica    51



This tells me that the model made 49 predictions that are Iris-versicolor.


Is the confusion matrix incorrectly labelled or did I make a mistake in interpreting the results?","['r', 'h2o', 'confusion-matrix']",mauna,https://stackoverflow.com/users/3775778/mauna,"1,118"
44281612,44281612,2017-05-31T09:59:01,2017-05-31 15:51:38Z,0,"I am trying to run optimizing grid for 2 algorithms (
random forest
 and 
gbm
) for different parts of a data set, using 
h2o
. My code looks like


for (...)
{
        read data

        # setup h2o cluster
        h2o <- h2o.init(ip = ""localhost"", port = 54321, nthreads = detectCores()-1)

        gbm.grid <- h2o.grid(""gbm"", grid_id = ""gbm.grid"", x = names(td.train.h2o)[!names(td.train.h2o)%like%segment_binary], y = segment_binary, 
                             seed = 42, distribution = ""bernoulli"",
                             training_frame = td.train.h2o, validation_frame = td.train.hyper.h2o,
                             hyper_params = hyper_params, search_criteria = search_criteria)

    # shutdown h2o
    h2o.shutdown(prompt = FALSE)

    # setup h2o cluster
    h2o <- h2o.init(ip = ""localhost"", port = 54321, nthreads = detectCores()-1)

    rf.grid <- h2o.grid(""randomForest"", grid_id = ""rf.grid"", x = names(td.train.h2o)[!names(td.train.h2o)%like%segment_binary], y = segment_binary, 
                        seed = 42, distribution = ""bernoulli"",
                        training_frame = td.train.h2o, validation_frame = td.train.hyper.h2o,
                        hyper_params = hyper_params, search_criteria = search_criteria)

    h2o.shutdown(prompt = FALSE)
}



The problem is that if i run the 
for loop
 in one go, i get the error 


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  : 
  Unexpected CURL error: Failed to connect to localhost port 54321: Connection refused



P.S.: I am using the line


 # shutdown h2o
h2o.shutdown(prompt = FALSE)

# setup h2o cluster
h2o <- h2o.init(ip = ""localhost"", port = 54321, nthreads = detectCores()-1)



So that I ""reset"" the 
h2o
, so that i do not run out of memory


I also read 
R H2O - Memory management
 but it is not clear to me how it works.


UPDATE


After following Matteusz comment, i 
init
 outside the 
for loop
 and inside of the 
for loop
 i use  
h2o.removeAll()
. So now my code looks like this 


 h2o <- h2o.init(ip = ""localhost"", port = 54321, nthreads = detectCores()-1)
for(...)
{
read data

gbm.grid <- h2o.grid(""gbm"", grid_id = ""gbm.grid"", x = names(td.train.h2o)[!names(td.train.h2o)%like%segment_binary], y = segment_binary, 
                             seed = 42, distribution = ""bernoulli"",
                             training_frame = td.train.h2o, validation_frame = td.train.hyper.h2o,
                             hyper_params = hyper_params, search_criteria = search_criteria)

h2o.removeAll()

rf.grid <- h2o.grid(""randomForest"", grid_id = ""rf.grid"", x = names(td.train.h2o)[!names(td.train.h2o)%like%segment_binary], y = segment_binary, 
                        seed = 42, distribution = ""bernoulli"",
                        training_frame = td.train.h2o, validation_frame = td.train.hyper.h2o,
                        hyper_params = hyper_params, search_criteria = search_criteria)

h2o.removeAll() }



It seems to work, but now i get this error (?) in the 
grid optimization
 for 
random forest




Any ideas what this might be ?","['r', 'memory', 'memory-management', 'machine-learning', 'h2o']",Unknown,,N/A
44277378,44277378,2017-05-31T06:37:12,2017-06-08 19:10:57Z,0,"I'm using h2o v3.10.4.6 version with R. Built a DL Binomial classification model using 
h2o.deeplearning()
 function, would like to find some thresholds using this function 
h2o.find_threshold_by_max_metric()
. However I got error like this:




Error in h2o.find_threshold_by_max_metric(NNmodel, ""F1"") :    No F1
  for H2OBinomialModel




I use 
this documentation
 as reference, but there is no examples


How can I solve this error?","['r', 'deep-learning', 'h2o']",llrs,https://stackoverflow.com/users/2886003/llrs,"3,397"
44275109,44275109,2017-05-31T03:26:58,2017-06-02 02:46:00Z,0,"I have installed HDP 2.6 in computer cluster with only 2 node. Each node has




Processor 2 Core


RAM 8 GB


Harddisk 40 GB




enter image description here


I also installed Apache Hadoop 2.7.3, too. Because of that, i can run H2O 3.11.4.8 using YARN. But, the error has occurred when i am trying to build Deep Learning Model using 500 MB dataset with R. This is the error 


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = urlSuffix,  : 
  Unexpected CURL error: Failed connect to 172.16.0.14:54321; Connection refused
Calls: h2o.deeplearning ... tryCatchOne -> doTryCatch -> .h2o.doSafeGET -> .h2o.doSafeREST
In addition: Warning message:
In .verify_dataxy(training_frame, x, y, autoencoder) :
  removing response variable from the explanatory variables
Execution halted
Error in .h2o.__checkConnectionHealth() : 
  H2O connection has been severed. Cannot connect to instance at http://172.16.0.14:54321/
Failed connect to 172.16.0.14:54321; Connection refused
Calls: <Anonymous> -> .h2o.__remoteSend -> .h2o.__checkConnectionHealth



Before using R, i also using Python, too. But, again i get an error similar like that. The error says that i get a problem with requests package because this package can not make a new connection with H2O.
This is an error when i'm using Python API.


H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries     exceeded with url: /3/Jobs/$0301ac10000e32d4ffffffff$_91d77c50e0aff3019565b9b6dddc4c69 (Caused by     NewConnectionError('<urllib3.connection.HTTPConnection object at 0xc21fa10>: Failed to establish a new     connection: [Errno 111] Connection refused',))
Error in sys.excepthook:
Traceback (most recent call last):
File ""/usr/lib/python2.7/site-packages/h2o/utils/debugging.py"", line 95, in _except_hook
_handle_soft_error(exc_type, exc_value, exc_tb)
File ""/usr/lib/python2.7/site-packages/h2o/utils/debugging.py"", line 225, in _handle_soft_error
args_str = _get_args_str(func, highlight=highlight)
File ""/usr/lib/python2.7/site-packages/h2o/utils/debugging.py"", line 316, in _get_args_str
s = str(inspect.signature(func))[1:-1]
AttributeError: 'module' object has no attribute 'signature'
Original exception was:
Traceback (most recent call last):
File ""hadoop-sed.py"", line 18, in <module>
y=""I_TORNADO_LOGICAL"", training_frame=training, validation_frame=validation)
File ""/usr/lib/python2.7/site-packages/h2o/estimators/estimator_base.py"", line 204, in train
model.poll()
File ""/usr/lib/python2.7/site-packages/h2o/job.py"", line 54, in poll
pb.execute(self._refresh_job_status)
File ""/usr/lib/python2.7/site-packages/h2o/utils/progressbar.py"", line 160, in execute
res = progress_fn()  # may raise StopIteration
File ""/usr/lib/python2.7/site-packages/h2o/job.py"", line 89, in _refresh_job_status
jobs = h2o.api(""GET /3/Jobs/%s"" % self.job_key)
File ""/usr/lib/python2.7/site-packages/h2o/h2o.py"", line 99, in api
return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
File ""/usr/lib/python2.7/site-packages/h2o/backend/connection.py"", line 410, in request
raise H2OConnectionError(""Unexpected HTTP error: %s"" % e)
h2o.exceptions.H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321):
Max retries exceeded with url: /3/Jobs/$0301ac10000e32d4ffffffff$_91d77c50e0aff3019565b9b6dddc4c69 (Caused by     NewConnectionError('<urllib3.connection.HTTPConnection object at 0xc21fa10>: Failed to establish a new     connection: [Errno 111] Connection refused',))



From this error, i am trying to figure out why this is happen. I get a several important information about it.




H2O Documentation in Hadoop Section (I'm sorry not to give a link, my reputation is low), H2O should run with 6 GB of RAM. With screenshot that i provided before. The RAM is not a problem.


Community H2O Question ""H2O Memory Requirements"", it says that RAM size should be 4x from dataset size. Because my dataset is 500MB, it should be passed.




From this information, i have a conclusion that my cluster is good enough to process dataset without a problem. So, the problem should not from a hardware. 


And i get a better clue from a question similar to my question.




Community H2O Question ""Error in .h2o.doSafeREST: Could not resolve host: localhost"". It says in answer point number 2. This is happen because ""H2O is still serving previous request(s) and this request could not go through"".




I think API in R and Python using Curl and requests to connect with H2O Rest API. Because the request is too many, H2O Server is no capable to handle it and gives me this error.


I also trying to slow down the request, but i dont know how to do it. Are you have a better solution to this problem.


Thanks a lot 


P.S.
I also get this problem in Sparkling Water 1.6.11 and 2.1.8 using YARN. Both suddenly stopped working when try to build Deep Learning Model with same dataset. 


Container in 
yarn application -list
 is killed without my interference. I dont know why, but i think this is a same problem.","['python', 'r', 'deep-learning', 'h2o', 'sparkling-water']",Unknown,,N/A
44269267,44269267,2017-05-30T18:19:26,2017-07-17 17:54:02Z,0,"I've installed H2O 3.11.0.266 on a Ubuntu 16.04 with CUDA 8.0 and libcudnn.so.5.1.10 so I believe H2O should be able to find my GPUs.


However, when I start up my h2o.init() in Python, I do not see evidence that it is actually using my GPUs.  I see:




H2O cluster total cores:     8


H2O cluster allowed cores:   8




which is the same as I had in the previous version (pre GPU).


Also, 
http://127.0.0.1:54321/flow/index.html
 shows only 8 cores as well.


I wonder if I don't have something properly installed or whether the latest h2o.init() hasn't implemented info about what GPUs are available or what...


Many thanks in advance.


[edit]

I should have mentioned that 3.11.0.266 is supposed to be the version that supports GPUs.


[edit]

Thanks for all the suggestions.  I'm now running H2O 3.13.0.337


I found this command also useful:


 sudo watch -n 0.1 'ps f -o user,pgrp,pid,pcpu,pmem,start,time,command -p `/usr/bin/lsof -n -w -t /dev/nvidia*`'



But, I'm a tad puzzled.


When I run XGBoost, I clearly see that the GPUs are very active 30 to 40% utilization (as well as all 8 of my CPU cores, which I guess must be managing the GPUs.)  XGB finishes my classification problem in 20 seconds.


GLM runs pretty fast, so it's a little hard to tell if it's using my GPUs (done in less than a second.  It does start clock in the STARTED column displayed by the ps program.


USER      PGRP   PID %CPU %MEM  STARTED     TIME COMMAND
user      3380  3380  116 12.0 10:52:56 04:36:36 /usr/local/anaconda2/bin/java -ea -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -jar /usr/local/anaconda2/lib/python2.7/site-packages/h



Distributed Random Forest starts the clock too, but doesn't seem to use any GPU processing but it does use all the CPU cores.


GBM is similar.  It takes 1.5 minutes to train the same problem compared to 20sec for XGB.  Since the algorithms are similar, I would have expected them to take the similar amount of time and use the GPUs in a similar way.  I find this surprising.


I'm convinced that XGBoost is working the GPUs, but I'm not sure if any of the other algorithms are.


[added]


By way of comparison on H2O  3.13.0.341.  Noticed the difference in temperature(!) and percentage GPU


Here's what gpustat -cup shows when I run 
xgboost
:


[0] GeForce GTX 1080 | 64'C,  90 % |  1189 /  8105 MB | clem:java/31183(191M)



Here's what it shows when I run 
Distributed Random Forest
 (similar results occur for GBM and DeepLearning)


[0] GeForce GTX 1080 | 51'C,   5 % |  1187 /  8105 MB | clem:java/31183(189M)","['python', 'gpu', 'nvidia', 'h2o', 'xgboost']",Unknown,,N/A
44268964,44268964,2017-05-30T18:02:24,2017-09-18 15:36:24Z,496,"I'm trying to create and save a generated model directly from Java. The 
documentation
 specifies how to do this in R and Python, but not in Java. A similar 
question
 was asked before, but no real answer was provided (beyond linking to H2O doc, which doesn't contain a code example).


It'd be sufficient for my present purpose get some pointers to be able to translate the following reference code to Java. I'm mainly looking for guidance on the relevant JAR(s) to import from the 
Maven repository
. 


import h2o
h2o.init()
path = h2o.system_file(""prostate.csv"")
h2o_df = h2o.import_file(path)
h2o_df['CAPSULE'] = h2o_df['CAPSULE'].asfactor()
model = h2o.glm(y = ""CAPSULE"",
            x = [""AGE"", ""RACE"", ""PSA"", ""GLEASON""],
            training_frame = h2o_df,
            family = ""binomial"")
h2o.download_pojo(model)","['java', 'maven', 'h2o']",RDK,https://stackoverflow.com/users/1165786/rdk,943
44259044,44259044,2017-05-30T09:56:47,2017-05-31 08:23:18Z,103,"We are developing a Java programm with H2O 3.10.4.7 and we need to retrieve metadata about all columns in a frame such as column names and datatypes. Related question (not resolved, different problem) 
here
.


Our expectation was that the 
water.bindings.H2oApi
 Client works just like the REST endpoints and we wanted to use the the H2oApi method 
frameColumns(FrameKeyV3 frameId)
 described in Javadoc: 




""Return all the columns from a Frame.""




but the result does not include any column-related info. 
When we use the REST endpoint via Browser, we get detailed information about the columns in the frame like expected. Examples:


H2oApi:


FramesV3 params = new FramesV3();
params.frameId = stringToKey(frame, FrameKeyV3.class); // ""frame"" reference from import process, works

// try with frameColumns()
FramesV3 frameColumns = api.frameColumns(params);
System.out.println(frameColumns.toString()); // ""columns"" not included in JSON response



Result:


{
   ""column"":"""",
   ""row_offset"":0,
   ""row_count"":0,
   ""column_offset"":0,
   ""column_count"":0,
   ""find_compatible_models"":false,
   ""path"":"""",
   ""force"":false,
   ""num_parts"":1,
   ""frames"":[
      {
         ""frame_id"":{
            ""name"":""3d257aae-7266-48a1-8f35-1243436616ab"",
            ""type"":""Key\u003cFrame\u003e"",
            ""URL"":""/3/Frames/3d257aae-7266-48a1-8f35-1243436616ab""
         },
         ""byte_size"":1928,
         ""is_text"":false,
         ""_exclude_fields"":""""
      }
   ],
   ""_exclude_fields"":""""
}



REST endpoint:
 


http://localhost:54321/3/Frames/3d257aae-7266-48a1-8f35-1243436616ab/columns


Result:


{

    ""__meta"": {
        ""schema_version"": 3,
        ""schema_name"": ""FramesV3"",
        ""schema_type"": ""Frames""
    },
    ""_exclude_fields"": """",
    ""row_offset"": 0,
    ""row_count"": 0,
    ""column_offset"": 0,
    ""column_count"": 0,
    ""job"": null,
    ""frames"": [
        {
            ""__meta"": {
                ""schema_version"": 3,
                ""schema_name"": ""FrameV3"",
                ""schema_type"": ""Frame""
            },
            ""_exclude_fields"": """",
            ""frame_id"": {
                ""__meta"": {
                    ""schema_version"": 3,
                    ""schema_name"": ""FrameKeyV3"",
                    ""schema_type"": ""Key<Frame>""
                },
                ""name"": ""3d257aae-7266-48a1-8f35-1243436616ab"",
                ""type"": ""Key<Frame>"",
                ""URL"": ""/3/Frames/3d257aae-7266-48a1-8f35-1243436616ab""
            },
            ""byte_size"": 1928,
            ""is_text"": false,
            ""row_offset"": 0,
            ""row_count"": 100,
            ""column_offset"": 0,
            ""column_count"": 5,
            ""total_column_count"": 5,
            ""checksum"": -7731554748204616990,
            ""rows"": 150,
            ""num_columns"": 5,
            ""default_percentiles"": [
                0.001,
                0.01,
                0.1,
                0.2,
                0.25,
                0.3,
                0.3333333333333333,
                0.4,
                0.5,
                0.6,
                0.6666666666666666,
                0.7,
                0.75,
                0.8,
                0.9,
                0.99,
                0.999
            ],
            ""columns"": [
                {
                    ""__meta"": {
                        ""schema_version"": 3,
                        ""schema_name"": ""ColV3"",
                        ""schema_type"": ""Vec""
                    },
                    ""label"": ""sepal_length"",
                    ""missing_count"": 0,
                    ""zero_count"": 0,
                    ""positive_infinity_count"": 0,
                    ""negative_infinity_count"": 0,
                    ""mins"": [
                        4.3,
                        4.4,
                        4.4,
                        4.4,
                        4.5
                    ],
                    ""maxs"": [
                        7.9,
                        7.7,
                        7.7,
                        7.7,
                        7.7
                    ],
                    ""mean"": 5.843333333333334,
                    ""sigma"": 0.8280661279778637,
                    ""type"": ""real"",
                    ""domain"": null,
                    ""domain_cardinality"": 0,
                    ""data"": [ ... ]
            ""string_data"": null,
            ""precision"": 1,
            ""histogram_bins"": null,
            ""histogram_base"": 0.0,
            ""histogram_stride"": 0.0,?
            ""percentiles"": null
    ...
}



We are interested in the JSON entity ""columns"" to retrieve the metadata we need but it is not provided by the H2o Java Api.


Best regards!


Nico","['java', 'h2o']",entusias,https://stackoverflow.com/users/8047329/entusias,11
44257257,44257257,2017-05-30T08:31:25,2017-05-30 08:31:25Z,142,"Using the machine learning platform H2O (Python in this instance) how can I determine ""confidence"" on an unknown categorical prediction? 


The issue is that I can get great results for known categories, but I also want to run it against data which has categories the model has not seen before. Outputting unknown or N/A would be perfect for these results. As far as I can tell H2O uses sigmoid on the last layer of its deep learning models, so all of the categories add up to one.


Is there an easy way to determine the confidence? I have thought of the following, but neither of them seem like a great idea:




Include an ""unknown"" category and assign it random data


Create an entirely new model that takes the prediction of the first model and tries to determine if it is correct or not




Thanks",['h2o'],survot,https://stackoverflow.com/users/8085119/survot,11
44252106,44252106,2017-05-30T01:25:01,2019-08-13 18:45:13Z,0,"I am having trouble getting DeepWater up and running in R via H2O. I have downloaded the most recent .jar file ( I think) but I get the following error this install command:


install.packages(""C:\\Users\\..\\Documents\\R\\win-
library\\3.4\\h2o_3.10.3.99999.tar.gz"", repos = NULL, type = ""source"",
lib=""C:\\Users\\..\\Documents\\R\\win-library\\3.4"")
* installing *source* package 'h2o' ...
** R Error in .install_package_code_files(""."", instdir) :  files in 'C:/Users/.../AppData/Local/Temp/RtmpuyZ5fc/R.INSTALL3783508571d/h2o/R' missing from 'Collate' field:   xgboost.R ERROR: unable to collate and parse R files for package 'h2o'
* removing 'C:/Users/.../Documents/R/win-library/3.4/h2o'
* restoring previous 'C:/Users/.../Documents/R/win-library/3.4/h2o' Warning in install.packages :   running command '""C:/PROGRA~1/R/R-34~1.0/bin/x64/R"" CMD INSTALL -l ""C:\Users\...\Documents\R\win-library\3.4"" ""C:/Users/.../Documents/R/win-library/3.4/h2o_3.10.3.99999.tar.gz""' had status 1 Warning in install.packages :   installation of package ‘C:/Users/.../Documents/R/win-library/3.4/h2o_3.10.3.99999.tar.gz’ had non-zero exit status Error in if (file.exists(dest) && file.mtime(dest) > file.mtime(lib) &&  :    missing value where TRUE/FALSE needed","['r', 'deep-learning', 'gpu', 'h2o']",Mateusz Dymczyk,https://stackoverflow.com/users/217019/mateusz-dymczyk,15.1k
44241161,44241161,2017-05-29T11:14:03,2017-05-30 18:40:57Z,0,"library(rsparkling)
library(sparklyr)
library(h2o)
test <- as_h2o_frame(sc, partitions$test, strict_version_check = FALSE)  



the error is following:


Error: java.lang.ClassNotFoundException: org.apache.spark.h2o.H2OContext
at java.net.URLClassLoader.findClass(Unknown Source)
at java.lang.ClassLoader.loadClass(Unknown Source)
……



Could you help me solve this question? thanks.","['r', 'apache-spark', 'h2o', 'sparklyr']",Sébastien Rochette,https://stackoverflow.com/users/7405597/s%c3%a9bastien-rochette,"6,661"
44240487,44240487,2017-05-29T10:41:30,2018-02-19 22:26:14Z,0,"I try to run Automl of h2o in R:


  data_train_hex=as.h2o(data_train)
  data_val_hex=as.h2o(data_val)
  automl <-h2o.automl(x=training_vars,y=targetvar, training_frame= data_train_hex
                       ,validation_frame = data_val_hex
                       ,build_control = NULL, max_runtime_secs = 30)



but when I run previous code in second time it's displayed: 


java.lang.NullPointerException
    at hex.StackedEnsembleModel.predictScoreImpl(StackedEnsembleModel.java:117)
    at hex.Model.score(Model.java:1076)
    at hex.Model.score(Model.java:1044)
    at ai.h2o.automl.Leaderboard$1.atomic(Leaderboard.java:207)
    at ai.h2o.automl.Leaderboard$1.atomic(Leaderboard.java:176)
    at water.TAtomic.atomic(TAtomic.java:17)
    at water.Atomic.compute2(Atomic.java:56)
    at water.Atomic.fork(Atomic.java:39)
    at water.Atomic.invoke(Atomic.java:31)
    at ai.h2o.automl.Leaderboard.addModels(Leaderboard.java:176)
    at ai.h2o.automl.Leaderboard.addModel(Leaderboard.java:273)
    at ai.h2o.automl.AutoML.addModel(AutoML.java:1043)
    at ai.h2o.automl.AutoML.pollAndUpdateProgress(AutoML.java:425)
    at ai.h2o.automl.AutoML.learn(AutoML.java:911)
    at ai.h2o.automl.AutoML.run(AutoML.java:326)
    at ai.h2o.automl.H2OJob$1.compute2(H2OJob.java:32)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1314)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

    Error: java.lang.NullPointerException



Can some one explain me, and give me solution for that when I must do loop for different dataset?","['r', 'h2o']",Hack-R,https://stackoverflow.com/users/3604745/hack-r,23.1k
44237304,44237304,2017-05-29T07:55:06,2017-05-29 08:23:44Z,146,"Water meter (CPU meter) does not work on my Windows standalone h2o instance. It shows ""not linux"" message.


Using version h2o-3.10.4.6 of h2o on Windows 8.1.




Was the feature not yet implemented for Windows deployments or is there another thing i need to consider?


Thanks!","['windows-8.1', 'h2o']",Mateusz Dymczyk,https://stackoverflow.com/users/217019/mateusz-dymczyk,15.1k
44220933,44220933,2017-05-27T19:40:18,2017-05-27 20:34:32Z,0,"I have models that depend on version 3.10.0.6 of the R package 
h2o
. In a new machine I tried to install from the source file 
h2o_3.10.0.6.tar.gz
,


install.packages(""h2o_3.10.0.6.tar.gz"", repos=NULL, type=""source"")



After 10 min, it came back with this complaint,


Performing one-time download of h2o.jar from
 http://s3.amazonaws.com/h2o-release/h2o/rel-turing/6/Rjar/h2o.jar 
(This could take a few minutes, please be patient...)
Warning in download.file(url = h2o_url, destfile = temp_file, mode = ""wb"",  :
  downloaded length 60135652 != reported length 62422402
Warning in download.file(url = h2o_url, destfile = temp_file, mode = ""wb"",  :
  URL 'http://s3.amazonaws.com/h2o-release/h2o/rel-turing/6/Rjar/h2o.jar': status was 'Failure when receiving data from the peer'
Error in download.file(url = h2o_url, destfile = temp_file, mode = ""wb"",  : 
  download from 'http://s3.amazonaws.com/h2o-release/h2o/rel-turing/6/Rjar/h2o.jar' failed



I understand it's an older version and so JVM might have been removed. But could the h2o folks leave the old JVM somewhere so folks like me could still use a previous version.","['r', 'package', 'h2o']",horaceT,https://stackoverflow.com/users/2434201/horacet,651
44205039,44205039,2017-05-26T15:18:55,2017-05-26 20:21:34Z,164,"I am using Steam to attempt to build a prediction service using a python preprocessing script. When python passes the cleaned data to the prediction service in the 


variable:value var2:value2 var3:value3



format (as seen in the 
Spam Detection Example
) I get a 


ERROR PredictPythonServlet - Failed to parse



error from the service. When I look at the 
PredictPythonServlet.java
 file it seems to only use the 
strMapToRowData
 function which assumes every value in the input string is a number:


for (String p : pairs) {
    String[] a = p.split("":"");
    String term = a[0];
    double value = Float.parseFloat(a[1]);
    row.put(term, value);
  }



Are character values not allowed to be sent in this format? If so is there a way to get the 
PredictPythonServlet
 file to use the 
csvToRowData
 function that is defined but never used? I'd like to not have to use One-Hot encoding for my models so being able to pass the actual character string representation would be ideal. 


Additionally, I passed the numeric representation found in the model pojo file for the categorical variables and received the error:


hex.genmodel.easy.exception.PredictUnknownTypeException: Unexpected object type java.lang.Double for categorical column home_team



So it looks like the service expects a character string but I can't figure out how to pass it along to the actual model. Any help would be greatly appreciated!",['h2o'],Matt Mills,https://stackoverflow.com/users/3625934/matt-mills,608
44137342,44137342,2017-05-23T14:17:08,2017-05-23 16:09:36Z,0,"I have built model with TensorFlow backend on machine with GPU. 
Now i want to use this model on production machine which doesn't have GPU.
I cannot use this model without installed TensorFlow backend but I also cannot install TensorFlow backend on machine without GPU.


Is there any option to built TensorFlow backend for H2O without GPU support?","['r', 'tensorflow', 'gpu', 'h2o']",talonmies,https://stackoverflow.com/users/681865/talonmies,72.2k
44124312,44124312,2017-05-23T01:17:31,2017-05-29 00:27:58Z,112,"I have an H2O frame X from which I extract 3 different H2O frames X1, X2, X2 (based on some filtering condition)


I want to run a grid search on X1 and get Model1, a grid search on X2 and get Model2..etc


What is the best way to do that?",['h2o'],user3731994,https://stackoverflow.com/users/3731994/user3731994,11
44112659,44112659,2017-05-22T12:17:35,2017-05-22 13:39:39Z,331,"I want to store a created model within sparkling water as a binary file so that I can can reload it with a different application.


What is the best way?","['h2o', 'sparkling-water']",Unknown,,N/A
44110233,44110233,2017-05-22T10:10:22,2017-05-30 10:48:53Z,239,"I work on a Java project using the H2O (3.10.4.7) REST Api provided by the H2O Java bindings and I have the following problem:


We need to retrieve Metadata from existing H2O Frames like:




Column Names and


DataTypes of those columns,




preferrably using the H2oApi.class.


Our approach is to fetch one Row from the H2O Frame and then use it to get the Metadata we need. 


So far I tried the following: 


 FramesV3 targetFrame = new FramesV3();
 targetFrame.frameId = frameKey; // key provided by import process - works
 targetFrame.rowCount = 1; // get one row to figure out specs of all cols
 H2OApi h2oApi = new H2OApi(""http://localhost:54321/"");
 FramesV3 result = h2oApi.frames(targetFrame);



I get this Exception:


 java.lang.IllegalArgumentException: @Field parameters can only be used with form encoding. (parameter #1)
 for method Frames.list
 at retrofit2.ServiceMethod$Builder.methodError(ServiceMethod.java:695)
 at retrofit2.ServiceMethod$Builder.methodError(ServiceMethod.java:686)
 at retrofit2.ServiceMethod$Builder.parameterError(ServiceMethod.java:704)
 at retrofit2.ServiceMethod$Builder.parseParameterAnnotation(ServiceMethod.java:476)
 at retrofit2.ServiceMethod$Builder.parseParameter(ServiceMethod.java:328)
 at retrofit2.ServiceMethod$Builder.build(ServiceMethod.java:201)
 at retrofit2.Retrofit.loadServiceMethod(Retrofit.java:166)
 at retrofit2.Retrofit$1.invoke(Retrofit.java:145)
 at com.sun.proxy.$Proxy14.list(Unknown Source)
 at water.bindings.H2oApi.frames(H2oApi.java:882)
 at H2ORestCloudTests.getSpecFromFrameTest(H2ORestCloudTests.java:388)



If I use the REST API Frames Endpoint via Browser it works. 
Example:


""http://localhost:54321/3/Frames/89a05762-4bcd-41d8-a800-f9cfc3ac73dd?row_count=1""



Result in Browser:


 {

 ""__meta"": {
     ""schema_version"": 3,
     ""schema_name"": ""FramesV3"",
     ""schema_type"": ""Frames""
 },
 ""_exclude_fields"": """",
 ""row_offset"": 0,
 ""row_count"": 1,
 ""column_offset"": 0,
 ""column_count"": 0,
 ""job"": null,
 ""frames"": [
     {
         ""__meta"": {
             ""schema_version"": 3,
             ""schema_name"": ""FrameV3"",
             ""schema_type"": ""Frame""
         },
         ""_exclude_fields"": """",
         ""frame_id"": {
             ""__meta"": {
                 ""schema_version"": 3,
                 ""schema_name"": ""FrameKeyV3"",
                 ""schema_type"": ""Key<Frame>""
             },
             ""name"": ""89a05762-4bcd-41d8-a800-f9cfc3ac73dd"",
             ""type"": ""Key<Frame>"",
             ""URL"": ""/3/Frames/89a05762-4bcd-41d8-a800-f9cfc3ac73dd""
         },
         ""byte_size"": 1928,
         ""is_text"": false,
         ""row_offset"": 0,
         ""row_count"": 1,
         ""column_offset"": 0,
         ""column_count"": 5,
         ""total_column_count"": 5,
         ""checksum"": -7731554748204616990,
         ""rows"": 150,
         ""num_columns"": 5,
         ""default_percentiles"": [
             0.001,
             0.01,
             0.1,
             0.2,
             0.25,
             0.3,
             0.3333333333333333,
             0.4,
             0.5,
             0.6,
             0.6666666666666666,
             0.7,
             0.75,
             0.8,
             0.9,
             0.99,
             0.999
         ],
         ""columns"": [
             {
                 ""__meta"": {
                     ""schema_version"": 3,
                     ""schema_name"": ""ColV3"",
                     ""schema_type"": ""Vec""
                 },
                 ""label"": ""sepal_length"",
                 ""missing_count"": 0,
                 ""zero_count"": 0,
                 ""positive_infinity_count"": 0,
                 ""negative_infinity_count"": 0,
                 ""mins"": [
                     4.3,
                     4.4,
                     4.4,
                     4.4,
                     4.5
                 ],
                 ""maxs"": [
                     7.9,
                     7.7,
                     7.7,
                     7.7,
                     7.7
                 ],
                 ""mean"": 5.843333333333334,
                 ""sigma"": 0.8280661279778637,
                 ""type"": ""real"",
                 ""domain"": null,
                 ""domain_cardinality"": 0,
                 ""data"": [
                     5.1000000000000005
                 ],
                 ""string_data"": null,
                 ""precision"": 1,
                 ""histogram_bins"": null,
                 ""histogram_base"": 0.0,
                 ""histogram_stride"": 0.0,
                 ""percentiles"": null
             },
             ...
 }



Am I missing something or is there maybe a better way to get those Frame infos via the H2O Java Rest API?


Thank you in advance and have a nice day!


Nico","['java', 'h2o']",entusias,https://stackoverflow.com/users/8047329/entusias,11
44094810,44094810,2017-05-21T08:34:54,2017-05-22 05:36:47Z,655,I am abit investigating the h2o framework to work with its extra machine learning tool. I am just curious what is the differences between an H20 dataframes and Spark RDDs. Is the h2o dataframes can be cached or persisted like Spark RDDs?,['h2o'],Luckylukee,https://stackoverflow.com/users/7886897/luckylukee,585
44092152,44092152,2017-05-21T00:45:04,2020-07-01 13:45:13Z,0,"I would like to know, what is the specific method / formula to calculate the variable importance of the GBM model in h2o package, both for continuous and categorical variables.","['h2o', 'gbm']",suprvisr,https://stackoverflow.com/users/4363456/suprvisr,127
44089162,44089162,2017-05-20T17:56:19,2017-05-21 00:18:08Z,0,"I am building a binary classification model in H2O with Python. My 'y' values are 'ok' and 'bad'. I need the metrics to be computed with ok = negative class = 0 and bad = positive class = 1. However, I do not see any way to set this in H2O. For example here is the output of the predictions and confusion matrix:


confusion matrix
        bad    ok  Error              Rate
  bad  3859   631 0.1405    (631.0/4490.0)
   ok   477  1069 0.3085    (477.0/1546.0)
Total  4336  1700 0.1836   (1108.0/6036.0)


>>> predictions.head(10)
  predict       bad        ok
0     bad  0.100604  0.899396
1     bad  0.100604  0.899396
2     bad  0.112232  0.887768
3      ok  0.068917  0.931083
4      ok  0.089706  0.910294
5      ok  0.089706  0.910294
6      ok  0.089706  0.910294
7     bad  0.126182  0.873818
8     bad  0.126182  0.873818
9      ok  0.092306  0.907694



H2O seems to arbitrarily decide based on alphabetical order among the labels. If I change the labels to 'ok' and 'sad' here is what I get:


confusion matrix
         ok   sad  Error             Rate
   ok   798   732 0.4784   (732.0/1530.0)
  sad   211  4381 0.0459   (211.0/4592.0)
Total  1009  5113 0.1540   (943.0/6122.0)


>>> predictions.head(10)
  predict        ok       sad
0     sad  0.215206  0.784794
1     sad  0.211073  0.788927
2     sad  0.211073  0.788927
3      ok  0.236190  0.763810
4      ok  0.241641  0.758359
5      ok  0.241641  0.758359
6      ok  0.236099  0.763901
7     sad  0.162072  0.837928
8     sad  0.162072  0.837928
9     sad  0.206146  0.793854



There must be a way to programmatically set which label is the positive class and which is the negative class?","['python', 'classification', 'h2o']",denson,https://stackoverflow.com/users/1810559/denson,"2,456"
44084312,44084312,2017-05-20T09:42:08,2017-05-24 20:50:27Z,0,"setwd(""D:/Santander"")

## import train dataset
train<-read.csv(""train.csv"",header=T)


dim(train)

summary(train)

str(train)

prop.table(table(train2$TARGET))

stats<-function(x){
  length<-length(x)
  nmiss<-sum(is.na(x))
  y<-x[!is.na(x)]
  freq<-as.data.frame(table(y))
  max_freq<-max(freq[,2])/length
  min<-min(y)
  median<-median(y)
  max<-max(y)
  mean<-mean(y)
  freq<-length(unique(y))
  return(c(nmiss=nmiss,min=min,median=median,mean=mean,max=max,freq=freq,max_freq=max_freq))
}


var_stats<-sapply(train,stats)

var_stats_1<-t(var_stats)

###将最大频数类别比例超过0.9999，其它类别小于1/10000的变量全删除

exclude_var<-rownames(var_stats_1)[var_stats_1[,7]>0.9999]

train2<-train[,! colnames(train) %in% c(exclude_var,""ID"")]




rm(list=setdiff(ls(),""train2""))

train2<-train2[1:10000,]

write.csv(train2,""example data.csv"",row.names = F)

##随机将数据分为训练集与测试集
set.seed(1)
ind<-sample(c(1,2),size=nrow(train2),replace=T,prob=c(0.8,0.2))

train2$TARGET<-factor(train2$TARGET)
train_set<-train2[ind==1,]
test_set<-train2[ind==2,]

rm(train2)
##1\用R randomForest构建预测模型 100棵树
library(randomForest)

memory.limit(4000)

random<-randomForest(TARGET~.,data=train_set,ntree=50)

print(random)

random.importance<-importance(random)

p_train<-predict(random,train_set,type=""prob"")

pred.auc<-prediction(p_train[,2],train_set$TARGET)

performance(pred.auc,""auc"")

##train_set auc=0.8177


## predict test_set
p_test<-predict(random,newdata = test_set,type=""prob"")

pred.auc<-prediction(p_test[,2],test_set$TARGET)
performance(pred.auc,""auc"")

##test_set auc=0.60


#________________________________________________#

##_________h2o.randomForest_______________

library(h2o)
h2o.init()

train.h2o<-as.h2o(train_set)
test.h2o<-as.h2o(test_set)

random.h2o<-h2o.randomForest(,""TARGET"",training_frame = train.h2o,ntrees=50)


importance.h2o<-h2o.varimp(random.h2o)

p_train.h2o<-as.data.frame(h2o.predict(random.h2o,train.h2o))

pred.auc<-prediction(p_train.h2o$p1,train_set$TARGET)

performance(pred.auc,""auc"")

##auc=0.9388, bigger than previous one

###test_set prediction

p_test.h2o<-as.data.frame(h2o.predict(random.h2o,test.h2o))

pred.auc<-prediction(p_test.h2o$p1,test_set$TARGET)

performance(pred.auc,""auc"")

###auc=0.775



I tried to make predictions with Kaggle competitions: Santander customer satisfaction: 
https://www.kaggle.com/c/santander-customer-satisfaction

When i use randomForest package in R, i got final result in test data of AUC=0.57, but when i use h2o.randomForest, i got final result in test data of AUC=0.81.the parameters in both function are same, i only used the default parameters with ntree=100.
So why h2o.randomForest make much better predictions than randomForest package itself?","['r', 'random-forest', 'h2o']",JC. Sun,https://stackoverflow.com/users/8040168/jc-sun,31
44056120,44056120,2017-05-18T19:15:35,2017-05-19 13:48:23Z,235,"Having a trained DRFModel instance in scala, what's the best way of generating the corresponding MojoModel object for scoring? from the api s I've seen so far, mostly are around exporting to a file and then loading back up using the MojoModel.load(path) for instance but no direct conversion?","['h2o', 'sparkling-water']",x89a10,https://stackoverflow.com/users/1367070/x89a10,691
44034944,44034944,2017-05-17T21:17:26,2020-04-30 17:29:43Z,0,"My apologies if I'm missing something obvious. I've been thoroughly enjoying working with h2o in the last few days using R interface. I would like to evaluate my model, say a random forest, by plotting an ROC. The documentation seems to suggest that there is a straightforward way to do that:




Interpreting a DRF Model




By default, the following output displays:


Model parameters (hidden)


A graph of the scoring history (number of trees vs. training MSE)


A graph of the ROC curve (TPR vs. FPR)


A graph of the variable importances
...






I've also seen that in python you can apply 
roc
 function 
here
. But I can't seem to be able to find the way to do the same in R interface. Currently I'm extracting predictions from the model using 
h2o.cross_validation_holdout_predictions
 and then use pROC package from R to plot the ROC. But I would like to be able to do it directly from the H2O model object, or, perhaps, a H2OModelMetrics object.


Many thanks!","['r', 'h2o', 'roc']",Community,https://stackoverflow.com/users/-1/community,1
44034563,44034563,2017-05-17T20:50:07,2017-05-18 12:33:12Z,0,"I have built latest h2o version (3.11.0.99999) with deepwater and I'm getting error after running h2o.deepwater with TensorFlow backend. 


This is my code:


model_tf<-h2o.deepwater(x=2:ncol(train),
                        y=1,
                        backend = ""tensorflow"",
                        training_frame = train
                        )



This is error:




java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: resource mlp_1093x1x1_32.meta not found.


java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: resource mlp_1093x1x1_32.meta not found.
    at hex.deepwater.DeepWaterModelInfo.setupNativeBackend(DeepWaterModelInfo.java:259)
    at hex.deepwater.DeepWaterModelInfo.(DeepWaterModelInfo.java:206)
    at hex.deepwater.DeepWaterModel.(DeepWaterModel.java:227)
    at hex.deepwater.DeepWater$DeepWaterDriver.buildModel(DeepWater.java:131)
    at hex.deepwater.DeepWater$DeepWaterDriver.computeImpl(DeepWater.java:118)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:173)
    at hex.deepwater.DeepWater$DeepWaterDriver.compute2(DeepWater.java:111)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1240)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)


Error: java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: resource mlp_1093x1x1_32.meta not found.




The same code but with ""mxnet"" backend is working well.","['r', 'tensorflow', 'h2o']",filius_arator,https://stackoverflow.com/users/7515482/filius-arator,139
44033458,44033458,2017-05-17T19:39:17,2017-05-17 19:39:17Z,145,"I have been able to run the ChicagoCrimeDemo.py script using spark-submit successfully (spark-submit --master=yarn-client --py-files /opt/sparkling-water-1.6.10/py/build/dist/h2o_pysparkling_1.6-1.6.10-py2.7.egg /opt/sparkling-water-1.6.10/py/examples/scripts/ChicagoCrimeDemo.py) .


Although when I try to execute the same script using Livy(Spark), I am getting the following error:","['h2o', 'sparkling-water']",Yandy Perez Ramos,https://stackoverflow.com/users/5787432/yandy-perez-ramos,46
44033089,44033089,2017-05-17T19:16:37,2017-05-17 19:38:05Z,0,"I have a h2o data table with 40 columns and 1 million rows. I want do a random selection of 0.3 million rows without replacement. The H2o.sample function i looked online gives the error (I've already start h2o cluster)


Error: could not find function ""h2o.sample""



Is there any other way i can do this? Thanks in advance!",['h2o'],Connie Chen,https://stackoverflow.com/users/6441095/connie-chen,113
44028468,44028468,2017-05-17T15:01:01,2019-01-31 15:42:37Z,114,"I have tried the example under the steam/prediction-service-builder/examples/spam-detection-python , but get some error about
python:


This is an example of NOT SPAM
""Sorry din lock my keypad""

http://localhost:55001/pypredict?Sorry%20%20din%20lock%20my%20keypad
<html>
  <head>
    <meta http-equiv=""Content-Type"" content=""text/html; charset=ISO-8859-1""/>
  <title>Error 406 1</title>
  </head>

  <body><h2>HTTP ERROR 406</h2>
    <p>Problem accessing /pypredict. Reason:
    <pre>    1</pre></p><hr /><i><small>Powered by Jetty://</small></i> 
    <br/>

  </body>
</html>",['h2o'],Rob,https://stackoverflow.com/users/162698/rob,15.1k
44019122,44019122,2017-05-17T08:12:13,2017-05-17 16:13:36Z,0,"I have the following 
data.table
.


  fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density   pH
   1:           7.0             0.27        0.36           20.7     0.045                  45                  170 1.00100 3.00
   2:           6.3             0.30        0.34            1.6     0.049                  14                  132 0.99400 3.30
   3:           8.1             0.28        0.40            6.9     0.050                  30                   97 0.99510 3.26
   4:           7.2             0.23        0.32            8.5     0.058                  47                  186 0.99560 3.19
   5:           7.2             0.23        0.32            8.5     0.058                  47                  186 0.99560 3.19   

sulphates alcohol   quality
   1:      0.45     8.8  Bad wine
   2:      0.49     9.5  Bad wine
   3:      0.44    10.1  Bad wine
   4:      0.40     9.9  Bad wine
   5:      0.40     9.9  Bad wine



I can run 


system.time(model_glm <- h2o.glm(x = 1:11, y = 12, training_frame = wine.train.h2o,
                                 validation_frame = wine.test.h2o, seed = 42,
                                 family = ""binomial""))



to train a glm on this data set. Later in order to get the partial dependency plot i can use 


glm_pp <- rbindlist(lapply(glm_pp, function(x){melt(x, id.vars=""mean_response"")}))
ggplot(glm_pp, aes(x=value, y=mean_response)) + geom_point()  + facet_wrap(~variable, scale=""free_x"") +
  geom_smooth(method=""loess"") + theme_pl() + ggtitle(""Partial dependence plot"")



In my case my 
y
 is the 
quality
, which is a 
binary variable
.


How could I get a partial dependency plot if my 
dependent variable
 had 3 or more categories, so if i ran glm using 
family = multinomial
 ?","['r', 'plot', 'h2o']",quant,https://stackoverflow.com/users/5868293/quant,"4,438"
44011357,44011357,2017-05-16T20:51:21,2018-06-25 13:31:32Z,0,"I am running h2o through Rstudio Server on a linux server with 64 GB of RAM. When I initialize the cluster it says that the total cluster memory is only 9.78 GB. I have tried using the max_mem_size parameter but still only using 9.78 GB.


localH2O <<- h2o.init(ip =  ""localhost"", port = 54321, nthreads = -1, max_mem_size = ""25g"")
H2O is not running yet, starting it now...
java version ""1.8.0_131""
Java(TM) SE Runtime Environment (build 1.8.0_131-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)
    Connection successful!
    R is connected to the H2O cluster: 
        H2O cluster uptime:         5 hours 10 minutes 
        H2O cluster version:        3.10.4.6 
        H2O cluster version age:    19 days  
        H2O cluster name:           H2O_started_from_R_miweis_mxv543 
        H2O cluster total nodes:    1 
        H2O cluster total memory:   9.78 GB 
        H2O cluster total cores:    16 
        H2O cluster allowed cores:  16 
        H2O cluster healthy:        TRUE 
        H2O Connection ip:          localhost 
        H2O Connection port:        54321 
        H2O Connection proxy:       NA 
        H2O Internal Security:      FALSE 
        R Version:                  R version 3.3.3 (2017-03-06) 



I ran the following on the server to insure the amount of memory available:


cat /proc/meminfo
MemTotal:       65806476 kB



EDIT:


I was looking more into this issue and it seems like it is a default within the JVM. When I started h2o directly in Java I was able to pass in the command  
-Xmx32g
 and it did increase the memory. I could then connect to that h2o instance in Rstudio and have access to the increases memory. I was wondering if there was a way to change this default value in the JVM and allow more memory so I don't have to first start the h2o instance from the command line then connect to it from Rstudio server.","['r', 'rstudio', 'h2o', 'rstudio-server']",Unknown,,N/A
43995293,43995293,2017-05-16T07:42:05,2017-05-17 01:43:39Z,0,"i am currently setting up a neural network in R to predict demand forecasts. i use the h20-package with a regression model because i would like to forecast demands based on historical data.
Currently the prototype is there and i receive some predictions, but some of the predicted values are negative and i dont know why.
Could you guys help me with that?


context
: 

I have 2 tables with each several hundret columns. The values are aggregated per week.
The first table contains configurations.
The second table shows the demand that results from these configurations


The structure of my code
:




load data


add 1 column of demand table to configurations (that i want to predict and train the neural network with) 


feature scaling (except last column)


train neural network


predict last column with test data




the code for my model:


model = h2o.deeplearning(y = [column to predict],
                           training_frame = as.h2o(training_data),
                           activation = 'Rectifier',
                           hidden = c(100,100),
                           epochs = 50,
                           train_samples_per_iteration = -2)



My problem now is my predictions contain negative values even though i use the Rectifier activation function.
Where is my mistake?


If u need any further information, let me know. I am a rooky in here :)


Thanks already for your help.","['r', 'neural-network', 'regression', 'prediction', 'h2o']",Stephan V.,https://stackoverflow.com/users/8015698/stephan-v,11
43940136,43940136,2017-05-12T14:24:32,2017-05-12 15:31:36Z,0,"We have been experiencing some odd (broken) behaviour at work when using large indexing sets with h2o data frames


The following code makes no changes to iris_h2o:


library(h2o)
h2o.init()
data(iris)

iris<-rbind(iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris)
iris<-rbind(iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris)
iris<-rbind(iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris)

iris_h2o<-as.h2o(iris)

iris_h2o[c(1:1000),""Sepal.Length""]<-iris_h2o[c(1:1000),""Sepal.Length""]
iris_h2o



Output:


  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa



However, if you increase the size of the indexing set to 1001, then suddenly it seems that the indexing between left and right sides becomes inconsistent


library(h2o)
h2o.init()
data(iris)

iris<-rbind(iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris)
iris<-rbind(iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris)
iris<-rbind(iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris,iris)

iris_h2o<-as.h2o(iris)


iris_h2o[c(1:1001),""Sepal.Length""]<-iris_h2o[c(1:1001),""Sepal.Length""]
iris_h2o



Output:


  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          5.1         3.0          1.4         0.2  setosa
3          4.9         3.2          1.3         0.2  setosa
4          4.7         3.1          1.5         0.2  setosa
5          4.6         3.6          1.4         0.2  setosa
6          5.0         3.9          1.7         0.4  setosa



This is confirmed on R 3.3.2 with h2o 3.10.0.8 as well as R 3.4.0 with the latest version of h2o available on CRAN (3.10.4.6)


We'd like to know what's causing this and how to avoid issues with this in the future","['r', 'h2o']",Unknown,,N/A
43936578,43936578,2017-05-12T11:20:39,2017-05-14 04:39:25Z,0,"H2o cannot start successfully, but I still can access link
localhost:12345/flow/index.html


Log in R Studio


Error in h2o.init(nthreads = -1, max_mem_size = ""4g"", ip = ""localhost"",  : 
  H2O failed to start, stopping execution.


Log in OS


[root@xxxx]# tail -200f /tmp/RtmpYou3gq/h2o_tuongnv4_started_from_r.out


: ----- H2O started  -----
: Build git branch: rel-ueno
: Build git hash: f521cfeb6cf6c41ca6a728cdbaafe96509170907
: Build git describe: jenkins-3.10.4.5-4-gf521cfe
: Build project version: 3.10.4.6 (latest version: unknown)
: Build age: 15 days
: Built by: 'jenkins'
: Built on: '2017-04-26 22:11:30'
: Watchdog Build git branch: (unknown)
: Watchdog Build git hash: (unknown)
: Watchdog Build git describe: (unknown)
: Watchdog Build project version: (unknown)
: Watchdog Built by: (unknown)
: Watchdog Built on: (unknown)
: Processed H2O arguments: [-name, H2O_started_from_R_tuongnv4_tju329, -ip, localhost, -port, 12345, -ice_root, /tmp/RtmpYou3gq]
: Java availableProcessors: 8
: Java heap totalMemory: 238.0 MB
: Java heap maxMemory: 3.56 GB
: Java version: Java 1.8.0_131 (from Oracle Corporation)
: JVM launch parameters: [-Xmx4g, -ea]
: OS version: Linux 3.10.0-514.el7.x86_64 (amd64)
: Machine physical memory: 15.44 GB
: X-h2o-cluster-id: 1494587541855
: User name: 'tuongnv4'
: Opted out of sending usage metrics.
: IPv6 stack selected: false
: Possible IP Address: enp0s31f6 (enp0s31f6), 192.168.170.166
: Possible IP Address: lo (lo), 127.0.0.1
: Selected H2O.CLOUD_MULTICAST_IF: name:lo (lo) doesn't support multicast
: H2O node running in unencrypted mode.
: Internal communication uses port: 12346
: Listening for HTTP and REST traffic on http://127.0.0.1:12345/
: H2O cloud name: 'H2O_started_from_R_tuongnv4_tju329' on localhost/127.0.0.1:12345, discovery address /228.246.241.107:58614
: If you have trouble connecting, try SSH tunneling from your local machine (e.g., via port 55555):
:   1. Open a terminal and run 'ssh -L 55555:localhost:12345 
[email protected]
'
:   2. Point your browser to http://localhost:55555
: Log dir: '/tmp/RtmpYou3gq/h2ologs'
: Cur dir: '/home/tuongnv4'
: HDFS subsystem successfully initialized
: S3 subsystem successfully initialized
: Flow dir: '/home/tuongnv4/h2oflows'
: Cloud of size 1 formed [localhost/127.0.0.1:12345]
: Registered parsers: [GUESS, ARFF, XLS, SVMLight, AVRO, PARQUET, CSV]
: Watchdog extension initialized
: Registered 1 extensions in: 2mS
: Registered: 148 REST APIs in: 283mS
: Registered: 209 schemas in 333ms
: H2O started in 2128ms
: 
: Open H2O Flow in your web browser: http://127.0.0.1:12345
: 
: GET /flow/index.html, parms: {}
: GET /flow/index.html, parms: {}
: GET /3/Metadata/endpoints, parms: {}
: Locking cloud to new members, because hex.schemas.DeepLearningV3
: GET /flow/help/catalog.json, parms: {}
: GET /3/ModelBuilders, parms: {}
: GET /3/NodePersistentStorage/categories/environment/names/clips/exists, parms: {}
: GET /3/About, parms: {}
: GET /3/NodePersistentStorage/notebook, parms: {}","['rstudio', 'h2o']",Tuong Nguyen,https://stackoverflow.com/users/7700946/tuong-nguyen,143
43879388,43879388,2017-05-09T20:35:53,2021-03-30 09:57:12Z,0,"I'm just testing out h2o, in particular its deep learning capabilities, since I've heard great things about it. So far I've been using the following code:


     library(h2o)
library(caret)
data(""iris"")

# Initiate H2O --------------------
h2o.removeAll() # Clean up. Just in case H2O was already running
h2o.init(nthreads = -1, max_mem_size=""22G"")  # Start an H2O cluster with all threads available

# Get training and tournament data -------------------
a <- createDataPartition(iris$Species, list=FALSE)
training <- iris[a,]
test <- iris[-a,]

# Convert target to factor -------------------
target <- as.factor(iris$Species)

feature_names <- names(train)[1:(ncol(train)-1)]

train_h2o <- as.h2o(train)
test_h2o <- as.h2o(test)

prob <- test[, ""id"", drop = FALSE]

model_dl <- h2o.deeplearning(x = feature_names, y = ""target"", training_frame = train_h2o, stopping_metric = ""logloss"")
h2o.logloss(model_dl)

pred_dl <- predict(model_dl, newdata = tourn_h2o)
prob <- cbind(prob, as.data.frame(pred_dl$p1, col.names = ""dl""))
write.table(prob[, c(""id"", ""dl"")], paste0(model_dl@model_id, "".csv""), sep = "","", row.names = FALSE, col.names = c(""id"", ""probability""))



The relevant part is really that last line, where I got the following error:


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Object 'DeepLearning_model_R_1494350691427_70' not found in function: predict for argument: model



Has anyone come across this before? Are there any easy solutions to this that I might be missing? Thanks in advance.


EDIT: With the updated code I get the error:


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Illegal argument(s) for DeepLearning model: DeepLearning_model_R_1494428751150_1.  Details: ERRR on field: _train: Training data must have at least 2 features (incl. response).
ERRR on field: _stopping_metric: Stopping metric cannot be logloss for regression.



I assume this has to do with the the way the Iris dataset is being read in.","['r', 'h2o']",Unknown,,N/A
43879240,43879240,2017-05-09T20:26:41,2017-05-09 20:26:41Z,69,"Background:

Given that 




I have a large table (~20 million rows, 40 columns).


I am using h2o to fit some models on the data in the table.


I found there is ""h2o.difflag1"" that gives a lag-1 transform.


I want to create a column that is ""lead ~8000"".




Question

How do I make a 3600 lead without making 8000 lags on 39 columns using h2o on my 20-million row dataset?  Is there a better way to do this?","['lag', 'h2o']",EngrStudent,https://stackoverflow.com/users/2259468/engrstudent,"1,972"
43877962,43877962,2017-05-09T19:04:42,2017-05-10 17:47:04Z,0,"I'm just testing out h2o since I've heard great things about it. So far I've been using the following code:


 library(h2o)

h2o.removeAll() # Clean up. Just in case H2O was already running
h2o.init(nthreads = -1, max_mem_size=""22G"")  # Start an H2O cluster with all threads available

train <- read.csv(""TRAIN"")
test <- read.csv(""TEST"")

target <- as.factor(train$target)

feature_names <- names(train)[1:(ncol(train)-1)]

train_h2o <- as.h2o(train)
test_h2o <- as.h2o(test)

prob <- test[, ""id"", drop = FALSE]

model_glm <- h2o.glm(x = feature_names,  y = ""target"", training_frame = train_h2o)
h2o.performance(model_glm) 

pred_glm <- predict(model_glm, newdata = test_h2o)



The relevant part is really that last line, where I got the following error:


DistributedException from localhost/127.0.0.1:54321, caused by java.lang.ArrayIndexOutOfBoundsException

DistributedException from localhost/127.0.0.1:54321, caused by java.lang.ArrayIndexOutOfBoundsException
    at water.MRTask.getResult(MRTask.java:478)
    at water.MRTask.getResult(MRTask.java:486)
    at water.MRTask.doAll(MRTask.java:390)
    at water.MRTask.doAll(MRTask.java:396)
    at hex.glm.GLMModel.predictScoreImpl(GLMModel.java:1198)
    at hex.Model.score(Model.java:1030)
    at water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:345)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1241)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
Caused by: java.lang.ArrayIndexOutOfBoundsException



Has anyone come across this before? Are there any easy solutions to this that I might be missing? Thanks in advance.","['r', 'h2o']",114,https://stackoverflow.com/users/2551655/114,926
43873603,43873603,2017-05-09T15:12:28,2017-05-09 19:58:26Z,75,"I have follow the instruction to install flow 

https://www.youtube.com/watch?v=_HVx9Jqr34Q


it works perfectly. But if I restart my laptop, then i open ""
http://localhost:54321
"" it shows not connected.


Should I rerun the command ""java -jar h2o.jar""? is that alway required if I want to open flow after computer reboot? is that an easy short cut to start flow?",['h2o'],Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
43865336,43865336,2017-05-09T08:54:48,2017-05-09 15:10:06Z,826,How to integrate H2O models with Java code? If you have any reference materials for H2O then please share.,"['java', 'h2o']",demongolem,https://stackoverflow.com/users/236247/demongolem,"9,697"
43862280,43862280,2017-05-09T06:06:57,2017-05-09 11:53:58Z,159,"I am trying to connect to Sparkling Water using R and also analyze my data frames on the H20 flow. I could connect to Spark instance from R using sparkly and sparklingR package and generate a few H20 dataframes. Please advise how can I access the Flow using the same Spark context as created by the spark_connect.
Using the following versions




H2o Flow 3.0.4.7


R v. 3.4


R Studio version v. 1.0.143 


Sparkling Water v. 2.1.0


Spark  v. 2.1.0","['h2o', 'sparkling-water']",Arun Lakhotia,https://stackoverflow.com/users/7984336/arun-lakhotia,51
43851748,43851748,2017-05-08T15:37:19,2017-08-07 21:35:22Z,0,"I try to load table into R using h2o but had the following error


my_data <- h2o.import_sql_table(my_sql_conn, table, username, password)



ERROR: Unexpected HTTP Status code: 500 Server Error (url = 
http://localhost:54321/99/ImportSQLTable
)


java.lang.RuntimeException [1] ""java.lang.RuntimeException: SQLException: No suitable driver found for jdbc:mysql://10.140.20.29/MySQL?&useSSL=false\nFailed to connect and read from SQL database with connection_url: jdbc:mysql://10.140.20.29/MySQL?&useSSL=false""


Can someone help me with this? Thank you so much!",['h2o'],Connie Chen,https://stackoverflow.com/users/6441095/connie-chen,113
43850650,43850650,2017-05-08T14:46:26,2017-05-10 15:23:54Z,81,"I have a GBM model I would like to add to a project in Steam, however the model is not showing up in the list of Model options. The ""Select a Model Category List"" does not include an option for any type of model after I select the data frame I wish to include in the project (if the GBM is the only model on the server). If I make a GLM model on the same data frame then ""Regression"" is listed as an option but only the GLM model is available to select to create the project with; the GBM is still hidden. I don't see anywhere in the docs that GBM is not available for projects in Steam but is this the case? Is there a bug where GBM Regression models are not being found as options? I'm able to view the GBM model through R and Flow so it's definitely there on the server. I am running h2o version 3.10.4.6 and Steam version 1.1.16.


Any help would be appreciated!",['h2o'],Matt Mills,https://stackoverflow.com/users/3625934/matt-mills,608
43848376,43848376,2017-05-08T13:02:10,2017-05-10 04:06:37Z,646,"I have been introduced to RL by Sutton's book. In order to further this knowledge I would like to explore how the agent learns from raw pixels and try to implement an example using H2O. I want to use the Java API.




Is Sparkling water the distribution I should use ?


How do I stream raw pixels to h2o ? How can a ping-pong game, for example, be used to make the h2o RL agent learn  ? What Deep Learning h2o API is used ?




I would appreciate if the answers pertain to h2o as I refer to other literature to learn about RL.


Update : 
http://h2o2016.wpengine.com/wp-content/themes/h2o2016/images/resources/DeepLearningBooklet.pdf


But still I need to figure out how to use Java to stream image pixels from a game to help the h2o RL agent learn. Examples use R and Python mostly.","['h2o', 'reinforcement-learning']",Unknown,,N/A
43840178,43840178,2017-05-08T05:11:12,2019-05-23 19:02:21Z,428,"I am building gbm model using h2o. The training data is randomly split into 70% development data and 30% in-time validation data. The training data has 1.4% bad rate and I also need to assign weight for each observation (data has a weight column). Observation is: the model built with weight has much higher performance on development data (DEV) compared to the model built without weight (VAL). Model built with weight has big performance difference between development and in-time validation data. For instance, model build with weight shows below top 10% capture rate


DEV: 56%

Validation: 25%  


While model build without weight shows below top 10% capture rate:


DEV: 35%

Validation: 23%


Seems use weight in this case helped on model performance on both development and in-time validation data. Wondering how exactly is weight used in the h2o? With weight used in the model building, does the bigger performance difference of the model on DEV and VAL illustrate higher instability of the gbm model building in h2o?




Blue curve is the DEV, orange curve is for 
VAL>
. For no weight case, log loss for DEV and VAL started from the same point. While for weighted case, log loss for DEV and VAL started from two different points. How to interpret this log loss chart, why weight in h2o gbm created such different in log loss function output?","['h2o', 'gbm', 'weighted-graph']",double-beep,https://stackoverflow.com/users/10607772/double-beep,"5,464"
43793479,43793479,2017-05-04T22:03:33,2017-05-08 01:48:23Z,39,"While going over sparkling-water examples, a common pattern that is seen is for scoring and collection scores over a h2oframe is to do the following:


val predictionH2OFrame = dlModel.score(result)('predict)
val predictionsFromModel = asRDD[DoubleHolder](predictionH2OFrame).collect.map ( _.result.getOrElse(""NaN"") )



I need to understand if the order of scores in the original frame will be maintained using this approach. In particular, will the scores order be maintained through the the cast to a RDD followed with the collect?","['apache-spark', 'h2o', 'sparkling-water']",x89a10,https://stackoverflow.com/users/1367070/x89a10,691
43791092,43791092,2017-05-04T19:15:42,2017-05-05 18:23:03Z,525,"Is there a way to save the whole environment in H2o Flow, so you can get all the grids, models, and predictions intact after you restart the h2o? When I do Download a flow, or Save/Load it loads only a flow with no data.","['python', 'h2o']",Unknown,,N/A
43785970,43785970,2017-05-04T14:41:20,2017-05-06 04:49:43Z,0,"I am running on a Linux system; R v3.3.2; H2O v3.10.2.1.


I installed H2O v3.10.2.1 via:


install.packages( ""h2o"", type=""source"", 
  repos=(c(""http://h2o-release.s3.amazonaws.com/h2o/rel-tutte/1/R"")))



During installation, I saw a few warning messages, for example:


Rd warning: /tmp/Rtmp2c0lfT/R.INSTALL13f787112f6e2/h2o/man/use.package.Rd:24: file link ‘fwrite’ in package ‘data.table’ does not exist and so has been treated as a topic



I am now unable to access help:


> ?h2o.init
Error in .getHelpFile(file) : 
  package ‘h2o_v3.10.2.1’ exists but was not installed under R >= 2.10.0 so help cannot be accessed

> help(h2o.init)
Error in .getHelpFile(file) : 
  package ‘h2o_v3.10.2.1’ exists but was not installed under R >= 2.10.0 so help cannot be accessed



Because I needed to find a version of H2O that worked around an issue I have in one of my R/H2O scripts, I installed many versions of H2O.  My 
.libPaths()
 directory looks like this:


drwxrwxr-x 10 userme 4.0K Apr 17 13:10 dplyr
drwxrwxr-x  6 userme  107 Apr 17 13:10 dtplyr
lrwxrwxrwx  1 userme   13 May  3 10:13 h2o -> h2o_v3.10.2.1
drwxrwxr-x  9 userme 4.0K Apr 11 07:43 h2o_v3.10.0.8
drwxrwxr-x  9 userme 4.0K Apr 11 20:00 h2o_v3.10.2.1
drwxrwxr-x  9 userme 4.0K Apr 11 19:05 h2o_v3.10.3.1
drwxrwxr-x  9 userme 4.0K Apr 11 11:52 h2o_v3.10.3.2
drwxrwxr-x  9 userme 4.0K Apr 10 13:38 h2o_v3.10.3.5
drwxrwxr-x  9 userme 4.0K Apr 11 10:24 h2o_v3.10.4.3
drwxrwxr-x  9 userme 4.0K Apr 11 08:59 h2o_v3.11.0.3839
drwxrwxr-x  9 userme 4.0K May  3 08:48 h2o_v3.6.0.8
drwxrwxr-x  9 userme 4.0K Apr 10 20:58 h2o_v3.8.1.4
drwxrwxr-x 16 userme 4.0K Apr 17 13:05 Rcpp



Where 
h2o
 is a symbolic link to a specific directory.


What do I need to do to get these help files?


Thank you!","['r', 'linux', 'installation', 'h2o']",BA88,https://stackoverflow.com/users/7733787/ba88,79
43785580,43785580,2017-05-04T14:24:11,2019-07-20 07:07:20Z,0,"I just start to learn H2O. I am confused about if i run H2O at home just for leaning purpose. When I simply run ""h2o.init()"" then start data clean or modeling using H2O. Will it speed up the calculation speed for big data? Is it automatically connect to some H2O cluster online? Where is the H2O cluster located?",['h2o'],Gavin,https://stackoverflow.com/users/5732164/gavin,"1,521"
43774821,43774821,2017-05-04T05:49:47,2017-05-05 01:57:24Z,0,"Error on connecting to H2o server running on EMR core node from master node.


import h2o
h2o.connect(url=""http://IP:54321"")



Error Trace


Connecting to H2O server at http://IP:54321... successful.
Traceback (most recent call last):
  File ""/home/hadoop/TataCliqEMR/app/__init__.py"", line 3, in <module>
    h2o.connect(ip=""IP"", port=54321)
  File ""/usr/local/lib/python3.4/site-packages/h2o/h2o.py"", line 86, in connect
    h2oconn.cluster.show_status()
  File ""/usr/local/lib/python3.4/site-packages/h2o/backend/cluster.py"", line 190, in show_status
    [""H2O internal security:"",     self.internal_security_enabled],
  File ""/usr/local/lib/python3.4/site-packages/h2o/backend/cluster.py"", line 121, in internal_security_enabled
    return self._props[""internal_security_enabled""]
KeyError: 'internal_security_enabled'



It's also unanswered here","['python', 'hadoop', 'h2o']",Unknown,,N/A
43766401,43766401,2017-05-03T17:21:56,2018-07-26 14:08:50Z,211,"The h2o steam website said Python preprocess with pojo
As .War is an optional, but I can not find any examples about doing this step by step,
Where can I find out more details about this? Or I better do it in Java only?


The situation is I have one python preprocess program, mainly use pandas to do some data munging before calling h2o to train/score the model. I want to use the 
h2o steam as the score engine. The website mentions I can wrap the python and h2o pojo/mojo file together as a .war file, so I can call it through REST API.  But I 
can not find example or details about how to proceed. Also do I need to and if yes how to include these 
python library like pandas in the war file?",['h2o'],Unknown,,N/A
43762037,43762037,2017-05-03T13:51:55,2018-05-31 16:43:39Z,548,"I have just run an upgrade on the h2o package in python, but am only getting a version of 3.10.4.1. However, my recently upgraded h2o application is running 3.10.4.6 - can you please help me rectify this discrepancy? Thanks in advance.","['python', 'h2o']",Hannes Ovrén,https://stackoverflow.com/users/13565/hannes-ovr%c3%a9n,21.8k
43746769,43746769,2017-05-02T20:34:47,2017-07-12 09:10:06Z,0,"In a different post 
here
 I asked for help on parallel processing a call to 
h2o.gbm
 inside a 
foreach
 loop.


Following the answers provided, I run a script similar to this example:


library(h2o)
data(iris)
data <- as.h2o(iris)
ss <- h2o.splitFrame(data)
gbm <- h2o.gbm(x = 1:4, y = ""Species"", training_frame = ss[[1]])
h2o.saveModel(path=""some path"")
h2o.shutdown(prompt = FALSE)

library(foreach)
library(doParallel)


#setup parallel backend to use 12 processors
cl <- makeCluster(12)
registerDoParallel(cl)

#loop
df4 <- foreach(i = seq(20), .combine=rbind) %dopar% {
 library(h2o)
 port <- 54321 + 3*i
 print(paste0(""http://localhost:"", port))
 h2o.init(nthreads = 1, max_mem_size = ""10G"", port = port)  #my local machine runs 128GB
 df4 <- data.frame()
 gbm <- h2o.loadModel(path=""some path"")
 df4 <- as.data.frame(h2o.predict(gbm, ss[[2]]))[,1]
}  



It runs really well on a small sample of my real data (at least 50% faster than sequential)


But when I run this on all of my data, I get the following error code after 45 minutes:


Error in { : task 2 failed - ""

ERROR MESSAGE:

DistributedException from localhost/127.0.0.1:60984, caused by 
java.lang.IllegalStateException: Unable to clean up RollupStats after an 
exception (see cause). This could cause a key leakage, key=$05ff14000000feffffff$_b66dbd609dc068f0137cc88cb42a
""



I am not sure what causes this error code. I guess it has to do with a memory issue because this code will take up 85-95% of my RAM (128GB) and 100% of my CPU (12 threads).


Anyone any ideas to work around this?","['r', 'memory-management', 'foreach', 'parallel-processing', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
43733918,43733918,2017-05-02T09:00:02,2017-05-02 17:36:26Z,0,"I'm using 
h2o
 package and trying to create a learner using the below given code


install.packages(""h2o"")
library(""h2o"")
h2o.learner <- makeLearner(""regr.h2o.deeplearning"",predict.type = ""response"")



But I'm getting this error 


> h2o.learner <- makeLearner(""regr.h2o.deeplearning"",predict.type = ""response"")
Error: could not find function ""makeLearner""



Note
: Few months back I used this code without any problem. 


Any idea what could be possible thing for this error?","['r', 'h2o', 'mlr']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
43720117,43720117,2017-05-01T13:43:34,2017-05-03 03:00:37Z,736,"I am trying to build an ensemble of 3 models viz.....Deeplearning, RandomForest and Gradient Boosting. I have passed the models ids as a list to the ensemble function, but i get the following error:


java.lang.NullPointerException

 java.lang.NullPointerException
     at hex.StackedEnsembleModel.checkAndInheritModelProperties(StackedEnsembleModel.java:258)
     at hex.ensemble.StackedEnsemble$StackedEnsembleDriver.computeImpl(StackedEnsemble.java:116)
     at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:169)
     at water.H2O$H2OCountedCompleter.compute(H2O.java:1241)
     at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
     at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
     at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
     at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
     at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)

 Error: java.lang.NullPointerException



Here is my argument to the ensemble model:


my_ensemble <- h2o.stackedEnsemble(x=2:length(names(train)),y=1,
              training_frame = train,validation_frame = valid,
              base_models = list(ann1@model_id,rf1@model_id,
               gbm1@model_id),model_id = ""my_ensemble_1"")



Kindly advise as to where i have went wrong.


Note: I am trying to predict on a multinomial classification.","['h2o', 'ensemble-learning']",adimessi30,https://stackoverflow.com/users/7887145/adimessi30,495
43699454,43699454,2017-04-29T19:03:08,2018-02-28 07:22:57Z,0,"After creating the model using 
h2o.randomForest
, then using:


perf <- h2o.performance(model, test)
print(perf)



I get the following information (value 
H2OModelMetrics
 object)


H2OBinomialMetrics: drf

MSE:  0.1353948
RMSE:  0.3679604
LogLoss:  0.4639761
Mean Per-Class Error:  0.3733908
AUC:  0.6681437
Gini:  0.3362873

Confusion Matrix (vertical: actual; across: predicted) 
for F1-optimal threshold:
          0    1    Error        Rate
0      2109 1008 0.323388  =1008/3117
1       257  350 0.423394    =257/607
Totals 2366 1358 0.339689  =1265/3724

Maximum Metrics: Maximum metrics at their respective thresholds
                        metric threshold    value idx
1                       max f1  0.080124 0.356234 248
2                       max f2  0.038274 0.515566 330
3                 max f0point5  0.173215 0.330006 131
4                 max accuracy  0.288168 0.839957  64
5                max precision  0.941437 1.000000   0
6                   max recall  0.002550 1.000000 397
7              max specificity  0.941437 1.000000   0
8             max absolute_mcc  0.113838 0.201161 195
9   max min_per_class_accuracy  0.071985 0.621087 262
10 max mean_per_class_accuracy  0.078341 0.626921 251

Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` 
or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`



I use to look at sensitivity (recall) and specificity for comparing the quality of my prediction model, but with the information provided I am not able to understand in terms of such metrics. Based on above information how can I evaluate the quality of my prediction?


If I compute such metrics using the confusion matrix I get: 
sens=0.58
, 
spec=0.68
that is different from the information provided.


If there any way to get such values like we have using 
confusionMatrix
 from 
caret
 package?


For me it is more intuitive this metric:




than 
logLoss
 metric.","['r', 'h2o']",zx8754,https://stackoverflow.com/users/680068/zx8754,55.7k
43692058,43692058,2017-04-29T05:18:20,2017-04-29 08:31:13Z,0,"Closed.
 This question needs 
debugging details
. It is not currently accepting answers.
                                
                            
























 Edit the question to include 
desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem
. This will help others answer the question.






Closed 
7 years ago
.















                        Improve this question
                    








library(h2o)
h2o.init(nthreads=-1)
test <- h2o.importFile(path = ""C:/Users/AkshayJ/Documents/newapril/data/testdata.csv"")
train <- h2o.importFile(path = ""C:/Users/AkshayJ/Documents/newapril/data/traindata.csv"")
y <- ""Label""
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])
train[,""Allele1Top""] <- as.factor(train[,""Allele1Top""])
test[,""Allele1Top""] <- as.factor(test[,""Allele1Top""])
train[,""Allele2Top""] <- as.factor(train[,""Allele2Top""])
test[,""Allele2Top""] <- as.factor(test[,""Allele2Top""])
train[,""Allele1Forward""] <- as.factor(train[,""Allele1Forward""])
test[,""Allele1Forward""] <- as.factor(test[,""Allele1Forward""])
train[,""Allele2Forward""] <- as.factor(train[,""Allele2Forward""])
test[,""Allele2Forward""] <- as.factor(test[,""Allele2Forward""])
train[,""Allele1AB""] <- as.factor(train[,""Allele1AB""])
test[,""Allele1AB""] <- as.factor(test[,""Allele1AB""])
train[,""Allele2AB""] <- as.factor(train[,""Allele2AB""])
test[,""Allele2AB""] <- as.factor(test[,""Allele2AB""])
train[,""Chr""] <- as.factor(train[,""Chr""])
test[,""Chr""] <- as.factor(test[,""Chr""])
train[,""SNP""] <- as.factor(train[,""SNP""])
test[,""SNP""] <- as.factor(test[,""SNP""])
x <- setdiff(names(train),y)
model <- h2o.deeplearning(
x = x,
y = y,
training_frame = train,
validation_frame = test,
distribution = ""multinomial"",
activation = ""RectifierWithDropout"",
hidden = c(32,32,32),
input_dropout_ratio = 0.2,
sparse = TRUE,
l1 = 1e-5,
epochs = 10)
predic <- h2o.predict(model, newdata = test)
table(pred=predic, true = test[,21])



Everything is fine but the last line
    table(pred=predic, true = test[,21])
gives the error
Error in unique.default(x, nmax = nmax) : 
  invalid type/length (environment/0) in vector allocation","['r', 'machine-learning', 'deep-learning', 'h2o']",Akshay Jaiswal,https://stackoverflow.com/users/6364327/akshay-jaiswal,67
43672318,43672318,2017-04-28T05:04:36,2017-04-28 07:35:45Z,265,"My goal is take the outcome class probability predictions from another model (or wherever, really), and use them as an offset in h2o.gbm with distribution = ""multinomial"".


I noticed in the nnet package, the multinom() function 
allows for an offset
 with as many columns as there are outcome classes (K). Does something like this exist for h2o GBM's?","['offset', 'h2o', 'multinomial']",MatthiasK,https://stackoverflow.com/users/3251172/matthiask,47
43668371,43668371,2017-04-27T21:39:17,2017-04-28 20:36:29Z,515,"Is there a way I can use H2O to iterate over data that is larger than the cumulative memory size of the cluster? I have a big-data set which I need to iterate through in batches and feed into Tensorflow for gradient-descent. At a given time, I only need to load one batch (or a handful) in memory. Is there a way I can setup H2O to perform this kind of iteration without it loading the entire data-set into memory?


Here's a related question that was answered over a year ago, but doesn't solve my problem: 
Loading data bigger than the memory size in h2o","['memory', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
43665020,43665020,2017-04-27T18:06:01,2017-04-28 01:59:00Z,0,"I am trying to read a dataset in SVMLight format into h2o.  Writing it to a file on disk and reading it back is working OK but reading it directly from R's memory is not.  I would like to know if there is a different function or a different way of calling the function I have used below.


Here's an example 
R 3.3.3,  h2o 3.10.3.6
:


require(data.table)
require(h2o)

set.seed(1000)
tot_obs <- 100
tot_var <- 500
vars_per_obs <- round(.0*tot_var,0):round(.1*tot_var,0)

#randomly generated data
mat.dt <- do.call('rbind', lapply(1:tot_obs, function(n) {
    nvar <- sample(vars_per_obs,1)
    if(nvar>0) data.table(obs=n, var=sample(1:tot_var,nvar))[, value:=sample(10:50,.N,replace=TRUE)]
}))

event.dt <- data.table(obs=1:tot_obs)[, is_event:=sample(0:1,.N,prob=c(.9,.1),replace=TRUE)]

#SVMLight format
setorder(mat.dt, obs, var)
mat.agg.dt <- mat.dt[, .(feature=paste(paste0(var,"":"",value), collapse="" "")), obs]
mat.agg.dt <- merge(event.dt, mat.agg.dt, by=""obs"", sort=FALSE, all.x=TRUE)
mat.agg.dt[is.na(feature), feature:=""""]
mat.agg.dt[, svmlight:=paste(is_event,feature)][, c(""obs"",""is_event"",""feature""):=NULL]
fwrite(mat.agg.dt, file=""svmlight.txt"", col.names=FALSE)

#h2o
localH2o <- h2o.init(nthreads=-1, max_mem_size=""4g"")
h2o.no_progress()

#works
h2o.orig <- h2o.importFile(""svmlight.txt"", parse=TRUE)

#does NOT work
tmp <- as.h2o(mat.agg.dt)
h2o.orig.1 <- h2o.parseRaw(tmp, parse_type=""SVMLight"")","['r', 'data.table', 'h2o', 'svmlight']",Erin LeDell,https://stackoverflow.com/users/5451344/erin-ledell,"8,809"
43662373,43662373,2017-04-27T15:46:08,2020-02-05 18:08:51Z,0,"I'm trying to use some machine learning functions via H2o (using library 
rsparkling
) during a 
sparklyr
 session. I'm running off hadoop clusters.


Consider the following example:


library(dplyr)
library(sparklyr)
library(rsparkling)
library(h2o)

#configure the spark session and connect
sc = spark_connect(master = 'yarn-client',
                   spark_home = '/usr/hdp/current/spark-client',
                   app_name = 'sparklyr',
                   config = list(
                     ""sparklyr.shell.executor-memory"" = ""1G"",
                     ""sparklyr.shell.driver-memory""   = ""4G"",
                     ""spark.driver.maxResultSize""     = ""2G"" # may need to transfer a lot of data into R
                   )
)

mtcars_tbl <- copy_to(sc, mtcars, ""mtcars"")

mtcars_hf <- as_h2o_frame(sc=sc,x=mtcars_tbl,name='h_cars')



I'm getting the following error:


Error: java.lang.IllegalArgumentException: Unsupported argument: (spark.dynamicAllocation.enabled,true)
        at org.apache.spark.h2o.backends.internal.InternalBackendUtils$$anonfun$checkUnsupportedSparkOptions$1.apply(InternalBackendUtils.scala:48)
        at org.apache.spark.h2o.backends.internal.InternalBackendUtils$$anonfun$checkUnsupportedSparkOptions$1.apply(InternalBackendUtils.scala:40)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.h2o.backends.internal.InternalBackendUtils$class.checkUnsupportedSparkOptions(InternalBackendUtils.scala:40)
        at org.apache.spark.h2o.backends.internal.InternalH2OBackend.checkUnsupportedSparkOptions(InternalH2OBackend.scala:31)
        at org.apache.spark.h2o.backends.internal.InternalH2OBackend.checkAndUpdateConf(InternalH2OBackend.scala:61)
        at org.apache.spark.h2o.H2OContext.<init>(H2OContext.scala:96)
        at org.apache.spark.h2o.H2OContext$.getOrCreate(H2OContext.scala:294)
        at org.apache.spark.h2o.H2OContext$.getOrCreate(H2OContext.scala:316)
        at org.apache.spark.h2o.H2OContext.getOrCreate(H2OContext.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sparklyr.Invoke$.invoke(invoke.scala:94)
        at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
        at sparklyr.StreamHandler$.read(stream.scala:55)
        at sparklyr.BackendHandler.channelRead0(handler.scala:49)
        at sparklyr.BackendHandler.channelRead0(handler.scala:14)
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
        at java.lang.Thread.run(Thread.java:745)



Any ideas what's going on?","['r', 'hadoop', 'h2o', 'sparklyr']",schristel,https://stackoverflow.com/users/2847422/schristel,245
43652714,43652714,2017-04-27T08:36:35,2017-04-27 19:37:23Z,291,"I am trying to follow the instructions at the deepwater github page (
https://github.com/h2oai/deepwater
). I have successfully built the deepwater jar, but I am having trouble building h2o with GPU deep learning support.


I pulled the h2o, added the deepwater jar to h2o-3/lib and then I run ./gradlew build -x test like the instructions demonstrate, but in the building process I get a failure at the h2o-bindings:runGenerateRESTAPIBindingsSrc step. Below is the output from the building process




:h2o-bindings:runGenerateRESTAPIBindingsSrc Starting H2O cloud...
  + CMD: /usr/lib/jvm/java-8-oracle/bin/java -Xmx4g -ea -cp /home/kristinn/repos/h2o-3/build/h2o.jar water.H2OApp -name
  H2O_runit_kristinn_9840684 -baseport 48000 -ga_opt_out


ERROR: Too many retries starting cloud 0. Check the output log
  ../build/logs/java_0_0.out.txt.


:h2o-bindings:runGenerateRESTAPIBindingsSrc FAILED
  :h2o-bindings:runGenerateRESTAPIBindingsSrc took 30.098 secs


FAILURE: Build failed with an exception.


What went wrong: Execution failed for task ':h2o-bindings:runGenerateRESTAPIBindingsSrc'.
  Process 'command 'python'' finished with non-zero exit value 1




Anyone had this problem while building the h2o library?


Full stacktrace:




Exception is: org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':h2o-bindings:runGenerateRESTAPIBindingsSrc'.
          at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:84)
          at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:55)
          at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:62)
          at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58)
          at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88)
          at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:46)
          at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51)
          at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)
          at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)
          at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)
          at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:236)
          at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.execute(DefaultTaskGraphExecuter.java:228)
          at org.gradle.internal.Transformers$4.transform(Transformers.java:169)
          at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106)
          at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:61)
          at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:228)
          at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:215)
          at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:77)
          at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:58)
          at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:32)
          at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:113)
          at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37)
          at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)
          at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23)
          at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43)
          at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32)
          at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)
          at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30)
          at org.gradle.initialization.DefaultGradleLauncher$3.execute(DefaultGradleLauncher.java:196)
          at org.gradle.initialization.DefaultGradleLauncher$3.execute(DefaultGradleLauncher.java:193)
          at org.gradle.internal.Transformers$4.transform(Transformers.java:169)
          at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:106)
          at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:56)
          at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:193)
          at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:119)
          at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:102)
          at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:71)
          at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28)
          at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
          at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41)
          at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26)
          at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75)
          at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49)
          at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:44)
          at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:29)
          at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67)
          at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
          at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
          at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47)
          at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
          at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26)
          at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
          at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34)
          at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
          at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74)
          at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72)
          at org.gradle.util.Swapper.swap(Swapper.java:38)
          at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72)
          at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
          at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55)
          at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
          at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:60)
          at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
          at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
          at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72)
          at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
          at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
          at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50)
          at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:297)
          at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54)
          at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40)
  Caused by: org.gradle.process.internal.ExecException: Process 'command
  'python'' finished with non-zero exit value 1
          at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369)
          at org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:31)
          at org.gradle.api.tasks.AbstractExecTask.exec(AbstractExecTask.java:54)
          at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)
          at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.doExecute(DefaultTaskClassInfoStore.java:141)
          at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:134)
          at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:123)
          at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:632)
          at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:615)
          at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:95)
          at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:76)
          ... 70 more",['h2o'],Unknown,,N/A
43640828,43640828,2017-04-26T17:25:59,2017-05-01 20:58:21Z,383,"Starting H2O from a Cloudera Gateway node and it comes back with the following:


Open H2O Flow in your web browser: http://127.0.0.1:54321



less command on /etc/hosts shows the following (I needing it to use 10.x.2xx.2x or the DNS name)


127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.x.2xx.2x   xxx-xshcxxwd12 xxx-xshcxxwd12.xxx.net



Suggestion/insights/ideas?


  Update #1



Since our ifconfig and the output from the startup of H2O shows the following:


    <b>Startup of H2O:</b>
    Determining driver host interface for mapper->driver callback.
    [Possible callback IP address: 10.2.219.27]
    [Possible callback IP address: 10.18.73.77]
    [Possible callback IP address: 127.0.0.1]</code>

<b>ifconfig shows:</b>
    1. eth0 : addr:10.2.219.27
    2. eth1: addr:10.18.73.77 
    3. lo :  addr:127.0.0.1 



We utilized the -baseport and set it to 9100, which did change the port number. 

Open H2O Flow in your web browser: 
http://127.0.0.1:9100


From the same host we performed wget on all three Ip Addresses with results of:


    [@usadc-shdcxp01 ]$ wget 127.0.0.1:9100
    --2017-05-01 16:12:19--  http://127.0.0.1:9100/
    Connecting to 127.0.0.1:9100... failed: Connection refused.



Even with the default port of :54321 we got the same results.


Tom K: spoke of using 
 -network
 yet I am setting up a single gateway.. Setting a network range resulted in the following:
    'ERROR: network invalid: 10.18.73.0/77`","['cloudera', 'h2o']",Unknown,,N/A
43631804,43631804,2017-04-26T10:38:02,2017-04-27 20:05:28Z,122,"I keep having a problem with a deep learning model. I have a model trained on rrc data frame, and if I do: 


rrc['preds'] = dp.cross_validation_holdout_predictions().as_data_frame().predict


I always get misaligned the response columns and predictions. At the top of the data frame there are aligned, but at some point it seems that they are misaligned and if I calculate a correlation between them is very bad because of this misalignment. I have been trying to fix this for over 3 day but I have no idea how to do it.


I'm using H2O 3.10.4.5.
The model itself:


dp = H2ODeepLearningEstimator(activation = ""Tanh"", hidden = [10, 10, 10], epochs = 10000, 
                              keep_cross_validation_predictions=True, 
                              ignored_columns = ['fn', 'pdb_id','pdb_id_chain', 'pdb_id_chain_source', 'source'])
dp.train(x = list(set(rrch.col_names) - set(['rmsd_all'])), y =""rmsd_all"", training_frame = rrch, 
         fold_column=""cv"")


Edit: I think I found the problem (Cell #58) 
https://github.com/mmagnus/mmagnus.github.io/blob/master/mq-test.ipynb
 If I do 
rrc3 = rrc3[rrc3.rmsd_all < 10]
 to remove some rows that rmsd_all (the response column) value is higher than 10 and then I do 
rrc3h = h2o.H2OFrame(rrc3)
 caused the problem. I'm not sure why though. The dataset, 40mb 
https://www.dropbox.com/s/1et38o3xx47jw1m/rasp_rnakb_cv2.csv?dl=0","['python', 'deep-learning', 'cross-validation', 'h2o']",Unknown,,N/A
43592291,43592291,2017-04-24T15:36:52,2017-04-24 16:12:25Z,0,"I am using R v3.3.2 and H2O V3.10.2.1 on a Linux server.


I saved a model to MOJO via 
h2o.download_mojo
. This resulting file is a .zip file.  In the .zip file are these text files:


model.ini
domains/d000.txt
domains/d001.txt
domains/d002.txt
domains/d003.txt
domains/d004.txt
domains/d005.txt
domains/d006.txt
domains/d007.txt
domains/d008.txt



In the 
model.ini
 file there is a section 
[columns]
 that list the columns used to train my model:


[columns]
name
address01
address02
city
state
zip
phone number
age



In the 
model.ini
 file there is another section 
[domains]
 that lists filenames containing the data used during training for the categorical columns retained in the final model:


[domains]
1: 71 d000.txt
2: 71 d001.txt
3: 51 d002.txt
4: 3243 d003.txt
5: 3228 d004.txt
6: 2954 d005.txt
7: 2456 d006.txt
9: 616 d007.txt



How do I associated the column names listed in 
[columns]
 with their domain files listed in 
[domains]
?


For example, the first domain file 
d000.txt
 does 
not
 contain data for the first column listed 
name
.  In fact, 
d000.txt
 
does
 contain data for column 3, 
address02
.


How can I know this mapping:


1: 71 d000.txt    -->  address02
2: 71 d001.txt    -->  phone number
3: 51 d002.txt    -->  column 23
4: 3243 d003.txt  -->  column 58



A downloaded POJO (plain old java object) contained enough information to make this association.  I don't see a way to do this with a downloaded MOJO.


Something like a fourth column in the 
[domains]
 section would be helpful:


[domains]
1: 71 d000.txt ""address02""
2: 71 d001.txt ""phone number""
3: 51 d002.txt ""column 23""
4: 3243 d003.txt ""column 58""



Thanks for any help!","['r', 'h2o']",Unknown,,N/A
43591430,43591430,2017-04-24T14:54:59,2017-04-24 16:20:41Z,229,"I'm new to Sparkling Water and machine learning,


I've built GBM model with two datasets divided manually into train and test.
Task is classification with all numeric atributes (response column is converted to enum type). Code is in Scala.


val gbmParams = new GBMParameters()
  gbmParams._train = train
  gbmParams._valid = test
  gbmParams._response_column = ""response""
  gbmParams._ntrees = 50
  gbmParams._max_depth = 6

val gbm = new GBM(gbmParams)
val gbmModel = gbm.trainModel.get



In model summary I get four different - one on train data and one on test data before building individual trees with prediction. The result is with predicted value as 1 in each case - this is for test data:


CM: Confusion Matrix (vertical: actual; across: predicted):
       0    1   Error       Rate
    0  0  500  1,0000  500 / 500
    1  0  300  0,0000    0 / 300
Totals 0  800  0,6250  500 / 800



The second confusion matrix is similar with predicted value as 1 in each case for train data. Third and Fourth confusion matrix after built trees gaves normal results with values distributed in all sections of matrix.


I need to interpret first and second matrix. Why is Sparkling Water doing that? Can I work with these results or it's just some middle step?


Thank you.","['scala', 'h2o', 'gbm', 'sparkling-water']",velaciela,https://stackoverflow.com/users/7914316/velaciela,11
43578485,43578485,2017-04-24T01:35:57,2017-04-24 21:33:47Z,0,I have a data set with over 400 features that I am estimating with GBM  using H2O atop R. When I use the variable importance function (h2o.varimp) it only shows me the head and tail of the full ranked variable list. Is there a way to have the entire list displayed?,"['r', 'h2o', 'gbm']",dj_ski_mask,https://stackoverflow.com/users/7746844/dj-ski-mask,65
43556802,43556802,2017-04-22T07:59:33,2017-04-22 19:21:49Z,0,"i am currently trying to build a muti-class prediction model to predict the letter out of 26 English alphabets. I have currently built a few models using ANN, SVM, Ensemble and nB. But i am stuck at the evaluating the accuracy of these models. Although the confusion matrix shows me the Alphabet-wise True and False predictions, I am only able to get an overall accuracy of each model. Is there a way to evaluate the model's accuracy similar to the ROC and AUC values for a Binomial Classification.
Note: I am currently running the model using the H2o package as it saves me more time.","['r', 'precision', 'h2o', 'auc']",adimessi30,https://stackoverflow.com/users/7887145/adimessi30,495
43556784,43556784,2017-04-22T07:58:07,2017-04-22 17:22:45Z,316,"In my project, I will use h2o's machine learning algorithm. While I don't load the train date.
I use the folloing ways.


 var f = FileUtils.getFile(""D:\\from_2017_2_13\\untitled2\\src\\main\\resources\\extdata\\iris_wheader.csv"")
 println(11111)
 var frame = FrameUtils.parseFrame(Key.make(""iris_weather.hex""),f)
 println(22222)



The 11111 was output, then the program will being runing, and not stopping 


 11111



other way


 var f = FileUtils.getFile(""D:\\from_2017_2_13\\untitled2\\src\\main\\resources\\extdata\\iris_wheader.csv"")
 val parserSetup = H2OFrame.defaultParserSetup()
    parserSetup.setSeparator(',').setCheckHeader(ParseSetup.HAS_HEADER).setNumberColumns(5)
 val f3 = new H2OFrame(parserSetup, f)
    f3



the error


Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 65535
 at water.DKV.get(DKV.java:202)
 at water.DKV.get(DKV.java:175)
 at water.parser.ParseSetup.createHexName(ParseSetup.java:594)
 at water.fvec.H2OFrame.<init>(H2OFrame.scala:56)
 at water.fvec.H2OFrame.<init>(H2OFrame.scala:84)","['java', 'scala', 'h2o']",Unknown,,N/A
43549276,43549276,2017-04-21T18:02:56,2017-06-12 15:40:47Z,277,"When attempting to call 
H2OContext.getOrCreate
 with a valid 
SparkContext
, randomly we keep seeing failures to deploy:


17/04/21 17:21:32 ERROR TaskSchedulerImpl: Lost executor 0 on 172.17.0.4: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/21 17:21:38 ERROR LiveListenerBus: Listener ExecutorAddNotSupportedListener threw an exception
java.lang.IllegalArgumentException: Executor without H2O instance discovered, killing the cloud!
    at org.apache.spark.listeners.ExecutorAddNotSupportedListener.onExecutorAdded(H2OSparkListener.scala:27)
    at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:61)
    at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
    at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
    at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:63)
    at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:36)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:94)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:78)
    at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1252)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:77) 



The 
H2OContext.getOrCreate
 causes the error:


Context.spark_session = SparkSession.builder.getOrCreate()
Context.h2o_context = H2OContext.getOrCreate(Context.spark_session)



Any thoughts from the H2O Crew?","['apache-spark', 'h2o', 'sparkling-water']",Unknown,,N/A
43548878,43548878,2017-04-21T17:37:42,2018-09-20 20:09:17Z,0,"What is the Python equivalent of getTypes in R?
I'm trying to extract the variable types for each column from H2O data frame (enum, string, int etc.)


Also, broadly can someone send me a link to some documentation listing all the properties and functions for data frames for Python?
Things like. df.nrow, df.shape etc. I have really hard time finding such clear  source.","['python', 'h2o']",C8H10N4O2,https://stackoverflow.com/users/2573061/c8h10n4o2,18.9k
43545511,43545511,2017-04-21T14:40:02,2017-04-24 16:46:57Z,918,"My goal is to integrate H2O with TensorFlow without CUDA on a machine.


As TensorFlow supports both CPU and GPU execution, I expect H2O/TensorFlow integration to be possible without CUDA. But I'm pretty confused by mentioning of CUDA software in 
system specifications of Deep Water
.


I've tried to build Deep Water/TensorFlow model in H2O Flow but failed. The steps I've performed:




Downloaded 
H2O standalone JAR
;


Created data frame in H2O Flow as usual;


Tried to build a model with Deep Water and tensorflow chosen as an algorithm and backend respectively;


Got the following exception:




java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: No backend found. Cannot build a Deep Water model.
    at hex.deepwater.DeepWaterModelInfo.setupNativeBackend(DeepWaterModelInfo.java:246)
    at hex.deepwater.DeepWaterModelInfo.(DeepWaterModelInfo.java:193)
    at hex.deepwater.DeepWaterModel.(DeepWaterModel.java:225)
    at hex.deepwater.DeepWater$DeepWaterDriver.buildModel(DeepWater.java:127)
    at hex.deepwater.DeepWater$DeepWaterDriver.computeImpl(DeepWater.java:114)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:169)
    at hex.deepwater.DeepWater$DeepWaterDriver.compute2(DeepWater.java:107)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1220)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)


So my questions are:




Is it possible to build Deep Water/TensorFlow model in H2O without CUDA at all?


If it is, what should I do to get it working? If it is not, are there other options to integrate H2O and TensorFlow without CUDA?




Update 1
:


I've set the gpu parameter to false and tried to build model again with all possible backends. Both caffe and tensorflow produce the same stacktrace as shown above. mxnet also fails but with two different stacktraces.


mxnet (first attempt to build a model):


java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: null
    at hex.deepwater.DeepWaterModelInfo.setupNativeBackend(DeepWaterModelInfo.java:246)
    at hex.deepwater.DeepWaterModelInfo.(DeepWaterModelInfo.java:193)
    at hex.deepwater.DeepWaterModel.(DeepWaterModel.java:225)
    at hex.deepwater.DeepWater$DeepWaterDriver.buildModel(DeepWater.java:127)
    at hex.deepwater.DeepWater$DeepWaterDriver.computeImpl(DeepWater.java:114)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:169)
    at hex.deepwater.DeepWater$DeepWaterDriver.compute2(DeepWater.java:107)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1220)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)


mxnet (subsequent attempts):


java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: Could not initialize class deepwater.backends.mxnet.MXNetBackend$MXNetLoader
    at hex.deepwater.DeepWaterModelInfo.setupNativeBackend(DeepWaterModelInfo.java:246)
    at hex.deepwater.DeepWaterModelInfo.(DeepWaterModelInfo.java:193)
    at hex.deepwater.DeepWaterModel.(DeepWaterModel.java:225)
    at hex.deepwater.DeepWater$DeepWaterDriver.buildModel(DeepWater.java:127)
    at hex.deepwater.DeepWater$DeepWaterDriver.computeImpl(DeepWater.java:114)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:169)
    at hex.deepwater.DeepWater$DeepWaterDriver.compute2(DeepWater.java:107)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1220)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)


Update 2


Environment:




SW: CentOS Linux release 7.3.1611 (Core), Java HotSpot 64-Bit Server VM (build 25.121-b13, mixed mode);


HW: virtual machine running on Xeon CPU E5-2620 v4 with 4 cores and 8 GB RAM available. No physical GPU is available, 
lspci -vnn | grep VGA
 returns 
00:0f.0 VGA compatible controller [0300]: VMware SVGA II Adapter [15ad:0405] (prog-if 00 [VGA controller])




I've cleared my /tmp directory and tried mxnet again. On the first attempt I've got new exception:

java.lang.RuntimeException: Unable to initialize the native Deep Learning backend: /tmp/libmxnet.so: libcudart.so.8.0: cannot open shared object file: No such file or directory
    at hex.deepwater.DeepWaterModelInfo.setupNativeBackend(DeepWaterModelInfo.java:246)
    at hex.deepwater.DeepWaterModelInfo.(DeepWaterModelInfo.java:193)
    at hex.deepwater.DeepWaterModel.(DeepWaterModel.java:225)
    at hex.deepwater.DeepWater$DeepWaterDriver.buildModel(DeepWater.java:127)
    at hex.deepwater.DeepWater$DeepWaterDriver.computeImpl(DeepWater.java:114)
    at hex.ModelBuilder$Driver.compute2(ModelBuilder.java:169)
    at hex.deepwater.DeepWater$DeepWaterDriver.compute2(DeepWater.java:107)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1220)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)


The file 
/tmp/libmxnet.so
 is present, its permissions are 
-rw-rw-r--
.",['h2o'],Unknown,,N/A
43540038,43540038,2017-04-21T10:15:51,2017-04-22 06:09:49Z,0,"Closed
. This question needs to be more 
focused
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Update the question so it focuses on one problem only by 
editing this post
.






Closed 
7 years ago
.















                        Improve this question
                    








I’m new in deep learning and I’m trying to perform clustering on some cancer data to get patient subtypes, I found that autoencoders are the unsupervised learning algorithm that can help me, and I want to try h2o stacked autoencoder to get my clusters, my question is there anyone know where to find an implementation example. Or give me some suggestions.","['deep-learning', 'h2o', 'autoencoder']",mitchou,https://stackoverflow.com/users/7864414/mitchou,1
43515721,43515721,2017-04-20T09:37:03,2017-04-21 03:23:39Z,0,"I get the following error when I run a random forest model using H2o package.




Error: DistributedException from localhost/127.0.0.1:54321, caused by
  java.lang.IllegalArgumentException: Operation not allowed on string
  vector.




The code I ran was as follows:-


fit = h2o.randomForest(x = indep, y = dep, training_frame = QCAnalysis_sub_h2o, seed = 1234, ntrees = 500, mtries = 3, max_depth = 50)



Kindly clarify on the error.","['r', 'random-forest', 'h2o']",user3342643,https://stackoverflow.com/users/3342643/user3342643,749
43515062,43515062,2017-04-20T09:08:49,2017-04-21 15:43:08Z,0,"How can I increase the h2o startup 
timeout
 when starting an h2o server via R?
I have a multinode AWS EC2 cluster, where I start a separate h2o server on each node. After startup, some EC2 nodes can be a bit slow and I'd rather increase the timeout than to re-run the 
h2o
 initialization code on these nodes.


What I am currently doing is along the lines of


library(doParallel)
library(foreach)

workers=parallel::makePSOCKcluster(workerIPs,master=masterIP)
registerDoParallel(workers)

foreach(i=seq_along(workers),.inorder=FALSE,.multicombine=TRUE) %dopar% {
  library(h2o)
  h2o.init(nthreads=-1)
  paste0(capture.output(h2o.clusterStatus()),collapse=""\n"")
}



Slow nodes will throw an error at 
h2o.clusterStatus()
 if 
h2o.init(nthreads=-1)
 produced a timeout.


BTW: I am using h2o v 3.10.4.4 and I am on ubuntu 16.04.","['r', 'h2o']",cryo111,https://stackoverflow.com/users/983028/cryo111,"4,474"
43506620,43506620,2017-04-19T21:45:17,2017-04-19 23:11:30Z,38,"I am starting with H2O using a Form Web UI, I am trying to fit the model using Distributed Random Forest. When I select Distribuited Random Forest in the 
Build Model Section
 I can see the parameter: 
balance_classes
 under the section 
ADVANCED
. 




but once I select the 
response_column
 with my variable (INT). I don't see such parameter.




My decision variable as two values: 
0
,
1
, and the value 
1
 is imbalanced (15%).


Am I doing something wrong?, Thks",['h2o'],David Leal,https://stackoverflow.com/users/6237093/david-leal,"6,739"
43502724,43502724,2017-04-19T17:46:49,2018-03-06 06:56:23Z,650,"H2O Sparkling water often throws below exception, we are rerunning it manually whenever this happens. The Issue is the spark job doesn't exit when this exception occurs, they don't return exit status and we are not able to automate this process. 


App > Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 316 in stage 22.0 failed 4 times, most recent failure: Lost task 316.3 in stage 22.0 (TID 9470, ip-**-***-***-**.ec2.internal): java.lang.ArrayIndexOutOfBoundsException: 65535
App > at water.DKV.get(DKV.java:202)
App > at water.DKV.get(DKV.java:175)
App > at water.Key.get(Key.java:83)
App > at water.fvec.Frame.createNewChunks(Frame.java:896)
App > at water.fvec.FrameUtils$class.createNewChunks(FrameUtils.scala:43)
App > at water.fvec.FrameUtils$.createNewChunks(FrameUtils.scala:70)
App > at org.apache.spark.h2o.backends.internal.InternalWriteConverterContext.createChunks(InternalWriteConverterContext.scala:28)
App > at org.apache.spark.h2o.converters.SparkDataFrameConverter$class.org$apache$spark$h2o$converters$SparkDataFrameConverter$$perSQLPartition(SparkDataFrameConverter.scala:86)
App > at org.apache.spark.h2o.converters.SparkDataFrameConverter$$anonfun$toH2OFrame$1$$anonfun$apply$2.apply(SparkDataFrameConverter.scala:67)
App > at org.apache.spark.h2o.converters.SparkDataFrameConverter$$anonfun$toH2OFrame$1$$anonfun$apply$2.apply(SparkDataFrameConverter.scala:67)
App > at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
App > at org.apache.spark.scheduler.Task.run(Task.scala:85)
App > at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
App > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
App > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)","['apache-spark', 'apache-spark-mllib', 'h2o', 'apache-spark-ml', 'sparkling-water']",DINESHKUMAR MURUGAN,https://stackoverflow.com/users/7497534/dineshkumar-murugan,192
43485939,43485939,2017-04-19T03:40:59,2017-06-23 15:14:10Z,0,"I ran GBM model through R code in H2O and got below error. The same code was running fine a couple of weeks. Wondering if this is H2O side error Or configuration on the user system? 


water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: gbm-2017-04-18-15-29-53.  Details: ERRR on field: _ntrees: The tree model will not fit in the driver node's memory (23.2 MB per tree x 1000 > 3.32 GB) - try decreasing ntrees and/or max_depth or increasing min_rows!","['model', 'h2o', 'gbm']",Eric_IL,https://stackoverflow.com/users/5031282/eric-il,171
43463178,43463178,2017-04-18T03:40:19,2018-11-27 13:27:07Z,0,"I am suspecting that both h2o's and caret's data partitioning functions may be leaking data somehow. The reason why I suspect this is that I get two, completely different results when using either h2o's h2o.splitFrame function or caret's createDataPartition function - vs when I manually partition the data myself:


In my dataframe with time-series data, 3000-4000 data points, and using 10-fold CV, I'm obtaining very acceptable results in all data sets: training, validation, cross-validation, and test sets when using either caret's xgboost or h2o. However, these high r2/low RMSE (good) results occur 
only
 when I use caret's createDataPartition function and h2o's h2o.splitFrame function.


On the other hand, if I 
manually
 remove a portion of data myself to create a completely separate test set dataframe (while still using appropriate partitioning function to split data into train and validation sets), then the 
manually-created
 test set results are poor - while the the training, validation, and cross-validation results remain good (high r2/low RMSE).


To establish a control, I can intentionally mess up the data - use random numbers, remove multiple features/columns, etc - and the predictive results are bad in training, validation, 10-fold CV, and test sets.


For example, here is the h2o code I used to partition the data. These strange, inconsistent results that I'm seeing when I use package-specific, data partitioning functions makes me wonder if all this ""amazing accuracy"" that can be read about in the media about machine learning - could be partially due to data leaking when using partitioning functions?


#Pre-process
df_h2o <- write.csv(df, file = ""df_h2o.csv"")
df <- h2o.importFile(path = normalizePath(""df_h2o.csv""),  destination_frame 
= ""df"")

## Pre moodel
#Splitting the data
splits <- h2o.splitFrame(df, c(0.6,0.2), seed=1234)
train <- h2o.assign(splits[[1]], ""train"")   
valid <- h2o.assign(splits[[2]], ""valid"")   
test <- h2o.assign(splits[[3]], ""test"") ### <--- My test results are poor if 
                                        ### I manually partition test data                                            
                                        ### myself (i.e. without using any
                                        ### partitioning function). 
                                        ### Otherwise, results are good.   

#Identify the variable that will predict and independent variables
y <- ""Target""
x <- setdiff(colnames(train),y)

#Build model
model<-h2o.deeplearning(x=x,              
                    y=y,
                    seed = 1234,
                    training_frame = train,
                    validation_frame = valid,
                    hidden = c(40,40),
                    nfolds = 10,
                    stopping_rounds = 7,
                    epochs = 500,
                    overwrite_with_best_model = TRUE,
                    standardize = TRUE,
                    activation = ""Tanh"",
                    loss = ""Automatic"",
                    distribution = ""AUTO"",
                    stopping_metric = ""MSE"",
                    variable_importances=T)","['r', 'validation', 'partitioning', 'r-caret', 'h2o']",jmuhlenkamp,https://stackoverflow.com/users/6850554/jmuhlenkamp,"2,150"
43444333,43444333,2017-04-17T02:27:03,2018-12-10 04:21:26Z,0,"I am setting up a piece of code to parallel processes some computations for N groups in my data using 
foreach
.


I have a computation that involves a call to 
h2o.gbm
.


In my current, sequential set-up, I use up to about 70% of my RAM.


How do I correctly set-up my h2o.init() within the parallel piece of code? I am afraid that I might run out of RAM when I use multiple cores.


My Windows 10 machine has 12 cores and 128GB of RAM.


Would something like this pseudo-code work?


library(foreach)
library(doParallel)

#setup parallel backend to use 12 processors
cl<-makeCluster(12)
registerDoParallel(cl)

#loop
df4 <-foreach(i = as.numeric(seq(1,999)), .combine=rbind) %dopar% {
  df4 <- data.frame()
  #bunch of computations
  h2o.init(nthreads=1, max_mem_size=""10G"")
  gbm <- h2o.gbm(train_some_model)
  df4 <- data.frame(someoutput)
   }

fwrite(df4, append=TRUE)

stopCluster(cl)","['r', 'memory', 'foreach', 'parallel-processing', 'h2o']",wake_wake,https://stackoverflow.com/users/3587303/wake-wake,"1,214"
43440345,43440345,2017-04-16T18:02:12,2017-10-26 07:27:41Z,0,"I am new in Pysparkling. I work with yarn cluster, Spark 1.6, Cloudera CDH 5.8.0,python 2.7.6 and i have problem with 
hc=H2OContext.getOrCreate(sc)
. Do you have some ideas ?


from pysparkling import * import h2o hc = H2OContext.getOrCreate(sc) 

17/04/16 17:13:59 INFO spark.SparkContext: Added JAR /root/.cache/Python-Eggs/h2o_pysparkling_1.6-1.6.10-py2.7.eg g-tmp/sparkling_water/sparkling_water_assembly.jar at spark://147.232.202.114:47251/jars/sparkling_water_assembly .jar with timestamp 1492355639066 
17/04/16 17:13:59 WARN internal.InternalH2OBackend: Increasing 'spark.locality.wait' to value 30000 17/04/16 17:13:59 WARN internal.InternalH2OBackend: Due to non-deterministic behavior of Spark broadcast-based jo ins We recommend to disable them by configuring spark.sql.autoBroadcastJoinThreshold variable to value -1: sqlContext.sql(""SET spark.sql.autoBroadcastJoinThreshold=-1"") 
17/04/16 17:13:59 WARN internal.InternalH2OBackend: The property 'spark.scheduler.minRegisteredResourcesRatio' is not specified! We recommend to pass --conf spark.scheduler.minRegisteredResourcesRatio=1 
17/04/16 17:13:59 WARN internal.InternalH2OBackend: Unsupported options spark.dynamicAllocation.enabled detected! 
17/04/16 17:13:59 WARN internal.InternalH2OBackend: The application is going down, since the parameter (spark.ext.h2o.fail.on.unsupported.spark.param,true) is true! If you would like to skip the fail call, please, specify the value of the parameter to false.

Traceback (most recent call last): File """", line 1, in File ""build/bdist.linux-x86_64/egg/pysparkling/context.py"", line 128, in getOrCreate File ""/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway. py"", line 813, in call File ""/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/lib/spark/python/pyspark/sql/utils.py"", line 45, in deco return f(a, *kw) File ""/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/lib/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py"", line 308, in get_return_value py4j.protocol.Py4JJavaError: An error occurred while calling o54.invoke. : java.lang.IllegalArgumentException: Unsupported argument: (spark.dynamicAllocation.enabled,true) at org.apache.spark.h2o.backends.internal.InternalBackendUtils$$anonfun$checkUnsupportedSparkOptions$1.ap ply(InternalBackendUtils.scala:48) at org.apache.spark.h2o.backends.internal.InternalBackendUtils$$anonfun$checkUnsupportedSparkOptions$1.ap ply(InternalBackendUtils.scala:40) at scala.collection.immutable.List.foreach(List.scala:318) at org.apache.spark.h2o.backends.internal.InternalBackendUtils$class.checkUnsupportedSparkOptions(Interna lBackendUtils.scala:40) at org.apache.spark.h2o.backends.internal.InternalH2OBackend.checkUnsupportedSparkOptions(InternalH2OBack end.scala:31) at org.apache.spark.h2o.backends.internal.InternalH2OBackend.checkAndUpdateConf(InternalH2OBackend.scala: 61) at org.apache.spark.h2o.H2OContext.(H2OContext.scala:96) at org.apache.spark.h2o.H2OContext$.getOrCreate(H2OContext.scala:294) at org.apache.spark.h2o.H2OContext.getOrCreate(H2OContext.scala) at org.apache.spark.h2o.JavaH2OContext.getOrCreate(JavaH2OContext.java:191) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381) at py4j.Gateway.invoke(Gateway.java:259) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:209) at java.lang.Thread.run(Thread.java:745)","['h2o', 'sparkling-water']",Community,https://stackoverflow.com/users/-1/community,1
43419308,43419308,2017-04-14T21:10:07,2017-11-11 14:50:06Z,280,"I'm trying run h2o on hadoop:




hadoop 2.7.1 (single node) 


h2o driver - h2o-3.10.4.3-mapr5.1




with the following command: 


hadoop jar h2odriver.jar -nodes 1 -mapperXmx 6g -output hdfsOutputDirName


and getting this error:






----- YARN cluster metrics -----
Number of YARN worker nodes: 1

----- Nodes -----
Node: http://DESKTOP-BPF0VCF:8042 Rack: /default-rack, RUNNING, 0 containers used, 0,0 / 16,0 GB used, 0 / 4 vcores used

----- Queues -----
Queue name:            default
    Queue state:       RUNNING
    Current capacity:  0,00
    Capacity:          1,00
    Maximum capacity:  1,00
    Application count: 0

Queue 'default' approximate utilization: 0,0 / 16,0 GB used, 0 / 4 vcores used

----------------------------------------------------------------------

ERROR: Unable to start any H2O nodes; please contact your YARN administrator.

       A common cause for this is the requested container size (6,6 GB)
       exceeds the following YARN settings:

           yarn.nodemanager.resource.memory-mb
           yarn.scheduler.maximum-allocation-mb

----------------------------------------------------------------------

For YARN users, logs command is 'yarn logs -applicationId application_1492203365290_0001'








Here's my yarn-site.xml:






<configuration>
   <property>
       <name>yarn.nodemanager.aux-services</name>
       <value>mapreduce_shuffle</value>
   </property>
   <property>
       <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
       <value>org.apache.hadoop.mapred.ShuffleHandler</value>
   </property>
      <property>
       <name>yarn.resourcemanager.scheduler.class</name>
       <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
   </property>

   <property>
       <name>yarn.scheduler.maximum-allocation-mb</name>
       <value>16384</value>
   </property>

   <property>
       <name>yarn.nodemanager.resource.memory-mb</name>
       <value>16384</value>
   </property>

   <property>
       <name>yarn.scheduler.maximum-allocation-vcores</name>
       <value>8</value>
   </property>

   <property>
       <name>yarn.nodemanager.resource.cpu-vcores</name>
       <value>4</value>
   </property>
   
</configuration>








So, it's obvious that memory is enough, but it still doesn't work.
This post didn't help too:

h2o starting on YARN not working","['hadoop', 'hadoop-yarn', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
43415466,43415466,2017-04-14T16:25:35,2017-04-14 16:34:51Z,440,"I am using H2O to develp model. After initiated H2O instance I got an IP and port for opening H2O flow in web browser. I used below command in HDFS to initiate the H2O instance. The problem is when I run hyperparameter search, the job takes multiple hours and my shell session got inactive and will automatically log me out. This will kill the console session and H2O instance will be killed as well. I am using Rstudio interface with H2O. Is there any way to keep H2O instance longer without auto log out/shut down due to inactivity in 


start h20 cluster


hadoop jar /dsap/devl/h2o/h2o-3.10.4.1-hdp2.4/h2odriver.jar -nodes 30 -mapperXmx 8g -output /user/userid1/h2o1 -baseport 6335","['instance', 'h2o']",Eric_IL,https://stackoverflow.com/users/5031282/eric-il,171
43404805,43404805,2017-04-14T03:32:48,2017-04-14 04:22:18Z,0,"I see h2o model performance metric contains AUC, logloss etc. There is one model performance metric called lift_top_group, is it lift on top decile? 


Also can user specify the band for h2o to output gains chart such as top 5%, 5%-10%, 10%-15% ....... The function I can find is h2o.gainsLift","['model', 'h2o']",Eric_IL,https://stackoverflow.com/users/5031282/eric-il,171
43381075,43381075,2017-04-12T23:13:37,2017-04-13 00:10:26Z,139,"I've tried to connect my h2o node to h2o cluster, using -flatfile. It works fine.
But I'm trying now another thing and I've got a problem:


is it possible to connect h2o node to existing h2o cluster which is running on hadoop cluster?","['java', 'h2o']",Markiza,https://stackoverflow.com/users/6227309/markiza,444
43377012,43377012,2017-04-12T18:27:04,2019-05-26 11:01:19Z,0,"I am using h2o.grid hyperparameter search function to fine tune gbm model. h2o gbm allows add a weight column to specify the weight of each observation. However when I tried to add that in h2o.grid, it always error out saying illegal argument/missing value, even though the weight volume is populated. 
Any one has similar experience? Thanks


Hyper-parameter: max_depth, 20
[2017-04-12 13:10:05] failure_details: Illegal argument(s) for GBM model: depth_grid_model_11.  Details: ERRR on field: _weights_columns: Weights cannot have missing values.
ERRR on field: _weights_columns: Weights cannot have missing values.


============================


hyper_params = list( max_depth = c(4,6,8,12,16,20) ) ##faster for larger datasets

grid <- h2o.grid(
  ## hyper parameters
  hyper_params = hyper_params,

  ## full Cartesian hyper-parameter search
  search_criteria = list(strategy = ""Cartesian""),  ## default is Cartesian

  ## which algorithm to run
  algorithm=""gbm"",

  ## identifier for the grid, to later retrieve it
  grid_id=""depth_grid"",

  ## standard model parameters
  x = X,  #predictors, 
  y = Y,  #response, 
  training_frame = datadev, #train, 
  validation_frame = dataval, #valid,
    **weights_column = ""Adj_Bias_correction"",**

  ## more trees is better if the learning rate is small enough 
  ## here, use ""more than enough"" trees - we have early stopping
  ntrees = 10000,                                                            

  ## smaller learning rate is better
  ## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
  learn_rate = 0.05,                                                         

  ## learning rate annealing: learning_rate shrinks by 1% after every tree 
  ## (use 1.00 to disable, but then lower the learning_rate)
  learn_rate_annealing = 0.99,                                               

  ## sample 80% of rows per tree
  sample_rate = 0.8,                                                       

  ## sample 80% of columns per split
  col_sample_rate = 0.8, 

  ## fix a random number generator seed for reproducibility
  seed = 1234,                                                             

  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
  stopping_rounds = 5,   stopping_tolerance = 1e-4,   stopping_metric = ""AUC"", 

  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
  score_tree_interval = 10                                                
)

## by default, display the grid search results sorted by increasing logloss (since this is a classification task)
grid","['r', 'grid', 'h2o']",Bhargav Rao,https://stackoverflow.com/users/4099593/bhargav-rao,51.9k
43369270,43369270,2017-04-12T12:15:32,2017-04-14 16:48:01Z,0,"I would like to use h2o model (either run in R or in flow) as an operational predictions. However, I would like to run it directly from Oracle sql. Is there currently a way that h2o translate .predict in sql code?
Thanks","['sql', 'r', 'oracle11g', 'h2o']",Michalis,https://stackoverflow.com/users/6410809/michalis,21
43364961,43364961,2017-04-12T09:01:38,2017-04-12 16:13:14Z,0,"I' am new with the h2o package of R.
I would like to know how to not display the execution process bar in the h2o.function.","['r', 'machine-learning', 'h2o']",axel,https://stackoverflow.com/users/4087922/axel,77
43359169,43359169,2017-04-12T02:25:50,2017-04-12 06:13:22Z,0,"Closed
. This question needs 
details or clarity
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Add details and clarify the problem by 
editing this post
.






Closed 
6 years ago
.















                        Improve this question
                    








I am creating a classification and regression models using Random forest (DRF) and GBM in H2O.ai. I believe that I don't need to normalize (or scale) the data as it's un-neccessary rather more harmful as it might smooth out the nonlinear nature of the model. Could you please confirm if my understanding is correct.","['random-forest', 'xgboost', 'h2o']",Gaurav Gupta,https://stackoverflow.com/users/5746807/gaurav-gupta,104
43324287,43324287,2017-04-10T13:19:24,2023-08-02 09:47:09Z,0,"I have been working with H2O.ai (version 3.10.3.6) in combination with R.


I am struggling to replicate the results from glm with h2o.glm. I would expect exactly the same result (evaluated, in this case, in terms of mean square error), but I am seeing must worse accuracy with h2o. Since my model is Gaussian, I would expect both cases to be ordinary least squares (or maximum likelihood) regressions.


Here is my example:


train <- model.matrix(~., training_df)
test <- model.matrix(~., testing_df)

model1 <- glm(response ~., data=data.frame(train))
yhat1 <- predict(model1 , newdata=data.frame(test))
mse1 <- mean((testing_df$response - yhat1)^2) #5299.128

h2o_training <- as.h2o(train)[-1,]
h2o_testing <- as.h2o(test)[-1,]

model2 <- h2o.glm(x = 2:dim(h2o_training)[2], y = 1,
                  training_frame = h2o_training,
                  family = ""gaussian"", alpha = 0)

yhat2 <- h2o.predict(model2, h2o_testing)
yhat2 <- as.numeric(as.data.frame(yhat2)[,1])
mse2 <- mean((testing_df$response - yhat2)^2) #8791.334



The MSE is 60% higher for the h2o model. Is my hypothesis that glm ≈ h2o.glm wrong? I will look to provide an example dataset asap (the training dataset is confidential and 350000 rows x 350 columns).


An extra question: for some reason, as.h2o adds an extra row full of NAs, so that h2o_training and h2o_testing have an additional row. Removing it (as I do here: as.h2o(train)[-1,]) before building the model does not affect the regression performance. There are no NA values passed to either glm or h2o.glm; i.e. the training matrices do not have NA values.","['r', 'h2o']",Unknown,,N/A
43313681,43313681,2017-04-10T01:15:15,2017-04-10 19:02:12Z,0,"I have some data that follows an unknown multidimensional nonlinear relationship, for example: 


x1 <- seq(0, 10, 0.1)
x2 <- seq(5, 15, 0.1)
y1 <- sin(x1)+exp(x2**2)+rnorm(length(x1))
y2 <- log10(abs(x1*x2)+1)+rnorm(length(x1))



I want to learn how to transform (x1,x2) into (y1,y2). 


How can I use the package 
h2o
 in R to achieve this?","['r', 'machine-learning', 'deep-learning', 'regression', 'h2o']",rhombidodecahedron,https://stackoverflow.com/users/354979/rhombidodecahedron,"7,922"
43291544,43291544,2017-04-08T07:49:26,2017-06-27 01:56:15Z,943,"I am trying to build h2o source code for ubuntu 16.04.

Here is the output :


:h2o-r:classes UP-TO-DATE
:h2o-r:jar UP-TO-DATE
:h2o-r:assemble UP-TO-DATE
:h2o-r:getRVersion
:h2o-r:gitbranch
:h2o-r:pdflatex
:h2o-r:setProperties
    Git Branch: master
    R Version: 3.3.3
    PDF LATEX: /usr/bin/pdflatex
:h2o-r:cpH2OAppJar
:h2o-r:setDevPackageFiles
:h2o-r:setPackageFiles
:h2o-r:buildPackageDocumentation FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':h2o-r:buildPackageDocumentation'.
> Process 'command 'R'' finished with non-zero exit value 1

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 33.713 secs

Task timings:
  14.641 secs  :h2o-assemblies:main:shadowJar
   8.099 secs  :h2o-bindings:runGenerateRESTAPIBindingsSrc
   4.404 secs  :h2o-py:buildDist
   1.360 secs  :h2o-web:installNpmPackages
   0.780 secs  :h2o-py:verifyDependencies
   0.771 secs  :h2o-bindings:compileJava
   0.596 secs  :h2o-assemblies:main:copyJar
   1.585 secs  All others



I have installed all prerequisites. Its able to build all packages & breaks down at documentation. How can I fix this?","['ubuntu', 'h2o']",Joe C,https://stackoverflow.com/users/6815131/joe-c,15.7k
43284577,43284577,2017-04-07T18:09:26,2017-04-12 21:17:56Z,0,"This is the popular pre-trained word vector file as supplied by the Stanford GLOVE project. It is commonly used for natural language applications.


The parse errors might be the root cause of some unpredictable behavior I am trying to fix. The strange stuff (not shown) appears after performing as.data.frame on this hex frame. So I'd like to fix this earlier error some way.


# How to eliminate parse error?
g6B = h2o.importFile(path = ""/mnt/fastssd/glove/glove.6B/glove.6B.100d.txt"", header=FALSE, na.strings=NA, sep="" "")

WARNING: ParseError at file nfs://mnt/fastssd/glove/glove.6B/glove.6B.100d.txt  at byte offset 7674; error = 'Unmatched quote char ""' 
  |======================================================================| 100%
Warning message in doTryCatch(return(expr), name, parentenv, handler):
“ParseError at file /mnt/fastssd/glove/glove.6B/glove.6B.100d.txt  at byte offset 7674; error = 'Unmatched quote char ""'”



Is there a way within H2O to fix this error?  If not then what is another way? Thanks","['r', 'word2vec', 'h2o']",Geoffrey Anderson,https://stackoverflow.com/users/39123/geoffrey-anderson,"1,554"
43267641,43267641,2017-04-07T00:01:36,2020-04-08 13:36:10Z,0,"I'm a beginner in machine learning. I'm doing clustering using autoencoder in R (h2o package). For now, I've done the following codes:


`mydata = h2o.importFile(path = mfile)

NN_model = h2o.deeplearning(
  x = 2:9,
  training_frame = mydata,
  hidden = c(2),
  epochs = 100,
  activation = ""Tanh"",
  autoencoder = TRUE
)

train_supervised_features = h2o.deepfeatures(NN_model, mydata, layer=1)`



For my data, there are not too many columns (as you can see in the codes, only 8 variables now), but lots of rows.


Then I plot the 2 nodes in ""train_supervised_features"" obtained above. And I got the plot like this 
It is clear that there are 8 clusters of my data (right?)


My question is, how can I extract the labels from the autoencoder results? I want to add the labels to original data, and plot in colors using these labels, something like this:","['r', 'deep-learning', 'cluster-analysis', 'h2o', 'autoencoder']",Has QUIT--Anony-Mousse,https://stackoverflow.com/users/1060350/has-quit-anony-mousse,77.4k
43236377,43236377,2017-04-05T16:08:14,2017-04-06 18:14:26Z,0,"I have a few questions or doubts on sparkling water and why is it needed.


Lets assume that I have a generated h2o model with both binary and pojo. 


Now I want to deploy the model into production and have an option for using pojo and binary (sparkling water) both.




Which one should I use? Direct spark with pojo or sparkling water with Binary.


What is the exact use of sparkling water, when we can easily deploy a model using pojo and spark itself?


Is sparkling water needed only when you have to train model on huge amounts of data? Or it can be used in PROD deployments of model's as well.




Example: 
https://github.com/h2oai/h2o-droplets/blob/master/h2o-pojo-on-spark-droplet/src/main/scala/examples/PojoExample.scala


Uses spark to run a pojo model. 


Example: 
https://github.com/h2oai/h2o-droplets/blob/master/sparkling-water-droplet/src/main/scala/water/droplets/SparklingWaterDroplet.scala


Trains / Runs a model in sparkling water.


What are the advantages which sparkling water h2o provides over normal spark?","['h2o', 'sparkling-water']",Lalit Agarwal,https://stackoverflow.com/users/3467351/lalit-agarwal,"2,354"
43236362,43236362,2017-04-05T16:07:41,2017-04-06 23:32:28Z,0,"I am using H2O machine learning package to do natural language predictions, including the functions h2o.word2vec and h2o.transform.  I need sentence level aggregation, which is provided by the AVERAGE parameter value:


h2o.transform(word2vec, words, aggregate_method = c(""NONE"", ""AVERAGE""))



However, in my case I strongly wish to avoid equal weighting of ""the"" and ""platypus"" for example.


Here's a scheme I concocted to achieve custom word-weightings.  If H2O's word2vec ""AVERAGE"" option uses all the words including duplicates that might appear, then I could effect a custom word weighting when calling h2o.transform by adding additional duplicates of certain words to my sentences, when I want to weight them more heavily than other words.


Can any H2O experts confirm that that the word2vec AVERAGE parameter is using all the words rather than just the unique words when computing AVERAGE of the words in sentence?


Alternatively, is there a better way?  I tried but I find myself unable to imagine any correct math to multiply the sentence average by some factor, after it was already computed.","['word2vec', 'h2o']",Geoffrey Anderson,https://stackoverflow.com/users/39123/geoffrey-anderson,"1,554"
43222103,43222103,2017-04-05T04:54:16,2017-04-05 16:14:31Z,373,"To whom may it concern,


With pojo, I was able to write two line java codes to score a csv file.


However, with mojo, I am not sure how to do the same. I was searching but could not find a suitable example.


Can you kindly guide me how to do it?


Many thanks in advance,
Kere",['h2o'],K Klein,https://stackoverflow.com/users/7783921/k-klein,11
43216126,43216126,2017-04-04T19:33:50,2017-04-05 05:35:17Z,0,"This is probably a dumb question, but when I use the H2O Predict function in R, I am wondering if there is a way that I can specify that it keep a column or columns from the scoring data. Specifically I want to keep my unique ID key.


As it stands now, I end up doing the really inefficient approach of assigning an index key to the original data set and one to the scores, then merging the scores to the scoring data set. I'd rather just say ""score this data set and keep x,y,z....columns as well."" Any advice?


Inefficient code:


#Use H2O predict function to score new data
NL2L_SCore_SetScored.hex = h2o.predict(object = best_gbm, newdata = 
NL2L_SCore_Set.hex)

#Convert scores hex to data frame from H2O
NL2L_SCore_SetScored.df<-as.data.frame(NL2L_SCore_SetScored.hex)
#add index to the scores so we can merge the two datasets
NL2L_SCore_SetScored.df$ID <- seq.int(nrow(NL2L_SCore_SetScored.df))



#Convert orignal scoring set to data frame from H2O
NL2L_SCore_Set.df<-as.data.frame(NL2L_SCore_Set.hex)
#add index to original scoring data so we can merge the two datasets
NL2L_SCore_Set.df$ID <- seq.int(nrow(NL2L_SCore_Set.df))


#Then merge by newly created ID Key so I have the scores on my scoring data 
#set. Ideally I wouldn't have to even create this key and could keep 
#original Columns from the data set, which include the customer id key

Full_Scored_Set=inner_join(NL2L_SCore_Set.df,NL2L_SCore_Set.df, by=""ID"" )","['r', 'data-manipulation', 'predict', 'h2o', 'scoring']",Unknown,,N/A
43209823,43209823,2017-04-04T14:15:55,2019-02-24 01:16:18Z,0,"I have a big hex frame in H2O, for which I need to compute euclidean distance between the two points that are in each row. Although it produces the correct result, the following H2O R code runs too slowly. 30 minutes have elapsed already and it is still running.  I even had time to post this question to stackoverflow while it runs.  


Is there a more efficient design possible for this h2o code?


# H2O R code to row-wise compute Euclidean distance between two points s1 and s2 contained in each row.
# Is this the most efficient H2O code that is possible? Real world will run on a big hex frame.
h2odistance = function(hex, cols1, cols2) {
    nr = h2o.nrow(hex)
    for (r in 1:nr) {
        dif = hex[r,cols1] - hex[r,cols2]
        sq = dif * dif
        sm = h2o.sum(sq)
        rt[r] = h2o.sqrt(sm)
    }
    rt  
}



Here is plain old R code of it, for comparison. I am including a small test case dataframe for correctness checking:


(df = data.frame(s1_c1=c(1,3), s1_c2=c(2,20), s1_c3=c(3,3), s2_c1=c(9,21), s2_c2=c(10,22), s2_c3=c(0,0)))
fn <- function(z) {sqrt(sum((z[1:3] - z[4:6])^2))}
(rt = apply(df, 1, fn))



This is the correct output of the plain R code for reference:


11.7046999107196 18.3575597506858


The h2o code outputs the correct value too:


h2odistance(as.h2o(df), 1:3, 4:6)



11.7046999107196 18.3575597506858","['r', 'h2o']",JJJ,https://stackoverflow.com/users/3367799/jjj,"1,029"
43189340,43189340,2017-04-03T16:05:11,2023-11-28 12:44:00Z,0,"I've read the PythonBooklet.pdf by H2O.ai and the 
python API documentation
, but still can't find a clean way to do this. I know I can do either of the following:




Convert H2OFrame to Spark DataFrame and do a 
flatMap
 + 
collect
 or 
collect
 + list comprehension.


Use H2O's 
get_frame_data
, which gives me a string of header and data separated by 
\n
; then convert it a list (a numeric list in my case).




Is there a better way to do this? Thank you.","['apache-spark', 'apache-spark-sql', 'h2o']",Unknown,,N/A
43183229,43183229,2017-04-03T11:15:47,2017-04-04 18:21:13Z,352,"In a fairly balanced binomial classification response problem, I am observing unusual level of error in h2o.gbm classification for determining class 0, on train set itself. It is from a competition which is over, so interest is only towards understanding what is going wrong.


Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
            0      1    Error            Rate
0      147857 234035 0.612830  =234035/381892
1       44782 271661 0.141517   =44782/316443
Totals 192639 505696 0.399260  =278817/698335



Any expert suggestions to treat the data and reduce the error is welcome.
Following approaches are tried and error is not found decreasing.
Approach 1: Selecting top 5 important variables via h2o.varimp(gbm)
Approach 2: Converting the negative normalized variable as zero and possitive as 1.


    #Data Definition

# Variable                        Definition

#Independent Variables

# ID                                Unique ID for each observation
# Timestamp                       Unique value representing one day
# Stock_ID                        Unique ID representing one stock
# Volume                            Normalized values of volume traded of                  given stock ID on that timestamp
# Three_Day_Moving_Average        Normalized values of three days moving average of Closing price for given stock ID (Including Current day)
# Five_Day_Moving_Average           Normalized values of five days moving average of Closing price for given stock ID (Including Current day)
# Ten_Day_Moving_Average            Normalized values of ten days moving average of Closing price for given stock ID (Including Current day)
# Twenty_Day_Moving_Average       Normalized values of twenty days moving average of Closing price for given stock ID (Including Current day)
# True_Range                        Normalized values of true range for given stock ID
# Average_True_Range                Normalized values of average true range for given stock ID
# Positive_Directional_Movement   Normalized values of positive directional movement for given stock ID
# Negative_Directional_Movement   Normalized values of negative directional movement for given stock ID

#Dependent Response Variable
# Outcome                           Binary outcome variable representing whether price for one particular stock at the tomorrow’s market close is higher(1) or lower(0) compared to the price at today’s market close


temp <- tempfile()
download.file('https://github.com/meethariprasad/trikaal/raw/master/Competetions/AnalyticsVidhya/Stock_Closure/test_6lvBXoI.zip',temp)
test <- read.csv(unz(temp, ""test.csv""))
unlink(temp)


temp <- tempfile()
download.file('https://github.com/meethariprasad/trikaal/raw/master/Competetions/AnalyticsVidhya/Stock_Closure/train_xup5Mf8.zip',temp)
#Please wait for 60 Mb file to load.
train <- read.csv(unz(temp, ""train.csv""))
unlink(temp)

summary(train)

#We don't want the ID
train<-train[,2:ncol(train)]
# Preserving Test ID if needed
ID<-test$ID
#Remove ID from test
test<-test[,2:ncol(test)]
#Create Empty Response SalePrice
test$Outcome<-NA
#Original
combi.imp<-rbind(train,test)

rm(train,test)
summary(combi.imp)

#Creating Factor Variable
combi.imp$Outcome<-as.factor(combi.imp$Outcome)
combi.imp$Stock_ID<-as.factor(combi.imp$Stock_ID)
combi.imp$timestamp<-as.factor(combi.imp$timestamp)

summary(combi.imp)


#Brute Force NA treatment by taking only complete cases without NA.
train.complete<-combi.imp[1:702739,]
train.complete<-train.complete[complete.cases(train.complete),]
test.complete<-combi.imp[702740:804685,]

library(h2o)
y<-c(""Outcome"")
features=names(train.complete)[!names(train.complete) %in% c(""Outcome"")]
h2o.shutdown(prompt=F)
#Adjust memory size based on your system.
h2o.init(nthreads = -1,max_mem_size = ""5g"")

train.hex<-as.h2o(train.complete)
test.hex<-as.h2o(test.complete[,features])

#Models
gbmF_model_1 = h2o.gbm( x=features,
                        y = y,
                        training_frame =train.hex,
                        seed=1234
)
h2o.performance(gbmF_model_1)","['h2o', 'gbm', 'balanced-groups']",Hari Prasad,https://stackoverflow.com/users/3548327/hari-prasad,"1,891"
43174049,43174049,2017-04-02T22:06:19,2017-04-03 17:49:40Z,0,"Any way to allow an H2O cluster to save/load directly to S3?


model.save('s3n://my-domain/gbm-from-the-future')
model.load('s3n://my-domain/gbm-from-the-future')



Historically, I have achieved this by:
- Saving to a file-system off of the Cluster
- Syncing with S3
- Downloading from S3
- Loading from the file-system


Obviously, there has to be a better way from the cluster itself.","['amazon-s3', 'h2o']",deepelement,https://stackoverflow.com/users/1195652/deepelement,"2,506"
43171653,43171653,2017-04-02T18:01:25,2017-06-23 15:20:40Z,0,"23Jun2017: Yet another update...
 


11Apr2017: I added another update below...
 


I added an update below...


We have developed a model using gradient boosting machine (GBM).  This model was originally developed using H2O v3.6.0.8 via R v3.2.3 on a Linux machine:


$ uname -a
Linux xrdcldapprra01.unix.medcity.net 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux



The following code has been working fine for months:


modelname <- 'gbm_34325f.hex'
h2o.gbm(x = predictors, y = ""outcome"", training_frame = modified.hex,
    validation_frame = modified_holdout.hex, distribution=""bernoulli"",
    ntrees = 6000, learn_rate = 0.01, max_depth = 5,
    min_rows = 40, model_id = modelname)
gbm <- h2o.getModel(modelname)
h2o.saveModel( gbm, path='.', force = TRUE )



Last week we upgraded the Linux machine to:




R:   v 3.3.2 


H2O: v 3.10.4.2




As shown here in the output from 
h2o.init()
:


> h2o.init()
 Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         2 days 1 hours 
    H2O cluster version:        3.10.4.2 
    H2O cluster version age:    14 days, 22 hours and 48 minutes  
    H2O cluster name:           H2O_started_from_R_bac_ytl642 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   18.18 GB 
    H2O cluster total cores:    64 
    H2O cluster allowed cores:  64 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    R Version:                  R version 3.3.2 (2016-10-31) 



I am now rebuilding this model from scratch in the newer version of R and H2O.  When I run the above R/H2O code, it hangs on this command:


h2o.saveModel( gbm, path='.', force = TRUE )



While my program is hung at 
h2o.saveModel
, I started another R/H2O session and connected to the currently hung process.  I can successfully get the model.  I can successfully run 
h2o.saveModelDetails
 and save it as JSON.  And I can save it as MOJO.  However, I cannot save it as a native 'hex' model via 
h2o.saveModel
.


These are my commands and output from my connected session (while the original session remains hung up):


> h2o.init()
 Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         2 days 1 hours 
    H2O cluster version:        3.10.4.2 
    H2O cluster version age:    14 days, 22 hours and 48 minutes  
    H2O cluster name:           H2O_started_from_R_bac_ytl642 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   18.18 GB 
    H2O cluster total cores:    64 
    H2O cluster allowed cores:  64 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    R Version:                  R version 3.3.2 (2016-10-31) 

> modelname <- 'gbm_34325f.hex'
> gbm <- h2o.getModel(modelname)
> gbm
Model Details:
==============

H2OBinomialModel: gbm
Model ID:  gbm_34325f.hex 
Model Summary: 
  number_of_trees number_of_internal_trees model_size_in_bytes min_depth
1            6000                     6000           839613730         5
  max_depth mean_depth min_leaves max_leaves mean_leaves
1         5    5.00000          6         32    17.51517
[ snip ]

> model_path <- h2o.saveModelDetails( object=gbm, path='.', force=TRUE )
> model_path
[1] ""/home/bac/gbm_34325f.hex.json""

# file created:
# -rw-rw-r-- 1 bac bac      552K Apr  2 12:20 gbm_34325f.hex.json
#
# first few characters are:
# {""__meta"":{""schema_version"":3,""schema_name"":""GBMModelV3"",""schema_type"":""GBMModel""},

> h2o.saveMojo( gbm, path='.', force=TRUE )
[1] ""/home/bac/gbm_34325f.hex.zip""

# file created:
# -rw-rw-r-- 1 bac bac   7120899 Apr  2 11:57 gbm_34325f.hex.zip
#
# when I unzip this file, things look okay (altho MOJOs are new to me).

> h2o.saveModel( gbm, path='.', force=TRUE )
[ this hangs and never returns; i have to kill the entire R session ]

# empty file created:
# -rw-rw-r-- 1 bac bac         0 Apr  2 12:00 gbm_34325f.hex



I then access this hung-up process via the web interface H2OFlow.  Again, I can load and view the model.  When I try to export the model, an empty 
.hex
 file is created and I see the message:


Waiting for 2 responses...



(
2 responses
 because I exported twice.)




Snapshot of Export via H2OFlow
 


Snapshot of 'Waiting for 2 responses' message from exportModel




To be clear, I am 
not
 loading an old model.  Rather, I am rebuilding the model from scratch in the new R/H2O environment.  I am, however, using the same R/H2O code that was successful in the older environment.


Any ideas of what is going on?
Thanks.




UPDATE:


The problem I have -- 
h2o.saveModel
 hangs -- is related to 
OOM
 (out of memory).


I see these messages in the 
.out
 file created when I 
h2o.init
:


Note:  In case of errors look at the following log files:
    /tmp/RtmpOnJn83/h2o_bfo7328_started_from_r.out
    /tmp/RtmpOnJn83/h2o_bfo7328_started_from_r.err

$ tail -n 6 h2o_bfo7328_started_from_r.out
[ I removed the timestamp / IP info to help made this readable ]

FJ-1-107  INFO:  2017-04-04 01:27:04 30 min 56.196 sec            6000       0.25485          0.22119      0.96950       3.54582                       0.08634
2946-780 INFO: GET /3/Models/gbm_34325f.hex, parms: {}
2946-780 INFO: GET /3/Models/gbm_34325f.hex, parms: {}
946-1102 INFO: GET /99/Models.bin/gbm_34325f.hex, parms: {dir=/opt/app/STUFF/bpci/training/facility_models/gbm_34325f.hex, force=TRUE}
946-1102 WARN: Unblock allocations; cache below desired, but also OOM: OOM, (K/V:3.15 GB + POJO:Zero   + FREE:441.54 GB == MEM_MAX:444.44 GB), desiredKV=299.74 GB OOM!
946-1102 WARN: Unblock allocations; cache below desired, but also OOM: OOM, (K/V:3.15 GB + POJO:Zero   + FREE:441.54 GB == MEM_MAX:444.44 GB), desiredKV=299.74 GB OOM!



Once I realized this was an OOM issue, I changed my 
h2o.init
 to include 
max_mem_size
:


localH2O = h2o.init(ip = ""localhost"", port = 54321, nthreads = -1, max_mem_size = '500G')


Even with 
max_mem_size = '500G'
 set this high, I still get a OOM error (see above).


When I was running H2O v3.6.0.8, I didn't explicitly define 
max_mem_size
.

I am curious: Now that I've upgraded to H2O v3.10.4.2, is there a larger memory demand?  What was the default 
max_mem_size
 in H2O v3.6.0.8?


Any idea of what changed memory-wise between the two versions of H2O?  And how I can get this to run again?


Thanks!




11Apr2017 UPDATE:


I hoped to share the dataset that generates this error. Unfortunately, the data contains protected information so I cannot share it. I created a 'scrubbed' version of this file -- contains nonsense data -- but I found it much too difficult to run this scrubbed data through our model training R code because of various dependencies and validation checks.


I have a general sense of what sorts of parameters cause the OOM (out of memory) error during 
h2o.saveModel
. 

Causes errors:




51380 records with 1413 columns of data used to train


ntrees = 6000




Does not cause errors:




51380 records with 1413 columns of data used to train


ntrees = 3750 (but ntrees = 4000 causes an error)




Does not cause errors:




25000 records with 1413 columns of data used to train (but 40000 records causes an error)


ntrees = 6000




There is some combination of number of records, number of columns, and ntrees that eventually causes OOM.


Setting 
max_mem_size
 does not help at all.  I set it to '100G', '200G', and '300G' and still OOM during 
h2o.saveModel
.


Testing earlier versions of H2O


Because I cannot compromise on number of records and number of columns used to train and on the number of trees needed in the GBM, I had to go back to an earlier version of h2o.


After working with ten different versions of h2o, I found the most recent released version that does 
not
 produce OOM.  The versions and the results are:




v3.6.0.8  - success (original version used to create model)


v3.8.1.4  - success 


v3.10.0.8 - success


v3.10.2.1 - success 


v3.10.3.1 - error: OOM 


v3.10.3.2 - error: OOM


v3.10.3.5 - error: OOM 


v3.10.4.2 - error: OOM (upgraded to this; found OOM error)


v3.10.4.3 - error: OOM


v3.11.0.3839 - success




I am not using v3.11.0.3839 since it seems to be 'bleeding edge'.  I am currently running with v3.10.2.1.


I hope this helps someone track down this bug.




23Jun2017 UPDATE:


I was able to fix this problem by:




upgrading to v3.10.5.1


setting 
both
 
min_mem_size
 and 
max_mem_size
 during 
h2o.init()




See:

https://stackoverflow.com/a/44724813/7733787","['r', 'linux', 'h2o']",Unknown,,N/A
43149508,43149508,2017-03-31T20:54:12,2020-01-16 10:55:16Z,0,"Apologies if this has been answered elsewhere but i couldn't find anything.


I'm using h2o (latest release) in R. I've created a random forest model using h2o.grid (for parameter tuning) and called this 'my_rf'


My steps are as follows:




train grid of `randomForests with parameter tuning & cross validation (nfolds = 5)


get the sorted grid of models (by AUC) and set my_rf = best model


use h2o performance(my_rf, test) to assess auc, accuracy etc on a test set


predict on test set using h2o.predict and export results




The exact line I've used for h2o.performance is: 


h2o.performance(my_rf, newdata = as.h2o(test))



.... which gives me a confusion matrix, from which I can calculate accuracy (as well as giving me AUC, max F1 score etc)


I would have thought that using 


h2o.predict(my_rf, newdata = as.h2o(test)) 



I would be able to replicate the confusion matrix from h2o.performance. But the accuracy is different - 3% worse in fact. 


Is anyone able to explain why this is so?


Also, is there any way to return the predictions that make up the confusion matrix in h2o.performance?


Edit: here is the relevant code:


library(mlbench)
data(Sonar)
head(Sonar)

mainset <- Sonar
mainset$Class <- ifelse(mainset$Class == ""M"", 0,1)          #binarize
mainset$Class <- as.factor(mainset$Class)

response <- ""Class""
predictors <- setdiff(names(mainset), c(response, ""name""))

# split into training and test set

library(caTools)
set.seed(123)
split = sample.split(mainset[,61], SplitRatio = 0.75)
train = subset(mainset, split == TRUE)
test =  subset(mainset, split == FALSE)

# connect to h2o

Sys.unsetenv(""http_proxy"")
Sys.setenv(JAVA_HOME='C:\\Program Files (x86)\\Java\\jre7')                #set JAVA home for 32 bit
library(h2o)
h2o.init(nthread = -1)

# stacked ensembles

nfolds <- 5
ntrees_opts <- c(20:500)             
max_depth_opts <- c(4,8,12,16,20)
sample_rate_opts <- seq(0.3,1,0.05)
col_sample_rate_opts <- seq(0.3,1,0.05)

rf_hypers <- list(ntrees = ntrees_opts, max_depth = max_depth_opts,
                  sample_rate = sample_rate_opts,
                  col_sample_rate_per_tree = col_sample_rate_opts)

search_criteria <- list(strategy = 'RandomDiscrete', max_runtime_secs = 240, max_models = 15,
stopping_metric = ""AUTO"", stopping_tolerance = 0.00001, stopping_rounds = 5,seed = 1)

my_rf <- h2o.grid(""randomForest"", grid_id = ""rf_grid"", x = predictors, y = response,
                                                                training_frame = as.h2o(train),
                                                                nfolds = 5,
                                                                fold_assignment = ""Modulo"",
                                                                keep_cross_validation_predictions = TRUE,
                                                                hyper_params = rf_hypers,
                                                                search_criteria = search_criteria)

get_grid_rf <- h2o.getGrid(grid_id = ""rf_grid"", sort_by = ""auc"", decreasing = TRUE)                         # get grid of models built
my_rf <- h2o.getModel(get_grid_rf@model_ids[[1]])
perf_rf <- h2o.performance(my_rf, newdata = as.h2o(test))

pred <- h2o.predict(my_rf, newdata = as.h2o(test))
pred <- as.vectpr(pred$predict)

cm <- table(test[,61], pred)
print(cm)","['r', 'performance', 'predict', 'h2o']",Unknown,,N/A
43148636,43148636,2017-03-31T19:51:01,2023-04-12 15:21:00Z,0,"I was able to install h2o fine (in R) but get the following error when I run h2o.init()




h2o.init()




H2O is not running yet, starting it now...

Error in value[3L] : 
  You have a 32-bit version of Java. H2O works best with 64-bit Java.
Please download the latest Java SE JDK 7 from the following URL:

http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html


I updated java SE JDK version to 7 (and got the 64 bit) and am still receiving this error. Why is this?﻿","['r', 'rstudio', 'h2o']",Mike Stack,https://stackoverflow.com/users/7722372/mike-stack,41
43137325,43137325,2017-03-31T09:34:23,2017-03-31 18:48:42Z,376,"I am new to h2o. I am using h2o-3.10.4.2 on Windows 7, java 1.8. I am using h2o flow. Just going through the k-modes demo. If I import seeds_dataset.txt     file from 
https://archive.ics.uci.edu/ml/machine-learning-databases/00236/
 and try to parse it then at record number 8, parser fails to capture column 7 and 8. This is most probably due to 2 tabs. I want the parser to consider any white spaces as a separator. Please note that if i use ""
http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/seeds_dataset.txt
"" then i do not face any issues as tabs are sorted out.


So finally i want to know that when i use parseFiles in h2o how do i set any whitespaces as separator. i tried using startoftext etc but i am not able to get all the columns.","['java', 'h2o']",Mad Physicist,https://stackoverflow.com/users/2988730/mad-physicist,113k
43130279,43130279,2017-03-31T00:04:14,2017-03-31 00:04:14Z,0,"I am trying to convert the levelone matrix of cross validation predictions from my h2o.ensemble into a normal R object (dataframe) so I can work with the data. I can retrieve the levelone matrix from my h2o.ensemble (fit). When I try to convert to a dataframe I get a bunch of java errors.


> levelone_h2o <- fit$levelone
> print(levelone_h2o)
  h2o.glm.wrapper h2o.randomForest.wrapper h2o.gbm.wrapper h2o.deeplearning.wrapper     y
1        6647.155                 1609.284        4617.418                 3839.470  1600
2        9383.599                13436.505       18496.432                13803.004 13500
3        2499.189                 8807.916        9859.840                 5697.766  2200
4       11541.062                 8067.081       12338.893                 6955.093  5750
5       12884.708                14544.741       16254.272                14843.988 14500
6        5757.545                 6501.096       10247.327                 4794.154  6250

[122653 rows x 5 columns] 
> df <- as.data.frame(levelone_h2o)

ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://localhost:54321/3/DownloadDataset?frame_id=RTMP_sid_bce4_12&hex_string=1)

java.lang.NullPointerException
 [1] ""java.lang.NullPointerException""                                                                                     
 [2] ""    water.api.DatasetServlet.doGet(DatasetServlet.java:35)""  



I can manipulate the h2o.frame in this simple example, but it breaks with the levelone object (which I don't think I can provide easily).


> df <- as.data.frame(cbind(labels = letters[1:5],numbers = 1:5))
> h2o_df <- as.h2o(df)
  |========================================================================================| 100%
> print(h2o_df)
  labels numbers
1      a       1
2      b       2
3      c       3
4      d       4
5      e       5

[5 rows x 2 columns] 
> new_df <- as.data.frame(h2o_df, stringsAsFactors = FALSE)
> print(new_df) #converts to factors, but it's okay
  labels numbers
1      a       1
2      b       2
3      c       3
4      d       4
5      e       5



thanks in advance.","['r', 'h2o']",Ken C.,https://stackoverflow.com/users/7752330/ken-c,43
43120159,43120159,2017-03-30T14:00:56,2017-04-01 00:17:16Z,119,"I have been intermittently getting distribution error when running a sample IRIS model in sparkling water. 


Sparkling water: 2.1
Spark streaming kafka - 0.10.0.0
Running locally using spark submit - Only master


DistributedException from xxx:54321, caused by java.lang.NullPointerException
            at water.MRTask.getResult(MRTask.java:478)
            at water.MRTask.getResult(MRTask.java:486)
            at water.MRTask.doAll(MRTask.java:390)
            at water.MRTask.doAll(MRTask.java:396)
            at hex.Model.predictScoreImpl(Model.java:1103)
            at hex.Model.score(Model.java:964)
            at hex.Model.score(Model.java:932)
    ....
    Caused by: java.lang.NullPointerException
        at water.fvec.Vec.chunkForChunkIdx(Vec.java:1014)
        at water.fvec.CategoricalWrappedVec.chunkForChunkIdx(CategoricalWrappedVec.java:49)
        at water.MRTask.compute2(MRTask.java:618)
        at water.MRTask.compute2(MRTask.java:591)
        at water.MRTask.compute2(MRTask.java:591)
        at water.H2O$H2OCountedCompleter.compute1(H2O.java:1223)
        at hex.Model$BigScore$Icer.compute1(Model$BigScore$Icer.java)
        at water.H2O$H2OCountedCompleter.compute(H2O.java:1219)
        at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
        at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
        at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
        at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
        at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)","['spark-streaming', 'h2o', 'sparkling-water']",Lalit Agarwal,https://stackoverflow.com/users/3467351/lalit-agarwal,"2,354"
43109777,43109777,2017-03-30T06:06:33,2019-08-23 11:49:24Z,899,"Has anyone managed to run a H2O Cluster in Kubernetes?


I tried 2 options both using flatfile 1) using StatefulSet, but since the ip generated for the pod can change the cluster is unreliable 2) using a bunch of pairs of service/deployments and specifying the the flatfile the dns name of the service but the cluster doesn't start up correctly


none of the above work. Is there any way to make it work?","['kubernetes', 'h2o']",Alessandro Magnani,https://stackoverflow.com/users/4297959/alessandro-magnani,61
43103022,43103022,2017-03-29T19:58:46,2017-03-29 20:11:18Z,588,"What is correct way to implement MAPE under h2o framework?


I am interested to convert below function to h2o concept


def mape(a, b): 
    mask = a <> 0
    return (np.fabs(a - b)/a)[mask].mean()","['python', 'python-3.x', 'pandas', 'dataframe', 'h2o']",SpanishBoy,https://stackoverflow.com/users/4437954/spanishboy,"2,205"
43102725,43102725,2017-03-29T19:41:05,2017-11-10 02:02:27Z,287,"I am using H2O, Randomforest for 36 class classification problem. this model works really good and we decide to deploy this model for our real world application which is in C#.


for this, I had to download the POJO format (which is java), covert it to C#, make a dll and use the dll in our C# application. it works!


BUT is there any way to make the whole process easier? or maybe in future, can H2O provide randomforest or any other models deployed in other languages rather than java?","['java', 'c#', 'random-forest', 'h2o']",Mahdi Mohammadi,https://stackoverflow.com/users/7728958/mahdi-mohammadi,43
43095970,43095970,2017-03-29T14:09:26,2017-03-29 20:42:33Z,0,"How can I convert pandas object to h2o dataframe safely? 


import h2o
import pandas as pd

df = pd.DataFrame({'col1': [1,1,2], 'col2': ['César Chávez Day', 'César Chávez Day', 'César Chávez Day']})
hf = h2o.H2OFrame(df)  #gives error





UnicodeEncodeError: 'ascii' codec can't encode character '\xe9' in
  position 4: ordinal not in range(128)




Environment:
 Python 3.5, h2o 3.10.4.2","['python', 'python-3.x', 'dataframe', 'unicode', 'h2o']",SpanishBoy,https://stackoverflow.com/users/4437954/spanishboy,"2,205"
43089222,43089222,2017-03-29T09:13:31,2018-01-23 07:35:28Z,394,"I was following the steps for running Sparkling water with external backend from 
here
. I am using 
spark 1.4.1
, 
sparkling-water-1.4.16
, I've build the extended h2o jar and exported the 
H2O_ORIGINAL_JAR
 and 
H2O_EXTENDED_JAR
 system variables. I start the h2o backend with 


java -jar $H2O_EXTENDED_JAR -md5skip -name test



But when I start sparkling water via 


./bin/sparkling-shell



and in it try to get the 
H2OConf
 with 


import org.apache.spark.h2o._
val conf = new H2OConf(sc).setExternalClusterMode().useManualClusterStart().setCloudName(""test”)
val hc = H2OContext.getOrCreate(sc, conf)



it fails on the second line with 


<console>:24: error: trait H2OConf is abstract; cannot be instantiated
   val conf = new H2OConf(sc).setExternalClusterMode().useManualClusterStart().setCloudName(""test"")
              ^



I've tried adding the newly build extended h2o jar with 
--jars
 parameter either to sparkling water or standalone spark with no progress. Does any one have any hints?","['scala', 'apache-spark', 'h2o']",JaKu,https://stackoverflow.com/users/2350272/jaku,"1,126"
43085556,43085556,2017-03-29T06:05:12,2017-03-30 17:53:15Z,922,"To whom may it concern,


Is it possible to plot an exploratory variable versus the target in h2o? I want to know whether it is possible to carry out basic data exploration in h2o, or whether it is not designed for that.


Many thanks in advance,
Kere",['h2o'],K Klein,https://stackoverflow.com/users/7783921/k-klein,11
43075650,43075650,2017-03-28T17:02:36,2017-11-10 02:01:00Z,346,"I recently upgraded to H2O-3.11.0.3820. My web based flow ui is not working. 
When I go to the link, i get a light blue colored screen with all the options that I used to get earlier, missing. 


Unable to find anything relevant on stackoverflow. Any one else facing similar issue? Any help will be much appreciated!


Checking whether there is an H2O instance running at http://localhost:54321..... not found.
Attempting to start a local H2O server...
; OpenJDK 64-Bit Server VM (Zulu 8.17.0.3-win64) (build 25.102-b14, mixed mode)
Starting server from C:\Users\shekh\Anaconda2\lib\site-packages\h2o\backend\bin\h2o.jar
Ice root: c:\users\shekh\appdata\local\temp\tmpvrxqvd
JVM stdout: c:\users\shekh\appdata\local\temp\tmpvrxqvd\h2o_shekh_started_from_python.out
JVM stderr: c:\users\shekh\appdata\local\temp\tmpvrxqvd\h2o_shekh_started_from_python.err
Server is running at http://127.0.0.1:54321
Connecting to H2O server at http://127.0.0.1:54321... successful.
--------------------------  ------------------------------
H2O cluster uptime:         03 secs
H2O cluster version:        3.11.0.3820
H2O cluster version age:    5 days
H2O cluster name:           H2O_from_python_shekh_vdbwfl
H2O cluster total nodes:    1
H2O cluster free memory:    51.56 Gb
H2O cluster total cores:    0
H2O cluster allowed cores:  0
H2O cluster status:         accepting new members, healthy
H2O connection url:         http://127.0.0.1:54321
H2O connection proxy:
H2O internal security:      False
Python version:             2.7.12 final
--------------------------  ------------------------------","['python-2.7', 'h2o']",spartacus,https://stackoverflow.com/users/7757913/spartacus,147
43072344,43072344,2017-03-28T14:31:33,2019-02-15 12:52:09Z,0,"I'm training a Random Forest using h2o and R on a large (~6 million) row dataset and ~50 output levels. Despite the progress bar hitting 100% the console (and the processor!) is still busy and hangs for over an hour (so far!). Definitely not resource limitations, I have 120gb of RAM and a couple of dozen cores. 


Hard to give a fully reproducible example given the nature of the issue but there are 35 variables, half of which are factors, I'm running the model training through R with the following options:


rforest <- h2o.randomForest(y = y.var
                          , x = x.vars
                          , training_frame = trainData.h2o
                          , validation_frame = testData.h2o
                          , ntrees = 100
                          , stopping_rounds = 3
                          , seed = 42
                          , model_id = modCode
                          , mtries = -1)



Has anyone encountered a similar issue/has an explanation/knows a workaround, please?","['r', 'h2o']",DataMacGyver,https://stackoverflow.com/users/6814598/datamacgyver,446
43054383,43054383,2017-03-27T18:58:46,2017-03-27 23:54:47Z,0,"I am running a classification model in H2O R. I would like  to extract fitted model predictions for my training dataset.


Code:


train <- as.h2o(train)
test <- as.h2o(test)
y <- ""class""
x <- setdiff(names(train), y)
family <- ""multinomial""
nfolds <- 5 
gbm1 <- h2o.gbm(x = x, y = y, distribution = family,
            training_frame = train,
            seed = 1,
            nfolds = nfolds,
            fold_assignment = ""Modulo"",
            keep_cross_validation_predictions = TRUE)
h2o.getFrame(gbm1@model$cross_validation_predictions[[gbm1@allparameters$nfolds]]$name)[,2:4]","['r', 'h2o']",Unknown,,N/A
43048126,43048126,2017-03-27T13:42:49,2017-03-29 20:52:18Z,941,"I am loading Spark dataframes into H2O (using Python) for building machine learning models. It has been recommended to me that I should allocate an H2O cluster with RAM 2-4x as big as the frame I will be training on, so that the analysis fits comfortably within memory. But I don't know how to precisely estimate the size of an H2O frame.


So supposing I have an H2O frame already loaded into Python, how do I actually determine its size in bytes? An approximation within 10-20% is fine.","['python', 'h2o']",abeboparebop,https://stackoverflow.com/users/1892435/abeboparebop,"7,735"
43035149,43035149,2017-03-26T22:07:34,2017-03-27 23:44:39Z,0,"After importing a relatively big table from MySQL into H2O on my machine, I tried to run a hashing algorithm (
murmurhash
 from the R digest package
) on one of its columns and save it back to H2O. As I found out, using 
as.data.frame
 on a H2OFrame object is not always advised: originally my H2OFrame is ~43k rows large, but the coerced DataFrame contains usually only ~30k rows for some reason (the same goes for using 
base::apply
/
base::sapply
/etc on the H2OFrame). 


I found out there is an 
apply
 function used for H2OFrames as well, but as I see, it can only be used with built-in R functions. 


So, for example my code would look like this: 


data[, ""subject""] <- h2o::apply(data[, ""subject""], 2, function(x) 
                                digest(x, algo = ""murmur32""))



I get the following error: 


Error in .process.stmnt(stmnt, formalz, envs) : 
  Don't know what to do with statement: digest



I understand the fact that only the predefined functions from the Java backend can be used to manipulate H2O data, but is there perhaps another way to use the digest package from the client side without converting the data to DataFrame? I was thinking that in the worst case, I will have to use the R-MySQL driver to load the data first, manipulate it as a DataFrame and then upload it to the H2O cloud. Thanks for help in advance.","['r', 'h2o']",Hadron,https://stackoverflow.com/users/2506617/hadron,33
43002941,43002941,2017-03-24T15:07:04,2017-03-25 16:23:38Z,456,"In h2o.randomForest, lets say I have 5 input features x=c(""A"",""B"",""C"",""D"",""E""), is there anyway to force the algorithm to always choose A,B AND one of the remaining features?","['random-forest', 'h2o']",Mahdi Mohammadi,https://stackoverflow.com/users/7728958/mahdi-mohammadi,43
42989596,42989596,2017-03-24T00:57:01,2017-03-24 01:16:22Z,0,"I modeled an object using h2o. But, when I try to predict the test dataset using h2o.predict, the output is an error: 


Error in paste0(""Predictions/models/"", object@model_id, ""/frames/"", h2o.getId(newdata)) : 
  trying to get slot ""model_id"" from an object (class ""H2OFrame"") that is not an S4 object 


The test and train test are from the same dataset.","['r', 'h2o']",Maximilian Follet,https://stackoverflow.com/users/7731766/maximilian-follet,37
42986668,42986668,2017-03-23T20:47:46,2018-02-05 13:47:10Z,0,"I am using H2O GLRM's to build a binary recommender system (example data files attached). I am using the current version of H2O, 3.10.3.6 in R., following suggestions from 
recommender
 and 
meetup
.


I generated a train (train.wmiss) from another data set (train.nomiss) allowing for 20% of missing values. All columns in train data sets are binary factors (encoded with N, Y).


I was trying to fit a glrm using following command:


#generate a train and a test set
train.h2o.wmiss<-h2o.insertMissingValues(train.h2o, fraction = 0.2) #train has 20% missing value
train.h2o.nomiss<-as.h2o(train) #test has no missing value

 glrm_k <- 3
 glrm_cols <- colnames(train.h2o.nomiss)
 ncols<-h2o.ncol(train.h2o.nomiss)
 #fitting a logistic GLRM
 base_glrm <- h2o.glrm(training_frame = train.h2o.wmiss, cols = glrm_cols, k = glrm_k, 
                       validation_frame = train.h2o.nomiss, seed = 1,
                       loss_by_col=rep(""Logistic"",ncols),
                       regularization_x = ""Quadratic"", regularization_y = ""Quadratic"", gamma_x = 0.1, gamma_y = 0.1, transform = ""NONE"", impute_original = TRUE, 
                       model_id = ""myglrm"")`



I am noticing that the train and the test set shows the same classification error. Is it right or a potential bug?","['r', 'h2o']",Giorgio Spedicato,https://stackoverflow.com/users/1259856/giorgio-spedicato,"2,483"
42982176,42982176,2017-03-23T16:46:03,2017-03-23 17:58:12Z,0,"H2O recently added word2vec in its API.  It is great to be able to easily train your own word vectors on a corpus you provide yourself. 


However even greater possibilities exist from using big data and big computers, of the type that software vendors like Google or H2O.ai, but not so many end-users of H2O, may have access to, due to network bandwidth and compute power limitations.


Word embeddings can be seen as a type of unsupervised learning. As such, great value can be had in a data science pipeline by using pretrained word vectors that were built on a very large corpus as infrastructure in specific applications. Using general purpose pretrained word vectors can be seen as a form of transfer learning.  Reusing word vectors is analogous to computer vision deep learning generic lowest layers that learn to detect edges in photographs. Higher layers detect specific kinds of objects composed from the edge layers below them.


For example Google provides some pretrained word vectors with their word2vec package.  The more examples the better is often true with unsupervised learning.  Further, sometimes it's practically difficult for an individual data scientist to download a giant corpus of text on which to train your own word vectors.  And there is no good reason for every user to recreate the same wheel by training word vectors themselves on the same general purpose corpuses (corpi?) like wikipedia.


Word embeddings are very important and have the potential to be the bricks and mortar of a galaxy of possible applications.  TF-IDF, the old basis for many natural language data science applications, stands to be made obsolete by using word embeddings instead.


Three questions:


1 - Does H2O currently provide any general purpose pretrained word embeddings (word vectors), for example trained on text found at legal or other public-owned (government) websites, or wikipedia or twitter or craigslist, or other free or Open Commons sources of human-written text?


2 - Is there a community site where H2O users can share their trained word2vec word vectors that are built on more specialized corpuses, such as medicine and law?


3 - Can H2O import Google's pretrained word vectors from their word2vec package?","['word2vec', 'h2o', 'unsupervised-learning']",Geoffrey Anderson,https://stackoverflow.com/users/39123/geoffrey-anderson,"1,554"
42981259,42981259,2017-03-23T16:04:35,2017-05-29 08:25:07Z,627,"I had a use-case that I thought was really simple but couldn't find a way to do it with h2o. I thought you might know.


I want to train my model once, and then evaluate its ROC on a few different test sets (e.g. a validation set and a test set, though in reality I have more than 2) without having to retrain the model. The way I know to do it now requires retraining the model each time:


train, valid, test = fr.split_frame([0.2, 0.25], seed=1234)
rf_v1 = H2ORandomForestEstimator( ... )
rf_v1.train(features, var_y, training_frame=train, validation_frame=valid)
roc = rf_v1.roc(valid=1)

rf_v1.train(features, var_y, training_frame=train, validation_frame=test) # training again with the same training set - can I avoid this?
roc2 = rf_v1.roc(valid=1)



I can also use model_performance(), which gives me some metrics on an arbitrary test set without retraining, but not the ROC. Is there a way to get the ROC out of the H2OModelMetrics object?


Thanks!",['h2o'],spartacus,https://stackoverflow.com/users/7757913/spartacus,147
42973949,42973949,2017-03-23T10:53:14,2019-02-08 13:44:43Z,0,"This is the error message:


   > h2o.init()
Error in dirname(path) : path too long
In addition: There were 12 warnings (use warnings() to see them)



This is one of the warning messages (the others are similar): 


> warnings()
    Warning messages:
    1: In normalizePath(path.expand(path), winslash, mustWork) :
      path[1]=""\\FILE-EM1-06/USERDATA2$/john134/My Documents/./../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../.."": The filename or extension is too long



Any idea how to work around this error?


Thanks","['r', 'h2o']",John McClain,https://stackoverflow.com/users/7592095/john-mcclain,51
42913138,42913138,2017-03-20T20:14:42,2017-04-01 16:06:02Z,0,"I faced an error trying to import a CSV into R which had multiple duplicate columns, is there a way I can ignore those columns?
It's easy to do that in case of small files and small number of columns but mine is a big one ~3k columns and 10M rows.","['r', 'rstudio', 'h2o', 'import-from-csv', 'sklearn-pandas']",Ayush,https://stackoverflow.com/users/2931946/ayush,518
42865609,42865609,2017-03-17T19:23:44,2018-11-03 08:04:45Z,0,"I am using H2O, on a large dataset, 8 Million rows and 10 col. I trained my randomForest using h2o.randomForest. The model was trained fine and also prediction worked correctly. Now I would like to convert my predictions to a data.frame. I did this : 


A2=h2o.predict(m1,Tr15_h2o)
pred2=as.data.frame(A2)



but it is too slow, takes forever. Is there any faster way to do the conversion from H2o to data.frame or data.table?","['performance', 'dataframe', 'h2o']",0xF,https://stackoverflow.com/users/5041140/0xf,586
42844585,42844585,2017-03-16T20:54:43,2021-06-14 21:45:12Z,0,"I'm using H2O 3.10.4.1


I'm trying to fit a Bernoulli model with GBM using some initial predictions from some other model and I'm getting worse Likelihoods than starting predictions. I was able to reproduce it using titanic data.


I was able to use R's gbm to do what I want. R's gbm.fit asks for offset on the link scale, which is not restricted, it could be very high or very low negative values.


However, when I try to do the same in H2O GBM, it throws an error:




water.exceptions.H2OModelBuilderIllegalArgumentException: Illegal argument(s) for GBM model: GBM_model_R_1489164084643_3568.  Details: ERRR on field: _offset_column: Offset cannot be larger than 1 for Bernoulli distribution.




My Jupyter notebook is here:

Github


UPDATE

I was able to use offset, but only for a dataframe where ProbabilityLink is less than 1. Since H2O complains about it. See cells 65-68 in the Notebook.


I believe that this is a bug in H2O. They should just remove the requirement that offset must me less than 1 for Bernoulli. It can be anything. And then it should work fine.","['r', 'h2o', 'gbm']",Martin Gal,https://stackoverflow.com/users/12505251/martin-gal,16.9k
42842993,42842993,2017-03-16T19:15:59,2017-03-17 14:47:17Z,0,"I am connected to H2O Cluster and was trying to read the .csv file in R but not able to read it.


Giving below command after connecting to Cluster:


data = h2o.importFile(path = ""/tmp"", pattern = "".*.csv"", destination_frame = ""train-0.01m"")


Please suggest if I am doing anything wrong.","['r', 'h2o']",srivakin,https://stackoverflow.com/users/7711796/srivakin,1
42796452,42796452,2017-03-14T21:00:20,2017-03-15 00:13:37Z,0,"I am using the r h2o package 3.6.0.3. I cannot update to a newer version. How do I sort an h2o frame by a certain  column? I've looked through the manual 
here
, but I was unable to find a sort function.  


Thank you!","['r', 'h2o']",Vincent Lous,https://stackoverflow.com/users/3453731/vincent-lous,127
42739051,42739051,2017-03-11T18:33:38,2017-03-12 01:02:42Z,42,In GBM Model - I have near to 150 columns used to train and create a model - I have a case where for some records I won't be getting all the columns. In that case will the model work - I don't want to set the values to 0 in that case.?,"['apache-spark-mllib', 'h2o', 'supervised-learning', 'gbm', 'sparkling-water']",DINESHKUMAR MURUGAN,https://stackoverflow.com/users/7497534/dineshkumar-murugan,192
42669040,42669040,2017-03-08T10:39:31,2017-03-13 11:12:14Z,825,"I am trying to run 
Sparkling Water
 on my Local instance of Spark 2.1.0.
I followed documentation on 
H2o
 for Sparling Water. But when I try to execute 


sparkling-shell.cmd



I am getting following error :




The filename, directory name, or volume label syntax is incorrect.




I look into the batch file and I am getting this error when the following command is executed:


C:\Users\Mansoor\libs\spark\spark-2.1.0/bin/spark-shell.cmd --jars C:\Users\Mansoor\libs\H2o\sparkling\bin\../assembly/build/libs/sparkling-water-assembly_2.11-2.1.0-all.jar --driver-memory 3G --conf spark.driver.extraJavaOptions=""-XX:MaxPermSize=384m""



When I remove 
--conf spark.driver.extraJavaOptions=""-XX:MaxPermSize=384m""
, Spark starts but I am unable to import the packages of H2o.


import org.apache.spark.h2o._





error: object h2o is not a member of package org.apache.spark




I tried everything I could but unable to solve this issue. Could someone help me in this? Thanks","['scala', 'apache-spark', 'h2o', 'sparkling-water']",Mansoor,https://stackoverflow.com/users/4520274/mansoor,"1,259"
42614890,42614890,2017-03-05T22:34:04,2017-03-06 00:57:41Z,52,When I try to add H2O with spark and use GBM model. I'm getting this exception while packaging it. This is my first time running H2O with spark. And I just tried adding the H2O libraries in my spark app and used the GBM within H2O.,"['apache-spark', 'machine-learning', 'h2o', 'gbm', 'sparkling-water']",DINESHKUMAR MURUGAN,https://stackoverflow.com/users/7497534/dineshkumar-murugan,192
42603673,42603673,2017-03-05T02:11:23,2017-03-05 17:22:23Z,0,"I'm using the following sample code to download a pojo that I found from this 
post
:


import h2o
 h2o.init()
 iris_df = h2o.import_file(""https://s3.amazonaws.com/h2o-public-test-data/smalldata/iris/iris.csv"")
 from h2o.estimators.glm import H2OGeneralizedLinearEstimator
 predictors = iris_df.columns[0:4]
 response_col = ""C5""
 train,valid,test = iris_df.split_frame([.7,.15], seed =1234)
 glm_model = H2OGeneralizedLinearEstimator(family=""multinomial"")
 glm_model.train(predictors, response_col, training_frame = train, validation_frame = valid)
 h2o.download_pojo(glm_model, path = '/Users/your_user_name/Desktop/', get_jar = True)



When I open the downloaded java file I'm given some instructions for how to compile it.  The following compiles successfully:


javac -cp h2o-genmodel.jar -J-Xmx2g -J-XX:MaxPermSize=128m GLM_model_python_1488677745392_2.java



Now, I'm not sure how to use it.  I've tried the following:


java -cp h2o-genmodel.jar javac -cp h2o-genmodel.jar -J-Xmx2g -J-XX:MaxPermSize=128m GLM_model_python_1488677745392_2.java



The following is the code in the pojo:


/*
  Licensed under the Apache License, Version 2.0
    http://www.apache.org/licenses/LICENSE-2.0.html

  AUTOGENERATED BY H2O at 2017-03-05T01:51:46.237Z
  3.10.3.2

  Standalone prediction code with sample test data for GLMModel named GLM_model_python_1488677745392_2

  How to download, compile and execute:
      mkdir tmpdir
      cd tmpdir
      curl http:/10.0.0.4/10.0.0.4:54321/3/h2o-genmodel.jar > h2o-genmodel.jar
      curl http:/10.0.0.4/10.0.0.4:54321/3/Models.java/GLM_model_python_1488677745392_2 > GLM_model_python_1488677745392_2.java
      javac -cp h2o-genmodel.jar -J-Xmx2g -J-XX:MaxPermSize=128m GLM_model_python_1488677745392_2.java

     (Note:  Try java argument -XX:+PrintCompilation to show runtime JIT compiler behavior.)
*/
import java.util.Map;
import hex.genmodel.GenModel;
import hex.genmodel.annotations.ModelPojo;

@ModelPojo(name=""GLM_model_python_1488677745392_2"", algorithm=""glm"")
public class GLM_model_python_1488677745392_2 extends GenModel {
  public hex.ModelCategory getModelCategory() { return hex.ModelCategory.Multinomial; }

  public boolean isSupervised() { return true; }
  public int nfeatures() { return 4; }
  public int nclasses() { return 3; }

  // Names of columns used by model.
  public static final String[] NAMES = NamesHolder_GLM_model_python_1488677745392_2.VALUES;
  // Number of output classes included in training data response column.
  public static final int NCLASSES = 3;

  // Column domains. The last array contains domain of response column.
  public static final String[][] DOMAINS = new String[][] {
    /* C1 */ null,
    /* C2 */ null,
    /* C3 */ null,
    /* C4 */ null,
    /* C5 */ GLM_model_python_1488677745392_2_ColInfo_4.VALUES
  };
  // Prior class distribution
  public static final double[] PRIOR_CLASS_DISTRIB = {0.2818181818181818,0.33636363636363636,0.38181818181818183};
  // Class distribution used for model building
  public static final double[] MODEL_CLASS_DISTRIB = null;

  public GLM_model_python_1488677745392_2() { super(NAMES,DOMAINS); }
  public String getUUID() { return Long.toString(-5598526670666235824L); }

  // Pass in data in a double[], pre-aligned to the Model's requirements.
  // Jam predictions into the preds[] array; preds[0] is reserved for the
  // main prediction (class for classifiers or value for regression),
  // and remaining columns hold a probability distribution for classifiers.
  public final double[] score0( double[] data, double[] preds ) {
    final double [] b = BETA.VALUES;
    for(int i = 0; i < 0; ++i) if(Double.isNaN(data[i])) data[i] = CAT_MODES.VALUES[i];
    for(int i = 0; i < 4; ++i) if(Double.isNaN(data[i + 0])) data[i+0] = NUM_MEANS.VALUES[i];
    preds[0] = 0;
    for(int c = 0; c < 3; ++c){
      preds[c+1] = 0;
      for(int i = 0; i < 4; ++i)
        preds[c+1] += b[0+i + c*5]*data[i];
      preds[c+1] += b[4 + c*5]; // reduce intercept
    }
    double max_row = 0;
    for(int c = 1; c < preds.length; ++c) if(preds[c] > max_row) max_row = preds[c];
    double sum_exp = 0;
    for(int c = 1; c < preds.length; ++c) { sum_exp += (preds[c] = Math.exp(preds[c]-max_row));}
    sum_exp = 1/sum_exp;
    double max_p = 0;
    for(int c = 1; c < preds.length; ++c) if((preds[c] *= sum_exp) > max_p){ max_p = preds[c]; preds[0] = c-1;};
    return preds;
  }
    public static class BETA implements java.io.Serializable {
      public static final double[] VALUES = new double[15];
      static {
        BETA_0.fill(VALUES);
      }
      static final class BETA_0 implements java.io.Serializable {
        static final void fill(double[] sa) {
          sa[0] = -1.4700470387418272;
          sa[1] = 4.26067731522767;
          sa[2] = -2.285756276489862;
          sa[3] = -4.312931422791621;
          sa[4] = 5.231215014401568;
          sa[5] = 1.7769023115830205;
          sa[6] = -0.2534145823550425;
          sa[7] = -0.9887536067536575;
          sa[8] = -1.2706135235877678;
          sa[9] = -4.319817154759757;
          sa[10] = 0.0;
          sa[11] = -3.024835247270209;
          sa[12] = 3.8622405283810464;
          sa[13] = 7.018262604176258;
          sa[14] = -22.702291637028203;
        }
      }
}
// Imputed numeric values
    static class NUM_MEANS implements java.io.Serializable {
      public static final double[] VALUES = new double[4];
      static {
        NUM_MEANS_0.fill(VALUES);
      }
      static final class NUM_MEANS_0 implements java.io.Serializable {
        static final void fill(double[] sa) {
          sa[0] = 5.90272727272727;
          sa[1] = 3.024545454545454;
          sa[2] = 3.9490909090909097;
          sa[3] = 1.2872727272727267;
        }
      }
}
// Imputed categorical values.
    static class CAT_MODES implements java.io.Serializable {
      public static final int[] VALUES = new int[0];
      static {
      }
}
    // Categorical Offsets
    public static final int[] CATOFFS = {0};
}
// The class representing training column names
class NamesHolder_GLM_model_python_1488677745392_2 implements java.io.Serializable {
  public static final String[] VALUES = new String[4];
  static {
    NamesHolder_GLM_model_python_1488677745392_2_0.fill(VALUES);
  }
  static final class NamesHolder_GLM_model_python_1488677745392_2_0 implements java.io.Serializable {
    static final void fill(String[] sa) {
      sa[0] = ""C1"";
      sa[1] = ""C2"";
      sa[2] = ""C3"";
      sa[3] = ""C4"";
    }
  }
}
// The class representing column C5
class GLM_model_python_1488677745392_2_ColInfo_4 implements java.io.Serializable {
  public static final String[] VALUES = new String[3];
  static {
    GLM_model_python_1488677745392_2_ColInfo_4_0.fill(VALUES);
  }
  static final class GLM_model_python_1488677745392_2_ColInfo_4_0 implements java.io.Serializable {
    static final void fill(String[] sa) {
      sa[0] = ""Iris-setosa"";
      sa[1] = ""Iris-versicolor"";
      sa[2] = ""Iris-virginica"";
    }
  }
}



Now, I think I need to call score0.  I've figured out how to create my own main.java and create an entrypoint to main() so that I can instantiate the object and call score0, but I have no idea how it's supposed to work.  I'm expecting to feed in 4 doubles and get back a category, but instead, the function takes two double[] and I can't figure out exactly what to put where and how to read the results.  Here's  my main:


public class Main {
  public static void main(String[] args) {
      double[] input = {4.6, 3.1, 1.5, 0.2};
      double[] output = new double[4];
      GLM_model_python_1488677745392_2 m = new GLM_model_python_1488677745392_2();
      double[] t = m.score0(input,output);
      for(int i = 0; i < t.length; i++) System.out.println(t[i]);
  }
}



I'm actually getting a bunch of data returned, but I don't know what any of it means.  I think I'm completely using the second argument incorrectly, but I'm not sure what to do. Here's the output:


0.0
0.9976588811416329
0.0023411188583572825
9.662837354438092E-15","['java', 'python', 'pojo', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
42491661,42491661,2017-02-27T17:08:12,2017-06-06 17:48:44Z,0,"I am trying to import a large .csv file using h2o.importfile in R


library(h2o)
h2o.init()
dataFile <- ""big_file.csv"" 
h2o.importFile(dataFile,header=TRUE,destination_frame = ""data.hex"")



The file has a number of id columns. I get the following error message. 


Error: water.parser.ParseDataset$H2OParseException: Exceeded categorical limit on columns [id1, id2]. Consider reparsing these columns as a string.


Is there way to specify these colum types to be strings similar to data.frame(stringAsFactors = FALSE)","['r', 'h2o']",iboboboru,https://stackoverflow.com/users/4128894/iboboboru,"1,102"
42470472,42470472,2017-02-26T15:33:30,2018-08-24 07:06:58Z,0,"I have a problem with h2o (deeplearning) in RStudio. 
It's the first time I'm using h2o and everytime I try to get a model with h2o.deeplearning() the prozess freezes when it reaches 100%. And when I want to build multiple models in a loop (as added below) it reaches 100% for the first model an then freezes. 
I attach my code and the error message below:


 <<>>= remove/install packages
# The following two commands remove any previously installed H2O packages     for R.
if (""package:h2o"" %in% search()) { detach(""package:h2o"", unload=TRUE) }
if (""h2o"" %in% rownames(installed.packages())) { remove.packages(""h2o"") }

# Next, we download packages that H2O depends on.
pkgs <- c(""methods"",""statmod"",""stats"",""graphics"",""RCurl"",""jsonlite"",""tools"",""utils"")
for (pkg in pkgs) {
if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}

# Now we download, install and initialize the H2O package for R.
install.packages(""h2o"", type=""source"", repos=(c(""http://h2o-release.s3.amazonaws.com/h2o/rel-tverberg/5/R"")))
@
<<>>==

library(h2o)
localH2O=h2o.init(nthreads = -1)
@



I get this result after init(nthreads=-1)


<<>>= #load data
training <- read.csv(""training.csv"", row.names = ""ID"", colClasses = c(Affection = ""factor""))
testing <- read.csv(""testing.csv"", row.names = ""ID"", colClasses = c(DRB1_1 = ""factor""))
training_h2o <- as.h2o(training)
testing_h2o <- as.h2o(testing)
@



I then try to build multiple models similar to the explanation here: 
githup explanation


<<>>=#tune Parameter

#compute multiple models
models <- c()
for (i in 1:5){
rand_activation <- c(""Tanh"", ""Rectifier"")[sample(1:2,1)]
rand_epochs <- sample(c(10, 15),1)
rand_hidden <- c(sample(100:500,2))
rand_l1 <- runif(1, 0, 1e-3)
rand_l2 <- runif(1, 0, 1e-3)
rand_input_dropout <- runif(1, 0, 0.5)
dl.model <- h2o.deeplearning(x = 2:3979, y = 1, training_frame =  training_h2o, nfolds = 10, keep_cross_validation_predictions = TRUE,   variable_importances = TRUE, adaptive_rate = TRUE,
 activation = rand_activation, hidden = rand_hidden, l1=rand_l1, l2 =     rand_l2, epochs = rand_epochs, input_dropout_ratio = rand_input_dropout)  
models <- c(models, dl.model)
}



But as mentioned before this always freezes and when I cancel it I get the following error (shortened):


ERROR: Unexpected HTTP Status code: 404 Not Found (url =   http://localhost:54321/3/Models/DeepLearning_model_R_1488120350711_1)

water.exceptions.H2OKeyNotFoundArgumentException
[1] ""water.exceptions.H2OKeyNotFoundArgumentException: Object    'DeepLearning_model_R_1488120350711_1' not found for argument: key""
 [2] ""    water.api.ModelsHandler.getFromDKV(ModelsHandler.java:110)""                                                             
 [3] ""    water.api.ModelsHandler.fetch(ModelsHandler.java:129)""                                                                  
 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                            
 [5] ""    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)""                                                            

[35] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                                       
[36] ""        org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                        
[37] ""    java.lang.Thread.run(Unknown Source)""                                                                                   

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix =    page,  : 


ERROR MESSAGE:

Object 'DeepLearning_model_R_1488120350711_1' not found for argument: key



Can someone help me with this? I never used h2o before and am not sure I used the commands correctly but tried to stay close to the h2o.pdf. 
I also can't open the url given in the error.


Thanks for your help!","['r', 'deep-learning', 'h2o']",Lauren,https://stackoverflow.com/users/7618426/lauren,21
42424156,42424156,2017-02-23T19:08:00,2017-08-04 14:17:29Z,405,"I am trying to submit my sparkling water application in yarn cluster mode but it fails. However, it runs in client mode.


I am using the following to submit my jar:


spark2-submit --class <main_class_name> --conf spark.ext.h2o.topology.change.listener.enabled=false --conf spark.ext.h2o.fail.on.unsupported.spark.param=false --conf spark.ext.h2o.repl.enabled=false --conf spark.executor.memory=5g --conf spark.driver.memory=5g --num-executors 5 --conf spark.dynamicAllocation.enabled=false --master yarn --deploy-mode cluster <name_of_the_jar>.jar



I get the following error when running in cluster mode:


ERROR executor.Executor: Exception in task 1.0 in stage 6.0 (TID 165)
java.io.InvalidClassException: org.apache.commons.lang3.time.FastDateFormat; local class incompatible: stream classdesc serialVersionUID = 2, local class serialVersionUID = 1
    at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:621)
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1623)
    at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)
    at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)
    at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)
    at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)
    at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
    at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
    at org.apache.spark.scheduler.Task.run(Task.scala:86)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)","['scala', 'apache-spark', 'h2o', 'sparkling-water']",Souradeep Basu,https://stackoverflow.com/users/5361376/souradeep-basu,83
42408295,42408295,2017-02-23T06:10:03,2017-09-07 17:42:10Z,0,"Accoding to the Description of function:


h2o.stack:
 This function creates a ""Super Learner"" (stacking) ensemble using a list of existing H2O base models specified by the user.


h2o.ensemble:
 This function creates a ""Super Learner"" (stacking) ensemble using the H2O base learning algorithms specified by the user.","['r', 'h2o']",Tao Hu,https://stackoverflow.com/users/6325806/tao-hu,301
42397089,42397089,2017-02-22T16:29:04,2023-08-04 11:36:45Z,0,"I was training a machine learning model using h2o, but the process crashed while parsing data. I restarted the python kernel, but now when I try to call 
h2o.connect()
 again the script raises the following error:


  File ""<ipython-input-7-3b2ccf9d3f4c>"", line 1, in <module>
    h2o.connect()

  File ""/Users/victormayrink/anaconda/lib/python3.5/site-packages/h2o/h2o.py"", line 74, in connect
    cluster_id=cluster_id, cookies=cookies, verbose=verbose)

  File ""/Users/victormayrink/anaconda/lib/python3.5/site-packages/h2o/backend/connection.py"", line 175, in open
    conn._cluster = conn._test_connection(retries, messages=_msgs)

  File ""/Users/victormayrink/anaconda/lib/python3.5/site-packages/h2o/backend/connection.py"", line 437, in _test_connection
    raise H2OServerError(""Cluster reports unhealthy status"")

H2OServerError: Cluster reports unhealthy status","['python', 'h2o']",Victor Mayrink,https://stackoverflow.com/users/4862402/victor-mayrink,"1,126"
42385017,42385017,2017-02-22T07:19:28,2017-02-23 19:20:33Z,753,"I am getting following error while executing 
sparkling-shell2.cmd
 bat file. I walked through and I am getting this error while executing 
spark-shell.cmd
 with following paramters


cd %TOPDIR%
%SPARK_HOME%/bin/spark-shell.cmd --jars %TOPDIR%/assembly/build/libs/%FAT_JAR% --driver-memory %DRIVER_MEMORY% --conf spark.driver.extraJavaOptions=""-XX:MaxPermSize=384m"" %*





Error: The input line is too long.




How do I solve this issue?


Thanks","['apache-spark', 'h2o', 'sparkling-water']",Mansoor,https://stackoverflow.com/users/4520274/mansoor,"1,259"
42291510,42291510,2017-02-17T07:07:25,2017-02-21 06:34:10Z,336,"I am trying to integrate Collaborative algorithm in Spark MLLib with H2o Ai using Sparkling water for product recommendation.  I followed this link


http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html


and updated code as in below


System.setProperty(""hadoop.home.dir"", ""D:\\backup\\lib\\winutils"")
    val conf = new SparkConf()
      .setAppName(""Spark-InputFile processor"")
      .setMaster(""local"")

    val sc = new SparkContext(conf)

    val inputFile = ""src/main/resources/test.data""

    val data = sc.textFile(inputFile)

    val ratings = data.map(x=>{
      val mapper = x.split("","")
      Rating(mapper(0).toInt,mapper(1).toInt,mapper(2).toDouble)
    })
    // Build the recommendation model using ALS
    val rank = 10
    val numIterations = 10
    val model = ALS.train(ratings, rank, numIterations, 0.01)



    // Save and load model
    model.save(sc, ""target/tmp/myCollaborativeFilter"")
    val sameModel = MatrixFactorizationModel.load(sc, ""target/tmp/myCollaborativeFilter"")

    val modelRdd = sameModel.recommendProductsForUsers(100)

    implicit val sqlContext = SparkSession.builder().getOrCreate().sqlContext
    import sqlContext.implicits._

    val modelDf = modelRdd.toDF(""Rdd"",""Rdd1"")




    @transient val hc = H2OContext.getOrCreate(sc)

    val h2oframe:H2OFrame = hc.asH2OFrame(modelDf)



When I run the code in Intellij I am getting the below error


Exception in thread ""main"" java.util.NoSuchElementException: key not found: StructType(StructField(user,IntegerType,false), StructField(product,IntegerType,false), StructField(rating,DoubleType,false))
    at scala.collection.MapLike$class.default(MapLike.scala:228)
    at scala.collection.AbstractMap.default(Map.scala:59)
    at scala.collection.MapLike$class.apply(MapLike.scala:141)
    at scala.collection.AbstractMap.apply(Map.scala:59)
    at org.apache.spark.h2o.utils.ReflectionUtils$.vecTypeFor(ReflectionUtils.scala:132)
    at org.apache.spark.h2o.converters.SparkDataFrameConverter$$anonfun$3.apply(SparkDataFrameConverter.scala:68)
    at org.apache.spark.h2o.converters.SparkDataFrameConverter$$anonfun$3.apply(SparkDataFrameConverter.scala:68)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    at scala.collection.Iterator$class.foreach(Iterator.scala:893)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.spark.h2o.converters.SparkDataFrameConverter$.toH2OFrame(SparkDataFrameConverter.scala:68)
    at org.apache.spark.h2o.H2OContext.asH2OFrame(H2OContext.scala:132)
    at org.apache.spark.h2o.H2OContext.asH2OFrame(H2OContext.scala:130)
    at com.poc.sample.RecommendataionAlgo$.main(RecommendataionAlgo.scala:54)
    at com.poc.sample.RecommendataionAlgo.main(RecommendataionAlgo.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)



How can I solve this error?


Thanks in advance.","['apache-spark', 'h2o', 'sparkling-water']",Unknown,,N/A
42284205,42284205,2017-02-16T20:41:32,2020-06-23 00:38:36Z,0,"I'm trying to do something with 
h2o
 in 
Rstudio
, but have problems when using 
as.h2o()
. It always gives back the following 
error
.


For example:


library(h2o)
localH2O = h2o.init()

finaldata.hex = as.h2o(finaldata)


ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://localhost:54321/3/PostFile?destination_frame=%2Fprivate%2Fvar%2Ffolders%2F8z%2F29h4lb311gbdhg58mj704g580000gn%2FT%2FRtmpr83spR%2Ffile12ab3b8df30c.csv_sid_a24d_3)

Error: lexical error: invalid char in json text.
                                       <html> <head> <meta http-equiv=
                     (right here) ------^



Would you please help me to figure out how to fix this 
error
?


Thanks,","['r', 'rstudio', 'h2o']",Daniel,https://stackoverflow.com/users/7003536/daniel,"1,252"
42233552,42233552,2017-02-14T18:24:04,2017-02-18 23:47:09Z,383,"I’m trying to train a GBM on an EMR cluster with 60 c4.8xlarge nodes using Sparkling Water. The process runs successfully up to a specific data size. Once I hit a certain data size (number of training examples) the process freezes in the collect stage in SpreadRDDBuilder.scala and dies after an hour. While this is happening the network memory continues to grow to capacity while there’s no progress in Spark stages (see below) and very little CPU usage and network traffic. I’ve tried increasing the executor and driver memory and num-executors but I’m seeing the exact same behavior under all configurations.


Thanks for looking at this. It’s my first time posting here so please let me know if you need any more information.


Parameters


spark-submit --num-executors 355 --driver-class-path h2o-genmodel-3.10.1.2.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/* --driver-memory 20G --executor-memory 10G --conf spark.sql.shuffle.partitions=10000 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --driver-java-options -Dlog4j.configuration=file:${PWD}/log4j.xml --conf spark.ext.h2o.repl.enabled=false --conf spark.dynamicAllocation.enabled=false --conf spark.locality.wait=3000 --class com.X.X.X.Main X.jar -i s3a://x



Other parameters that I’ve tried with no success:


 conf spark.ext.h2o.topology.change.listener.enabled=false
 conf spark.scheduler.minRegisteredResourcesRatio=1
 conf spark.task.maxFailures=1
 conf spark.yarn.max.executor.failures=1



Spark UI


collect at SpreadRDDBuilder.scala:105 118/3551
collect at SpreadRDDBuilder.scala:105 109/3551
collect at SpreadRDDBuilder.scala:105 156/3551
collect at SpreadRDDBuilder.scala:105 151/3551
collect at SpreadRDDBuilder.scala:105 641/3551



Driver logs


17/02/13 22:43:39 WARN LiveListenerBus: Dropped 49459 SparkListenerEvents since Mon Feb 13 22:42:39 UTC 2017
 [Stage 9:(641 + 1043) / 3551][Stage 10:(151 + 236) / 3551][Stage 11:(156 + 195) / 3551]



stderror for yarn containers


t.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
     at java.lang.Thread.run(Thread.java:745)
 Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
     at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
     at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
     at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
     at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
     at scala.concurrent.Await$.result(package.scala:190)
     at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:81)
     ... 14 more
 17/02/13 22:56:34 WARN Executor: Issue communicating with driver in heartbeater
 org.apache.spark.SparkException: Error sending message [message = Heartbeat(222,[Lscala.Tuple2;@c7ac58,BlockManagerId(222, ip-172-31-25-18.ec2.internal, 36644))]
     at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:119)
     at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:518)
     at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547)
     at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
     at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
     at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953)
     at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547)
     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
     at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
     at java.lang.Thread.run(Thread.java:745)
 Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
     at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
     at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
     at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
     at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
     at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
     at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
     ... 13 more
 Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
     at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
     at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
     at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
     at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
     at scala.concurrent.Await$.result(package.scala:190)
     at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:81)
     ... 14 more
 17/02/13 22:56:41 WARN TransportResponseHandler: Ignoring response for RPC 8189382742475673817 from /172.31.27.164:37563 (81 bytes) since it is not outstanding
 17/02/13 22:56:41 WARN TransportResponseHandler: Ignoring response for RPC 7998046565668775240 from /172.31.27.164:37563 (81 bytes) since it is not outstanding
 17/02/13 22:56:41 WARN TransportResponseHandler: Ignoring response for RPC 8944638230411142855 from /172.31.27.164:37563 (81 bytes) since it is not outstanding","['apache-spark', 'hadoop-yarn', 'emr', 'h2o', 'sparkling-water']",Amir Ziai,https://stackoverflow.com/users/6896683/amir-ziai,148
42228715,42228715,2017-02-14T14:29:19,2017-02-16 09:36:45Z,0,"I know how to access packages in 
R
 scripts in 
Azure machine
 learning by either using the 
Azure
 supported ones or by zipping up the packages.


My problem now is that 
Azure
 machine learning does not support the 
h2o package
 and when I tried using the zipped file - it gave an 
error
. 


Has anyone figured out how to use 
h2o
 in 
R
 in 
Azure machine
 learning?","['r', 'azure', 'h2o', 'azure-machine-learning-service']",Daniel,https://stackoverflow.com/users/7003536/daniel,"1,252"
42212737,42212737,2017-02-13T20:15:24,2017-02-14 15:16:10Z,199,"I am using H2O with python. I need to do a transformation on a subset of my columns , raise each element to a power of 1.5 . How do I apply an element-wise and/or column-wise power function on a H2OFrame without converting it to a pandas object?","['python', 'h2o']",Unknown,,N/A
42201397,42201397,2017-02-13T10:07:04,2017-02-13 11:09:24Z,0,"Could I export model trained with 
h2o
 to Java code when using 
h2o
 from level of 
R
 package ?","['java', 'r', 'production-environment', 'h2o']",Qbik,https://stackoverflow.com/users/783421/qbik,"6,097"
42079678,42079678,2017-02-07T00:24:25,2017-02-11 09:51:02Z,148,"How to find the variables that contributes the most for a particular prediction in case of a decision tree? For eg. If there are features A,B,C,D,E and we build a decision tree on top of the dataset. Then for a sample x, lets say variables C,D contributes the most to the prediction(x). How to find these variables that contributed the most for prediction(x) in H2O? I know the H2O gives global importance of variables once the decision tree is built. My question applies in the case when we are using that particular tree to make a decision and in finding the variables that contributed to that particular decision. Scikit learn has functions to extract the rules that were used to predict a sample. Does H2O have any such functionality?","['decision-tree', 'h2o']",10land,https://stackoverflow.com/users/2202252/10land,185
42033052,42033052,2017-02-03T21:09:01,2017-03-08 17:09:33Z,480,"I am new to the sparkling water. I now how to run my program from sparkling-shell. However, I am not sure how to build a standalone application that I can give as an input to spark submit. What are the jars that I need to include to build my application?","['apache-spark', 'h2o', 'sparkling-water']",Souradeep Basu,https://stackoverflow.com/users/5361376/souradeep-basu,83
42002989,42002989,2017-02-02T13:03:56,2017-02-02 19:32:03Z,0,Does h2o supports multidimensional output ? I would like to train NN on data where input is 81-dimensional and output variable is also 81-dimensional vector.,"['r', 'h2o']",Qbik,https://stackoverflow.com/users/783421/qbik,"6,097"
41983520,41983520,2017-02-01T15:28:42,2017-02-02 01:51:04Z,0,"I have a H2O frame in R with two character columns and I would like to create a new column by concatenating them. I tried the following but it failed as Paste function is not supported by H2O. Any other ideas? I searched for a solution but haven't found one so far. Thank you.


df$Col3 = paste(df$Col1, df$Col2)","['concatenation', 'paste', 'h2o']",user3634351,https://stackoverflow.com/users/3634351/user3634351,111
41896004,41896004,2017-01-27T14:15:22,2017-01-28 09:47:19Z,0,"I am new with H2O and I am trying to use it, as initial test, to train a Neural network to interpolate a function.
I am trying with several, but no one works!!!
I get a NN which does not match the training set. I also tried by rescaling data (not here attached for simplicity), but it does not change.
Here is my code:


x<- seq(-50, 50, by=0.01)
y1<- x
f1<-data.frame(x, y1)
f1.hex<-as.h2o(f1)
random.vec <- h2o.runif(f1.hex)
train <- f1.hex[random.vec < 0.6, ]
valid <- f1.hex[(random.vec > 0.6) && (random.vec < 0.8),]
test <- f1.hex[random.vec > 0.8, ]


  m1 <- h2o.deeplearning(
  training_frame=train, 
  validation_frame=valid,   
  x=1,
  y=2,
  activation=""RectifierWithDropout"",   ## default
  hidden=c(25,25),             
  epochs=100,
  input_dropout_ratio = 0,
  hidden_dropout_ratios = c(0.5, 0.5), 
  stopping_rounds = 5,
  stopping_metric = ""AUTO"", 
  stopping_tolerance = 0.001  
  )

plot(m1, timestep = ""duration"", metric = ""deviance"")  
summary(m1)
pred <- h2o.predict(m1, f1.hex[,1])
pred.r<- as.data.frame(pred)



I get a wrong predicition with a huge deviance...
Altro tried to cheange epochs and other parameters...
Where am I wrong?
Thanks","['r', 'h2o']",Ricky71,https://stackoverflow.com/users/7479054/ricky71,11
41840596,41840596,2017-01-24T23:07:07,2017-01-25 00:03:32Z,0,"I don't understand why I'm getting the error below. I've imported csv data as an h2oFrame. It's clearly an h2oFrame since I can perform methods such as describe() and such. But, when I pass it in to the glm function, I'm getting the rror saying that it's not the proper datatype. Why is this occurring? 


dat = h2o.import_files(data_dir)
glm_normal = H2OGeneralizedLinearEstimator(family='gaussian') 
glm_normal.train(x=x, y=y, train_frame=dat) 



Error:
 
H2OTypeError: Argument 
training_frame
 should be an H2OFrame, got NoneType None","['python', 'h2o']",MLhacker,https://stackoverflow.com/users/5641469/mlhacker,"1,492"
41821274,41821274,2017-01-24T06:18:41,2017-01-25 01:36:41Z,0,"Consider the below code :


library(h2o)
library(plyr)

h2o.init()
data1x <- ""x row1
1 1
1 2
1 3
1 4
2 1
2 2
2 3
3 1
4 2""
data1x <- read.table(textConnection(data1x), header=TRUE)
data1xH2O <- as.h2o(data1x)

fun = function(df) {
  1:2
}

h2o.ddply(data1xH2O, ""x"", fun)

ddply(data1x, ""x"", fun)



The h20 version of ddply gives below error.


ERROR: Unexpected HTTP Status code: 400 Bad Request (url = http://localhost:54321/99/Rapids)

water.rapids.Rapids.IllegalASTException
 [1] ""water.rapids.Rapids$IllegalASTException: Missing a number""                                                   
 [2] ""    water.rapids.Rapids.number(Rapids.java:312)""                                                             
 [3] ""    water.rapids.Rapids.parseNumList(Rapids.java:243)""                                                       
 [4] ""    water.rapids.Rapids.parseList(Rapids.java:208)""                                                          
 [5] ""    water.rapids.Rapids.parseNext(Rapids.java:140)""                                                          
 [6] ""    water.rapids.Rapids.parseFunctionDefinition(Rapids.java:193)""                                            
 [7] ""    water.rapids.Rapids.parseNext(Rapids.java:139)""                                                          
 [8] ""    water.rapids.Rapids.parseFunctionApplication(Rapids.java:158)""                                           
 [9] ""    water.rapids.Rapids.parseNext(Rapids.java:138)""                                                          
[10] ""    water.rapids.Rapids.parseFunctionApplication(Rapids.java:158)""                                           
[11] ""    water.rapids.Rapids.parseNext(Rapids.java:138)""                                                          
[12] ""    water.rapids.Rapids.parse(Rapids.java:48)""                                                               
[13] ""    water.rapids.Rapids.exec(Rapids.java:81)""                                                                
[14] ""    water.api.RapidsHandler.exec(RapidsHandler.java:39)""                                                     
[15] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                             
[16] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                           
[17] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                   
[18] ""    java.lang.reflect.Method.invoke(Method.java:483)""                                                        
[19] ""    water.api.Handler.handle(Handler.java:61)""                                                               
[20] ""    water.api.RequestServer.serve(RequestServer.java:412)""                                                   
[21] ""    water.api.RequestServer.doGeneric(RequestServer.java:263)""                                               
[22] ""    water.api.RequestServer.doPost(RequestServer.java:200)""                                                  
[23] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                            
[24] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                            
[25] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                  
[26] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                              
[27] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)""                           
[28] ""    org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)""                             
[29] ""    org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)""                       
[30] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                      
[31] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                               
[32] ""    org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)""                        
[33] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                       
[34] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                           
[35] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                   
[36] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                         
[37] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                 
[38] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""          
[39] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""           
[40] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)""                
[41] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)""
[42] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)""                                        
[43] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)""                                   
[44] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                  
[45] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""            
[46] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                        
[47] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                         
[48] ""    java.lang.Thread.run(Thread.java:745)""                                                                   

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 

ERROR MESSAGE:

Missing a number



The same code for ddply from plyr package works.
Please give any suggestion.","['r', 'h2o']",Unknown,,N/A
41804489,41804489,2017-01-23T10:46:29,2017-01-25 01:42:59Z,0,"Consider the following code.


library(data.table)

data1x <- ""x row1
1 1
1 2
1 3
1 4
2 1
2 2
2 3
3 1
4 2""

data1 <- read.table(textConnection(data1x), header=TRUE)
setDT(data1)
data1[, seqN := 1:.N, by=""x""]



I want to do similar thing on h2o data frame. I went through the user manual, but am not able to figure out how to do  this using h20.


EDIT :
 I tried the below code, for h2o, but it is giving error :


library(h2o)
h2o.init()
data1x <- ""x row1
1 1
1 2
1 3
1 4
2 1
2 2
2 3
3 1
4 2""
data1x <- read.table(textConnection(data1x), header=TRUE)
data1xH2O <- as.h2o(data1x)

fun = function(df) {
  1:nrow(df)
}

h2o.ddply(data1xH2O, ""x"", fun)



This results in following error :


ERROR: Unexpected HTTP Status code: 400 Bad Request (url = http://localhost:54321/99/Rapids)

water.rapids.Rapids.IllegalASTException
 [1] ""water.rapids.Rapids$IllegalASTException: Missing a number""                                                   
 [2] ""    water.rapids.Rapids.number(Rapids.java:312)""                                                             
 [3] ""    water.rapids.Rapids.parseNumList(Rapids.java:243)""                                                       
 [4] ""    water.rapids.Rapids.parseList(Rapids.java:208)""                                                          
 [5] ""    water.rapids.Rapids.parseNext(Rapids.java:140)""                                                          
 [6] ""    water.rapids.Rapids.parseFunctionDefinition(Rapids.java:193)""                                            
 [7] ""    water.rapids.Rapids.parseNext(Rapids.java:139)""                                                          
 [8] ""    water.rapids.Rapids.parseFunctionApplication(Rapids.java:158)""                                           
 [9] ""    water.rapids.Rapids.parseNext(Rapids.java:138)""                                                          
[10] ""    water.rapids.Rapids.parseFunctionApplication(Rapids.java:158)""                                           
[11] ""    water.rapids.Rapids.parseNext(Rapids.java:138)""                                                          
[12] ""    water.rapids.Rapids.parse(Rapids.java:48)""                                                               
[13] ""    water.rapids.Rapids.exec(Rapids.java:81)""                                                                
[14] ""    water.api.RapidsHandler.exec(RapidsHandler.java:39)""                                                     
[15] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                             
[16] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                           
[17] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                   
[18] ""    java.lang.reflect.Method.invoke(Method.java:483)""                                                        
[19] ""    water.api.Handler.handle(Handler.java:61)""                                                               
[20] ""    water.api.RequestServer.serve(RequestServer.java:412)""                                                   
[21] ""    water.api.RequestServer.doGeneric(RequestServer.java:263)""                                               
[22] ""    water.api.RequestServer.doPost(RequestServer.java:200)""                                                  
[23] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                            
[24] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                            
[25] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                  
[26] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                              
[27] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)""                           
[28] ""    org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)""                             
[29] ""    org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)""                       
[30] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                      
[31] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                               
[32] ""    org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)""                        
[33] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                       
[34] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                           
[35] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                   
[36] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                         
[37] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                 
[38] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""          
[39] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""           
[40] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)""                
[41] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)""
[42] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)""                                        
[43] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)""                                   
[44] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                  
[45] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""            
[46] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                        
[47] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                         
[48] ""    java.lang.Thread.run(Thread.java:745)""                                                                   

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Missing a number



Any suggestion, what I can do to remove this error.","['r', 'data.table', 'h2o']",Unknown,,N/A
41788232,41788232,2017-01-22T07:02:51,2017-01-24 20:04:21Z,0,"What does the key argument does in h2o? why we need to specify it?


I looked for an answer but couldn't find anything satisfactory. It appears as if I am missing something very obvious. I haven't seen it in R.


h2o.assign(data, key)

Arguments
data    An H2OFrame object
key     The hex key to be associated with the H2O parsed data object



e.g.


df <- h2o.importFile(path = normalizePath(""../data/covtype.full.csv""))
splits <- h2o.splitFrame(df, c(0.6,0.2), seed=1234)
valid  <- h2o.assign(splits[[2]], ""valid.hex"")","['r', 'h2o']",Unknown,,N/A
41725997,41725997,2017-01-18T17:40:35,2017-04-14 17:54:36Z,78,"I have created a flow to predict something with the distributed random forest model and now i want to predict every few days, without using the flow gui.


So is there a way to automate your H2O Flow or to translate the entire script into java/python to automatically run the flow?","['apache-spark', 'h2o', 'sparkling-water']",Peter Hall,https://stackoverflow.com/users/493729/peter-hall,58.1k
41680746,41680746,2017-01-16T16:22:56,2017-01-25 05:12:00Z,928,"Just getting started on h2o.ai, I'm loving the algos and distributed computing but stuck on data manipulation...


How would I be able to filter a H2OFrame on 
Timestamp('2011-01-01 00:00:00')
 AND 
numpy.datetime64('2011-01-02T00:00:00.000000000')
?


So for example I'd like to do the equivalent of this pandas date slicing


import pandas as pd
import h2o
h2o.init()

dft=pd.DataFrame({'date':pd.date_range('1/1/2011', periods=10), 'value': range(10)})
dft.ix[dft['date']>dft['date'].min()]
dft.ix[dft['date']>dft['date'].values[1]]



I've failed at this


dfh2o=h2o.H2OFrame(dft)
dfh2o[dfh2o['date']>dft['date'].min()] # causes error
dfh2o[dfh2o['date']>dft['date'].values[1]] # causes error
dfh2o['date2']=dfh2o['date'].asdate() # this function doesn't exist



NOTE that I'd like to filter on the original date values because I get them from elsewhere and it will be difficult to include them in the H2OFrame. That is I don't just want something like this


dfh2o[dfh2o['date']>dfh2o['date'].min()]
dfh2o[dfh2o['date']>dfh2o[1,'date']]



And I'm not sure how the Timestamp gets converted by H2OFrame


dft['date'].astype('int64').min()
Out[16]: 1293840000000000000

dfh2o['date'].min()
Out[17]: 1293858000000.0","['python', 'h2o']",Unknown,,N/A
41627290,41627290,2017-01-13T04:14:36,2020-12-17 00:03:39Z,0,"I have a large dataset in 
csv
 format to build a prediction model. Because of its size, I planned to use 
h2o
 package in 
R
 to build the model. However, the data, in multiple columns of the 
data.frame
, contains some Chinese Simplified characters and 
h2o
 is having difficulty receiving the data.


I've tried two different approaches. The first approach involved directly reading from the file using the 
h2o.importFile()
 function to import the data. However, this approach ends up converting the Chinese characters into some messy codes. 


The second approach I've tried to first bring the data into 
R
 using 
readr
 and base R 
read_csv
/
read.csv
 functions. After the data is loaded correctly into 
R
, I tried to convert the 
data.frame
 into 
h2o
 frame using 
as.h2o
 function. Though, the end result of this approach also resulted in a messed up translation.


To illustrate, I've written the following piece of codes as an example:


require(h2o)
dat<-data.frame(x=rep(c(""北京"",""上海""),50),
                y=rnorm(mean=10,sd=3,n=100))
h2o.init(nthreads=-1)
h2o.dat<-as.h2o(dat)","['r', 'h2o']",coatless,https://stackoverflow.com/users/1345455/coatless,20.7k
41501232,41501232,2017-01-06T07:47:32,2017-01-11 11:45:58Z,515,"I am new to Sparkling Water, I want to ask some quick questions: 




Does Sparking Water support all the algorithms that both Spark MLlib and H2O provides


Does Sparkling  Water itself provide algorithms that Spark MLlib and H2O don't support?


If I want to write code with pure Spark MLlib within Sparkling Water context, should I have to use 
H2OContext
 or Sparkling Water related API? 




Per the above 3 questions, I think what I want to understand is how Sparkling Water works. (For present, I know no more than that Sparkling Water brings Spark and H2O together)


Thanks.


Questions-2017-01-11



I am able to run the 
AirlinesWithWeatherDemo2
example with 
run-example.sh
successfully, but I got two questions:




H2O Flow web ui is opened during application running(can be accessed through 54321 port), but when the application is finished, 
the process that opens 54321 port is also shut down(the web ui is inaccessible any more), I would ask when I am running the example, what functionality does this flow UI provide since it may be short-lived


Sparkling water is meant to integrate Spark and H2O, when I submit the example, I only need the sparkling-water-assembly_2.11-2.0.3-all as the applicaiton jar(It contains the example classes), 
It looks that if I want to run H2O algorithms that Sparkling water doesn't provide, I should add the H2O jars(h2o.jar) as the dependent jars?","['h2o', 'sparkling-water']",Unknown,,N/A
41498435,41498435,2017-01-06T03:26:17,2017-01-06 03:26:17Z,304,"I am trying to run sparkling water deep learning demo in IntelliJ IDEA
The code link is:


https://github.com/h2oai/sparkling-water/blob/RELEASE-2.0.3/examples/src/main/scala/org/apache/spark/examples/h2o/DeepLearningDemo.scala


If fails to start, the exception is:


17/01/06 11:18:41 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(1,1483672721446,JobFailed(org.apache.spark.SparkException: Job 1 cancelled because SparkContext was shut down))
    at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:187)
    at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:177)
    at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
    at java.lang.Thread.getStackTrace(Thread.java:1108)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1930)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
    at org.apache.spark.rdd.RDD.collect(RDD.scala:911)
    at org.apache.spark.h2o.backends.internal.InternalBackendUtils$class.startH2O(InternalBackendUtils.scala:163)
    at org.apache.spark.h2o.backends.internal.InternalBackendUtils$.startH2O(InternalBackendUtils.scala:262)
    at org.apache.spark.h2o.backends.internal.InternalH2OBackend.init(InternalH2OBackend.scala:99)
    at org.apache.spark.h2o.H2OContext.init(H2OContext.scala:102)
    at org.apache.spark.h2o.H2OContext$.getOrCreate(H2OContext.scala:279)
    at org.apache.spark.h2o.H2OContext$.getOrCreate(H2OContext.scala:301)
    at com.xyz.HelloSparklingWater$.main(HelloSparklingWater.scala:35)
    at com.xyz.HelloSparklingWater.main(HelloSparklingWater.scala)



It looks exception is thrown when constructing 
H2OContext
 and 
InternalH2OBackend
.


I would ask how to run this example in spark local mode that is run within IDE","['h2o', 'sparkling-water']",Tom,https://stackoverflow.com/users/4443784/tom,"6,282"
41439307,41439307,2017-01-03T08:30:40,2019-07-09 16:00:48Z,0,"I would like to measure models performance by looking for AUC or Accuracy. In the grid search I get results with 
residual deviance
,how can I tell h2o deep learning grid to have AUC instead of residual deviance and present the results as atable like the one attached below ?


train <- read.table(text = ""target birds    wolfs     snakes
                              0        9         7 a
                              0        8         4 b
                              1        2         8 c
                              1        2         3 a
                              1        8         3 a
                              0        1         2 a
                              0        7         1 b
                              0        1         5 c
                              1        9         7 c
                              1        8         7 c
                              0        2         7 b
                              1        2         3 b
                              1        6         3 c
                              0        1         1 a
                              0        3         9 a
                              1        1         1 b "",header = TRUE)
trainHex <- as.h2o(train)

g <- h2o.grid(""deeplearning"",
              hyper_params = list(
                  seed = c(123456789,12345678,1234567),
                  activation = c(""Rectifier"", ""Tanh"", ""TanhWithDropout"", ""RectifierWithDropout"", ""Maxout"", ""MaxoutWithDropout"")
              ),
              reproducible = TRUE,
              x = 2:4,
              y = 1,
              training_frame = trainHex,
              validation_frame = trainHex,
              epochs = 50,
              )
g
model_ids <- g@summary_table
model_ids<-as.data.frame(model_ids)



The results table that I got:


     Hyper-Parameter Search Summary: ordered by increasing residual_deviance
             activation      seed                                                  model_ids   residual_deviance
1                Maxout  12345678 Grid_DeepLearning_train_model_R_1483217086840_112_model_10 0.07243775676256235
2                Maxout   1234567 Grid_DeepLearning_train_model_R_1483217086840_112_model_16 0.10060885040861599
3     MaxoutWithDropout 123456789  Grid_DeepLearning_train_model_R_1483217086840_112_model_5  0.1706496158406441
4                Maxout 123456789  Grid_DeepLearning_train_model_R_1483217086840_112_model_4 0.17243125875659948
5                  Tanh 123456789  Grid_DeepLearning_train_model_R_1483217086840_112_model_1 0.18326527198894926
6                  Tanh  12345678  Grid_DeepLearning_train_model_R_1483217086840_112_model_7 0.18763395264761593
7                  Tanh   1234567 Grid_DeepLearning_train_model_R_1483217086840_112_model_13 0.18791531211136187
8       TanhWithDropout 123456789  Grid_DeepLearning_train_model_R_1483217086840_112_model_2 0.19808063817007837
9       TanhWithDropout  12345678  Grid_DeepLearning_train_model_R_1483217086840_112_model_8 0.19815190962052193
10      TanhWithDropout   1234567 Grid_DeepLearning_train_model_R_1483217086840_112_model_14 0.19832946889767458
11            Rectifier 123456789  Grid_DeepLearning_train_model_R_1483217086840_112_model_0 0.20679125165086842
12    MaxoutWithDropout   1234567 Grid_DeepLearning_train_model_R_1483217086840_112_model_17 0.21971759565380736
13 RectifierWithDropout 123456789  Grid_DeepLearning_train_model_R_1483217086840_112_model_3 0.22337599298253263
14    MaxoutWithDropout  12345678 Grid_DeepLearning_train_model_R_1483217086840_112_model_11 0.22440661112729862
15 RectifierWithDropout   1234567 Grid_DeepLearning_train_model_R_1483217086840_112_model_15  0.2284671685474275
16 RectifierWithDropout  12345678  Grid_DeepLearning_train_model_R_1483217086840_112_model_9 0.23163744415703522
17            Rectifier   1234567 Grid_DeepLearning_train_model_R_1483217086840_112_model_12  0.2516917276707789
18            Rectifier  12345678  Grid_DeepLearning_train_model_R_1483217086840_112_model_6  0.2642221616447725","['r', 'h2o', 'auc']",mql4beginner,https://stackoverflow.com/users/1024441/mql4beginner,"2,223"
41428543,41428543,2017-01-02T14:19:54,2017-01-02 16:09:51Z,0,"I would like to create a data frame that will present the accuracy of different seeds number and deep learning methods.I have created the code that contains two loops (see below) but I got an error, How can I create this loop correctly


Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
ERROR MESSAGE:

Can only append one column



Attached is my code:


attach(iris)
train<-iris
test<-iris
invisible(capture.output(h2o.init(nthreads = -1))) # initalising with all cpu cores
trainHex <- as.h2o(train[1:200,])
testHex <- as.h2o(test)
x_names  <- colnames(trainHex[1:4])
SEED<-c(123456789,12345678,1234567)
method<-c(""Rectifier"", ""Tanh"", ""TanhWithDropout"", ""RectifierWithDropout"", ""Maxout"", ""MaxoutWithDropout"")
Res<-data.frame()

for(i in 1:6){
    for(j in 1:3){

        system.time(ann <- h2o.deeplearning(
            reproducible = TRUE,
            seed = SEED[j],
            x = x_names,
            y = ""Species"",
            training_frame = trainHex,epochs = 50,
            standardize = TRUE,
            nesterov_accelerated_gradient = T, # for speed
            activation = method[i] 
        ))
        #ann
        testHex$h20<-ifelse(predict(ann,newdata = testHex)>0.5,1,0)
        testHex<-as.data.frame(testHex)
        s<-xtabs(~Species +h20,data=testHex )
        accuracy<-sum(diag(s))/sum(s)
        tmp<-data.frame(seed=SEED[j],method=method[i],result=accuracy)
        Res<-rbind(Res,tmp)

    }
}
Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Can only append one column","['r', 'loops', 'h2o']",mql4beginner,https://stackoverflow.com/users/1024441/mql4beginner,"2,223"
41340086,41340086,2016-12-27T06:26:32,2021-05-30 08:21:08Z,0,"I have data in a normalised, tidy ""long"" data structure I want to upload to 
H2O
 and if possible analyse on a single machine (or have a definitive finding that I need more hardware and software than currently available).  The data is large but not enormous; perhaps 70 million rows of 3 columns in its efficient normalised form, and 300k by 80k when it has been cast into a sparse matrix (a big majority of cells being zeroes).  


The analytical tools in H2O need it to be in the latter, wide, format.  Part of the overall motivation is seeing where the limits of various hardware setups is with analysing such data, but at the moment I'm struggling just to get the data into an H2O cluster (on a machine where R can hold it all in RAM) so can't make the judgments about size limits for analysis.


The trial data are like the below, where the three columns are ""documentID"", ""wordID"" and ""count"":


1 61 2
1 76 1
1 89 1
1 211 1
1 296 1
1 335 1
1 404 1



Not that it matters - because this isn't even a real life dataset for me, just a test set - this test data is from 
https://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/docword.nytimes.txt.gz
 (caution, large download).


To analyse, I need it in a matrix with a row for each documentID, a column for each wordID, and the cells are the counts (number of that word in that document).  In R (for example), this can be done with 
tidyr::spread
 or (as in this particular case the dense data frame created by 
spread
 would be too large) 
tidytext::cast_sparse
, which works fine with this sized data so long as I am happy for the data to stay in R.


Now, the very latest version of H2O (available from h2o.ai but not yet on CRAN) has the R function 
as.h2o
 which understands sparse matrices, and this works well with smaller but still non-trivial data (eg in test cases of 3500 rows x 7000 columns it imports a sparse matrix in 3 seconds when the dense version takes 22 seconds), but when it gets my 300,000 x 80,000 sparse matrix it crashes with this error message:




Error in asMethod(object) : 
    Cholmod error 'problem too large' at file ../Core/cholmod_dense.c, line 105




As far as I can tell there are two ways forward:




upload a long, tidy, efficient form of the data into H2O and do the reshaping ""spread"" operation in H2O.


do the data reshaping in R (or any other language), save the resulting sparse matrix to disk in a sparse format, and upload from there into H2O




As far as I can tell, H2O doesn't have the functionality to do #1 ie the equivalent of 
tidytext::cast_sparse
 or 
tidyr::spread
 in R.  Its 
data munging capabilities
 look to be very limited.  But maybe I've missed something?  So my first (not very optimistic) question is 
can (and how can) H2O ""cast"" or ""spread"" data from long to wide format?
.


Option #2 becomes the same as this 
older question
, for which the accepted answer was to save the data in SVMlight format.  However, it's not clear to me how to do this efficiently, and it's not clear that SVMlight format makes sense for data that is not intended to be modelled with a support vector machine (for example, the data might be just for an unsupervised learning problem).  It would be much more convenient if I could save my sparse matrix in MatrixMarket format, which is supported by the 
Matrix
 package in R, but as far as I can tell isn't by H2O.  The MatrixMarket format looks very similar to my original long data, it's basically a space-delimited file that looks like 
colno rowno cellvalue
 (with a two line header).","['r', 'sparse-matrix', 'tidyr', 'h2o', 'tidytext']",Ronak Shah,https://stackoverflow.com/users/3962914/ronak-shah,388k
41296728,41296728,2016-12-23T07:03:45,2016-12-26 09:34:32Z,0,"In my current project, I am using H2O machine learning library in SparkR. I have multiple .csv files and reading these .csv files through h2o data frame. Now, I would like to apply 
h2o.merge()
 function over the files to 
map primary key of one h2o data frame with the foreign key of another h2o data frame
. My main h2o data frame contains 14 columns. I get data types of all the columns using 
h2o.getTypes()
 function. 


In order to apply 
h2o.merge()
 function, the column should be of type string or numeric instead of enum or real. So to convert data type of columns, I am using 
h2o.ascharacter()
 and 
h2o.asfactor()
 functions. Now, I have converted enum columns to string columns to use 
h2o.merge()
 functions. When I used 
h2o.merge()
 function it displays following error:

 
Am I missing anything ? I have captured the syntax to use 
h2o.merge()
 function from this link 
Syntax of h2o.merge function
.
How to merge h2o data frames?
Sample data set of factTable h2o data frame is shown below 
(SALES_ORG is a primary key)
:


Sample data set of regionTable h2o data frame is shown below 
(SALES_ORG is a foreign key)
:","['r', 'merge', 'sparkr', 'h2o']",Unknown,,N/A
41281064,41281064,2016-12-22T10:33:40,2016-12-22 15:05:58Z,217,"Closed
. This question needs to be more 
focused
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Update the question so it focuses on one problem only by 
editing this post
.






Closed 
7 years ago
.















                        Improve this question
                    








I'm pretty new on search engines and pretty newbie on machine learning. But I wanted to know if there is a way to combine functionalities of search engines like elasticsearch or Apache Solr and machine learning project like Apache Mahout, H2O or PredictionIO.


For exemple, if you work on a travel website where you can search for a destination. You start type ""au"", so the first suggestions are ""AUstria"", ""AUstralia"", ""mAUrice island"", ""mAUritania""... etc... This is typically what elasticsearch can do.


But you know that this user has already travelled on Mauritania three times, so you want that Mauritania goes on the first place of suggestions. And I guess that's typically what machine learning can do.


Is there bridges between this two type of technologies ? Can machine learning ensure the work of search engine efficiently ?


I'm open to all answers, regardless of the technologies used. If you have ever experienced this type of problems, my ears are wide open :-)


Thank you","['search', 'machine-learning', 'mahout', 'h2o', 'predictionio']",Oreste Viron,https://stackoverflow.com/users/4939853/oreste-viron,"3,785"
41270947,41270947,2016-12-21T19:57:21,2020-10-27 18:02:33Z,0,"This relates to the h2o package in R. I'm working with multiple jobs running in parallel, some may come in later than others. Is it possible to detect whether a h2o instance already exists and make a connection to that instance. 


I see that if I know an instance is already running, i just do 
h2o.init(startH2O=FALSE)
, but what if I don't know that?","['r', 'jvm', 'h2o']",horaceT,https://stackoverflow.com/users/2434201/horacet,651
41213721,41213721,2016-12-18T22:52:12,2016-12-19 08:32:43Z,0,"Tinkering with gradient boosting and I noticed R's 
gbm
 package produces different results than 
h2o
 on a minimal example.  Why?




Data


library(gbm)
library(h2o)

h2o.init()

train <- data.frame(
  X1 = factor(c(""A"", ""A"", ""A"", ""B"", ""B"")),
  X2 = factor(c(""A"", ""A"", ""B"", ""B"", ""B"")),
  Y = c(0, 1, 3, 4, 7)
)
  X1 X2 Y
1  A  A 0
2  A  A 1
3  A  B 3
4  B  B 4
5  B  B 7



gbm


# (gbm, 1 round, mae)
model.gbm <- gbm(
  Y ~ X1 + X2, data = train, distribution=""laplace"", n.tree = 1, shrinkage = 1, n.minobsinnode=1, bag.fraction=1, 
  interaction.depth = 1, verbose=TRUE
)
train$Pred.mae.gbm1 <- predict(model.gbm, newdata=train, n.trees=model.gbm$n.trees)



h2o


# (h2o, 1 round, mae)
model.h2o <- h2o.gbm(
  x=c(""X1"", ""X2""), y=""Y"", training_frame=as.h2o(train), distribution=""laplace"", ntrees=1, max_depth=1, 
  learn_rate = 1, min_rows=1
)
train$Pred.mae.h2o1 <- as.data.frame(h2o.predict(model.h2o, as.h2o(train)))$predict



Results


train
  X1 X2 Y Pred.mae.gbm1 Pred.mae.h2o1
1  A  A 0           1.0           0.5
2  A  A 1           1.0           0.5
3  A  B 3           1.0           4.0
4  B  B 4           5.5           4.0
5  B  B 7           5.5           4.0","['r', 'machine-learning', 'h2o', 'gbm']",Ben,https://stackoverflow.com/users/2146894/ben,21.5k
41177401,41177401,2016-12-16T04:43:47,2016-12-20 19:28:34Z,384,"I am trying to converting Spark DataFrame to H2O DataFrame


For spark setup, I am using


 .setMaster(""local[1]"")
 .set(""spark.driver.memory"", ""4g"")
 .set(""spark.executor.memory"", ""4g"")



and I tried H2O 2.0.2 and H2O 1.6.4. I got both the same error at:


 val trainsetH2O: H2OFrame = trainsetH
 val testsetH2O: H2OFrame = testsetH



The error message is:


 ERROR Executor: Exception in task 49.0 in stage 3.0 (TID 62)
 java.lang.OutOfMemoryError: PermGen space
     at sun.misc.Unsafe.defineClass(Native Method)
     at sun.reflect.ClassDefiner.defineClass(ClassDefiner.java:63)
     at sun.reflect.MethodAccessorGenerator$1.run(MethodAccessorGenerator.java:399)
     at sun.reflect.MethodAccessorGenerator$1.run(MethodAccessorGenerator.java:396)
     at java.security.AccessController.doPrivileged(Native Method)
     at sun.reflect.MethodAccessorGenerator.generate(MethodAccessorGenerator.java:395)
     at sun.reflect.MethodAccessorGenerator.generateSerializationConstructor(MethodAccessorGenerator.java:113)
     at sun.reflect.ReflectionFactory.newConstructorForSerialization(ReflectionFactory.java:331)
     at java.io.ObjectStreamClass.getSerializableConstructor(ObjectStreamClass.java:1376)
     at java.io.ObjectStreamClass.access$1500(ObjectStreamClass.java:72)
     at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:493)
     at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:468)
     at java.security.AccessController.doPrivileged(Native Method)
     at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:468)
     at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:365)
     at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:602)
     at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)
     at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
     at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
     at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
     at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
     at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
     at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
     at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
     at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
     at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
     at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
     at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)



where is wrong? The data in the trainset and testset are less than 10K, so it is actually pretty small.","['apache-spark', 'apache-spark-sql', 'h2o', 'sparkling-water']",lserlohn,https://stackoverflow.com/users/3804098/lserlohn,"6,166"
41104540,41104540,2016-12-12T15:50:14,2016-12-15 11:31:07Z,0,"I am trying to import a frame by creating a h2o frame from a spark parquet file.
The File is 2GB has about 12M rows and Sparse Vectors with 12k cols.
It is not that big in parquet format but the import takes forever.
In h2o it is actually reported as 447mb compressed size. Quite small actually.


Am I doing it wrong and when I actually finish importing (took 39min), Is there any form in h2o to save the frame to disk for a fast loading next time??


I understand h2o does some magic behind the scene which takes so long but I only found a download csv option which is slow and huge for a 11k x 1M sparse data and I doubt it is any faster to import.


I feel like there is a part missing. Any Info about h2o data import/export is appreciated.
Model save/load works great but train/val/test data loading seems an unreasonably slow procedure.


I got 10 sparkworkers with 10g each and gave the driver 8g. This should be plenty.","['h2o', 'sparkling-water']",samst,https://stackoverflow.com/users/4011761/samst,556
41075416,41075416,2016-12-10T12:17:16,2017-12-14 13:32:03Z,0,"After running h2o.deeplearning for a binary classification problem I then run the h2o.predict and obtain the following results


  predict        No       Yes
1      No 0.9784425 0.0215575
2     Yes 0.4667428 0.5332572
3     Yes 0.3955087 0.6044913
4     Yes 0.7962034 0.2037966
5     Yes 0.7413591 0.2586409
6     Yes 0.6800801 0.3199199



I was hoping to get a confusion matrix with only two rows. But this seems to be quite different. How do I interpret these results? Is there any way of getting something like a confusion matrix with actual and predicted values and error percentage?","['r', 'machine-learning', 'neural-network', 'deep-learning', 'h2o']",Sujay DSa,https://stackoverflow.com/users/4051357/sujay-dsa,"1,192"
41058856,41058856,2016-12-09T10:57:49,2016-12-10 21:09:15Z,0,"I have run h2o deeplearning and obtained a model as follows


best_model<- h2o.deeplearning( activation = ""RectifierWithDropout"",
                                            hidden = c(200, 200, 200, 200, 200),
                                            hidden_dropout_ratio = c(0.1, 0.1, 0.1, 0.1, 0.1),
                                            loss = ""CrossEntropy"",
                                            l1 = 1e-5,
                                            epochs = EPOCHS,
                                            distribution = ""multinomial"",
                                            seed = 5000,
                                            balance_classes = TRUE,
                                            y = c(""Churn""),
                                            x = columns,
                                            validation_frame = churn_validation,
                                            training_frame = churn_training

                                            )



Now I try to test it with my test data like this


churn_prediction <- h2o.predict(best_model, my_test)



I get this error:


Error in chk.H2OFrame(x) : must be an H2OFrame



Any suggestions please?


EDIT: The example from the documentation which seems to work fine


library(h2o)
h2o.init()
iris.hex <- as.h2o(iris)
iris.dl <- h2o.deeplearning(x = 1:4, y = 5, training_frame = iris.hex)

# now make a prediction
predictions <- h2o.predict(iris.dl, iris.hex)","['r', 'deep-learning', 'h2o']",Unknown,,N/A
41054025,41054025,2016-12-09T06:00:45,2016-12-19 07:20:40Z,0,"Closed
. This question is 
opinion-based
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Update the question so it can be answered with facts and citations by 
editing this post
.






Closed 
7 years ago
.















                        Improve this question
                    








Currently, I am doing survey on Machine Learning library using 
h2o.ai
 and 
SparkMLlib
. I have identified that more number of ML algorithms are supported by 
h2o.ai
 library as compare to 
SparkMLlib
, and partition of Spark data frame in to training and testing set seems to be difficult (need to convert spark data frame to R/h2o data frame which is also time/resource consuming). 


What are the others advantages/disadvantages of using 
h2o.ai
 library over 
SparkMLib
 or vice-versa ?
 I am focusing 
h2o.ai
 and 
SparkMLlib
 into R based implementation (SparkR). So the dataframes for h2o 
(as.h2o)
 and SparkMLlib 
(as.DataFrame)
 are different.","['r', 'machine-learning', 'sparkr', 'h2o']",Saurabh Chauhan,https://stackoverflow.com/users/5835763/saurabh-chauhan,"3,201"
41049921,41049921,2016-12-08T22:25:17,2017-04-12 15:40:20Z,0,"I am observing odd behavior when using the interactions argument within the h2o.glm function.  Specifically, while the coefficients match up with base R glm function, the predictions do not.  Given almost identical coefficients, I would expect almost identical predictions.  I've carefully run two versions of the glm in R and two versions with h2o to demonstrate this behavior below.  Why are the predictions from the h2o.glm model with interactions not matching up with the other glm predictions (despite having nearly identical coefficients)?


Here is code to reproduce this behavior along with comments noting where the predictions do not match up, but coefficients do.


# Load libraries and ingest data.
library(h2o)
h2o.init()
infile <- ""https://www.dropbox.com/s/itx2za2p63iez29/h2o_data2.csv?dl=1""
indf <- read.csv(infile, stringsAsFactors = FALSE)
indf$dow_x_hour <- paste(indf$dow, indf$hour)
indf[] <- lapply(indf[], as.factor)
str(indf)
# RESULT OF str(indf)
# 'data.frame': 8100 obs. of  4 variables:
# $ y         : Factor w/ 2 levels ""0"",""1"": 1 1 1 1 1 1 1 1 1 1 ...
# $ dow       : Factor w/ 3 levels ""Fri"",""Sat"",""Sun"": 1 1 1 2 2 2 3 3 3 1 ...
# $ hour      : Factor w/ 3 levels ""6"",""7"",""8"": 1 2 3 1 2 3 1 2 3 1 ...
# $ dow_x_hour: Factor w/ 9 levels ""Fri 6"",""Fri 7"",..: 1 2 3 4 5 6 7 8 9 1 ...
hf <- as.h2o(indf)


## FIRST TRY R ----------------
# Fit glm with R using interactions.
r_glm1 <- glm(y ~ dow + hour + dow:hour,
              family = ""binomial"",
              data = indf)

# Fit glm with R using concatenated column.
r_glm2 <- glm(y ~ dow_x_hour,
              family = ""binomial"",
              data = indf)

# These two R models generate near-identical predictions.
# RESULT: 4.496403e-15
max(abs(predict(r_glm2, type = ""response"") - predict(r_glm1, type = ""response"")))


## NOW H2O ----------------
# Fit glm with h2o using interactions.
h2o_glm1 <- h2o.glm(2:3,
                    1,
                    hf,
                    solver = ""IRLSM"",
                    family = ""binomial"",
                    interactions = 2:3,
                    lambda_search = FALSE,
                    lambda = 0,
                    compute_p_values = TRUE)

# Fit glm with h2o using concatenated column.
h2o_glm2 <- h2o.glm(4,
                    1,
                    hf,
                    solver = ""IRLSM"",
                    family = ""binomial"",
                    lambda_search = FALSE,
                    lambda = 0,
                    compute_p_values = TRUE)

# These two H2O models do not generate the same predictions.
# RESULT: 0.06211734
max(abs(h2o.predict(h2o_glm1, hf)$p1 - h2o.predict(h2o_glm2, hf)$p1))


## COMPARE R VS H2O PREDICTIONS ----------------

# The R and h2o models using concatenated column produce near idential predictions.
# RESULT: 3.356773e-07
max(abs(predict(r_glm2, type = ""response"") - as.data.frame(h2o.predict(h2o_glm2, hf))$p1))

# The R and h2o models using interactions DO NOT produce near idential predictions.
# RESULT: 0.06211732
max(abs(predict(r_glm1, type = ""response"") - as.data.frame(h2o.predict(h2o_glm1, hf))$p1))


## COMPARE R VS H2O COEFFIICENTS ----------------

# The R and h2o models using interactions produce near idential coefficients 
# (we manually matched them up here).
# RESULT: 3.341192e-06
df_coef <- cbind(h2o_glm1@model$coefficients_table, r_coef = coef(r_glm1)[c(1,6,8,7,9,2:5)])
max(abs(df_coef$coefficients - df_coef$r_coef))","['r', 'glm', 'h2o']",jmuhlenkamp,https://stackoverflow.com/users/6850554/jmuhlenkamp,"2,150"
41029009,41029009,2016-12-07T23:08:23,2016-12-07 23:15:06Z,0,"I'm working on a remote server with 196 GB of ram but when I use h2o.init(nthreads = -1) it says that the total cluster memory is .96 GB. Furthermore, when I try to use max_mem_size = ""2g"" I get an error saying that the JVM could not be created.


Now, the server is using a 32 bit Java version and I do not have admin access. What do I need to do to get more cluster memory?


Thanks","['java', 'r', 'h2o']",Stephen,https://stackoverflow.com/users/6592213/stephen,324
41027353,41027353,2016-12-07T21:05:21,2016-12-07 23:44:19Z,0,"I would like to estimate coefficient for a predictor by a categorical variable level in h2o glm. For example, if my data frame has product price (continuous variable) and product type (categorical variable), then I want to estimate a coefficient for price by product. In SAS, you can easily accomplish this by specifying model effect as price*type. How can I do the same in h2o or R? 


There is an interactions() function, but it cannot handle interaction between a continuous and categorical variables. Any tips to get around this problem?


Many thanks,","['r', 'glm', 'h2o']",user3634351,https://stackoverflow.com/users/3634351/user3634351,111
41023086,41023086,2016-12-07T23:08:23,2016-12-07 23:15:06Z,0,"I'm working on a remote server with 196 GB of ram but when I use h2o.init(nthreads = -1) it says that the total cluster memory is .96 GB. Furthermore, when I try to use max_mem_size = ""2g"" I get an error saying that the JVM could not be created.


Now, the server is using a 32 bit Java version and I do not have admin access. What do I need to do to get more cluster memory?


Thanks","['java', 'r', 'h2o']",Stephen,https://stackoverflow.com/users/6592213/stephen,324
41006726,41006726,2016-12-06T23:18:24,2016-12-12 13:30:27Z,469,"I am trying the simple droplet 
https://github.com/h2oai/sparkling-water
 program, but I am unable to make it run successfully using spark-submit. 


I used sparkling water 1.6.4, as used in the sample code.


 spark-submit --jars sparkling-water-assembly-1.6.4-all.jar swtest_2.10-1.0.jar



I didn't use gradel way, provided in the sample code. I just used very simple sbt build.


name := ""SWTest""

version := ""1.0""

scalaVersion := ""2.10.4""

libraryDependencies += ""ai.h2o"" % ""sparkling-water-core_2.10"" % ""1.6.4""
libraryDependencies += ""ai.h2o"" % ""sparkling-water-examples_2.10"" % ""1.6.4""



The program runs fine, until it reaches:


val trainRDD = h2oContext.asRDD[StringHolder](irisData('class))
val predictRDD = h2oContext.asRDD[StringHolder](predict)    

val numMispredictions = trainRDD.zip(predictRDD).filter( i => {
      val act = i._1
      val pred = i._2
      act.result != pred.result
    }).collect()

It looks like the as.RDD needs a generic type, and here is ""StringHolder""



However, it reports error "" Unable to find class: org.apache.spark.h2o.package$StringHolder"":


12-06 15:03:53.442 127.0.0.1:54321       489    FJ-1-3    INFO:  Number of Trees Model Size in Bytes Min. Depth Max. Depth Mean Depth Min. Leaves Max. Leaves Mean Leaves
12-06 15:03:53.442 127.0.0.1:54321       489    FJ-1-3    INFO:               15                2176          1          5    4.20000           2           9     7.20000
12-06 15:03:53.442 127.0.0.1:54321       489    FJ-1-3    INFO: Scoring History:
12-06 15:03:53.442 127.0.0.1:54321       489    FJ-1-3    INFO:            Timestamp   Duration Number of Trees Training MSE Training LogLoss Training Classification Error
12-06 15:03:53.442 127.0.0.1:54321       489    FJ-1-3    INFO:  2016-12-06 15:03:50  0.261 sec               0      0.44444          1.09861                       0.64000
12-06 15:03:53.442 127.0.0.1:54321       489    FJ-1-3    INFO:  2016-12-06 15:03:51  1.607 sec               1      0.36474          0.92664                       0.04000
12-06 15:03:53.442 127.0.0.1:54321       489    FJ-1-3    INFO:  2016-12-06 15:03:52  1.987 sec               2      0.29854          0.79143                       0.04667
12-06 15:03:53.442 127.0.0.1:54321       489    FJ-1-3    INFO:  2016-12-06 15:03:52  2.364 sec               3      0.24482          0.68353                       0.04667
12-06 15:03:53.442 127.0.0.1:54321       489    FJ-1-3    INFO:  2016-12-06 15:03:53  2.668 sec               4      0.20083          0.59453                       0.04667
12-06 15:03:53.442 127.0.0.1:54321       489    FJ-1-3    INFO:  2016-12-06 15:03:53  3.007 sec               5      0.16523          0.52069                       0.04667
gbm prediction
12-06 15:03:53.846 127.0.0.1:54321       489    main      INFO: Confusion Matrix (vertical: actual; across: predicted):
12-06 15:03:53.846 127.0.0.1:54321       489    main      INFO:                 Iris-setosa Iris-versicolor Iris-virginica  Error      Rate
12-06 15:03:53.846 127.0.0.1:54321       489    main      INFO:     Iris-setosa          50               0              0 0.0000 =  0 / 50
12-06 15:03:53.846 127.0.0.1:54321       489    main      INFO: Iris-versicolor           0              48              2 0.0400 =  2 / 50
12-06 15:03:53.846 127.0.0.1:54321       489    main      INFO:  Iris-virginica           0               5             45 0.1000 =  5 / 50
12-06 15:03:53.846 127.0.0.1:54321       489    main      INFO:          Totals          50              53             47 0.0467 = 7 / 150
12-06 15:03:53.847 127.0.0.1:54321       489    main      INFO: Top-3 Hit Ratios:
12-06 15:03:53.847 127.0.0.1:54321       489    main      INFO: K  Hit Ratio
12-06 15:03:53.847 127.0.0.1:54321       489    main      INFO: 1   0.953333
12-06 15:03:53.847 127.0.0.1:54321       489    main      INFO: 2   1.000000
12-06 15:03:53.847 127.0.0.1:54321       489    main      INFO: 3   1.000000
computer number of mispredictions
computer number of mispredictions
16/12/06 15:03:55 ERROR TaskResultGetter: Exception while getting task result
com.esotericsoftware.kryo.KryoException: Unable to find class: org.apache.spark.h2o.package$StringHolder
    at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138)
    at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115)
    at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:610)
    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:721)
    at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:41)
    at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
    at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:338)
    at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:293)
    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
    at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:311)
    at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:97)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:60)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:51)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:51)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
    at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:50)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.h2o.package$StringHolder
    at java.lang.ClassLoader.findClass(ClassLoader.java:531)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at org.apache.spark.repl.h2o.InterpreterClassLoader.loadClass(InterpreterClassLoader.scala:37)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:274)
    at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136)
    ... 19 more
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: Unable to find class: org.apache.spark.h2o.package$StringHolder
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
    at scala.Option.foreach(Option.scala:236)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
    at org.apache.spark.rdd.RDD.collect(RDD.scala:926)
    at swtest$.main(swtest.scala:68)
    at swtest.main(swtest.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:735)
    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)



I think I included sparkling-water-assembly-1.6.4-all.jar, which should contains everything.


Anyone would suggests any ideas?","['scala', 'apache-spark', 'h2o', 'sparkling-water']",Unknown,,N/A
40864176,40864176,2016-11-29T10:59:30,2016-11-30 13:08:38Z,0,"I have run a grid search, with epochs as one of the hyper parameters. Now after choosing the best model, how can I determine which epoch was chosen for this particular model?


Below is the summary of the model
    Model Details:
    ==============


H2OBinomialModel: deeplearning
Model ID:  dl_grid_model_19 
Status of Neuron Layers: predicting Churn, 2-class classification, bernoulli distribution, CrossEntropy loss, 4,226 weights/biases, 44.1 KB, 47,520 training samples, mini-batch size 1
  layer units             type dropout       l1       l2 mean_rate rate_rms momentum mean_weight weight_rms
1     1    30            Input  0.00 %                                                                     
2     2    32 RectifierDropout 20.00 % 0.000010 0.000010  0.009995 0.000000 0.501901   -0.011006   0.210611
3     3    32 RectifierDropout 20.00 % 0.000010 0.000010  0.009995 0.000000 0.501901   -0.035854   0.191687
4     4    32 RectifierDropout 20.00 % 0.000010 0.000010  0.009995 0.000000 0.501901   -0.029072   0.185352
5     5    32 RectifierDropout 20.00 % 0.000010 0.000010  0.009995 0.000000 0.501901   -0.057359   0.186863
6     6     2          Softmax         0.000010 0.000010  0.009995 0.000000 0.501901    0.122655   0.406789
  mean_bias bias_rms
1                   
2  0.401924 0.136989
3  0.938406 0.041128
4  0.950918 0.043826
5  0.915588 0.060796
6  0.019925 0.175195


H2OBinomialMetrics: deeplearning
** Reported on training data. **
** Metrics reported on full training frame **

MSE:  0.1946901
RMSE:  0.441237
LogLoss:  0.5731371
Mean Per-Class Error:  0.194215
AUC:  0.8767996
Gini:  0.7535992

Confusion Matrix for F1-optimal threshold:
         No  Yes    Error       Rate
No     1755  614 0.259181  =614/2369
Yes     308 2075 0.129249  =308/2383
Totals 2063 2689 0.194024  =922/4752

Maximum Metrics: Maximum metrics at their respective thresholds
                        metric threshold    value idx
1                       max f1  0.216316 0.818218 266
2                       max f2  0.058723 0.889206 348
3                 max f0point5  0.306487 0.801744 216
4                 max accuracy  0.217122 0.805976 265
5                max precision  0.730797 1.000000   0
6                   max recall  0.006754 1.000000 398
7              max specificity  0.730797 1.000000   0
8             max absolute_mcc  0.216316 0.616944 266
9   max min_per_class_accuracy  0.257957 0.795636 242
10 max mean_per_class_accuracy  0.217122 0.805792 265

Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`
H2OBinomialMetrics: deeplearning
** Reported on validation data. **
** Metrics reported on full validation frame **

MSE:  0.1418929
RMSE:  0.3766867
LogLoss:  0.4374728
Mean Per-Class Error:  0.2603761
AUC:  0.8306744
Gini:  0.6613489

Confusion Matrix for F1-optimal threshold:
         No Yes    Error       Rate
No     1075 201 0.157524  =201/1276
Yes     162 284 0.363229   =162/446
Totals 1237 485 0.210801  =363/1722

Maximum Metrics: Maximum metrics at their respective thresholds
                        metric threshold    value idx
1                       max f1  0.323830 0.610097 183
2                       max f2  0.087110 0.740000 319
3                 max f0point5  0.514027 0.608666  94
4                 max accuracy  0.514027 0.800232  94
5                max precision  0.668538 0.875000  21
6                   max recall  0.011443 1.000000 389
7              max specificity  0.717464 0.999216   0
8             max absolute_mcc  0.323830 0.466764 183
9   max min_per_class_accuracy  0.229876 0.746082 238
10 max mean_per_class_accuracy  0.173814 0.753367 273

Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`","['r', 'deep-learning', 'h2o', 'grid-search', 'hyperparameters']",Sujay DSa,https://stackoverflow.com/users/4051357/sujay-dsa,"1,192"
40858769,40858769,2016-11-29T05:59:11,2019-08-08 16:26:04Z,0,Will all functions in R and python work on the h2o package? Are they any restrictions like some functions work and other functions do not work?,"['python', 'r', 'package', 'h2o']",Ferdi,https://stackoverflow.com/users/6359698/ferdi,560
40849546,40849546,2016-11-28T16:53:43,2019-10-25 00:58:53Z,0,"I get a java.lang.AssertionError when I try to run the tutorial code with my dataset. Can you please let me know where I'm going wrong and how to correct it?


response <- ""Churn""
predictors <- setdiff(names(churn), response)

hyper_params <- list(
  hidden=list(c(32,32,32),c(64,64)),
  input_dropout_ratio=c(0,0.05),
  rate=c(0.01,0.02),
  rate_annealing=c(1e-8,1e-7,1e-6)
)
grid <- h2o.grid(
  algorithm=""deeplearning"",
  grid_id=""dl_grid"", 
  training_frame=churn_training,
  validation_frame=churn_validation, 
  x=predictors, 
  y=response,
  epochs=1,
  stopping_metric=""AUTO"",     ## Changed this to AUTO for classification
  stopping_tolerance=1e-2,       
  stopping_rounds=2,
  score_validation_samples=10000, 
  score_duty_cycle=0.025,         
  adaptive_rate=F,                
  momentum_start=0.5,             
  momentum_stable=0.9, 
  momentum_ramp=1e7, 
  l1=1e-5,
  l2=1e-5,
  activation=c(""Rectifier""),
  max_w2=10,                      
  hyper_params=hyper_params
)



EDIT: Here's a snapshot of the data. It is biased in the original as well


https://github.com/sujaydsa/sample_data/blob/master/ex.csv","['r', 'neural-network', 'deep-learning', 'h2o', 'grid-search']",Unknown,,N/A
40827940,40827940,2016-11-27T10:32:38,2016-11-28 08:31:52Z,0,"For a certain combination of parameters in the deeplearning function of h2o, I get different results each time I run it. 


args <- list(list(hidden = c(200,200,200), 
                  loss = ""CrossEntropy"",  
                  hidden_dropout_ratio = c(0.1, 0.1,0.1), 
                  activation = ""RectifierWithDropout"",  
                  epochs = EPOCHS))

run   <- function(extra_params) {
  model <- do.call(h2o.deeplearning, 
                   modifyList(list(x = columns, y = c(""Response""),  
                   validation_frame = validation, distribution = ""multinomial"",
                   l1 = 1e-5,balance_classes = TRUE, 
                   training_frame = training), extra_params))
}

model <- lapply(args, run) 



What would I need to do in order to get consistent results for the model each time I run this?","['r', 'machine-learning', 'neural-network', 'deep-learning', 'h2o']",Martin Schmelzer,https://stackoverflow.com/users/1777111/martin-schmelzer,23.8k
40779282,40779282,2016-11-24T06:20:40,2018-01-30 08:48:29Z,0,"In the below code, they use autoencoder as supervised clustering or classification because they have data labels.

http://amunategui.github.io/anomaly-detection-h2o/

But, can I use autoencoder to cluster data if I did not have its labels.?
Regards","['deep-learning', 'h2o', 'autoencoder']",forever,https://stackoverflow.com/users/7143885/forever,149
40776608,40776608,2016-11-24T01:09:03,2016-11-24 05:04:44Z,296,"I have developed a web application where user can choose machine learning framework/ number of iterations/ some other tuning parameter. How can I invoke Spark job from user interface by passing all the inputs and display response to user. Depending on the framework (dl4j/ spark mllib/ H2o) user can either upload input csv or the data can be read from Cassandra.




How can I call SPARK job from user interface?


How can I display the result back to user?




Please help.","['apache-spark', 'cassandra', 'h2o', 'dl4j']",lalithark,https://stackoverflow.com/users/4435563/lalithark,107
40651735,40651735,2016-11-17T09:57:17,2016-11-17 16:52:45Z,0,"Closed.
 This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet 
Stack Overflow guidelines
. It is not currently accepting answers.
                                
                            
























 We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.






Closed 
7 years ago
.















                        Improve this question
                    








I want to use h2o through R. The latest h2o on is 
3.10.0.10
, and the latest integrated h2o version for R is 3.10.0.8, which is not compatible. where can i download previous versions of h2o?","['r', 'h2o']",sebastian-c,https://stackoverflow.com/users/1465387/sebastian-c,15.4k
40605544,40605544,2016-11-15T08:53:50,2016-11-15 10:40:36Z,0,"I built a deep learning model using h2o in R.
I saved the model using the command


save(model,file=""my_model.RData"")



But now when I am predicting using the saved model ,it is giving me error message.I should have saved the model using h2o.savemodel.


h2o.savemodel(model)



Is there a way around it so that I can use my model now for prediction?","['r', 'deep-learning', 'h2o']",mathkid,https://stackoverflow.com/users/5700070/mathkid,367
40543314,40543314,2016-11-11T07:42:45,2016-11-14 08:21:47Z,423,"My code:


curl -X POST http://localhost:54321/3/ParseSetup --data 'source_frames=[""/root/documents/my_file.csv""]'



Error:




java.lang.IllegalArgumentException: Key not loaded: Key  at
  water.api.ParseSetupHandler.guessSetup(ParseSetupHandler.java:31)




Help: 


Could anyone help to resolve this? Am I missing any parameter?


Used H2O version: 
h2o-3.10.0.10","['java', 'rest', 'api', 'curl', 'h2o']",swiftBoy,https://stackoverflow.com/users/1371853/swiftboy,35.8k
40398676,40398676,2016-11-03T10:03:50,2016-11-13 13:35:37Z,0,"I'm trying to install h2oEnsemble package on R 3.2.3. I have H2O 3.10.0.8 package. When I write these commands:


library(devtools) 
install_github(""h2oai/h2o-3/h2o-r/ensemble/h2oEnsemble-package"")


I get the next error:


Downloading GitHub repo h2oai/h2o-3@master
from URL 
https://api.github.com/repos/h2oai/h2o-3/zipball/master

Installing h2oEnsemble
""C:/Program Files/R/R-3.2.3/bin/x64/R"" --no-site-file --no-environ --no-save  \
  --no-restore --quiet CMD INSTALL  \
  ""C:/Users/User/AppData/Local/Temp/Rtmpi0Q2AV/devtools9583548338a/h2oai-h2o-3-a0a4bbc/h2o-r/ensemble/h2oEnsemble-package""  \
  --library=""C:/Users/User/Documents/R/win-library/3.2"" --install-tests 
""C:\Program"" ­Ҝ пү«пҜвбп ү­гваҜ­­Ҝ© Ё«Ё ү­Ҝи­Ҝ©
Ө®¬ ­¤®©, ЁбÜ®«­пҜ¬®© Üа®Ҹа ¬¬®© Ё«Ё Ü ӨҜв­л¬ д ©«®¬.
Error: Command failed (1)


Please, help me to solve this problem. Thanks.","['r', 'git', 'devtools', 'h2o']",jangorecki,https://stackoverflow.com/users/2490497/jangorecki,16.7k
40365149,40365149,2016-11-01T17:15:55,2016-11-01 23:27:36Z,449,"I recently updated my h2o from 3.6.0.8 to 3.10.0.9 on hadoop and am using the python api. Previously (using 3.6) I used the following command to load in a model I had saved: 


model_to_load = h2o.load_model('hdfs://nameservice1/path/to/model/model_directory')



I now use the same command (in 3.10) and get the following error: 


java.io.FileNotFoundException: Path is not a file: /path/to/model/model_directory



I tried downgrading back to 3.6.0.8 and it works just fine, just as it did before. Within the directory I have a file for each tree (this is a random forest model), two model metric files, and two files one named 


__h2o_bin.mbin 



and one named 


model_directory.bin



I tried using each of those in the load_model path but no luck. 


Has anyone experienced this and do you know the right path I should use to load my model?","['python', 'hadoop', 'hdfs', 'h2o']",lilyrobin,https://stackoverflow.com/users/2022628/lilyrobin,73
40358476,40358476,2016-11-01T10:44:44,2016-11-02 08:29:31Z,0,"My laptop has 8GB RAM with 4 cores. 


My h2o version is as follows,`


Package: h2o
Type: Package
Version: 3.10.0.8
Branch: rel-turing
Date: Mon Oct 10 13:47:51 PDT 2016
License: Apache License (== 2.0)
Depends: R (>= 2.13.0), RCurl, jsonlite, statmod, tools, methods, utils`



I initialized it as follows,


h2o.init(nthreads = -1,max_mem_size = ""8g"")



But the output i get is as follows,


R is connected to the H2O cluster: 
    H2O cluster uptime:         13 hours 21 minutes 
    H2O cluster version:        3.10.0.8 
    H2O cluster version age:    21 days, 13 hours and 33 minutes  
    H2O cluster name:           H2O_started_from_R_hp_ubq027 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   1.33 GB 
    H2O cluster total cores:    4 
    H2O cluster allowed cores:  2 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    R Version:                  R version 3.3.1 (2016-06-21) 



Why is the allowed cores only 2 and allowed memory only 1.33gb while almost 8GB is available?","['r', 'h2o']",mathkid,https://stackoverflow.com/users/5700070/mathkid,367
40355123,40355123,2016-11-01T06:29:23,2016-11-02 09:39:03Z,249,"I watched the video demo

http://www.lectoro.com/index.php?action=search&ytq=H2O%20TensorFlow%20Deep%20Learning%20Demo


I am able to set up the env using the same spark and sparkling-water versions. The tensorflow runs on python3. Apparently, the example uses python2 code. I am getting SyntaxError: Missing parentheses in call to 'print'. It points to a file with an extreme long path starting with /private ends in context.py. 


Is there a way I can get this demo to work with my python3 environment?","['python-2.7', 'python-3.x', 'tensorflow', 'jupyter', 'h2o']",bhomass,https://stackoverflow.com/users/1058511/bhomass,"3,552"
40275946,40275946,2016-10-27T04:06:59,2019-07-09 22:10:54Z,0,"I have a data frame in R that I am passing to H2O using the 
as.h2o()
. 


dataset.h2o <- as.h2o(dataset,destination_frame = ""dataset.h2o"")



Doing an 
str()
 on the data frame, we can see that the week_of_date class is of datatype Date 




$ primary_account_id            : int  31 31 31 31 31 31 31 31 31 31 ...

  $ week_of_date                  : Date, format: ""2015-08-31"" ""2015-09-07"" ""2015-09-14"" ""2015-09-21"" ...




However, when viewed in H2O Flow, it seems to be converted to a datatype called time - which is of the format 


week_of_date    time    0   0   0   0   1440943200000.0 1447592400000.0 1444480409625.8884  2013362534.5706


When I bring back the data to R using as.data.frame


returned.dataset <- as.data.frame(dataset.h2o)



it is stored in a format that I am unable to understand and therefore parse back




$ primary_account_id: int  31 31 698 1060 1060 1060 1060 1060 1060 1133 ...

  $ week_of_date      :Class 'POSIXct'  num [1:194] 1442757600000 1446382800000 1446382800000 1442152800000 1442757600000 ...




Could you please point me in the direction of how I can achieve better interoperability with dates between R and H2O? 


Thanks!","['r', 'h2o']",kangaroo_cliff,https://stackoverflow.com/users/3651529/kangaroo-cliff,"6,222"
40227519,40227519,2016-10-24T21:03:24,2016-10-27 17:05:28Z,213,"I'm trying to use the 
GridSearch
 api within Scala for 
H2O
.  I have found 
this
 documentation which shows what to do in R and Python but the Java documentation stops before actually getting the best model.  Can someone tell me the last few lines?  IE given a 
Grid<MP>
 how can I either get the best hyperparameter combinations or the best model?


EDIT


Here is how my code looks so far (it's in Scala but obviously I'm calling into the Java code within H2O).


private[this] def hyperopt(hyperParams: Map[String, Array[Object]]): M = {
  val search = GridSearch.startGridSearch(
    null,
    params,
    hyperParams.asJava,
    new SimpleParametersBuilderFactory[P],
    new HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria)
  val grid = search.get().asInstanceOf[Grid[P]]
  val scoringInfo = grid.getScoringInfos.last // This is best scoringInfo cause it gets sorted along the way
  //val bestParams = scoringInfo.???
  //grid.getModel(bestParams).asInstanceOf[M]
}



The key problem is I don't know how to write the last 2 lines.


So after some digging I've come up with this solution which I don't like, but which does work


private[this] def hyperopt(hyperParams: (Map[String, Array[Object]], String)): M = {
  val search = GridSearch.startGridSearch(
    null,
    params,
    hyperParams._1.asJava,
    new SimpleParametersBuilderFactory[P],
    new HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria)
  val grid = search.get()
  val summary = grid.createSummaryTable(grid.getModelKeys, hyperParams._2, true)

  info(""Hyper parameter results"")
  info(summary.toString)

  // TODO Fix this hack.  I don't know a better way to do this now.
  val bestModelId = summary.get(0, 1)
  grid.getModels.find(m => m._key.toString == bestModelId).get.asInstanceOf[M]
}","['java', 'scala', 'machine-learning', 'h2o', 'hyperparameters']",Unknown,,N/A
40204743,40204743,2016-10-23T15:48:24,2016-10-24 08:13:05Z,834,"I am building a Random Forest model using a grid search with the H2O Python API. I split the data in train and validation and use 
k
-fold cross validation to select the best model in the grid search.


I am able to retrieve the model with the best 
MSE
 on the training set but I want to retrieve the model with the highest 
AUC
 on the validation set.


I could code everything in Python but I was wondering whether there is a H2O approach to solve this. Any suggestions on how I could do this?","['python', 'h2o']",Stereo,https://stackoverflow.com/users/4099925/stereo,"1,148"
40184473,40184473,2016-10-21T19:50:48,2018-01-26 16:36:11Z,0,"Is it possible to create a H2OFrame using the H2O's REST API and if so how?


My main objective is to utilize models stored inside H2O so as to make predictions on 
external
 H2OFrames. 


I need to be able to generate those H2OFrames externally from JSON (I suppose by calling an endpoint)  


I read the API documentation but couldn't find any clear explanation.


I believe that the closest endpoints are 


/3/CreateFrame
 which creates random data and 
/3/ParseSetup
 


but I couldn't find any reliable tutorial.","['rest', 'h2o']",Unknown,,N/A
40179875,40179875,2016-10-21T14:58:51,2016-10-21 22:35:14Z,958,"I am performing a GridSearch with H2O using the Python API using the following code,


from h2o.estimators.random_forest import H2ORandomForestEstimator
from h2o.grid import H2OGridSearch

hyper_parameters = {'ntrees':[10, 50, 100, 200], 'max_depth':[5, 10, 15, 20, 25], 'balance_classes':[True, False]}

search_criteria = {
    ""strategy"": ""RandomDiscrete"",
    ""max_runtime_secs"": 600,
    ""max_models"": 30,
    ""stopping_metric"": 'AUTO',
    ""stopping_tolerance"": 0.0001,
    'seed': 42
}

grid_search = H2OGridSearch(H2ORandomForestEstimator, hyper_parameters, search_criteria=search_criteria)
grid_search.train(x=events_names_x, 
                  y=""total_rsvps"", 
                  training_frame=train,
                  validation_frame=test)



Once run I want to print the models and predict in order of 
AUC
,


grid_search.sort_by('auc', False)



I get the following error,


---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-272-b250bf2b838e> in <module>()
----> 1 grid_search.sort_by('auc', False)

/Users/stereo/.pyenv/versions/3.5.2/lib/python3.5/site-packages/h2o/grid/grid_search.py in sort_by(self, metric, increasing)
    663 
    664         if metric[-1] != ')': metric += '()'
--> 665         c_values = [list(x) for x in zip(*sorted(eval('self.' + metric + '.items()'), key=lambda k_v: k_v[1]))]
    666         c_values.insert(1, [self.get_hyperparams(model_id, display=False) for model_id in c_values[0]])
    667         if not increasing:

/Users/stereo/.pyenv/versions/3.5.2/lib/python3.5/site-packages/h2o/grid/grid_search.py in <module>()

/Users/stereo/.pyenv/versions/3.5.2/lib/python3.5/site-packages/h2o/grid/grid_search.py in auc(self, train, valid, xval)
    606         :return: The AUC.
    607         """"""
--> 608         return {model.model_id: model.auc(train, valid, xval) for model in self.models}
    609 
    610     def aic(self, train=False, valid=False, xval=False):

/Users/stereo/.pyenv/versions/3.5.2/lib/python3.5/site-packages/h2o/grid/grid_search.py in <dictcomp>(.0)
    606         :return: The AUC.
    607         """"""
--> 608         return {model.model_id: model.auc(train, valid, xval) for model in self.models}
    609 
    610     def aic(self, train=False, valid=False, xval=False):

/Users/stereo/.pyenv/versions/3.5.2/lib/python3.5/site-packages/h2o/model/model_base.py in auc(self, train, valid, xval)
    669         tm = ModelBase._get_metrics(self, train, valid, xval)
    670         m = {}
--> 671         for k, v in viewitems(tm): m[k] = None if v is None else v.auc()
    672         return list(m.values())[0] if len(m) == 1 else m
    673 

/Users/stereo/.pyenv/versions/3.5.2/lib/python3.5/site-packages/h2o/model/metrics_base.py in auc(self)
    158         :return: Retrieve the AUC for this set of metrics.
    159         """"""
--> 160         return self._metric_json['AUC']
    161 
    162     def aic(self):

KeyError: 'AUC'



Any advise on:




can print the models in order of performance


forecast with the model with the highest 
AUC","['python', 'h2o']",Stereo,https://stackoverflow.com/users/4099925/stereo,"1,148"
40157468,40157468,2016-10-20T14:36:07,2017-02-21 20:26:28Z,347,"I've been reading the H2O documentation for a while, and I haven't found a clear example of how to load 
model
 trained and saved using the Python API. I was following the next example.


import h2o
from h2o.estimators.naive_bayes import H2ONaiveBayesEstimator

model = H2ONaiveBayesEstimator()
h2o_df = h2o.import_file(""http://s3.amazonaws.com/h2o-public-test-data/smalldata/airlines/allyears2k_headers.zip"")
model.train(y = ""IsDepDelayed"", x = [""Year"", ""Origin""], 
            training_frame = h2o_df, 
            family = ""binomial"", 
            lambda_search = True, 
            max_active_predictors = 10)
h2o.save_model(model, path=models)



But if you check the official 
documentation
 it states that you have to download the model as a 
POJO
 from the flow UI. 
Is it the only way?
 or, 
may I achieve the same result via python?
 Just for information, I show the doc's example below. I need some guidance.


import java.io.*;
import hex.genmodel.easy.RowData;
import hex.genmodel.easy.EasyPredictModelWrapper;
import hex.genmodel.easy.prediction.*;

public class main {
  private static String modelClassName = ""gbm_pojo_test"";

  public static void main(String[] args) throws Exception {
    hex.genmodel.GenModel rawModel;
    rawModel = (hex.genmodel.GenModel) Class.forName(modelClassName).newInstance();
    EasyPredictModelWrapper model = new EasyPredictModelWrapper(rawModel);
    //
    // By default, unknown categorical levels throw PredictUnknownCategoricalLevelException.
    // Optionally configure the wrapper to treat unknown categorical levels as N/A instead:
    //
    //     EasyPredictModelWrapper model = new EasyPredictModelWrapper(
    //                                         new EasyPredictModelWrapper.Config()
    //                                             .setModel(rawModel)
    //                                             .setConvertUnknownCategoricalLevelsToNa(true));

    RowData row = new RowData();
     row.put(""Year"", ""1987"");
     row.put(""Month"", ""10"");
     row.put(""DayofMonth"", ""14"");
     row.put(""DayOfWeek"", ""3"");
     row.put(""CRSDepTime"", ""730"");
     row.put(""UniqueCarrier"", ""PS"");
     row.put(""Origin"", ""SAN"");
     row.put(""Dest"", ""SFO"");

    BinomialModelPrediction p = model.predictBinomial(row);
    System.out.println(""Label (aka prediction) is flight departure delayed: "" + p.label);
    System.out.print(""Class probabilities: "");
    for (int i = 0; i < p.classProbabilities.length; i++) {
      if (i > 0) {
        System.out.print("","");
      }
      System.out.print(p.classProbabilities[i]);
    }
    System.out.println("""");
  }
}","['java', 'python', 'h2o']",tRuEsAtM,https://stackoverflow.com/users/2349082/truesatm,"3,699"
40144894,40144894,2016-10-20T03:23:19,2016-10-20 20:41:41Z,0,"I wish to fit a logistic regression with 
h2o.glm
 including some interactions between factors.  However, simple usage of 
h2o.interaction
 followed by 
h2o.glm
 ends up including too many dummy variables in the regression.  Here is a reproducible example.


# model.matrix function in R returns a matrix 
# with the intercept, 1 dummy for Age, 1 dummy for Sex, and 1 dummy for Age:Sex
colnames(model.matrix(Survived ~ Age + Sex + Age:Sex, data = Titanic))
[1] ""(Intercept)""        ""AgeAdult""           ""SexFemale""          ""AgeAdult:SexFemale""

# create an H2OFrame with the interaction of Age and Sex as a factor
library(h2o)
h2o.init()
Titanic.hex <- as.h2o(Titanic)
interact.hex <- h2o.cbind(Titanic.hex[,c(""Survived"",""Age"",""Sex"")]
                          ,h2o.interaction(Titanic.hex
                          ,factors = list(c(""Age"", ""Sex""))
                          ,pairwise = T
                          ,max_factors = 99
                          ,min_occurrence = 1))

# Age_Sex interaction column has 4 levels
h2o.levels(interact.hex$Age_Sex)
[1] ""Child_Male""   ""Child_Female"" ""Adult_Male""   ""Adult_Female""

# Because Age_Sex interaction column has 4 levels 
# we end up with 3 dummies to represent Age:Sex
interact.h2o.glm <- h2o.glm(2:ncol(interact.hex)
                            ,""Survived""
                            ,interact.hex
                            ,family = 'binomial'
                            ,lambda = 0)
h2o.varimp(interact.h2o.glm)$names
[1] ""Age_Sex.Child_Female"" ""Age_Sex.Adult_Male""   ""Age_Sex.Adult_Female"" ""Sex.Male""            
[5] ""Age.Child""            """"



What is a good way to do interactions between factors with h2o such that the 
h2o.glm
 behaves like 
model.matrix
?  In the example above, I would like to see only 1 dummy variable for the interaction between 
Age
 and 
Sex
 instead of 3 dummy variables.","['r', 'h2o']",jmuhlenkamp,https://stackoverflow.com/users/6850554/jmuhlenkamp,"2,150"
40121673,40121673,2016-10-19T03:35:24,2016-10-19 08:05:12Z,0,"I'm using H2O to generate predictions on a large data set with user ID as one of the columns. However, once I score the data set the predictions data set does not contain the ID... The only thing that keeps things working is the order of the scores matches the order of the input data set, which is pretty sloppy IMO.


Is there a way to instruct H2O to either retain the ID column in the predictions data set or alternatively to add it post-scoring but still in H2O?


I'm less excited about bringing the scores to python or R along with the data set with the IDs and using cbind and the likes but please chime in if this is the only option.","['python', 'h2o']",thasainta,https://stackoverflow.com/users/6622647/thasainta,103
40006311,40006311,2016-10-12T18:56:10,2016-10-14 18:24:21Z,0,"I need to perform parameter optimization on a gbm model on RH2o. I am relatively new to H2o and I think I need to convert ntrees and learn_rate(below) into a H2o vector before performing the below.
How do I perform this operation?
Thanks!


ntrees <- c(100,200,300,400)
learn_rate <- c(1,0.5,0.1)
for (i in ntrees){
  for j in learn_rate{
    n = ntrees[i]
    l= learn_rate[j]
    gbm_model <- h2o.gbm(features, label, training_frame = train, validation_frame = valid, ntrees=ntrees[[i]],max_depth = 5,learn_rate=learn_rate[j])
    print(c(ntrees[i],learn_rate[j],h2o.mse(h2o.performance(gbm_model, valid = TRUE))))

  }
}","['r', 'machine-learning', 'xgboost', 'h2o']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
39963181,39963181,2016-10-10T16:56:45,2019-04-26 19:09:30Z,0,"I get this error when I run the code below.


import h2o
from h2o.estimators.gbm import H2OGradientBoostingEstimator as GBM
from sklearn import datasets
import numpy as np
import pandas as pd

h2o.init(ip='192.168.0.4',port=54321)

# writing data to CSV so that h2o can read it
digits = datasets.load_digits()
predictors = digits.data[:-1]
targets = digits.target[:-1]
record_count = targets.shape[0]
targets = targets.reshape([record_count,1])
data = predictors
data = np.concatenate((data, targets), axis=1)
write_df = pd.DataFrame(data).to_csv(path_or_buf='data.csv',index=False)
model = GBM(ntrees=3,distribution='multinomial',max_depth=3)
everything = h2o.import_file(path='data.csv')
everything[64] = everything[64].asfactor()
model.start(training_frame=everything,x=list(range(64)),y=64,validation_frame=everything)

# model seems to be None for some reason
predictions = model.predict(everything)



The specific error is:


Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ryanzotti/anaconda/lib/python3.4/site-packages/h2o/model/model_base.py"", line 148, in predict
    j = H2OJob(h2o.api(""POST /4/Predictions/models/%s/frames/%s"" % (self.model_id, test_data.frame_id)),
  File ""/Users/ryanzotti/anaconda/lib/python3.4/site-packages/h2o/h2o.py"", line 83, in api
    return h2oconn.request(endpoint, data=data, json=json, filename=filename, save_to=save_to)
  File ""/Users/ryanzotti/anaconda/lib/python3.4/site-packages/h2o/backend/connection.py"", line 259, in request
    return self._process_response(resp, save_to)
  File ""/Users/ryanzotti/anaconda/lib/python3.4/site-packages/h2o/backend/connection.py"", line 586, in _process_response
    raise H2OResponseError(data)
h2o.exceptions.H2OResponseError: Server error water.exceptions.H2OKeyNotFoundArgumentException:
  Error: Object 'None' not found in function: predict for argument: model
  Request: POST /4/Predictions/models/None/frames/py_1_sid_a5e2



There are no other errors prior to this one.


H2O Version:
 3.11.0.3645


Python Version:
 3.4.4","['python', 'h2o']",user554481,https://stackoverflow.com/users/554481/user554481,"2,115"
39838518,39838518,2016-10-03T18:55:15,2016-10-24 19:19:15Z,497,"According to 
this
 blog by the Sparkling water guys, you are now able to use the Spark ML pipelines components to build a DL model in the latest versions. I tried adding the latest versions in my 
build.sbt


""org.apache.spark"" % ""spark-mllib_2.10"" % ""2.0.0"" % ""provided"",
""ai.h2o"" % ""sparkling-water-core_2.10"" % ""1.6.5"" % ""provided""



but no luck, trying to import 
org.apache.spark.ml.h2o.H2OPipeline
 doesn't work. The 
h2o
 package inside 
spark.ml
 doesn't seem to exist in the spark jars. Even though it seems to work in the above link as well as 
here
.I really want to reuse my spark-mllib feature transformers to create a DL model using h2o, as shown in the blog. 


Any help appreciated!


Thanks.","['scala', 'apache-spark', 'apache-spark-mllib', 'h2o', 'sparkling-water']",void,https://stackoverflow.com/users/2408270/void,"2,533"
39754003,39754003,2016-09-28T17:25:18,2016-09-28 21:15:53Z,152,"while following the 
dev setup guide
 for h2o steam I encounter this error:




as text 



➜  steam git:(master) pwd
/Users/m/workspace/go-workspace/src/github.com/h2oai/steam
➜  steam git:(master) make
go build
lib/yarn/yarn.go:22:2: cannot find package ""context"" in any of:
    /Users/m/workspace/go-workspace/src/github.com/h2oai/steam/vendor/context (vendor tree)
    /usr/local/go/src/context (from $GOROOT)
    /Users/m/workspace/go-workspace/src/context (from $GOPATH)
make: *** [build] Error 1



how can I run 
make
 successfully?","['go', 'hadoop-yarn', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
39709155,39709155,2016-09-26T17:43:11,2016-09-28 10:42:17Z,0,"I used h2o library for classification. I want to know the detail of weight from every node it made. Assume I named the model with 
model
, if I use 
summary(model)
, it will only show the average weight and the average bias of every layer, and I need to know the detail of each weight. Is it possible to print every detail weight? 
Any suggestions would be appreciated.
Sorry for the terrible English


train[1,] 
0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  1,  1,  0,  0,  0,  0,  0,  1,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1

train[2,] 
1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  1,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  1,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  2

model = h2o.deeplearning(x = 1:100,y = 101
                         training_frame = train,
                         activation = ""Tanh"",
                         balance_classes = TRUE, 
                         hidden = c(15,15),
                         momentum_stable = 0.99,
                         epochs = 50)

Scoring History: 
            timestamp   duration training_speed   epochs iterations     samples training_rmse training_logloss
1 2016-09-26 23:50:53  0.000 sec                 0.00000          0    0.000000                               
2 2016-09-26 23:50:53  0.494 sec  8783 rows/sec  5.00000          1  650.000000       0.81033          2.04045
3 2016-09-26 23:50:53  1.053 sec 10586 rows/sec 50.00000         10 6500.000000       0.23170          0.22766
  training_classification_error
1                              
2                       0.63077
3                       0.00000



here is some part of the summary of my model


 layer units    type dropout       l1       l2 mean_rate rate_rms momentum mean_weight weight_rms mean_bias bias_rms
1     1   100   Input  0.00 %                                                                                        
2     2    15    Tanh  0.00 % 0.000000 0.000000  0.005683 0.001610 0.000000    0.004570   0.148204 -0.019728 0.061853
3     3    15    Tanh  0.00 % 0.000000 0.000000  0.003509 0.000724 0.000000    0.003555   0.343449  0.007262 0.110244
4     4    26 Softmax         0.000000 0.000000  0.010830 0.006383 0.000000    0.005078   0.907516 -0.186089 0.166363","['r', 'model', 'h2o']",Unknown,,N/A
39597281,39597281,2016-09-20T14:50:40,2016-09-22 08:04:21Z,0,"I wanted to estimate h2o.glm model with pre-defined maximum number of active predictors (non-default max_active_predictors column). Here is the example:


set.seed(123)

par1 <- matrix(c(100, 200, 300, 400, 40, 30, 20, 10), 4, 2)
par2 <- c(1000, 2000, 3000, 4000)

coef <- c(0.5, -0.5, 1, -1, 1.5, -1.5, 2, -2)

mat <- as.data.frame(cbind(apply(par1, 1, function(x) rnorm(1000, mean = x[1], sd = x[2])),
                           sapply(par2, function(x) rpois(1000, lambda = x))))
mat$Y <- as.numeric(t(coef %*% t(mat)))

h2o.init(nthreads = -1)
mat_h2o <- as.h2o(mat, ""mat.h2o"")

glm_base <- h2o.glm(x = setdiff(colnames(mat), ""Y""), 
                    y = ""Y"",
                    training_frame = mat_h2o,
                    solver = ""IRLSM"",
                    family = ""gaussian"",
                    link = ""family_default"",
                    alpha = 1,
                    lambda_search = TRUE,
                    nlambdas = 10)

summary(glm_base)

glm_restr <- h2o.glm(x = setdiff(colnames(mat), ""Y""), 
                     y = ""Y"",
                     training_frame = mat_h2o,
                     solver = ""IRLSM"",
                     family = ""gaussian"",
                     link = ""family_default"",
                     alpha = 1,
                     lambda_search = TRUE,
                     nlambdas = 10,
                     max_active_predictors = 3)

summary(glm_restr)



Summary from glm_base looks exactly how I feel it should (eight non-zero predictors), but the latter is counter-intuitive (also eight non-zero predictors). How I can force the algorithm to restrict the complexity of final model to the predefined number of variables.","['r', 'h2o']",user2280549,https://stackoverflow.com/users/2280549/user2280549,"1,234"
39567131,39567131,2016-09-19T07:01:09,2016-09-19 08:37:56Z,0,"I have saved a trained model (deep net, but it is more general I think) in 
H2O
. Now I want to load it by another instance of H2O and use it for scoring, but the problem is, that the version of H2O used for training (
3.10.0.3
) was different than the one I started the production cluster with (
3.10.0.6
). The error message is quite self-explanatory


ERROR MESSAGE:

Found version 3.10.0.3, but running version 3.10.0.6



Is there a way to migrate the saved model between versions? Or am I stuck with using the same version of H2O for training and scoring?","['r', 'deployment', 'machine-learning', 'deep-learning', 'h2o']",JaKu,https://stackoverflow.com/users/2350272/jaku,"1,126"
39536372,39536372,2016-09-16T16:37:38,2016-09-17 17:18:00Z,0,"I naively thought it's straight forward to make multiple calls to h2o.gbm in parallel inside a foreach loop. But got a strange error. 


Error in { : 
         task 3 failed - ""java.lang.AssertionError: Can't unlock: Not locked!""



Codes below 


library(foreach)
library(doParallel)
library(doSNOW)

Xtr.hf = as.h2o(Xtr)
Xval.hf = as.h2o(Xval)

cl = makeCluster(6, type=""SOCK"")
registerDoSNOW(cl)
junk <- foreach(i=1:6, 
            .packages=c(""h2o""), 
            .errorhandling = ""stop"",
            .verbose=TRUE) %dopar% 
{
   h2o.init(ip=""localhost"", nthreads=2, max_mem_size = ""5G"") 
   for ( j in 1:3 ) { 
     bm2 <- h2o.gbm(
     training_frame = Xtr.hf,  
     validation_frame = Xval.hf, 
     x=2:ncol(Xtr.hf),
     y=1,          
     distribution=""gaussian"",
     ntrees = 100,
     max_depth = 3,
     learn_rate = 0.1,
     nfolds = 1)
  }
  h2o.shutdown(prompt=FALSE)    
  return(iname)
}
stopCluster(cl)","['r', 'foreach', 'h2o', 'gbm', 'doparallel']",horaceT,https://stackoverflow.com/users/2434201/horacet,651
39500529,39500529,2016-09-14T22:20:32,2022-04-14 04:29:27Z,183,"I have build a model in h2o flow.  Once completed I save the file to my local machine.  Next, I import some new data via Flow and python shell, I then load the model and run predict on the dataframe.  


I should get the exact same results as the dataframe and model are both the same.  Just different environments.  However, I get a different predictions in the different environments.  Anybody have a similar experience?","['python', 'machine-learning', 'h2o']",General Grievance,https://stackoverflow.com/users/4294399/general-grievance,"4,967"
39430734,39430734,2016-09-10T21:29:17,2017-09-20 05:09:34Z,161,"Say I have 20 frames on a 4-node H2O cluster: a1..a5, b1..b5, c1..c5, d1..d5. And I want to combine them into one big frame, from which I will build a model.


Is it better to combine sets of columns, then combine rows:


h2o.rbind(
  h2o.cbind(a1, b1, c1, d1),
  h2o.cbind(a2, b2, c2, d2),
  h2o.cbind(a3, b3, c3, d3),
  h2o.cbind(a4, b4, c4, d4),
  h2o.cbind(a5, b5, c5, d5)
  )



Or, to combine the rows first, then the columns:


h2o.cbind(
  h2o.rbind(a1, a2, a3, a4, a5),
  h2o.rbind(b1, b2, b3, b4, b5),
  h2o.rbind(c1, c2, c3, c4, c5),
  h2o.rbind(d1, d2, d3, d4, d5)
  )



For the sake of argument, 1/2/3/4/5 might each represent one month of data, which is why they got imported separately. And a/b/c/d are different sets of features, which again explains why they were imported separately. Let's say, a1..a5 have 1728 columns, b1..b5 have 113 columns, c1..c5 have 360 columns, and d1..d5 is a single column (the answer I'll be modelling). (Though I suspect, as H2O is a column database, that the relative number of columns in a/b/c/d does not matter?)


By ""better"" I mean quicker, but if there is a memory-usage difference in one or the other, that would also be good to know: I'm mainly interested in the Big Data case, where the combined frame is big enough that I wouldn't be able to fit it in the memory of just a a single node.","['h2o', 'bigdata']",Community,https://stackoverflow.com/users/-1/community,1
39372495,39372495,2016-09-07T14:22:21,2016-09-07 15:58:10Z,779,"I've got a cluster on AWS where I've installed H2O, Sparkling Water and H2O Flow for Machine Learning purposes on lots of data.


Now, these files come in a JSON format from a streaming job. Let's say they are placed in S3 in a folder called 
streamed-data
. 


From Spark, using the SparkContext, I could easily read them in one go to create an RDD as (this is Python, but is not important):


sc = SparkContext()
sc.read.json('path/streamed-data')



This reads them all, creates me the RDD and is very handy.


Now, I'd like to leverage the capabilities of H2O, hence I've installed it on the cluster, along with the other mentioned software. 


Looking from H2O flow, my problem is the lack of a JSON parser, so I'm wondering if I could import them into H2O in the first place, or if there's anything I could do to go round the problem.","['json', 'apache-spark', 'h2o']",martina.physics,https://stackoverflow.com/users/2352319/martina-physics,"9,764"
39215243,39215243,2016-08-29T21:07:51,2016-08-31 20:10:41Z,0,"I know that there is possibility to export/import h2o model, that was previously trained.


My question is - is there a way to transform h2o model to a non-h2o one (that just works in plain R)?


I mean that I don't want to launch the h2o environment (JVM) since I know that predicting on trained model is simply multiplying matrices, applying activation function etc.


Of course it would be possible to extract weights manually etc., but I want to know if there is any better way to do it.


I do not see any previous posts on SA about this problem.","['r', 'h2o']",Andrzej Pisarek,https://stackoverflow.com/users/6183527/andrzej-pisarek,271
39212635,39212635,2016-08-29T18:18:29,2016-08-30 07:30:46Z,0,"I want to tune a neural network with dropout using h2o in R. Here I provide a reproducible example for the iris dataset. I'm avoiding to tune 
eta
 and 
epsiplon
 (i.e. ADADELTA hyper-parameters) with the only purpose of making computations faster.


require(h2o)
h2o.init()
data(iris)
iris = iris[sample(1:nrow(iris)), ]
irisTrain = as.h2o(iris[1:90, ])
irisValid = as.h2o(iris[91:120, ])
irisTest = as.h2o(iris[121:150, ])
hyper_params <- list(
    input_dropout_ratio = list(0, 0.15, 0.3),
    hidden_dropout_ratios = list(0, 0.15, 0.3, c(0,0), c(0.15,0.15),c(0.3,0.3)),
    hidden = list(64, c(32,32)))
grid = h2o.grid(""deeplearning"", x=colnames(iris)[1:4], y=colnames(iris)[5],
                training_frame = irisTrain, validation_frame = irisValid,
                hyper_params = hyper_params, adaptive_rate = TRUE,
                variable_importances = TRUE, epochs = 50, stopping_rounds=5,
                stopping_tolerance=0.01, activation=c(""RectifierWithDropout""),
                seed=1, reproducible=TRUE)



The output is: 


Details: ERRR on field: _hidden_dropout_ratios: Must have 1 hidden layer dropout ratios.



The problem is in 
hidden_dropout_ratios
. Note that I'm including 0 for input_dropout_ratio and hidden_dropout_ratios since I also want to test the activation function without dropout. I'm aware that I could use 
activation=""Rectifier
 but I think that my configuration should lead to the same result. How do I tune 
hidden_dropout_ratios
 when tuning architectures with different numbers of layers?


Attempt 1: Unsuccessful and I'm not tuning 
hidden
.


hyper_params <- list(
    input_dropout_ratio = c(0, 0.15, 0.3),
    hidden_dropout_ratios = list(c(0.3,0.3), c(0.5,0.5)),
    hidden = c(32,32))
ERRR on field: _hidden_dropout_ratios: Must have 1 hidden layer dropout ratios.



Attempt 2: Successful but I'm not tuning 
hidden
.


hyper_params <- list(
    input_dropout_ratio = c(0, 0.15, 0.3),
    hidden_dropout_ratios = c(0.3,0.3),
    hidden = c(32,32))","['r', 'neural-network', 'deep-learning', 'h2o']",Unknown,,N/A
39207125,39207125,2016-08-29T13:08:15,2016-09-04 17:26:14Z,0,"I have a very noisy dataset with 2000 observations and 42 features (financial data) and I'm performing binary classification. Here I'm tuning the network using 
h2o.grid
 and providing a validation set. I've set 
epochs=1000
 and I'm imposing to stop the training when the misclassification error does not improve by >=1% for 5 scoring events (
stopping_rounds=5, stopping_tolerance=0.01
). I'm interested to know what is the value for 
epochs
 that minimises the validation error.


hyper_params = list(rho = c(0.9,0.95,0.99),
                 epsilon = 10^(c(-10, -8, -6, -4)),
                 hidden=list(c(64, 64)),
                 activation=c(""Tanh"", ""Rectifier"", ""RectifierWithDropout""))
grid = h2o.grid(""deeplearning"", x = predictors, y = response,
                training_frame = tempTrain, validation_frame = tempValid,
                grid_id=""h2oGrid10"", hyper_params = hyper_params,
                adaptive_rate = TRUE, stopping_metric=""misclassification"",
                variable_importances = TRUE, epochs = 1000,
                stopping_rounds=5, stopping_tolerance=0.01, max_w2 = 20)



According to 
this
 question, the solution should be the following:


gridErr = h2o.getGrid(""h2oGrid10"", sort_by=""err"", decreasing=FALSE)
best_model = h2o.getModel(gridErr@model_ids[[1]])
solution = rev(best_model@model$scoring_history$epochs)[1]



Where 
solution=1000
. Anyway, checking the 
scoring_history
 we observe the following output that is quite ambiguous. 


cbind(best_model@model$scoring_history$epochs,
+       best_model@model$scoring_history$validation_classification_error)
      [,1]      [,2]
 [1,]    0       NaN
 [2,]   10 0.4971347
 [3,]  160 0.4813754
 [4,]  320 0.4770774
 [5,]  490 0.4799427
 [6,]  660 0.4727794
 [7,]  840 0.4713467
 [8,] 1000 0.4727794
 [9,] 1000 0.4713467



In fact, it seems that the global minimum of the validation error is in correspondence of 840 epochs AND 1000 epochs. I've tried with different settings and I still get that the optimal number of epochs corresponds to the initially set number of epochs. Furthermore, I'm quite surprise to observe a so large number of optimal epochs given the conservative values for 
stopping_rounds=5
 and 
stopping_tolerance=0.01
 so I'm wondering whether I'm missing something important. How do I retrieve the optimal number of epochs, possibly in a finer scale (i.e. 1,2,... rather than 10,160,...)?


EDIT: The answer is in slide 8 
here
. What happens is that the best model is overwritten when performing the last iteration. Anyway, I've played for a while with the parameter 
train_samples_per_iteration
 but I'm not still able to observe the evolution of the validation error with the number of epochs in a finer scale. Any idea?","['r', 'neural-network', 'deep-learning', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
39204584,39204584,2016-08-29T10:58:04,2016-09-04 10:16:23Z,0,"I am using the H2O deeplearning Feed Forward Deep Neural network for doing a binary classification. My classes are  highly imbalanced and I want to use the parameters like    




balance_classes, class_sampling_factors 




Can any body give me a reproducible example about how to specifically intialize these parameters to handle class imbalance problems.","['r', 'machine-learning', 'deep-learning', 'h2o']",Boris Stitnicky,https://stackoverflow.com/users/1153747/boris-stitnicky,12.6k
39175806,39175806,2016-08-26T23:13:15,2016-08-27 16:53:17Z,0,"I have problems with data loading to H2O in R on Windows. When I run basic commands such as h2o.clusterInfo or as.h2o(localH2O, dat, key = 'dat'), I got an error message - 
Error in .... : unused argument (...).
 Like on screen. I use RTVS na Microsoft R Open 3.2.5","['r', 'h2o', 'rtvs']",jangorecki,https://stackoverflow.com/users/2490497/jangorecki,16.7k
39156265,39156265,2016-08-25T23:33:35,2016-08-26 23:48:27Z,0,"I am working on a multinomial classification model. The model is to predict transition probabilities. Among the variables used, one of them is the current state (one of the classes). As an example say a loan is presently current. It can transition to current, 1 month delinquent, defaulted, or paid off. But it should not transition to 2 months delinquent. In the training data, a current to 2 month delinquent transition does not occur. After training the model, I looked at the model predictions and there were still non-trivial probabilities to states which are known to be zero. Is it possible to enforce zero probabilities when using R’s h2o deeplearning function?","['r', 'h2o']",robert,https://stackoverflow.com/users/6759147/robert,13
39128865,39128865,2016-08-24T16:43:55,2018-06-12 10:27:21Z,0,"I have a model created in H2O using Python. I want to generate a POJO of that model, and save it. 


Say my model is called model_rf. 


I have tried: 


h2o.save_model(model_rf, path='./pojo_test', force=False)



This create a directory called ""pojo_test"", which contains a whole bunch of binary files. I want a java file though, something like model_rf.java, that is the POJO itself. 


I tried: 


h2o.download_pojo(model_rf, path='./pojo_test_2', get_jar = True)



Which gave the error message: 


IOError: [Errno 2] No such file or directory: u'./pojo_test_2/model_rf.java'



What am I missing? Probably a stupid question but I cannot for the life of me figure this out.","['python', 'pojo', 'h2o']",lilyrobin,https://stackoverflow.com/users/2022628/lilyrobin,73
39119275,39119275,2016-08-24T09:21:27,2016-09-15 08:50:57Z,129,"I looked for a generalized linear model implementation with regularization. I found that glmnet does not allow custom link function. However, h2o takes link function type as a parameter. Is it possible to define and use a custom link function under a family (optimization problem is same) in h2o?","['glm', 'h2o', 'regularized']",Naveen Mathew,https://stackoverflow.com/users/4981023/naveen-mathew,372
39071874,39071874,2016-08-22T05:24:12,2017-01-03 08:57:04Z,0,"I am learning h2o package now,




I installed h2o package from CRAN and couln't run this code  



## To import small iris data file from H\ :sub:`2`\ O's package
irisPath = system.file(""extdata"", ""iris.csv"", package=""h2o"")
iris.hex = h2o.importFile(localH2O, path = irisPath, key = ""iris.hex"")





I am getting the below error,




Error in h2o.importFile(localH2O, path = irisPath, key = ""iris.hex"") :
  unused argument (key = ""iris.hex"")






My second question is, Do we have good resources to learn h2o in R apart from this:




http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/rtutorial.html




My third question is I want to know how the h2o works in simple words.?","['r', 'h2o']",thelatemail,https://stackoverflow.com/users/496803/thelatemail,93.7k
39046032,39046032,2016-08-19T18:57:43,2016-08-19 19:59:52Z,0,"I'm experimenting with deeplearning binary classifiers using the 
h2o
 package. When I build a model and then use 
h2o.predict
 on some new (held-out) dataset, I notice that for some rows, the 
Predict
 output does not match the value with the highest probability.


Here's a reproducible example, adapted from 
h2o's deeplearning tutorial
:


library(h2o)

h2o.init(nthreads=-1, max_mem_size=""2G"")
h2o.removeAll()


df <- h2o.importFile(path = ""https://raw.githubusercontent.com/h2oai/h2o-tutorials/master/tutorials/data/covtype.full.csv"")

splits <- h2o.splitFrame(df, c(0.6,0.2), seed=1234)
train  <- h2o.assign(splits[[1]], ""train.hex"") # 60%
valid  <- h2o.assign(splits[[2]], ""valid.hex"") # 20%
test   <- h2o.assign(splits[[3]], ""test.hex"")  # 20%

response <- ""Cover_Type""
predictors <- setdiff(names(df), response)

train$bin_response <- ifelse(train[,response]==""class_1"", 0, 1)
train$bin_response <- as.factor(train$bin_response) ##make categorical

# apply same transforms to test
test$bin_response <- ifelse(test[,response]==""class_1"", 0, 1)
test$bin_response <- as.factor(test$bin_response)

dlmodel <- h2o.deeplearning(
  x=predictors,
  y=""bin_response"", 
  training_frame=train,
  hidden=c(10,10),
  epochs=0.1
  #balance_classes=T    ## enable this for high class imbalance
)

pred <- h2o.predict(dlmodel, test)



Now let's manipulate that to bring it into R and add some new columns, using 
dplyr
 for simplicity:


pred_df <- bind_cols(
  select(as.data.frame(test), actual = bin_response),
  as.data.frame(pred)
) %>%
  tbl_df %>%
  mutate(
    derived_predict = factor(as.integer(p1 > p0)),
    match = predict == derived_predict
  )



Now I would think the prediction should always match the column with the highest probability, but that's not always the case:


> pred_df %>% summarize(sum(match) / n())
# A tibble: 1 x 1
  sum(match)/n()
           <dbl>
1      0.9691755



Why isn't that value exactly 1? In my most recent run of the above code, the 
p0
 and 
p1
 values are fairly close


> pred_df %>% filter(!match)
# A tibble: 3,575 x 6
   actual predict        p0        p1 derived_predict match
   <fctr>  <fctr>     <dbl>     <dbl>          <fctr> <lgl>
1       1       1 0.5226679 0.4773321               0 FALSE
2       0       1 0.5165302 0.4834698               0 FALSE
3       0       1 0.5225683 0.4774317               0 FALSE
4       0       1 0.5120126 0.4879874               0 FALSE
5       1       1 0.5342851 0.4657149               0 FALSE
6       0       1 0.5335089 0.4664911               0 FALSE
7       0       1 0.5182881 0.4817119               0 FALSE
8       0       1 0.5094492 0.4905508               0 FALSE
9       0       1 0.5309947 0.4690053               0 FALSE
10      0       1 0.5234880 0.4765120               0 FALSE
# ... with 3,565 more rows



but that still doesn't explain why 
h2o.predict
 chooses the less probable value.


Am I doing something wrong here? Is this a bug in h2o? Does h2o intentionally use more information in picking a prediction than it presents to me here?


Interestingly, using my 
derived_predict
 yields slightly higher accuracy, by a hair:


> pred_df %>%
+   summarize(
+     original = sum(actual == predict)         / n(),
+     derived  = sum(actual == derived_predict) / n()
+   )
# A tibble: 1 x 2
   original   derived
      <dbl>     <dbl>
1 0.7794946 0.7827452","['r', 'h2o']",Unknown,,N/A
38977870,38977870,2016-08-16T14:40:49,2016-08-16 16:48:33Z,171,"I have a problem when starting h2o's steam application: When following the accompanying manual, I create the 
steam
 db user:


createuser -P steam



and then 


./create-database.sh



from the appropriate directory. And it seems fine. But when I try finally running steam with 


./steam serve master --compilation-service-address=""localhost:8080""



it fails with 


pq: password authentication failed for user ""steam""



I've tried modifying the 
pg_hba.conf
 with adding lines:


host steam steam 127.0.0.1/32 trust
local steam steam trust



but it didn't work. I've tried both 
YARN
 and the standalone 
steam
 version. Any ideas or workarounds will be appreciated.","['postgresql', 'h2o']",JaKu,https://stackoverflow.com/users/2350272/jaku,"1,126"
38972111,38972111,2016-08-16T10:10:00,2021-02-16 22:01:38Z,0,"I have a 5 node BigInsights hadoop cluster in Bluemix. I am getting error, when I am trying to install H2O ai R in BigInsights cluster.


install.packages(""h2o"", type=""source"", repos=(c(""
http://h2o-release.s3.amazonaws.com/h2o/rel-turing/3/R
"")))


ERROR: dependencies ‘statmod’, ‘RCurl’, ‘jsonlite’ are not available for package ‘h2o’
* removing ‘/home/opus/R/x86_64-redhat-linux-gnu-library/3.3/h2o’

The downloaded source packages are in
        ‘/tmp/RtmpJmcuyB/downloaded_packages’
Warning message:
In install.packages(""h2o"", type = ""source"", repos = (c(""http://h2o-release.s3.amazonaws.com/h2o/rel-turing/3/R""))) :
  installation of package ‘h2o’ had non-zero exit status



When I try to install 'RCurl' package of R in BigInsights cluster, getting the error message as:


trying URL 'https://cran.fhcrc.org/src/contrib/RCurl_1.95-4.8.tar.gz'
Content type 'application/x-gzip' length 916934 bytes (895 KB)
==================================================
downloaded 895 KB

* installing *source* package ‘bitops’ ...
** package ‘bitops’ successfully unpacked and MD5 sums checked
** libs
gcc -m64 -std=gnu99 -I/usr/include/R -DNDEBUG  -I/usr/local/include    -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpic -fPIC   -c bit-ops.c -o bit-ops.o
gcc -m64 -std=gnu99 -I/usr/include/R -DNDEBUG  -I/usr/local/include    -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpic -fPIC   -c cksum.c -o cksum.o
gcc -m64 -std=gnu99 -shared -L/usr/lib64/R/lib -o bitops.so bit-ops.o cksum.o -L/usr/lib64/R/lib -lR
installing to /home/opus/R/x86_64-redhat-linux-gnu-library/3.3/bitops/libs
** R
** preparing package for lazy loading
** help
*** installing help indices
  converting help for package ‘bitops’
    finding HTML links ... done
    bitAnd                                  html
    bitFlip                                 html
    bitShiftL                               html
    cksum                                   html
** building package indices
** testing if installed package can be loaded
* DONE (bitops)
* installing *source* package ‘RCurl’ ...
** package ‘RCurl’ successfully unpacked and MD5 sums checked
checking for curl-config... no
Cannot find curl-config
ERROR: configuration failed for package ‘RCurl’
* removing ‘/home/opus/R/x86_64-redhat-linux-gnu-library/3.3/RCurl’

The downloaded source packages are in
        ‘/tmp/RtmpJmcuyB/downloaded_packages’
Warning message:
In install.packages(""RCurl"") :
  installation of package ‘RCurl’ had non-zero exit status



Please help me to resolve the issue and help would be much appreciated.","['r', 'linux', 'ibm-cloud', 'h2o', 'biginsights']",ralphearle,https://stackoverflow.com/users/3692258/ralphearle,"1,694"
38968311,38968311,2016-08-16T06:51:04,2016-08-16 12:20:45Z,0,"I have a 10025x1417 TFIDF dfm matrix created with 
quanteda
. (The actual class is dfmSparse which is a subclass of 
dfm-matrix
).
When I convert to h2o with as.data.frame and then as.h2o, I incorrectly get 1002
6
x1417, with an unwanted extra first row of NaNs.
For performance reasons I don't want to create a temporary df with the full dense matrix.


The code is as follows (I was unable to reproduce on small toy data):


library(quanteda)
mat <- quanteda::weight(theDfm, type=""tfidf"")

# Convert to df then h2o, correctly gives 10025x1417 matrix
mat_df  <- as.data.frame(mat) # this will dispatch quanteda::as.data.frame for dfmSparse
mat_h2o <- as.h2o(mat_df)

# Convert in one go, get 10026x1417, get unwanted extra first row of NaNs
bad_h2o <- as.h2o(as.data.frame(mat))
dim(bad_h2o )
[1] 10026  1417

# Which as.data.frame method this uses
> showMethods(quanteda::as.data.frame)
Function: as.data.frame (package base)
x=""ANY""
x=""dfm""
x=""dfmSparse""
    (inherited from: x=""dfm"")
x=""matrix""
    (inherited from: x=""ANY"")

#########################################
# Ken Benoit requested sessionInfo()

R version 3.2.3 (2015-12-10)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows >= 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252    LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                           LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] h2o_3.8.3.3         statmod_1.4.22      quanteda_0.9.8      RevoUtilsMath_3.2.3

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.2      lattice_0.20-33  SnowballC_0.5.1  bitops_1.0-6     chron_2.3-47     grid_3.2.3       R6_2.1.1        
 [8] jsonlite_0.9.19  magrittr_1.5     httr_1.0.0       stringi_1.0-1    data.table_1.9.6 ca_0.58          Matrix_1.2-3    
[15] tools_3.2.3      stringr_1.0.0    RCurl_1.95-4.7   parallel_3.2.3","['r', 'dataframe', 'sparse-matrix', 'h2o', 'quanteda']",Unknown,,N/A
38950812,38950812,2016-08-15T07:18:35,2018-11-02 09:08:02Z,0,"I use the following code to install h2o-3 in R


# The following two commands remove any previously installed H2O packages for R.
    if (""package:h2o"" %in% search()) { detach(""package:h2o"", unload=TRUE) }
    if (""h2o"" %in% rownames(installed.packages())) { remove.packages(""h2o"") }

    # Next, we download packages that H2O depends on.
    pkgs <- c(""methods"",""statmod"",""stats"",""graphics"",""RCurl"",""jsonlite"",""tools"",""utils"")
    for (pkg in pkgs) {
      if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
    }

    # Now we download, install and initialize the H2O package for R.
    install.packages(""h2o"", type=""source"", repos=(c(""http://h2o-release.s3.amazonaws.com/h2o/rel-turing/3/R"")))
    library(h2o)
    localH2O = h2o.init(nthreads=-1)

    # Finally, let's run a demo to see H2O at work.
    demo(h2o.kmeans)



It shows the following error.


Warning in install.packages :
  running command '""C:/PROGRA~1/R/R-33~1.1/bin/x64/R"" CMD INSTALL -l ""C:\Program Files\R\R-3.3.1\library"" C:\Users\pintoo\AppData\Local\Temp\RtmpUxsC47/downloaded_packages/h2o_3.10.0.3.tar.gz' had status 65535
Warning in install.packages :
  installation of package ‘h2o’ had non-zero exit status


Then, as the above code, doesn't install package, and it shows it has been downloaded so i tried installing using the downloaded package using the below code


install.packages(""C:/Users/pintoo/AppData/Local/Temp/RtmpUL3Da2/downloaded_packages/h2o_3.10.0.3.tar.gz"",
                       repos = NULL, type = ""source"", dependencies = T)



It produced the below error


Warning in install.packages :
  running command '""C:/PROGRA~1/R/R-33~1.1/bin/x64/R"" CMD INSTALL -l ""C:\Program Files\R\R-3.3.1\library"" ""C:/Users/pintoo/AppData/Local/Temp/RtmpUL3Da2/downloaded_packages/h2o_3.10.0.3.tar.gz""' had status 65535
Warning in install.packages :
  installation of package ‘C:/Users/pintoo/AppData/Local/Temp/RtmpUL3Da2/downloaded_packages/h2o_3.10.0.3.tar.gz’ had non-zero exit status


MY version :


platform       x86_64-w64-mingw32

arch           x86_64

os             mingw32

system         x86_64, mingw32

status

major          3

minor          3.1

year           2016

month          06

day            21

svn rev        70800

language       R

version.string R version 3.3.1 (2016-06-21)
nickname       Bug in Your Hair 




Can any one help me out.


What is this non-zero exit status. 
  status 65535 meaning?
  Non- zero exit status of package means?","['r', 'h2o']",Unknown,,N/A
38894044,38894044,2016-08-11T10:39:31,2018-05-31 11:07:23Z,0,"There are multiple packages for 
R
 which help to print ""pretty"" tables (LaTeX/HTML/TEXT) from statistical models output AND to easily compare the results of alternative model specifications.


Some of these packages are 
apsrtable
, 
xtable
, 
memisc
, 
texreg
, 
outreg
, and 
stargazer
 (for examples see here: 
https://www.r-statistics.com/2013/01/stargazer-package-for-beautiful-latex-tables-from-r-statistical-models-output/
).


Is there any comparable 
R
 package that does support the models of the 
h2o
 package?


Here is an example of two simple GLM models with 
h2o
 which I like to print beside each other as ""beautiful"" tables.   


# Load package and setup h2o
library(h2o)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '4g')

# Load data
prostatePath <- system.file(""extdata"", ""prostate.csv"", package = ""h2o"")
prostate.hex <- h2o.importFile(path = prostatePath, destination_frame = ""prostate.hex"")

# Run GLMs
model.output.1 <- h2o.glm(y = ""CAPSULE"", x = c(""RACE"",""PSA"",""DCAPS""),
  training_frame = prostate.hex,family = ""binomial"", nfolds = 0, 
  alpha = 0.5, lambda_search = FALSE)
model.output.2 <- h2o.glm(y = ""CAPSULE"", x = c(""AGE"",""RACE"",""PSA"",""DCAPS""), 
  training_frame = prostate.hex, family = ""binomial"", nfolds = 0, 
  alpha = 0.5, lambda_search = FALSE)



This is how it would look like with an regular GLM object using 
screenreg()
 from the 
texreg
 package:


library(data.table)
library(texreg)
d <- fread(prostatePath)
model.output.1.glm <- glm(CAPSULE ~ RACE + PSA + DCAPS, data=d)
model.output.2.glm <- glm(CAPSULE ~ AGE + RACE + PSA + DCAPS, data=d)
screenreg(list(model.output.1.glm, model.output.2.glm))","['r', 'h2o', 'stargazer', 'texreg']",Unknown,,N/A
38870109,38870109,2016-08-10T10:00:02,2016-08-11 17:26:22Z,0,"I am working on a moderate data set (train_data). There are more 124 variables and 50,00,000 observations. For categorical variables, I have used feature hashing on it through hashed.model.matrix function in R.  


## feature hashing
b <- 2 ^ 22
f <- ~ .-1
X_train <- hashed.model.matrix(f, train_data, hash.size=b)



So, as a result , I have got a large dgCmatrix (a sparse matrix) as output (X_train). How can I use, H2o wrapper  on  this matrix and use different algorithms available in H2o ? Does H2o wrapper take sparse matrix (dgCmatrix). Any link / example of such usage will be helpful. Thanks in anticipation.


Looking forward to import X_train in H2o environment to do dollowing type of steps


# initialize connection to H2O server
  h2o.init(nthreads = -1)
 train.hex <- h2o.uploadFile('./X_train', destination_frame='train')

# list of features for training
feature.names <- names(train.hex)

# train random forest model, use ntrees = 500 
drf <- h2o.randomForest(x=feature.names, y='outcome', training_frame,train.hex, ntrees =500)","['r', 'h2o']",Harry,https://stackoverflow.com/users/6601038/harry,198
38853728,38853728,2016-08-09T14:35:24,2016-08-21 06:40:26Z,333,"As mentioned, how to calculate that with a h2o model, what's the most optimum way to do this with a large volume of data","['machine-learning', 'h2o']",HackToHell,https://stackoverflow.com/users/787563/hacktohell,"2,393"
38804546,38804546,2016-08-06T13:00:28,2016-08-08 22:16:23Z,0,"The following code:


library(randomForest)
z.auto <- randomForest(Mileage ~ Weight, 
                       data=car.test.frame,
                       ntree=1,
                       nodesize = 15)
tree <- getTree(z.auto,k=1,labelVar = T)
tree



Gives this as text output:


   left daughter right daughter split var split point status prediction
1              2              3    Weight      2567.5     -3   24.45000
2              0              0      <NA>         0.0     -1   30.66667
3              4              5    Weight      3087.5     -3   22.37778
4              6              7    Weight      2747.5     -3   24.00000
5              8              9    Weight      3637.5     -3   19.94444
6              0              0      <NA>         0.0     -1   25.20000
7             10             11    Weight      2770.0     -3   23.29412
8              0              0      <NA>         0.0     -1   21.18182
9              0              0      <NA>         0.0     -1   18.00000
10             0              0      <NA>         0.0     -1   22.50000
11             0              0      <NA>         0.0     -1   23.72727



From this data I can see the logic of an individual tree.


How do I get the much longer table, based on this, that describes all the trees in a random forest, from h2o?


I like 'h2o' because it cleanly uses all the cores, and goes at a pretty good clip on my system.  It is a nice tool.  It is, however, a library separate from 'r' so I am unsure how to access various parts of my data.


How do I get something like the above printed output, in the form of a csv file, from an h2o random forest?","['r', 'random-forest', 'h2o']",EngrStudent,https://stackoverflow.com/users/2259468/engrstudent,"1,972"
38710377,38710377,2016-08-02T01:18:36,2018-09-18 19:32:05Z,0,"I try to get the accurracy of my multiclass classifier using logistic regression.Is there any way to get the accuracy with a built-in function or do I have to write the function myself?


below my code so far:


multinomial_fit = H2OGeneralizedLinearEstimator(family=""multinomial"",max_iterations=100)

multinomial_fit.train(x=train_h2o_cro.columns[1:],y=train_h2o_cro.columns[0],training_frame=train_h2o)

prediction_glm_h2o = multinomial_fit.predict(test_h2o)

multinomial_fit.model_performance(test_h2o)



With the last line of code, I only get the mse and nothing else.


Thanks in advance.","['python', 'machine-learning', 'classification', 'h2o']",sedioben,https://stackoverflow.com/users/6524396/sedioben,945
38702463,38702463,2016-08-01T15:17:49,2017-03-16 00:50:59Z,795,"I am unable to start H2o in MRO3.3
the h2o.init() gave following output


08-02 00:40:36.543 127.0.0.1:54321       19672  main      INFO: ----- H2O started  -----
08-02 00:40:36.610 127.0.0.1:54321       19672  main      INFO: Build git branch: rel-turan
08-02 00:40:36.610 127.0.0.1:54321       19672  main      INFO: Build git hash: e2959c131831f8d5dad8c92eebdf0ad4a4e78d09
08-02 00:40:36.610 127.0.0.1:54321       19672  main      INFO: Build git describe: jenkins-rel-turan-3
08-02 00:40:36.610 127.0.0.1:54321       19672  main      INFO: Build project version: 3.8.1.3
08-02 00:40:36.610 127.0.0.1:54321       19672  main      INFO: Built by: 'jenkins'
08-02 00:40:36.611 127.0.0.1:54321       19672  main      INFO: Built on: '2016-03-06 14:54:19'
08-02 00:40:36.611 127.0.0.1:54321       19672  main      INFO: Java availableProcessors: 4
08-02 00:40:36.611 127.0.0.1:54321       19672  main      INFO: Java heap totalMemory: 150.0 MB
08-02 00:40:36.611 127.0.0.1:54321       19672  main      INFO: Java heap maxMemory: 1.72 GB
08-02 00:40:36.611 127.0.0.1:54321       19672  main      INFO: Java version: Java 1.8.0_91 (from Oracle Corporation)
08-02 00:40:36.611 127.0.0.1:54321       19672  main      INFO: JVM launch parameters: [-ea]
08-02 00:40:36.611 127.0.0.1:54321       19672  main      INFO: OS version: Linux 4.4.0-31-generic (amd64)
08-02 00:40:36.611 127.0.0.1:54321       19672  main      INFO: Machine physical memory: 7.73 GB
08-02 00:40:36.612 127.0.0.1:54321       19672  main      INFO: X-h2o-cluster-id: 1470078635582
08-02 00:40:36.612 127.0.0.1:54321       19672  main      INFO: User name: 'chaithanya'
08-02 00:40:36.612 127.0.0.1:54321       19672  main      INFO: Opted out of sending usage metrics.
08-02 00:40:36.612 127.0.0.1:54321       19672  main      INFO: Possible IP Address: wlo1 (wlo1), fe80:0:0:0:d5a6:487d:f375:a2e%wlo1
08-02 00:40:36.612 127.0.0.1:54321       19672  main      INFO: Possible IP Address: wlo1 (wlo1), 10.42.0.1
08-02 00:40:36.612 127.0.0.1:54321       19672  main      INFO: Possible IP Address: enp8s0 (enp8s0), fe80:0:0:0:1c9a:f547:b84:a5da%enp8s0
08-02 00:40:36.612 127.0.0.1:54321       19672  main      INFO: Possible IP Address: enp8s0 (enp8s0), 10.105.34.74
08-02 00:40:36.612 127.0.0.1:54321       19672  main      INFO: Possible IP Address: lo (lo), 0:0:0:0:0:0:0:1%lo
08-02 00:40:36.613 127.0.0.1:54321       19672  main      INFO: Possible IP Address: lo (lo), 127.0.0.1
08-02 00:40:36.613 127.0.0.1:54321       19672  main      FATAL: On /127.0.0.1 some of the required ports 54321, 54322 are not available, change -port PORT and try again.



Output :


Exception in thread ""main"" java.lang.AssertionError
        at water.AutoBuffer.<init>(AutoBuffer.java:165)
        at water.UDPRebooted$T.send(UDPRebooted.java:25)
        at water.H2O.shutdown(H2O.java:519)
        at water.H2O.die(H2O.java:1770)
        at water.init.NetworkInit.initializeNetworkSockets(NetworkInit.java:410)
        at water.H2O.startLocalNode(H2O.java:1294)
        at water.H2O.main(H2O.java:1705)
        at water.H2OStarter.start(H2OStarter.java:21)
        at water.H2OStarter.start(H2OStarter.java:36)
        at water.H2OApp.main(H2OApp.java:5)



i can't access h2o even if i changed the port
i tried reinstalling java and h2o but no luck so far","['java', 'port', 'h2o']",Sai Chaithanya Adapa,https://stackoverflow.com/users/6663994/sai-chaithanya-adapa,73
38685750,38685750,2016-07-31T15:53:27,2016-08-01 04:06:48Z,0,"Data:
 ""
https://github.com/estimate/pandas-exercises/blob/master/baby-names2.csv
""

In pandas:


df=pd.read_csv(""baby-names2.csv"")
df_group=df.groupby(""year"")
print df_group.head()



It prints the dataframe grouped by year.


How do I do the same thing in H2o Python ?

In H2o:


df=h2o.upload_file(""baby-names2.csv"")
df_group=df.group_by(""year"")
print df_group.head() ==> gives Error



Expected output: 


https://i.sstatic.net/SrS1V.png","['python', 'pandas', 'h2o']",Unknown,,N/A
38606606,38606606,2016-07-27T07:33:32,2018-08-13 20:24:07Z,0,"I fitted a random forest for my multinomial target with the 
randomForest
 package in R. Looking for the variable importance I found out 
permutation accuracy importance
 which is what I was looking for my analysis.
I fitted a random forest with the h2o package too, but the only measures it shows me are 
relative_importance,  scaled_importance,  percentage
.


My question is: can I extract a measure that shows me the level of the target which better classify the variable i want to take in exam?

Permutation accuracy importance
 is the best measure I can use in this case?


For example: I have a 3 levels target: A-B-C and 5 variables: v1-v2-v3-v4-v5 Is there a measure that shows me that v1 is more important for the level A of the target rather than level B (something similiar to the permutation accuracy importance)?","['r', 'random-forest', 'h2o', 'multinomial']",nicola,https://stackoverflow.com/users/6423513/nicola,83
38548789,38548789,2016-07-24T04:36:05,2016-07-27 02:11:28Z,260,"I am new to h2o machine learning platform and having the below issue while trying to build models.


When i was trying to build 5 GBM models with a not so large dataset, it has the following error:


gbm Model Build Progress: [##################################################] 100%

gbm Model Build Progress: [##################################################] 100%

gbm Model Build Progress: [##################################################] 100%

gbm Model Build Progress: [##################################################] 100%

gbm Model Build Progress: [#################                                 ] 34%

EnvironmentErrorTraceback (most recent call last)
<ipython-input-22-e74b34df2f1a> in <module>()
     13     params_model={'x': features_pca_all, 'y': response, 'training_frame': train_holdout_pca_hex, 'validation_frame':              validation_holdout_pca_hex, 'ntrees': ntree, 'max_depth':depth, 'min_rows': min_rows, 'learn_rate': 0.005}
     14 
---> 15     gbm_model=h2o.gbm(**params_model)
     16 
     17     #store model

C:\Anaconda2\lib\site-packages\h2o\h2o.pyc in gbm(x, y, validation_x, validation_y, training_frame, model_id, distribution, tweedie_power, ntrees, max_depth, min_rows, learn_rate, nbins, nbins_cats, validation_frame, balance_classes, max_after_balance_size, seed, build_tree_one_node, nfolds, fold_column, fold_assignment, keep_cross_validation_predictions, score_each_iteration, offset_column, weights_column, do_future, checkpoint)
   1058   parms = {k:v for k,v in locals().items() if k in [""training_frame"", ""validation_frame"", ""validation_x"", ""validation_y"", ""offset_column"", ""weights_column"", ""fold_column""] or v is not None}
   1059   parms[""algo""]=""gbm""
-> 1060   return h2o_model_builder.supervised(parms)
   1061 
   1062 

C:\Anaconda2\lib\site-packages\h2o\h2o_model_builder.pyc in supervised(kwargs)
     28   algo  = kwargs[""algo""]
     29   parms={k:v for k,v in kwargs.items() if (k not in [""x"",""y"",""validation_x"",""validation_y"",""algo""] and v is not None) or k==""validation_frame""}
---> 30   return supervised_model_build(x,y,vx,vy,algo,offsets,weights,fold_column,parms)
     31 
     32 def unsupervised_model_build(x,validation_x,algo_url,kwargs): return _model_build(x,None,validation_x,None,algo_url,None,None,None,kwargs)

C:\Anaconda2\lib\site-packages\h2o\h2o_model_builder.pyc in supervised_model_build(x, y, vx, vy, algo, offsets, weights, fold_column, kwargs)
     16   if not is_auto_encoder and y is None: raise ValueError(""Missing response"")
     17   if vx is not None and vy is None:     raise ValueError(""Missing response validating a supervised model"")
---> 18   return _model_build(x,y,vx,vy,algo,offsets,weights,fold_column,kwargs)
     19 
     20 def supervised(kwargs):

C:\Anaconda2\lib\site-packages\h2o\h2o_model_builder.pyc in _model_build(x, y, vx, vy, algo, offsets, weights, fold_column, kwargs)
     86   do_future = kwargs.pop(""do_future"") if ""do_future"" in kwargs else False
     87   future_model = H2OModelFuture(H2OJob(H2OConnection.post_json(""ModelBuilders/""+algo, **kwargs), job_type=(algo+"" Model Build"")), x)
---> 88   return future_model if do_future else _resolve_model(future_model, **kwargs)
     89 
     90 def _resolve_model(future_model, **kwargs):

C:\Anaconda2\lib\site-packages\h2o\h2o_model_builder.pyc in _resolve_model(future_model, **kwargs)
     89 
     90 def _resolve_model(future_model, **kwargs):
---> 91   future_model.poll()
     92   if '_rest_version' in kwargs.keys(): model_json = H2OConnection.get_json(""Models/""+future_model.job.dest_key, _rest_version=kwargs['_rest_version'])[""models""][0]
     93   else:                                model_json = H2OConnection.get_json(""Models/""+future_model.job.dest_key)[""models""][0]

C:\Anaconda2\lib\site-packages\h2o\model\model_future.pyc in poll(self)
      8 
      9     def poll(self):
---> 10         self.job.poll()
     11         self.x = None

C:\Anaconda2\lib\site-packages\h2o\job.pyc in poll(self)
     39       time.sleep(sleep)
     40       if sleep < 1.0: sleep += 0.1
---> 41       self._refresh_job_view()
     42       running = self._is_running()
     43     self._update_progress()

C:\Anaconda2\lib\site-packages\h2o\job.pyc in _refresh_job_view(self)
     52 
     53   def _refresh_job_view(self):
---> 54       jobs = H2OConnection.get_json(url_suffix=""Jobs/"" + self.job_key)
     55       self.job = jobs[""jobs""][0] if ""jobs"" in jobs else jobs[""job""][0]
     56       self.status = self.job[""status""]

C:\Anaconda2\lib\site-packages\h2o\connection.pyc in get_json(url_suffix, **kwargs)
    410     if __H2OCONN__ is None:
    411       raise ValueError(""No h2o connection. Did you run `h2o.init()` ?"")
--> 412     return __H2OCONN__._rest_json(url_suffix, ""GET"", None, **kwargs)
    413 
    414   @staticmethod

C:\Anaconda2\lib\site-packages\h2o\connection.pyc in _rest_json(self, url_suffix, method, file_upload_info, **kwargs)
    419 
    420   def _rest_json(self, url_suffix, method, file_upload_info, **kwargs):
--> 421     raw_txt = self._do_raw_rest(url_suffix, method, file_upload_info, **kwargs)
    422     return self._process_tables(raw_txt.json())
    423 

C:\Anaconda2\lib\site-packages\h2o\connection.pyc in _do_raw_rest(self, url_suffix, method, file_upload_info, **kwargs)
    476 
    477     begin_time_seconds = time.time()
--> 478     http_result = self._attempt_rest(url, method, post_body, file_upload_info)
    479     end_time_seconds = time.time()
    480     elapsed_time_seconds = end_time_seconds - begin_time_seconds

C:\Anaconda2\lib\site-packages\h2o\connection.pyc in _attempt_rest(self, url, method, post_body, file_upload_info)
    526 
    527     except requests.ConnectionError as e:
--> 528       raise EnvironmentError(""h2o-py encountered an unexpected HTTP error:\n {}"".format(e))
    529 
    530     return http_result

EnvironmentError: h2o-py encountered an unexpected HTTP error:
 ('Connection aborted.', BadStatusLine(""''"",))



My hunch is that the cluster memory has only around 247.5 MB which is not enough to handle the model building hence aborted the connection to h2o. Here are the codes I used to initiate h2o:


 #initialization of h2o module
import subprocess as sp
import sys
import os.path as p

# path of h2o jar file
h2o_path = p.join(sys.prefix, ""h2o_jar"", ""h2o.jar"")

# subprocess to launch h2o
# the command can be further modified to include virtual machine parameters
sp.Popen(""java -jar "" + h2o_path)

# h2o.init() call to verify that h2o launch is successfull
h2o.init(ip=""localhost"", port=54321, size=1, start_h2o=False, enable_assertions=False, \
         license=None, max_mem_size_GB=4, min_mem_size_GB=4, ice_root=None)



and here is the returned status table:




Any ideas on the above would be greatly appreciated!!","['python-2.7', 'machine-learning', 'h2o']",Gynteniuxas,https://stackoverflow.com/users/3884852/gynteniuxas,"7,085"
38510539,38510539,2016-07-21T17:19:10,2016-08-27 17:30:14Z,0,"I've trained a random forest model in Rstudio, using h2o library, and then i saved it as .rda file, now i would like to score other data using the model buildt. 
So i loaded the model i've turned the new dataset in a H2OFrame and i try to get score using 
predict( model, new_data)
 function.
What i get is the following error message:


    ERROR: Unexpected HTTP Status code: 404 Not Found (url = http://localhost:54321/4/Predictions/models/DRF_model_R_1468754145815_1/frames/file1840210c1889_sid_9b90_2)

water.exceptions.H2OKeyNotFoundArgumentException
 [1] ""water.exceptions.H2OKeyNotFoundArgumentException: Object 'DRF_model_R_1468754145815_1' not found in function: predict for argument: model""
 [2] ""    water.api.ModelMetricsHandler.predict2(ModelMetricsHandler.java:239)""                                                                 
 [3] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                                          
 [4] ""    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""                                                        
 [5] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""                                                
 [6] ""    java.lang.reflect.Method.invoke(Method.java:498)""                                                                                     
 [7] ""    water.api.Handler.handle(Handler.java:62)""                                                                                            
 [8] ""    water.api.RequestServer.handle(RequestServer.java:655)""                                                                               
 [9] ""    water.api.RequestServer.serve(RequestServer.java:596)""                                                                                
[10] ""    water.JettyHTTPD$H2oDefaultServlet.doGeneric(JettyHTTPD.java:745)""                                                                    
[11] ""    water.JettyHTTPD$H2oDefaultServlet.doPost(JettyHTTPD.java:681)""                                                                       
[12] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                                                         
[13] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                                         
[14] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                                               
[15] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                                                           
[16] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)""                                                        
[17] ""    org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)""                                                          
[18] ""    org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)""                                                    
[19] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                                                   
[20] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                                                            
[21] ""    org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)""                                                     
[22] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                                                    
[23] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                                        
[24] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                                
[25] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                                      
[26] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                                              
[27] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                                       
[28] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                                        
[29] ""    org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)""                                      
[30] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)""                      
[31] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)""                                                                     
[32] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)""                                                                
[33] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                                               
[34] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                                         
[35] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                                                     
[36] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                                      
[37] ""    java.lang.Thread.run(Thread.java:745)""                                                                                                

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

Object 'DRF_model_R_1468754145815_1' not found in function: predict for argument: model



I'm new to H2O.. Any idea on what is going wrong?","['r', 'random-forest', 'h2o']",jangorecki,https://stackoverflow.com/users/2490497/jangorecki,16.7k
38441250,38441250,2016-07-18T15:59:14,2017-11-14 15:31:41Z,0,"My intention is to start two or more h2o clusters / instances 
(not two or more nodes!)
 from within R on the same computer/server to enable multiple user to connect with h2o at the same time. In addition, I want to be able to shutdown and restart clusters separately, also from within R.  


I already know that I cannot controll multiple h2o clusters simply from within R, thus I tried to start two clusters from the command line in Windows 10: 


java -Xmx1g -jar h2o.jar -name testCluster1 -nthreads 1  -port 54321
java -Xmx1g -jar h2o.jar -name testCluster2 -nthreads 1  -port 54323



This works fine for me:


library(h2o)

h2o.init(startH2O = FALSE, ip = ""localhost"", port = 54321) 
Connection successful!

R is connected to the H2O cluster: 
H2O cluster uptime:         4 minutes 8 seconds 
H2O cluster version:        3.8.3.2 
H2O cluster name:           testCluster 
H2O cluster total nodes:    1 
H2O cluster total memory:   0.87 GB 
H2O cluster total cores:    4 
H2O cluster allowed cores:  1 
H2O cluster healthy:        TRUE 
H2O Connection ip:          localhost 
H2O Connection port:        54321 
H2O Connection proxy:       NA 
R Version:                  R version 3.2.5 (2016-04-14) 

h2o.init(startH2O = FALSE, ip = ""localhost"", port = 54323)
Connection successful!

R is connected to the H2O cluster: 
H2O cluster uptime:         3 minutes 32 seconds 
H2O cluster version:        3.8.3.2 
H2O cluster name:           testCluster2 
H2O cluster total nodes:    1 
H2O cluster total memory:   0.87 GB 
H2O cluster total cores:    4 
H2O cluster allowed cores:  1 
H2O cluster healthy:        TRUE 
H2O Connection ip:          localhost 
H2O Connection port:        54323 
H2O Connection proxy:       NA 
R Version:                  R version 3.2.5 (2016-04-14) 



Now, I want to do the same from within R via the system() command. 


launchH2O <-  as.character(""java -Xmx1g -jar h2o.jar -name testCluster -nthreads 1  -port 54321"")
system(command = launchH2O, intern =TRUE)



But I get an error message:


[1] ""Error: Unable to access jarfile h2o.jar""
attr(,""status"")
[1] 1
Warning message:
running command 'java -Xmx1g -jar h2o.jar -name testCluster -nthreads 1  -port 54321' had status 1 



Trying


system2(command = launchH2O)



I get a warning message and I am not able to connect with the cluster:


system2(command = launchH2O)
Warning message:
running command '""java -Xmx1g -jar h2o.jar -name testCluster -nthreads 1  -port 54321""' had status 127 

h2o.init(startH2O = FALSE, ip = ""localhost"", port = 54321)
Error in h2o.init(startH2O = FALSE, ip = ""localhost"", port = 54321) : 
Cannot connect to H2O server. Please check that H2O is running at http://localhost:54321/



Any ideas how to start / shutdown two or more h2o clusters from within R? 
Thank you in advance!


Note 1:
 I am only using my local Windows device for testing, I actually want to create multiple h2o clusters on a Linux server. 


Note 2:
 I tried it with both R GUI (3.2.5)  and R Studio (Version 0.99.892) and I ran them as admin. The h2o.jar file is in my working directory and my Java version is (Build 1.8.0_91-b14).


Note 3:
 System information:
- h2o & h2o R package version: 3.8.3.2
- Windows 10 Home, Version 1511
- 16 RAM, Intel Core i5-6200U CPU with 2,30 GHz","['r', 'windows', 'cmd', 'cluster-computing', 'h2o']",constiii,https://stackoverflow.com/users/6055629/constiii,648
38351835,38351835,2016-07-13T12:32:18,2017-07-26 03:25:41Z,0,"I was wondering about how to set up a h2o cluster using multiple AWS EC2 instances and R-Studio. I am not a computer scientist, so sorry for the trivial questions (!)


Based on this tutorial (
http://amunategui.github.io/h2o-on-aws/
) I sucessfully installed h2o and R-Studio on an AWS EC2 instance (Linux). But I rather want to create a multi-instance cluster with lets say 4 instance with 8 cores each.


Following this (
http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/deployment/multinode.html
) document, I need a flatfile.txt where I can list all IPs and ports of each EC2 instance. In a next step, I have to copy this file to each node in the cluster and afterwards I need to start a cluster via the java command line... Since I am not a computer scientist as I already mentioned, some questions emerged:




Where do I find the IPs and ports of each h2o instance?


How exactly can I copy the resulting file to each node?


From step 5 on I am completely confused; where do I have to insert this line / where can I find the java comand line?  


I dont want to use the Web UI of h2o, so how can I access the cluster from R-Studio (installed on one of the instances) ?




Thank you so much in advance!","['r', 'amazon-web-services', 'amazon-ec2', 'cluster-computing', 'h2o']",constiii,https://stackoverflow.com/users/6055629/constiii,648
38335068,38335068,2016-07-12T17:05:08,2018-01-19 01:11:52Z,0,"I've worked in the h2o R package for quite a while, now, but have recently had to move to the python package.


For the most part, an 
H2OFrame
 is designed to work like a pandas 
DataFrame
 object. However, there are several hurdles I haven't managed to get over... in Pandas, if I want to drop some rows:


df.drop([0,1,2], axis=0, inplace=True)



However, I cannot figure out how to do the same with an 
H2OFrame
:


frame.drop([0,1,2], axis=0)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-30-0eff75c48e35> in <module>()
----> frame.drop([0,1,2], axis=0)

TypeError: drop() got an unexpected keyword argument 'axis'



Their 
github
 source documents that the drop method is only for columns, so obviously the obvious way isn't working:


def drop(self, i):
    """"""Drop a column from the current H2OFrame.



Is there a way to drop rows from an 
H2OFrame
?","['python', 'h2o']",TayTay,https://stackoverflow.com/users/3015734/taytay,"7,120"
38281805,38281805,2016-07-09T12:29:36,2016-07-09 13:38:05Z,0,"I want to run a some Java or Scala code from within R (to be more specific, I'd like to run the POJO built by H2O from within R itself). Is there a way to do that? So far I just found solutions for the opposite case, of executing R programs from Java..","['java', 'r', 'scala', 'h2o']",shakedzy,https://stackoverflow.com/users/5863503/shakedzy,"2,893"
38275868,38275868,2016-07-08T21:51:18,2017-07-29 22:52:17Z,582,"I'm running Tweedie GLM using sparkling water for different sized data ie 20 MB, 400 MB, 2GB,25 GB. Code works fine for 
Sampling iteration 10
. But I have to test for large sampling scenario..



Sampling iteration is 500 



In this case code working well for 20 and 400 mb data.But It starts throwing issue when data is larger than 2 GB


After doing search I found one solution disabling change listener but that did not worked for large data.


--conf ""spark.scheduler.minRegisteredResourcesRatio=1"" ""spark.ext.h2o.topology.change.listener.enabled=false""


Here is my spark submit configuration 


spark-submit \
     --packages  ai.h2o:sparkling-water-core_2.10:1.6.1, log4j:log4j:1.2.17\
        --driver-memory 8g \
        --executor-memory 10g \
        --num-executors 10\
        --executor-cores 5 \
        --class TweedieGLM  target/SparklingWaterGLM.jar \
        $1\
        $2\
        --conf ""spark.scheduler.minRegisteredResourcesRatio=1"" ""spark.ext.h2o.topology.change.listener.enabled=false""



This is what I got as an error


16/07/08 20:39:55 ERROR YarnScheduler: Lost executor 2 on cfclbv0152.us2.oraclecloud.com: Executor heartbeat timed out after 175455 ms
    16/07/08 20:40:00 ERROR YarnScheduler: Lost executor 2 on cfclbv0152.us2.oraclecloud.com: remote Rpc client disassociated
    16/07/08 20:40:00 ERROR LiveListenerBus: Listener anon1 threw an exception
    java.lang.IllegalArgumentException: Executor without H2O instance discovered, killing the cloud!
            at org.apache.spark.h2o.H2OContext$$anon$1.onExecutorAdded(H2OContext.scala:203)
            at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:58)
            at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
            at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
            at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:56)
            at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)
            at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:79)
            at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1136)
            at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)","['apache-spark', 'machine-learning', 'glm', 'h2o', 'sparkling-water']",Unknown,,N/A
38265039,38265039,2016-07-08T11:00:46,2016-12-26 11:14:03Z,0,"I'm new to 
h2o
 and I'm having difficulty with this package on r.
I'm using a traning and test set 5100 and 2300 obs respectively with 18917 variables and a binary target (0,1) 
I ran a random forest:


train_h20<-as.h2o(train)
test_h20<-as.h2o(test)

forest <- h2o.randomForest(x = Words,
                           y = 18918,
                           training_frame = train_h20,
                           ntree = 250,
                           validation = test_h20,
                           seed = 8675309)



I know i can get the plot of logloss or mse or ... as the number of tree changes
But is there a way to plot an image of the model itself. I mean the final ensembled tree used for the final predictions?


Also, another question, in 
randomForest
 package I could use 
varImp
 function which returned me, as well as the absolute importance, the class-specific measures (computed as mean decrease in accuracy), i interpreted as a class-relative measure of variable importance.


varImp matrix, randomForest package:



In 
h2o
 package I only find the absolute importance measure, is there something similar?","['r', 'random-forest', 'h2o']",989,https://stackoverflow.com/users/1505504/989,12.9k
38150170,38150170,2016-07-01T16:31:10,2016-07-06 19:29:41Z,0,"Getting error when using as.h2o. Anyone have ideas?


library(pacman)
p_load(h2o)
data(iris)
localH2O = h2o.init(ip = 'XXX.XX.XX.XXX', port = XXXXX, strict_version_check = FALSE)
train_hex <-  as.h2o(iris, destination_frame = ""train_hex"")
|=====================================================| 100%
Error in class(obj) <- ""rs.scalar"" : attempt to set an attribute on NULL



If I try not to assign it, this is the error.


as.h2o(iris,  destination_frame = ""train_hex"")
|===================================================| 100%
    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
 1          5.1         3.5          1.4         0.2  setosa
 2          4.9         3.0          1.4         0.2  setosa
 3          4.7         3.2          1.3         0.2  setosa
 4          4.6         3.1          1.5         0.2  setosa
 5          5.0         3.6          1.4         0.2  setosa
 6          5.4         3.9          1.7         0.4  setosa
 Error in if (ncol(x) > 1) "" columns]"" else "" column]"" : 
 missing value where TRUE/FALSE needed



Here is some additional information about my environment and computing situation.


Version 0.99.896 – © 2009-2016 RStudio, Inc.


R is connected to the H2O cluster: 
H2O cluster uptime:         49 days 22 hours 
H2O cluster version:        3.9.1.99999 
H2O cluster name:           H2O_29276 
H2O cluster total nodes:    1 
H2O cluster total memory:   17.29 GB 
H2O cluster total cores:    24 
H2O cluster allowed cores:  4 
H2O cluster healthy:        TRUE 
H2O Connection ip:          
H2O Connection port:         
H2O Connection proxy:       NA 
R Version:                  R version 3.2.3 (2015-12-10)

R version 3.2.3 (2015-12-10)
Platform: x86_64-redhat-linux-gnu (64-bit)
Running under: Red Hat Enterprise Linux Server release 6.4 (Santiago)

locale:
[1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
[3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
[5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
[7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
[9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets 
[6] methods   base     

other attached packages:
[1] h2o_3.8.2.6    statmod_1.4.24 pacman_0.4.1  

loaded via a namespace (and not attached):
[1] tools_3.2.3     RCurl_1.95-4.8  jsonlite_0.9.20
[4] bitops_1.0-6","['r', 'rstudio', 'h2o']",Unknown,,N/A
38111805,38111805,2016-06-29T23:42:55,2016-06-30 07:22:30Z,0,"I'm starting with H2O and trying to ensemble a random forest and multiple linear regression in R f . The H2O data frame  I'm using is the following:


summary(training_frame)
 HS              AS              HST              AST              HF              AF             
 Min.   : 3.00   Min.   : 2.00   Min.   : 0.000   Min.   : 0.000   Min.   : 3.00   Min.   : 1.00  
 1st Qu.:11.00   1st Qu.: 8.00   1st Qu.: 3.000   1st Qu.: 2.000   1st Qu.:11.00   1st Qu.:11.00  
 Median :14.00   Median :11.00   Median : 5.000   Median : 4.000   Median :14.00   Median :14.00  
 Mean   :14.44   Mean   :11.53   Mean   : 5.211   Mean   : 4.063   Mean   :14.39   Mean   :14.03  
 3rd Qu.:18.00   3rd Qu.:15.00   3rd Qu.: 7.000   3rd Qu.: 5.000   3rd Qu.:17.00   3rd Qu.:17.00  
 Max.   :36.00   Max.   :28.00   Max.   :18.000   Max.   :13.000   Max.   :30.00   Max.   :27.00  
 HC               AC               HY              AY              HR               AR              
 Min.   : 0.000   Min.   : 0.000   Min.   :0.000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  
 1st Qu.: 4.000   1st Qu.: 3.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:0.0000   1st Qu.:0.0000  
 Median : 6.000   Median : 5.000   Median :2.000   Median :3.000   Median :0.0000   Median :0.0000  
 Mean   : 6.421   Mean   : 4.824   Mean   :2.563   Mean   :2.858   Mean   :0.1632   Mean   :0.2079  
 3rd Qu.: 8.000   3rd Qu.: 7.000   3rd Qu.:3.000   3rd Qu.:4.000   3rd Qu.:0.0000   3rd Qu.:0.0000  
 Max.   :17.000   Max.   :13.000   Max.   :8.000   Max.   :7.000   Max.   :2.0000   Max.   :3.0000  
 dif              
 Min.   :-5.0000  
 1st Qu.:-1.0000  
 Median : 0.0000  
 Mean   : 0.5026  
 3rd Qu.: 2.0000  
 Max.   : 6.0000  



Then, I try to set up the two models and the super learner to predict the variable ""dif"", the code is the following:


predictores <- names(X[,-13])


regre.1 <- function(..,family = ""gaussian"",lambda = 0) h2o.glm.wrapper(..,family = family,lambda = lambda)

randomforest.1 <- function(...,mtries = 5,ntree = 500) h2o.randomForest.wrapper(...,mtries = mtries,ntree = ntree)

h2o.glm.1 <- function(..., family = ""gaussian"",lambda = 0) h2o.glm.wrapper(..., family = family,lambda = lambda)

learner <- c(""regre.1"", ""randomforest.1"")

metalearner <- ""h2o.glm.1""

fit <- h2o.ensemble(x = predictores, y = ""dif"", 
                    training_frame = training_frame, 
                    learner = learner, 
                    metalearner = metalearner,
                    cvControl = list(V = 5))



However, I receive this error message:


|============================================================================================| 100%
[1] ""Cross-validating and training base learner 1: regre.1""
Error in match.fun(learner[l])(y = y, x = x, training_frame = training_frame,  : 
  unused arguments (y = y, x = x, training_frame = training_frame, validation_frame = NULL, fold_column = fold_column, keep_cross_validation_folds = TRUE)
Timing stopped at: 0 0 0 



What is wrong with my code?","['r', 'machine-learning', 'h2o', 'ensemble-learning']",CreamStat,https://stackoverflow.com/users/2825079/creamstat,"2,175"
38101052,38101052,2016-06-29T13:28:43,2016-06-29 20:47:38Z,0,"I am trying to use h2o with R. I have installed the package according to the instructions on H2o website as follows:


if (""package:h2o"" %in% search()) { detach(""package:h2o"", unload=TRUE) }
if (""h2o"" %in% rownames(installed.packages())) { remove.packages(""h2o"") }
pkgs <- c(""methods"",""statmod"",""stats"",""graphics"",""RCurl"",""jsonlite"",""tools"",""utils"")
for (pkg in pkgs) {
    if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
install.packages(""h2o"", type=""source"", repos=(c(""http://h2o-release.s3.amazonaws.com/h2o/rel-turchin/9/R"")))



Then I encountered a problem when trying do the initialisation as follows:


library(h2o)
localH2O <- h2o.init(nthreads = -1, max_mem_size = '2g')



The errors are as follows:


> localH2O <- h2o.init()

H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
        /var/folders/yq/rhplvy_x14935793590w3tl80000gn/T//Rtmp4i5gnZ/h2o_Fan_started_from_r.out
        /var/folders/yq/rhplvy_x14935793590w3tl80000gn/T//Rtmp4i5gnZ/h2o_Fan_started_from_r.err

java version ""1.8.0_20""
Java(TM) SE Runtime Environment (build 1.8.0_20-b26)
Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode)

Starting H2O JVM and connecting: ............................................................06-29 21:10:53.982 127.0.0.1:54321       18160  main      INFO: ----- H2O started  -----
06-29 21:10:54.020 127.0.0.1:54321       18160  main      INFO: Build git branch: rel-turchin
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: Build git hash: 3da08b14a0f219fa1086c1c8d475e62fa3e16c29
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: Build git describe: jenkins-rel-turchin-9
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: Build project version: 3.8.2.9
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: Built by: 'jenkins'
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: Built on: '2016-06-10 13:25:13'
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: Java availableProcessors: 8
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: Java heap totalMemory: 245.5 MB
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: Java heap maxMemory: 3.56 GB
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: Java version: Java 1.8.0_20 (from Oracle Corporation)
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: JVM launch parameters: [-ea]
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: OS version: Mac OS X 10.10.5 (x86_64)
06-29 21:10:54.021 127.0.0.1:54321       18160  main      INFO: Machine physical memory: 16.00 GB
06-29 21:10:54.022 127.0.0.1:54321       18160  main      INFO: X-h2o-cluster-id: 1467205853154
06-29 21:10:54.022 127.0.0.1:54321       18160  main      INFO: User name: 'Fan'
06-29 21:10:54.022 127.0.0.1:54321       18160  main      INFO: Possible IP Address: awdl0 (awdl0), fe80:0:0:0:40bf:6fff:fe9c:f5c5%awdl0
06-29 21:10:54.022 127.0.0.1:54321       18160  main      INFO: Possible IP Address: en0 (en0), fe80:0:0:0:82e6:50ff:fe12:3dd2%en0
06-29 21:10:54.022 127.0.0.1:54321       18160  main      INFO: Possible IP Address: en0 (en0), 192.168.0.103
06-29 21:10:54.022 127.0.0.1:54321       18160  main      INFO: Possible IP Address: lo0 (lo0), fe80:0:0:0:0:0:0:1%lo0
06-29 21:10:54.022 127.0.0.1:54321       18160  main      INFO: Possible IP Address: lo0 (lo0), 0:0:0:0:0:0:0:1
06-29 21:10:54.022 127.0.0.1:54321       18160  main      INFO: Possible IP Address: lo0 (lo0), 127.0.0.55
06-29 21:10:54.022 127.0.0.1:54321       18160  main      INFO: Possible IP Address: lo0 (lo0), 127.0.0.54
06-29 21:10:54.022 127.0.0.1:54321       18160  main      INFO: Possible IP Address: lo0 (lo0), 127.0.0.53
06-29 21:10:54.022 127.0.0.1:54321       18160  main      INFO: Possible IP Address: lo0 (lo0), 127.0.0.1
06-29 21:10:54.022 127.0.0.1:54321       18160  main      FATAL: On /127.0.0.1 some of the required ports 54321, 54322 are not available, change -port PORT and try again. 
[1] ""localhost""
[1] 54321
[1] TRUE
[1] -1
[1] ""Could not resolve host: localhost""
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                             Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: localhost
[1] 6
Error in h2o.init() : H2O failed to start, stopping execution.



I have searched over the internet and similar problems are mostly take place when the java is not 32-bit or the firewall is switched on. All these are not the reasons here. 


It would be great if you guys could provide some clues on it. FYI, I am running R3.3.1 on MacOS. But I have tried previous version of R, did not work either. Thanks in advance for the help.","['r', 'initialization', 'h2o']",rajafan,https://stackoverflow.com/users/2341936/rajafan,141
38081774,38081774,2016-06-28T16:38:17,2016-07-13 17:36:55Z,0,"I am currently working on a sparkling water application and I am a total beginner in spark and h2o.


What I want to do:




loading a input textfile


create a word2vec model


create a dataframe with a column 
word
 and a column 
Vector


using the dataframe as input for h2o




By creating the model i get a map, but i don't know how to create a dataframe of it. The output should look like that:


word
   | 
Vector


assert | [0.3, 0.4.....]


sense | [0.6, 0.2.....]
and so on.


This is my code so far: 


from pyspark import SparkContext
from pyspark.mllib.feature import Word2Vec
from pysparkling import *
import h2o

from pyspark.sql import SQLContext
from pyspark.mllib.linalg import Vectors
from pyspark.sql import Row


# Starting h2o application on spark cluster
hc = H2OContext(sc).start()

# Loading input file
inp = sc.textFile(""examples/custom/text8.txt"").map(lambda row: row.split("" ""))

# building the word2vec model with a vector size of 10
word2vec = Word2Vec()
model = word2vec.setVectorSize(10).fit(inp)

# Sanity check
model.findSynonyms(""property"",5)

# assign vector representation (map to variable
wordVectorsDF = model.getVectors()

# Transform wordVectorsDF word into dataframe



Is there any approach to that or functions provided by spark?


Thanks in advance","['apache-spark', 'machine-learning', 'pyspark', 'word2vec', 'h2o']",sedioben,https://stackoverflow.com/users/6524396/sedioben,945
38055840,38055840,2016-06-27T13:55:11,2016-06-27 14:30:58Z,0,"Manual says ""Logical. If enabled, automatically standardize the data. If disabled, the user must provide properly scaled input data.""


I tried learning with iris data.


R source code


library(h2o)
h2o.init()

xx <- data.frame(c(1:10), c(1:10) * 10, c(1:10) * 100)

iris.hex <- as.h2o(xx)
bb <- h2o.deeplearning(x = 1:2, y = 3, training_frame = iris.hex, standardize = TRUE, 
                   activation = ""Rectifier"", hidden = c(10, 10), export_weights_and_biases = TRUE)

predictions <- as.data.frame(h2o.predict(bb, iris.hex))

weights_01 <- as.matrix(h2o.weights(bb, matrix_id = 1))
weights_02 <- as.matrix(h2o.weights(bb, matrix_id = 2))
weights_03 <- as.matrix(h2o.weights(bb, matrix_id = 3))

biases_01 <- as.matrix(h2o.biases(bb, vector_id = 1))
biases_02 <- as.matrix(h2o.biases(bb, vector_id = 2))
biases_03 <- as.matrix(h2o.biases(bb, vector_id = 3))

xx_temp <- xx ## This step standardizes my data
for (i in 1:3){
  xx_temp[ , i] <- xx_temp[ , i] / max(xx_temp[ , i])
}

result_mat <- matrix(nrow = 10, ncol = 1)
for (j in 1:10){
  asdf <- as.matrix(xx_temp[j, 1:2])
  asdf <- weights_01 %*% t(asdf) + biases_01
  asdf[asdf < 0] <- 0

  asdf <- weights_02 %*% (asdf) + biases_02
  asdf[asdf < 0] <- 0

  asdf <- weights_03 %*% (asdf) + biases_03
  result_mat[j, 1] <- asdf
}
result_mat

cbind(predictions[1:10, 1], result_mat) ## << What is differences of two data 



My question is how can I calculate 'result mat' as 'predictions' ??","['r', 'h2o', 'standardized']",Baek Junghan,https://stackoverflow.com/users/6518818/baek-junghan,1
38027263,38027263,2016-06-25T10:03:30,2016-06-28 07:24:40Z,154,"How do I represent a set/list of items in the input data (data frame) for H2O?


I'm using sparkling water 1.6.5 with H2O Flow.
My input data (columns in the CSV file) look like this:


age: numeric
gender: enum
hobbies: ?
sports: ?



hobbies and sports are lists/sets with a limited number of possible entries (~20 each). H2O does not seem to have a suitable data type for this. How do I export these into a CSV file that can be processed by H2O Flow?","['machine-learning', 'h2o', 'sparkling-water']",Markus Kramer,https://stackoverflow.com/users/1008800/markus-kramer,411
38007814,38007814,2016-06-24T07:23:26,2016-09-04 08:08:15Z,736,"I'm trying to get h2o running on a Jupyter notebook with scala kernel, with no success so far. Maybe someone can give me a hint on what could be wrong? The code I'm executing at the moment is


classpath.add(""ai.h2o"" % ""sparkling-water-core_2.10"" % ""1.6.5"")

import org.apache.spark.h2o._
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

val conf = new SparkConf().setAppName(""appName"").setMaster(""local"")
val sc = new SparkContext(conf)
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

val h2oContext = new H2OContext(sc).start()



It fails on the last line with error


java.lang.NoClassDefFoundError: water/H2O
....



And prints out exception


java.lang.RuntimeException: Cannot launch H2O on executors: numOfExecutors=1, executorStatus=(driver,false) (Cannot launch H2O on executors: numOfExecutors=1, executorStatus=(driver,false))
org.apache.spark.h2o.H2OContextUtils$.startH2O(H2OContextUtils.scala:169)
org.apache.spark.h2o.H2OContext.start(H2OContext.scala:214)","['scala', 'jupyter', 'h2o']",psarka,https://stackoverflow.com/users/6110160/psarka,"1,812"
37798134,37798134,2016-06-13T19:57:04,2016-06-13 20:45:54Z,0,"I'm exploring h2o via the R interface and I'm getting a weird weight matrix. My task is as simple as they get: given x,y compute x+y.

I have 214 rows with 3 columns. The first column(x) was drawn uniformly from (-1000, 1000) and the second one(y) from (-100,100). I just want to combine them so I have a single hidden layer with a single neuron. 
This is my code:


library(h2o)
localH2O = h2o.init(ip = ""localhost"", port = 54321, startH2O = TRUE)
train <- h2o.importFile(path = ""/home/martin/projects/R NN Addition/addition.csv"")
model <- h2o.deeplearning(1:2,3,train, hidden = c(1), epochs=200, export_weights_and_biases=T, nfolds=5)
print(h2o.weights(model,1))
print(h2o.weights(model,2))



and the result is 


> print(h2o.weights(model,1))
          x          y
1 0.5586579 0.05518193

[1 row x 2 columns] 
> print(h2o.weights(model,2))
        C1
1 1.802469



For some reason the weight value for y is 0.055 - 10 times lower than for x. So, in the end the neural net would compute x+y/10. However, h2o.predict actually returns the correct values (even on a test set).

I'm guessing there's a preprocessing step that's somehow scaling my data. Is there any way I can reproduce the actual weights produced by the model? I would like to be able to visualize some pretty simple neural networks.","['r', 'neural-network', 'deep-learning', 'h2o']",Martin Boyanov,https://stackoverflow.com/users/4790540/martin-boyanov,416
37779076,37779076,2016-06-12T21:05:58,2020-03-22 06:42:55Z,0,"I am new with H2o. Based in the 
documentation
 I installed H2o for python 


$ pip install h2o



Then:


In:


import h2o
h2o.init()



Out:




OSError                                   Traceback (most recent call last)
<ipython-input-1-07f8bb8f27db> in <module>()
      1 import h2o
----> 2 h2o.init()

/usr/local/lib/python3.5/site-packages/h2o/h2o.py in init(ip, port, start_h2o, enable_assertions, license, nthreads, max_mem_size, min_mem_size, ice_root, strict_version_check, proxy, https, insecure, username, password, max_mem_size_GB, min_mem_size_GB, proxies, size)
    849                 nthreads=nthreads,max_mem_size=max_mem_size,min_mem_size=min_mem_size,ice_root=ice_root,
    850                 strict_version_check=strict_version_check,proxy=proxy,https=https,insecure=insecure,username=username,
--> 851                 password=password,max_mem_size_GB=max_mem_size_GB,min_mem_size_GB=min_mem_size_GB,proxies=proxies,size=size)
    852   return None
    853 

/usr/local/lib/python3.5/site-packages/h2o/connection.py in __init__(self, ip, port, start_h2o, enable_assertions, license, nthreads, max_mem_size, min_mem_size, ice_root, strict_version_check, proxy, https, insecure, username, password, max_mem_size_GB, min_mem_size_GB, proxies, size)
    173           raise EnvironmentError(""Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. ""
    174                                  ""This is a developer build, please contact your developer.""
--> 175                                  """".format(ver_h2o, str(ver_pkg)))
    176         else:
    177           raise EnvironmentError(""Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. ""

OSError: Version mismatch. H2O is version 3.8.2.99999, but the h2o-python package is version 3.8.2.6-1. This is a developer build, please contact your developer.



From this 
website
, I tried to solve this issue as follows:


h2o.shutdown



Then:


user@MacBook-Pro-of-User:~$ pip3 uninstall h2o
Uninstalling h2o-3.8.2.6-1:
  /usr/local/h2o_data/iris.csv
  /usr/local/h2o_data/prostate.csv
  /usr/local/h2o_jar/h2o.jar
  /usr/local/lib/python3.5/site-packages/h2o-3.8.2.6_1.dist-info/DESCRIPTION.rst
  /usr/local/lib/python3.5/site-packages/h2o-3.8.2.6_1.dist-info/INSTALLER
  /usr/local/lib/python3.5/site-packages/h2o-3.8.2.6_1.dist-info/METADATA
  /usr/local/lib/python3.5/site-packages/h2o-3.8.2.6_1.dist-info/RECORD
  /usr/local/lib/python3.5/site-packages/h2o-3.8.2.6_1.dist-info/WHEEL
  /usr/local/lib/python3.5/site-packages/h2o-3.8.2.6_1.dist-info/metadata.json
  /usr/local/lib/python3.5/site-packages/h2o-3.8.2.6_1.dist-info/top_level.txt
  /usr/local/lib/python3.5/site-packages/h2o/__init__.py
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/__init__.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/assembly.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/astfun.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/connection.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/cross_validation.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/demo.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/display.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/expr.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/frame.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/group_by.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/h2o.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/h2o_logging.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/job.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/__pycache__/two_dim_table.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/assembly.py
  /usr/local/lib/python3.5/site-packages/h2o/astfun.py
  /usr/local/lib/python3.5/site-packages/h2o/connection.py
  /usr/local/lib/python3.5/site-packages/h2o/cross_validation.py
  /usr/local/lib/python3.5/site-packages/h2o/demo.py
  /usr/local/lib/python3.5/site-packages/h2o/display.py
  /usr/local/lib/python3.5/site-packages/h2o/estimators/__init__.py
  /usr/local/lib/python3.5/site-packages/h2o/estimators/__pycache__/__init__.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/estimators/__pycache__/deeplearning.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/estimators/__pycache__/estimator_base.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/estimators/__pycache__/gbm.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/estimators/__pycache__/glm.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/estimators/__pycache__/glrm.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/estimators/__pycache__/kmeans.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/estimators/__pycache__/naive_bayes.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/estimators/__pycache__/random_forest.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/estimators/deeplearning.py
  /usr/local/lib/python3.5/site-packages/h2o/estimators/estimator_base.py
  /usr/local/lib/python3.5/site-packages/h2o/estimators/gbm.py
  /usr/local/lib/python3.5/site-packages/h2o/estimators/glm.py
  /usr/local/lib/python3.5/site-packages/h2o/estimators/glrm.py
  /usr/local/lib/python3.5/site-packages/h2o/estimators/kmeans.py
  /usr/local/lib/python3.5/site-packages/h2o/estimators/naive_bayes.py
  /usr/local/lib/python3.5/site-packages/h2o/estimators/random_forest.py
  /usr/local/lib/python3.5/site-packages/h2o/expr.py
  /usr/local/lib/python3.5/site-packages/h2o/frame.py
  /usr/local/lib/python3.5/site-packages/h2o/grid/__init__.py
  /usr/local/lib/python3.5/site-packages/h2o/grid/__pycache__/__init__.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/grid/__pycache__/grid_search.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/grid/__pycache__/metrics.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/grid/grid_search.py
  /usr/local/lib/python3.5/site-packages/h2o/grid/metrics.py
  /usr/local/lib/python3.5/site-packages/h2o/group_by.py
  /usr/local/lib/python3.5/site-packages/h2o/h2o.py
  /usr/local/lib/python3.5/site-packages/h2o/h2o_logging.py
  /usr/local/lib/python3.5/site-packages/h2o/job.py
  /usr/local/lib/python3.5/site-packages/h2o/model/__init__.py
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/__init__.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/autoencoder.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/binomial.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/clustering.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/confusion_matrix.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/dim_reduction.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/metrics_base.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/model_base.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/model_builder.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/model_future.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/multinomial.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/__pycache__/regression.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/model/autoencoder.py
  /usr/local/lib/python3.5/site-packages/h2o/model/binomial.py
  /usr/local/lib/python3.5/site-packages/h2o/model/clustering.py
  /usr/local/lib/python3.5/site-packages/h2o/model/confusion_matrix.py
  /usr/local/lib/python3.5/site-packages/h2o/model/dim_reduction.py
  /usr/local/lib/python3.5/site-packages/h2o/model/metrics_base.py
  /usr/local/lib/python3.5/site-packages/h2o/model/model_base.py
  /usr/local/lib/python3.5/site-packages/h2o/model/model_builder.py
  /usr/local/lib/python3.5/site-packages/h2o/model/model_future.py
  /usr/local/lib/python3.5/site-packages/h2o/model/multinomial.py
  /usr/local/lib/python3.5/site-packages/h2o/model/regression.py
  /usr/local/lib/python3.5/site-packages/h2o/transforms/__init__.py
  /usr/local/lib/python3.5/site-packages/h2o/transforms/__pycache__/__init__.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/transforms/__pycache__/decomposition.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/transforms/__pycache__/preprocessing.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/transforms/__pycache__/transform_base.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/transforms/decomposition.py
  /usr/local/lib/python3.5/site-packages/h2o/transforms/preprocessing.py
  /usr/local/lib/python3.5/site-packages/h2o/transforms/transform_base.py
  /usr/local/lib/python3.5/site-packages/h2o/two_dim_table.py
  /usr/local/lib/python3.5/site-packages/h2o/utils/__init__.py
  /usr/local/lib/python3.5/site-packages/h2o/utils/__pycache__/__init__.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/utils/__pycache__/shared_utils.cpython-35.pyc
  /usr/local/lib/python3.5/site-packages/h2o/utils/shared_utils.py
Proceed (y/n)? y
  Successfully uninstalled h2o-3.8.2.6-1
user@MacBook-Pro-of-User:~$



Then I tried to upgrade to the latest version:


user@MacBook-Pro-of-User:~$ pip3 install http://h2o-release.s3.amazonaws.com/h2o/rel-turchin/6/Python/h2o-3.8.2.6-py2.py3-none-any.whl



However, I got the same error message:


OSError: Version mismatch. H2O is version 3.8.2.99999, but the h2o-python package is version 3.8.2.6. This is a developer build, please contact your developer.



How to correctly install h2o for python?.","['python', 'python-3.x', 'machine-learning', 'h2o']",tumbleweed,https://stackoverflow.com/users/4140027/tumbleweed,"4,622"
37777147,37777147,2016-06-12T17:32:49,2018-09-06 19:15:02Z,0,"Could you please guide me on how to create and execute a machine learning models/statistical models (regression, Decision tree, K means clustering, Naive bayes, scorecard/linear/logistic regression etc. and GBM, GLM ) in Java/JVM based application (in production).


We have an ETL sort of Java based product where one can do most of data Preparation steps for machine learning, like data ingestion from JDBC, files, HDFS, No SQL etc., joins and aggregations etc.(which are required for Feature engineering) and now we want to add Analytics capabilities using machine learning/statistical modeling.


Right now, we are using JPMML- evaluator to score the models created in PMML format using R and python (and Knime) but it needs three separate and unconnected steps:-
 1- first step for data preparation in our Java/JVM application and save the sampling data (training and test) data in csv file or in DB, - 
 2-  Create a machine learning Model in R and python (and Knime) and export it in PMML 4.2 format -  
 3- Import/deploy the PMML in our Java based application and use JPMML evaluator to execute it in production. 


I am sure it's a common problem in machine learning as generally in Production JAVA is preferred over Python or R. Could you suggest what is the better approach(s) to create as well as execute a python/scikit based machine learning model in JVM based application. 


What are your thought to achieve the steps # 2 and #3 more seamlessly in a JVM based application, without compromising performance and usability:- 


1-  Call a java program which internally calls the 
python scikit script
 (under the hood) to 
create a model in PMML
 and then use JPMML evaluator. It will pretend to the user that he is in a single JVM based application (better usability). I am not sure what are the limitations and short coming of using PMML as not all features are supported in jpmml-sklearn.
2-  Call a java program which internally calls the python script and do the model creation as well as execution in an external python environment and serialized the model and the results in a file/csv or in memory DB (or cache, like hazelcast) from where the parent Java application will fetch the results etc.. I researched that I can’t use Jython for executing Sci-kit models.
3-  Can I use Jep (Embed Python in Java) to embed Cpython in JVM ? Does anybody tried it for sci-kit models?


Alternatively, I should explore to use Mahout or weka  - java based machine learning libraries in my JVM based application. (I need to support both windows and non-windows platforms)


I am also exploring H2Oai which is java based. Does anybody tried it.","['java', 'machine-learning', 'scikit-learn', 'h2o', 'pmml']",Unknown,,N/A
37748009,37748009,2011-08-11T17:01:50,2018-11-13 12:57:29Z,0,"data.table
 objects now have a := operator.  What makes this operator different from all other assignment operators?  Also, what are its uses, how much faster is it, and when should it be avoided?","['r', 'data.table', 'colon-equals']",moodymudskipper,https://stackoverflow.com/users/2270475/moodymudskipper,47.2k
37730358,37730358,2016-06-09T15:33:36,2018-12-05 17:02:33Z,0,"I am trying to use h2o to create an autoencoder using its deeplearning function. I am feeding a set of data about 4000x50 in size to the deeplearning function (hidden node c(200)) and then using h2o.mse to check its error and I am getting about 0.4, a fairly high value. 


Is there anyway to reduce that error by changing something in the deeplearning function?","['r', 'h2o']",Allen Huang,https://stackoverflow.com/users/6331459/allen-huang,394
37720792,37720792,2016-06-09T08:35:26,2016-06-10 07:31:26Z,0,"I have tried to follow steps in 
this
 tutorial to install and start 
h2o
 in R and got here:


> h2o.init(nthreads=-1)

H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    /home/pal/tmp/Rtmp8q7r3M/h2o_pal_started_from_r.out
    /home/pal/tmp/Rtmp8q7r3M/h2o_pal_started_from_r.err

java version ""1.8.0-ea""
Java(TM) SE Runtime Environment (build 1.8.0-ea-b114)
Java HotSpot(TM) 64-Bit Server VM (build 25.0-b56, mixed mode)

Starting H2O JVM and connecting: ... Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         3 seconds 258 milliseconds 
    H2O cluster version:        3.8.2.8 
    H2O cluster name:           H2O_started_from_R_pal_hmx971 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   19.13 GB 
    H2O cluster total cores:    44 
    H2O cluster allowed cores:  44 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    R Version:                  R version 3.2.2 (2015-08-14) 



However, after this I do not see the usual 
>
 so that I can start type commands, it is stalled at this point. Do you have any ideas what to do?
The log files: 
.err
 is empty, 
.out
 looks like this:


06-09 10:31:27.590 127.0.0.1:54321       14772  main      INFO: ----- H2O started  -----
06-09 10:31:27.680 127.0.0.1:54321       14772  main      INFO: Build git branch: rel-turchin
06-09 10:31:27.680 127.0.0.1:54321       14772  main      INFO: Build git hash: df8dd4c342aa5b1af1fc19b1510c361bcc8af0c4
06-09 10:31:27.681 127.0.0.1:54321       14772  main      INFO: Build git describe: jenkins-rel-turchin-8
06-09 10:31:27.681 127.0.0.1:54321       14772  main      INFO: Build project version: 3.8.2.8
06-09 10:31:27.681 127.0.0.1:54321       14772  main      INFO: Built by: 'jenkins'
06-09 10:31:27.681 127.0.0.1:54321       14772  main      INFO: Built on: '2016-06-02 18:35:57'
06-09 10:31:27.682 127.0.0.1:54321       14772  main      INFO: Java availableProcessors: 44
06-09 10:31:27.682 127.0.0.1:54321       14772  main      INFO: Java heap totalMemory: 1.29 GB
06-09 10:31:27.682 127.0.0.1:54321       14772  main      INFO: Java heap maxMemory: 19.13 GB
06-09 10:31:27.682 127.0.0.1:54321       14772  main      INFO: Java version: Java 1.8.0-ea (from Oracle Corporation)
06-09 10:31:27.683 127.0.0.1:54321       14772  main      INFO: JVM launch parameters: [-ea]
06-09 10:31:27.683 127.0.0.1:54321       14772  main      INFO: OS version: Linux 3.5.0-39-generic (amd64)
06-09 10:31:27.683 127.0.0.1:54321       14772  main      INFO: Machine physical memory: 86.09 GB
06-09 10:31:27.683 127.0.0.1:54321       14772  main      INFO: X-h2o-cluster-id: 1465461085307
06-09 10:31:27.684 127.0.0.1:54321       14772  main      INFO: User name: 'pal'
06-09 10:31:27.684 127.0.0.1:54321       14772  main      INFO: Possible IP Address: eth1 (eth1), fe80:0:0:0:216:3eff:fe11:2231%eth1
06-09 10:31:27.684 127.0.0.1:54321       14772  main      INFO: Possible IP Address: eth1 (eth1), 192.168.112.231
06-09 10:31:27.684 127.0.0.1:54321       14772  main      INFO: Possible IP Address: eth0 (eth0), fe80:0:0:0:216:3eff:fe21:8231%eth0
06-09 10:31:27.685 127.0.0.1:54321       14772  main      INFO: Possible IP Address: eth0 (eth0), 193.6.218.231
06-09 10:31:27.685 127.0.0.1:54321       14772  main      INFO: Possible IP Address: lo (lo), 0:0:0:0:0:0:0:1%lo
06-09 10:31:27.685 127.0.0.1:54321       14772  main      INFO: Possible IP Address: lo (lo), 127.0.0.1
06-09 10:31:27.685 127.0.0.1:54321       14772  main      INFO: Selected H2O.CLOUD_MULTICAST_IF: name:lo (lo) doesn't support multicast
06-09 10:31:27.702 127.0.0.1:54321       14772  main      INFO: Internal communication uses port: 54322
06-09 10:31:27.702 127.0.0.1:54321       14772  main      INFO: Listening for HTTP and REST traffic on http://127.0.0.1:54321/
06-09 10:31:27.703 127.0.0.1:54321       14772  main      INFO: H2O cloud name: 'H2O_started_from_R_pal_hmx971' on /127.0.0.1:54321, discovery address /238.86.237.46:61014
06-09 10:31:27.704 127.0.0.1:54321       14772  main      INFO: If you have trouble connecting, try SSH tunneling from your local machine (e.g., via port 55555):
06-09 10:31:27.704 127.0.0.1:54321       14772  main      INFO:   1. Open a terminal and run 'ssh -L 55555:localhost:54321 
[email protected]
'
06-09 10:31:27.704 127.0.0.1:54321       14772  main      INFO:   2. Point your browser to http://localhost:55555
06-09 10:31:27.704 127.0.0.1:54321       14772  main      INFO: Log dir: '/home/pal/tmp/Rtmp8q7r3M/h2ologs'
06-09 10:31:27.704 127.0.0.1:54321       14772  main      INFO: Cur dir: '/home/pal'
06-09 10:31:27.735 127.0.0.1:54321       14772  main      INFO: HDFS subsystem successfully initialized
06-09 10:31:27.736 127.0.0.1:54321       14772  main      INFO: S3 subsystem successfully initialized
06-09 10:31:27.736 127.0.0.1:54321       14772  main      INFO: Flow dir: '/home/pal/h2oflows'
06-09 10:31:27.758 127.0.0.1:54321       14772  main      INFO: Cloud of size 1 formed [/127.0.0.1:54321]
06-09 10:31:27.760 127.0.0.1:54321       14772  main      INFO: Registered 0 extensions in: 1058mS
06-09 10:31:28.787 127.0.0.1:54321       14772  main      INFO: Registered: 124 REST APIs in: 1026mS
06-09 10:31:29.560 127.0.0.1:54321       14772  main      INFO: Registered: 193 schemas in: 773mS
06-09 10:31:29.561 127.0.0.1:54321       14772  main      INFO: 
06-09 10:31:29.561 127.0.0.1:54321       14772  main      INFO: Open H2O Flow in your web browser: http://127.0.0.1:54321/
06-09 10:31:29.561 127.0.0.1:54321       14772  main      INFO: 
06-09 10:31:29.617 127.0.0.1:54321       14772  #31393-25 INFO: Method: GET   , URI: /, route: , parms: {}
06-09 10:31:29.642 127.0.0.1:54321       14772  #31393-26 INFO: Method: GET   , URI: /, route: , parms: {}
06-09 10:31:29.655 127.0.0.1:54321       14772  #31393-27 INFO: Method: GET   , URI: /, route: , parms: {}
06-09 10:31:29.852 127.0.0.1:54321       14772  #31393-31 INFO: Method: GET   , URI: /3/InitID, route: /3/InitID, parms: {}
06-09 10:31:29.854 127.0.0.1:54321       14772  #31393-31 INFO: Locking cloud to new members, because water.api.InitIDV3



I am running this on a server without any graphical interface.","['r', 'h2o']",Unknown,,N/A
37624789,37624789,2016-06-03T23:22:50,2016-06-04 08:36:48Z,530,"I am having trouble importing multiple csv files on H2O flow. The coffee code from Flow is


setupParse paths:[""file1.csv"", ""file2.csv"", ""file3.csv""...]  



However when the parsing is done I am left only with 
file1.hex
 and I am not sure where the other files went.","['csv', 'h2o']",Roman,https://stackoverflow.com/users/5032339/roman,"6,636"
37551299,37551299,2016-05-31T16:41:20,2016-05-31 16:41:20Z,0,"I am attempting to use the h2o package in R to query a MySQL database and store the results directly in an h2o instance I have running on my local machine.


Provided below is all of my code and errors received, I would very much appreciate a new set of eyes to help me troubleshoot - thanks!


Via Windows Command Prompt:


java -cp ""C:\Program Files\h2o-3.8.2.6\h2o.jar;C:\Program Files\MySQL\mysql-connector-java-5.1.39\mysql-connector-java-5.1.39-bin.jar"" water.H2OApp



Which returns:


05-31 12:20:41.601 127.0.0.1:54321       5280   main      INFO: HDFS subsystem successfully initialized
05-31 12:20:41.602 127.0.0.1:54321       5280   main      INFO: S3 subsystem successfully initialized
05-31 12:20:41.604 127.0.0.1:54321       5280   main      INFO: Flow dir: 'C:/Users/ekorne201/h2oflows'
05-31 12:20:41.925 127.0.0.1:54321       5280   main      INFO: Cloud of size 1 formed [/127.0.0.1:54321]
05-31 12:20:41.931 127.0.0.1:54321       5280   main      INFO: Registered 0 extensions in: 531mS
05-31 12:20:42.525 127.0.0.1:54321       5280   main      INFO: Registered: 124 REST APIs in: 553mS
05-31 12:20:42.995 127.0.0.1:54321       5280   main      INFO: Registered: 193 schemas in: 468mS
05-31 12:20:42.995 127.0.0.1:54321       5280   main      INFO:
05-31 12:20:42.996 127.0.0.1:54321       5280   main      INFO: Open H2O Flow in your web browser: http://127.0.0.1:54321/
05-31 12:20:42.998 127.0.0.1:54321       5280   main      INFO:



Then, via R I run the following
(db/server details masked):


#packages
require(h2o)

ip          <-      '127.0.0.1'
port        <-      54321
h2o.init(ip = ip, port = port)

#db details
dbip        <-      'XX.XX.XX.XX'
dbport      <-      'XXXX'
dblogin     <-      'login'
dbpass      <-      'password'

#test connection
test <- h2o.import_sql_select(
connection_url = paste(""jdbc:mysql://"", dbip, "":"", dbport, ""/myDB?useSSL=false"", sep = ''),
select_query = 'select * from myTable limit 1000',
username = dblogin,
password = dbpass)



which returns the following errors:


ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://127.0.0.1:54321/99/ImportSQLTable)

java.lang.RuntimeException
[1] ""java.lang.RuntimeException: SQLException: Communications link failure\n\nThe last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.\nFailed to connect and read from SQL database with connection_url: jdbc:mysql://XX.XX.XX.XX:XXXX/assumption_matrix?useSSL=false""
[2] ""    water.jdbc.SQLManager.importSqlTable(SQLManager.java:135)""                                                                                                                                                                                                                                                                              
[3] ""    water.api.ImportSQLTableHandler.importSQLTable(ImportSQLTableHandler.java:15)""                                                                                                                                                                                                                                                          
[4] ""    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                                                                                                                                                                                                                                                                            
[5] ""    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)""                                                                                                                                                                                                                                                                            
[6] ""    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)""                                                                                                                                                                                                                                                                        
[7] ""    java.lang.reflect.Method.invoke(Unknown Source)""                                                                                                                                                                                                                                                                                        
[8] ""    water.api.Handler.handle(Handler.java:62)""                                                                                                                                                                                                                                                                                              
[9] ""    water.api.RequestServer.handle(RequestServer.java:655)""                                                                                                                                                                                                                                                                                 
[10] ""    water.api.RequestServer.serve(RequestServer.java:596)""                                                                                                                                                                                                                                                                                  
[11] ""    water.JettyHTTPD$H2oDefaultServlet.doGeneric(JettyHTTPD.java:745)""                                                                                                                                                                                                                                                                      
[12] ""    water.JettyHTTPD$H2oDefaultServlet.doPost(JettyHTTPD.java:681)""                                                                                                                                                                                                                                                                         
[13] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                                                                                                                                                                                                                                                                           
[14] ""    javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                                                                                                                                                                                                                                                                           
[15] ""    org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""                                                                                                                                                                                                                                                                 
[16] ""    org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:503)""                                                                                                                                                                                                                                                             
[17] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)""                                                                                                                                                                                                                                                          
[18] ""    org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)""                                                                                                                                                                                                                                                            
[19] ""    org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)""                                                                                                                                                                                                                                                      
[20] ""    org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)""                                                                                                                                                                                                                                                     
[21] ""    org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429)""                                                                                                                                                                                                                                                              
[22] ""    org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)""                                                                                                                                                                                                                                                       
[23] ""    org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)""                                                                                                                                                                                                                                                      
[24] ""    org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)""                                                                                                                                                                                                                                                          
[25] ""    org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)""                                                                                                                                                                                                                                                  
[26] ""    org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)""                                                                                                                                                                                                                                                        
[27] ""    org.eclipse.jetty.server.Server.handle(Server.java:370)""                                                                                                                                                                                                                                                                                
[28] ""    org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)""                                                                                                                                                                                                                                         
[29] ""    org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)""                                                                                                                                                                                                                                          
[30] ""    org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)""                                                                                                                                                                                                                                               
[31] ""    org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)""                                                                                                                                                                                                                               
[32] ""    org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)""                                                                                                                                                                                                                                                                       
[33] ""    org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)""                                                                                                                                                                                                                                                                  
[34] ""    org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)""                                                                                                                                                                                                                                                 
[35] ""    org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)""                                                                                                                                                                                                                                           
[36] ""    org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)""                                                                                                                                                                                                                                                       
[37] ""    org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)""                                                                                                                                                                                                                                                        
[38] ""    java.lang.Thread.run(Unknown Source)""                                                                                                                                                                                                                                                                                                   

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 


ERROR MESSAGE:

SQLException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
Failed to connect and read from SQL database with connection_url: jdbc:mysql://XX.XX.XX.XX:XXXX/myDB?useSSL=false



I'm stumped and would appreciate any help.  I am able to connect to this server via ODBC without trouble.  Thanks!","['mysql', 'r', 'h2o']",93i7hdjb,https://stackoverflow.com/users/5724749/93i7hdjb,"1,196"
37529184,37529184,2016-05-30T15:24:14,2016-05-31 12:25:40Z,0,"If I start H2O using Jetty HashLoginService (i.e. 
-hash_login -login_conf realm.properties
) in order to protect my H2O instance with password, I cannot convert H2O object into R object.


For example in the following 
as.data.frame
 will not work, as if would work if I didn't use HashLoginService:


> require(h2o)
> h2o.init(port     = 54324,  startH2O = F, 
           password = ""pass"", username = ""uname"")
 Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         5 days 15 hours 
    H2O cluster version:        3.9.1.3405 
    H2O cluster name:           uname 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   22.19 GB 
    H2O cluster total cores:    8 
    H2O cluster allowed cores:  2 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54324 
    H2O Connection proxy:       NA 
    R Version:                  R version 3.1.1 (2014-07-10)

> packageVersion(""h2o"")
[1] ‘3.9.1.3405’

> df  <- data.frame(a = c(0.2, 0.3, 0.1, 0.5, 0.1, 0.2),
                    b = c(0.1, 0.1, 0.7, 0.3, 0.2, 0.9))

> h2o_df <- as.h2o(df, ""h2o_df"")
  |==============================================================================| 100%

> head(h2o_df)
     a    b
1  0.2  0.1
2  0.3  0.1
3  0.1  0.7
4  0.5  0.3
5  0.1  0.2
6  0.2  0.9

> class(h2o_df)
[1] ""H2OFrame""

> as.data.frame(h2o_df)
                                                                X.html.
1                                                                <head>
2  <meta http-equiv=Content-Type content=text/html;charset=ISO-8859-1/>
3                                 <title>Error 401 Unauthorized</title>
4                                                               </head>
5                                                                <body>
6                                              <h2>HTTP ERROR: 401</h2>
7                      <p>Problem accessing /3/DownloadDataset. Reason:
8                                       <pre>    Unauthorized</pre></p>
9                       <hr /><i><small>Powered by Jetty://</small></i>
10                                                                     
11                                                                     
12                                                                     
13                                                                     
14                                                                     
15                                                                     
16                                                                     
17                                                                     
18                                                                     
19                                                                     
20                                                                     
21                                                                     
22                                                                     
23                                                                     
24                                                                     
25                                                                     
26                                                                     
27                                                                     
28                                                                     
29                                                                     
30                                                              </body>
31                                                              </html>
Warning message:
In read.table(file = file, header = header, sep = sep, quote = quote,  :
  cols = 1 != length(data) = 11



Any idea how to resolve this problem?




UPDATE


This is a bug and you can track it here: 
https://0xdata.atlassian.net/browse/PUBDEV-2968","['r', 'jetty', 'h2o']",Unknown,,N/A
37504691,37504691,2016-05-28T23:28:09,2016-05-30 13:30:14Z,390,"I am a newbie to H2O and spark framework and I am having troubles with on boarding 
H2O+Spark (sparkling-water)
 PySparkling in Databricks. I have a 12 worker cluster running in Databricks in 1.5.2 environment.


Steps I took were as following: 

1. Attach (Installed) necessary libraries (six, requests, tabulate, and future) required by 
H2O
 to my cluster




Then, I took the necessary .egg file from 
sparkling-water-1.5.14/py/dist
 folder after unzipping it from the sparkling-water-1.5.14.zip package.


I also attached the 
sparkling-water-assembly-1.5.14.jar
 to my Databricks cluster


I am able to 
import h2o
 successfully. however, when I run the following cell in my python NB in Databricks, I am getting exception below:


Initiate H2OContext on top of Spark


from pysparkling import * 
hc = H2OContext(sc).start()
import h2o




I am getting following error


py4j.Py4JException: Method addURL([class java.net.URL]) does not exist



Sincerely appreciate any guidance on how to resolve this exception.","['python', 'pyspark', 'jupyter-notebook', 'h2o', 'sparkling-water']",Thomas K,https://stackoverflow.com/users/434217/thomas-k,40.2k
37492064,37492064,2016-05-27T20:51:25,2016-05-28 04:02:35Z,91,"I am wondering how to find and replace certain values of data in H2O flow.  For example, if a line of data is '0.003 8.938 0.005 9.999' I would like to find all of the '9.999' and replace with NaN, so sort of like sed?
Is there a way to do this directly in the Flow interface?
Thanks",['h2o'],xiansch,https://stackoverflow.com/users/6392750/xiansch,75
37482861,37482861,2016-05-27T11:53:36,2016-05-27 11:53:36Z,128,"Im trying to do a merge with two h2o.frames (frame1.shape: (470 Mio, 90), frame2.shape: (1Mio, 10)) but it raises my the same error all the time, it says Temp ID already exists.


frame1.merge(frame2, all_x=False, all_y=False, by_x=[3], by_y=[0])


H2O 3.8.2.3 python merge fails on big table >400 Mio rows with error 


ERROR MESSAGE:


Temp ID py_3 already exists




I restarted the Cluster already


It works just fine with smaller row size of frame1




Is this a bug? Or is there a limit to the size of the dataframes for merges?","['python', 'merge', 'h2o']",BenWhite,https://stackoverflow.com/users/6390755/benwhite,113
37436121,37436121,2016-05-25T11:43:51,2016-08-27 17:30:05Z,0,"So I've been trying H2O for some time now. Once I have a working model, how do I export it for external use? Meaning, I want to plug in the neural-network I just got to real and live data, and get predictions. I saw there's something called POJO (which is basically Java), but the tutorials I read weren't very clear on how I plug it in to real data. So how do I do it (using H2O Flow)?",['h2o'],jangorecki,https://stackoverflow.com/users/2490497/jangorecki,16.7k
37400821,37400821,2016-05-23T21:25:06,2019-03-29 09:53:41Z,858,"I am trying to use python h2o in my company.


After using the command:


>import h2o
> h2o.init()



I got


h2o\connection.py:110: UserWarning: Proxy environment variable `HTTP_PROXY` with value `http://username:password@proxy.**.com:8080` found. This may interfere with your H2O Connection.
warnings.warn(""Proxy environment variable `"" + name + ""` with value `"" + value + ""` found. This may interfere with your H2O Connection."")

h2o\connection.py:110: UserWarning: Proxy environment variable `HTTPS_PROXY` with value `https://username:password@proxy.**.com:8080` found. This may interfere with your H2O Connection.
warnings.warn(""Proxy environment variable `"" + name + ""` with value `"" + value + ""` found. This may interfere with your H2O Connection."")


No instance found at ip and port: localhost:54321. Trying to start local jar...


JVM stdout: c:\users\zchen6\appdata\local\temp\tmpqwjtad\h2o_zchen6_started_from_python.out
JVM stderr: c:\users\zchen6\appdata\local\temp\tmpegwtrj\h2o_zchen6_started_from_python.err
Using ice_root: c:\users\zchen6\appdata\local\temp\tmpftvwrm

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""h2o\h2o.py"", line 722, in init
 password=password,max_mem_size_GB=max_mem_size_GB,min_mem_size_GB=min_mem_size_GB,proxies=proxies,size=size)

  File ""h2o\connection.py"", line 133, in __init__
    cld = self._start_local_h2o_jar(max_mem_size, min_mem_size, enable_assertions, license, ice_root, jar_path, nthreads)

  File ""h2o\connection.py"", line 269, in _start_local_h2o_jar
    jver = subprocess.check_output([command, ""-version""], stderr=subprocess.STDOUT)

  File ""C:\Anaconda2\lib\subprocess.py"", line 566, in check_output
   process = Popen(stdout=PIPE, *popenargs, **kwargs)

  File ""C:\Anaconda2\lib\subprocess.py"", line 710, in __init__
   errread, errwrite)

  File ""C:\Anaconda2\lib\subprocess.py"", line 913, in _execute_child
   args = list2cmdline(args)

 File ""C:\Anaconda2\lib\subprocess.py"", line 616, in list2cmdline
  needquote = ("" "" in arg) or (""\t"" in arg) or not arg

TypeError: argument of type 'NoneType' is not iterable



I tried to set the value to the variable named proxy in init(), it still gave me the same kind of error.


After trying


h2o.init(start_h2o=False)



I got


h2o\connection.py:110: UserWarning: Proxy environment variable `HTTP_PROXY` with value `http://username:password@proxy.**.com:8080` found. This may interfere with your H2O Connection.
  warnings.warn(""Proxy environment variable `"" + name + ""` with value `"" + value + ""` found. This may interfere with your H2O Connection."")

h2o\connection.py:110: UserWarning: Proxy environment variable `HTTPS_PROXY` with value `https://username:password@proxy.**.com:8080` found. This may interfere with your H2O Connection.
  warnings.warn(""Proxy environment variable `"" + name + ""` with value `"" + value + ""` found. This may interfere with your H2O Connection."")


Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""h2o\h2o.py"", line 722, in init
    password=password,max_mem_size_GB=max_mem_size_GB,min_mem_size_GB=min_mem_size_GB,proxies=proxies,size=size)

  File ""h2o\connection.py"", line 123, in __init__
    if not start_h2o: raise ValueError(""Cannot connect to H2O server. Please check that H2O is running at {}"".format(H2OConnection.make_url("""")))
ValueError: Cannot connect to H2O server. Please check that H2O is running at http://localhost:54321/3/



By the way, the h2o version I used is 3.8.2.5.
Thanks in advance.","['python', 'windows', 'initialization', 'h2o']",Z. Chen,https://stackoverflow.com/users/5942875/z-chen,31
37400149,37400149,2016-05-23T20:40:41,2017-06-08 18:09:42Z,0,"I'm trying to connect to the cluster hosted on EC2 machine from R and getting the same error when trying both on Windows and Mac:


> h2o.init(ip = ""<Public IP>"")
 Connection successful!


ERROR: Unexpected HTTP Status code: 404 Not Found (url = http://<Public IP>:54321/3/Cloud?skip_ticks=true)

Error: lexical error: invalid char in json text.
                                       <!DOCTYPE html> <html lang=""en""
                     (right here) ------^





Cluster is reachable at 
http://<Public IP>:54321/


Starting a local cluster with 
h2o.init()
 also works fine in R, so the problem is only when trying to connect to 
remote
 one.




I've seen the following 
issue
 marked as resolved, but it doesn't help in my case. Have anybody experienced anything similar?




UPD:
 The answer was very simple. It turns out that the code example given in their 
guide for EC2
 is outdated and uses the old version of H2O. Using the most recent version (
3.9.1.5555
 at the moment) on EC2 machines has resolved the issue.","['r', 'amazon-ec2', 'h2o']",Unknown,,N/A
37344958,37344958,2016-05-20T10:59:37,2016-05-24 19:35:24Z,0,"How can i do a join on two frames in h2o flow? I want to join the first column of one frame with the first column a second frame, the second column of one frame with the second column of a second frame and so on.","['r', 'h2o']",dagrun,https://stackoverflow.com/users/3632362/dagrun,651
37311633,37311633,2016-05-18T23:25:46,2016-05-19 09:57:05Z,0,"I made a random forest in h2o using r.  It has trees and leaves.   


I want to know how many leaves.  I like to compare my total number of rows to leaves.


I have 200 trees, 8 layers deep, require 5 rows per terminal leaf.  Am I traversing much of my data?  I have 20k rows.  


Is there a clean way to count the number of leaves in the h2o randomForest?","['r', 'random-forest', 'h2o']",EngrStudent,https://stackoverflow.com/users/2259468/engrstudent,"1,972"
37286030,37286030,2016-05-17T20:51:23,2016-07-12 14:12:27Z,0,"I am reading an 
H2OFrame
 from a CSV file: 


val h2oFrame = new H2OFrame(new File(inputCsvFilePath))


How can I perform an equivalent of a 
.filter()
 operation (as available for Spark 
DataFrame
 or 
RDD
). For example, how do I get a new 
H2OFrame
 where ""label"" (which is a column name) is 
>1
? 


I have tried converting to a 
org.apache.spark.sql.DataFrame
 as below (simplified example):


val df = asDataFrame(h2oFrame)
val dff = df.filter(s""label > 1"")
print(dff.toString(0,15))



But this seems to throw 
OutOfMemoryError
 like below:




Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""Executor task launch worker-2""","['scala', 'h2o', 'sparkling-water']",Unknown,,N/A
37216280,37216280,2016-05-13T17:44:30,2017-11-08 12:00:40Z,295,"I start the sparkling-shell with the following command.  


./bin/sparkling-shell --num-executors 4 --executor-memory 4g --master yarn-client 


I only ever get two executors.  Is this an H2o problem, YARN problem, or Spark problem?


Mike","['hadoop', 'apache-spark', 'h2o', 'sparkling-water']",uh_big_mike_boi,https://stackoverflow.com/users/1472831/uh-big-mike-boi,"3,440"
37096589,37096589,2016-05-08T05:36:17,2016-05-18 16:27:14Z,679,"When I start H2o on a cdh cluster I get the following error.  I downloaded everything formt he wbesite and followed the tutorial. The command I ran was 


hadoop jar h2odriver.jar -nodes 2 -mapperXmx 1g -output hdfsOutputDirName  



It shows that containers are not being used.  It's not clear what settings these would be on hadoop.  I have given all settings memory.  It's the 0.0 for memory that doesnt make sense, and why are the containers not using memory.  Is the cluster even running now?


----- YARN cluster metrics -----
Number of YARN worker nodes: 3

----- Nodes -----
Node: http://data-node-3:8042 Rack: /default, RUNNING, 1 containers used, 1.0 / 6.0 GB used, 1 / 4 vcores used
Node: http://data-node-1:8042 Rack: /default, RUNNING, 0 containers used, 0.0 / 6.0 GB used, 0 / 4 vcores used
Node: http://data-node-2:8042 Rack: /default, RUNNING, 0 containers used, 0.0 / 6.0 GB used, 0 / 4 vcores used

----- Queues -----
Queue name:            root.default
    Queue state:       RUNNING
    Current capacity:  0.00
    Capacity:          0.00
    Maximum capacity:  -1.00
    Application count: 0

Queue 'root.default' approximate utilization: 0.0 / 0.0 GB used, 0 / 0 vcores used

----------------------------------------------------------------------

WARNING: Job memory request (2.2 GB) exceeds queue available memory capacity (0.0 GB)
WARNING: Job virtual cores request (2) exceeds queue available virtual cores capacity (0)

----------------------------------------------------------------------

For YARN users, logs command is 'yarn logs -applicationId application_1462681033282_0008'","['hadoop', 'hadoop-yarn', 'h2o']",uh_big_mike_boi,https://stackoverflow.com/users/1472831/uh-big-mike-boi,"3,440"
37062476,37062476,2016-05-06T00:42:21,2016-05-07 08:21:10Z,773,"I am training a 
DRFModel
 and while evaluating receiving an exception: 
Exception in thread ""main"" java.lang.ClassCastException: hex.ModelMetricsRegression cannot be cast to hex.ModelMetricsBinomial
.


The data has a column called ""label"" that contains 0 or 1, and that is the target column: 
dRFParameters._response_column = ""label""
. Looks like the model treats the target column values as real numbers. 


I had this problem with the python API as well and fixed by using the following on the 
H2OFrame
: 
hdf['label'] = hdf['label'].asfactor()
. I am new to scala and h2o. I was wondering what is the best way to force h2o to treat the target column in the 
H2OFrame
 to be binary (Integer). 


(This is my first question on stack overflow. Let me know if I need to be more specific or attach the entire code. Thanks.)","['scala', 'h2o', 'sparkling-water']",S.P.,https://stackoverflow.com/users/6298061/s-p,41
37017165,37017165,2016-05-04T00:25:33,2018-12-27 03:12:57Z,0,"Looking for an efficient way to plot trees in rstudio, H2O's Flow or in local html page from h2o's RF and GBM models similar to the one in the image in link below. 
Specifically, how do you plot trees for the objects, (fitted models) rf1 and gbm2 produced by code below perhaps by parsing h2o.download_pojo(rf1) or h2o.download_pojo(gbm1)?
 




# # The following two commands remove any previously installed H2O packages for R.
# if (""package:h2o"" %in% search()) { detach(""package:h2o"", unload=TRUE) }
# if (""h2o"" %in% rownames(installed.packages())) { remove.packages(""h2o"") }

# # Next, we download packages that H2O depends on.
# pkgs <- c(""methods"",""statmod"",""stats"",""graphics"",""RCurl"",""jsonlite"",""tools"",""utils"")
# for (pkg in pkgs) {
#   if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
# }
# 
# # Now we download, install h2o package
# install.packages(""h2o"", type=""source"", repos=(c(""http://h2o-release.s3.amazonaws.com/h2o/rel-turchin/3/R"")))
library(h2o)

h2o.init(nthreads = -1, max_mem_size = ""2G"")
h2o.removeAll()  ##clean slate - just in case the cluster was already running

## Load data - available to download from link below
## https://www.dropbox.com/s/gu8e2o0mzlozbu4/SampleData.csv?dl=0
df <- h2o.importFile(path = normalizePath(""../SampleData.csv""))

splits <- h2o.splitFrame(df, c(0.4, 0.3), seed = 1234)

train <- h2o.assign(splits[[1]], ""train.hex"")
valid <- h2o.assign(splits[[2]], ""valid.hex"")
test <- h2o.assign(splits[[2]], ""test.hex"")

predictor_col_start_pos <- 2
predictor_col_end_pos <- 169
predicted_col_pos <- 1

rf1 <- h2o.randomForest(training_frame = train, validation_frame = valid, 
                        x = predictor_col_start_pos:predictor_col_end_pos, y = predicted_col_pos, 
                        model_id = ""rf_covType_v1"", ntrees = 2000, stopping_rounds = 10, score_each_iteration = T, 
                        seed = 2001)

gbm1 <- h2o.gbm(training_frame = train, validation_frame = valid, x = predictor_col_start_pos:predictor_col_end_pos, 
            y = predicted_col_pos, model_id = ""gbm_covType2"", seed = 2002, ntrees = 20, 
            learn_rate = 0.2, max_depth = 10, stopping_rounds = 2, stopping_tolerance = 0.01, 
            score_each_iteration = T)


## Next step would be to plot trees for fitted models rf1 and gbm2
# print the model, POJO (Plain Old Java Object) to screen
h2o.download_pojo(rf1)
h2o.download_pojo(gbm1)","['r', 'data-visualization', 'random-forest', 'h2o', 'gbm']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
36927443,36927443,2016-04-29T00:08:06,2016-05-01 21:43:18Z,0,"I want to try out H2O at home, on my commodity computers. How can I join them into the cluster?


Do I need to create a Hadoop cluster first? 


Where can I find documentation, that could help me?",['h2o'],Adam Ryczkowski,https://stackoverflow.com/users/1261153/adam-ryczkowski,"8,054"
36919095,36919095,2016-04-28T15:26:38,2016-05-01 16:56:28Z,818,"I have an autogenerated Java class that I'm using in a Scala application. Something like:


public class Model123123 extends GenModel {
  public int nclasses() { return 4; }

  // Names of columns used by model.
  public static final String[] NAMES = NamesHolder_Model123123.VALUES;



I can create instances like this


val model = Class
          .forName(""Model123123"")
          .newInstance()
          .asInstanceOf[GenModel]



I'd like to access the static members of this Java class. I can do it directly, like this:


Model123123.NAMES



but don't understand how to do it via reflection. I've tried: 


scala> Class.forName(""Model123123"").NAMES
<console>:10: error: value NAMES is not a member of Class[?0]
              Class.forName(model_name).NAMES



and 


scala> model.getClass.NAMES
<console>:11: error: value NAMES is not a member of Class[?0]
              model.getClass.NAMES



I don't know a ton about Java or scala reflection, so I'm a bit lost. I'm trying to do this via reflection as I will have many classes that subclass the same parent class and I'd like to change the class dynamically at runtime. 


Thanks","['java', 'scala', 'reflection', 'h2o']",benlaird,https://stackoverflow.com/users/2339381/benlaird,849
36894000,36894000,2016-04-27T15:16:31,2016-04-27 17:55:51Z,411,"I have a 2000 rows data frame and I'm trying to slice the same data frame into two and combine them together. 


t1 = test[:10, :]
t2 = test[20:, :]
temp = t1.rbind(t2)
temp.show()



Then I got this error:


---------------------------------------------------------------------------
EnvironmentError                          Traceback (most recent call last)
<ipython-input-37-8daeb3375743> in <module>()
      2 t2 = test[20:, :]
      3 temp = t1.rbind(t2)
----> 4 temp.show()
      5 print len(temp)
      6 print len(test)

/usr/local/lib/python2.7/dist-packages/h2o/frame.pyc in show(self, use_pandas)
    383       print(""This H2OFrame has been removed."")
    384       return
--> 385     if not self._ex._cache.is_valid(): self._frame()._ex._cache.fill()
    386     if H2ODisplay._in_ipy():
    387       import IPython.display

/usr/local/lib/python2.7/dist-packages/h2o/frame.pyc in _frame(self, fill_cache)
    423 
    424   def _frame(self, fill_cache=False):
--> 425     self._ex._eager_frame()
    426     if fill_cache:
    427       self._ex._cache.fill()

/usr/local/lib/python2.7/dist-packages/h2o/expr.pyc in _eager_frame(self)
     67     if not self._cache.is_empty(): return self
     68     if self._cache._id is not None: return self  # Data already computed under ID, but not cached locally
---> 69     return self._eval_driver(True)
     70 
     71   def _eager_scalar(self):  # returns a scalar (or a list of scalars)

/usr/local/lib/python2.7/dist-packages/h2o/expr.pyc in _eval_driver(self, top)
     81   def _eval_driver(self, top):
     82     exec_str = self._do_it(top)
---> 83     res = ExprNode.rapids(exec_str)
     84     if 'scalar' in res:
     85       if isinstance(res['scalar'], list): self._cache._data = [float(x) for x in res['scalar']]

/usr/local/lib/python2.7/dist-packages/h2o/expr.pyc in rapids(expr)
    163       The JSON response (as a python dictionary) of the Rapids execution
    164     """"""
--> 165     return H2OConnection.post_json(""Rapids"", ast=expr,session_id=H2OConnection.session_id(), _rest_version=99)
    166 
    167 class ASTId:

/usr/local/lib/python2.7/dist-packages/h2o/connection.pyc in post_json(url_suffix, file_upload_info, **kwargs)
    515     if __H2OCONN__ is None:
    516       raise ValueError(""No h2o connection. Did you run `h2o.init()` ?"")
--> 517     return __H2OCONN__._rest_json(url_suffix, ""POST"", file_upload_info, **kwargs)
    518 
    519   def _rest_json(self, url_suffix, method, file_upload_info, **kwargs):

/usr/local/lib/python2.7/dist-packages/h2o/connection.pyc in _rest_json(self, url_suffix, method, file_upload_info, **kwargs)
    518 
    519   def _rest_json(self, url_suffix, method, file_upload_info, **kwargs):
--> 520     raw_txt = self._do_raw_rest(url_suffix, method, file_upload_info, **kwargs)
    521     return self._process_tables(raw_txt.json())
    522 

/usr/local/lib/python2.7/dist-packages/h2o/connection.pyc in _do_raw_rest(self, url_suffix, method, file_upload_info, **kwargs)
    592       raise EnvironmentError((""h2o-py got an unexpected HTTP status code:\n {} {} (method = {}; url = {}). \n""+ \
    593                               ""detailed error messages: {}"")
--> 594                               .format(http_result.status_code,http_result.reason,method,url,detailed_error_msgs))
    595 
    596 

EnvironmentError: h2o-py got an unexpected HTTP status code:
500 Server Error (method = POST; url = http://localhost:54321/99/Rapids). 
detailed error messages: []



If I count rows (len(temp)), it works find. Also if I change the slicing index a little bit, it works find too. For example, if I change to this, it shows the data frame.


t1 = test[:10, :]
t2 = test[:5,  :]



Do I miss something here? Thanks.","['python', 'h2o']",hamuchiwa,https://stackoverflow.com/users/6262091/hamuchiwa,21
36811862,36811862,2016-04-23T14:09:14,2016-04-23 16:54:49Z,0,"I am newbie in h2o implementation in R. I have such a data frame(df1):


df<-structure(list(v1 = c(5.24823, 0.839, 3.57348, 1.47869, 2.75093, 
1.69665, 0.46366, 1.53827, 2.0149, 2.32103, 1.87223, 2.3392, 
2.10579, 1.7236, 1.13056, 1.09144, 3.52515, 1.16248, 1.77885, 
0.9991, 0.47375, 2.91148, 1.237, 1.18971, 1.23953, 1.07049, 1.46971, 
1.65649, 3.3021, 1.04816), v100 = c(19.60784, 9.27047, 0.5523, 
15.05735, 0.93231, 11.73979, 19.53795, 6.22754, 4.54464, 17.0922, 
3.60958, 18.23052, 0.06395, 17.17605, 5.52724, 17.85276, 15.57143, 
0.05825, 19.85401, 14.51163, 6.64372, 19.60284, 16.40279, 16.89205, 
19.6748, 14.64446, 19.34747, 9.04215, 11.37993, 16.81159), v101 = c(10.71683, 
7.13707, 3.61956, 9.75558, 4.21413, 8.49785, 6.79572, 5.19486, 
7.39523, 6.05496, 2.91676, 9.82552, 5.5107, 5.40719, 10.82138, 
12.37154, 5.56351, 3.8549, 9.87455, 5.37746, 3.57747, 8.11406, 
6.61883, 7.3667, 7.74248, 12.44785, 12.38174, 5.99648, 7.10452, 
8.27756)), .Names = c(""v1"", ""v100"", ""v101""), row.names = c(85671L, 
92268L, 44249L, 68218L, 3250L, 105583L, 4874L, 94393L, 83502L, 
61414L, 42987L, 50200L, 80887L, 9321L, 39565L, 79644L, 26265L, 
75272L, 104819L, 72782L, 57101L, 59037L, 78810L, 88619L, 21564L, 
39198L, 55030L, 44193L, 6116L, 101448L), class = ""data.frame"")



I want to make glm using h2o package. So I have the below code:


  library(h2o)
  library(h2oEnsemble)

  modellm<-h2o.glm(y=""v1"", x=""v100"",training_frame=df ,family=""gaussian"",
                   nfolds = 0, alpha = 0.1, lambda_search = FALSE)



However, I get the below error after executing the code:


Error in value[[3L]](cond) : 
  argument ""training_frame"" must be a valid H2OFrame or ID



I tried the below topic:


h2oensemble Error in value[[3L]](cond) : argument ""training_frame"" must be a valid H2O H2OFrame or id


However, it didn't solve my problem. I get the below after execution of the recommended solution at the above link:


> library(devtools)
> install_github(""h2oai/h2o-3/h2o-r/ensemble/h2oEnsemble-package"")
Downloading github repo h2oai/h2o-3@master
Installing h2oEnsemble
""C:/PROGRA~1/R/R-32~1.4R~/bin/x64/R"" --no-site-file --no-environ  \
  --no-save --no-restore CMD INSTALL  \
  ""C:/Users/ozgur/AppData/Local/Temp/RtmpAfGU5K/devtools8f064866e23/h2oai-h2o-3-30ef929/h2o-r/ensemble/h2oEnsemble-package""  \
  --library=""C:/Users/ozgur/Documents/R/win-library/3.2""  \
  --install-tests 

* installing *source* package 'h2oEnsemble' ...
** R
** tests
** preparing package for lazy loading
Warning: package 'h2o' was built under R version 3.2.5
Warning: package 'statmod' was built under R version 3.2.5
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
*** arch - i386
Warning: package 'h2o' was built under R version 3.2.5
Warning: package 'statmod' was built under R version 3.2.5
*** arch - x64
Warning: package 'h2o' was built under R version 3.2.5
Warning: package 'statmod' was built under R version 3.2.5
* DONE (h2oEnsemble)
Reloading installed h2oEnsemble
h2oEnsemble (beta) for H2O >=3.0
Version: 0.1.8
Package created on 2016-03-29  



I will be very glad for any help. Thanks a lot.","['r', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
36811319,36811319,2016-04-23T13:15:27,2016-04-23 13:15:27Z,75,"I start-up H2O with: 
h2o.init(nthreads=-1, max_mem_size=""256m"")
 or 
h2o.init(nthreads=2, max_mem_size=""6g"")
, etc. What I would like to know is the minimum 
max_mem_size
 I can get away with.


E.g. on my development machine I might be happy to give it 6g, but when I move it to a cloud server, where the cost is often proportional to the memory I request, if I can get away with, say, a 1gb instance, it would be wasteful to be renting an 8gb instance.


I think the memory usage is mainly on the data sets? But is there a way to find out the peak memory that was used by a data set, plus the training of a model on it?",['h2o'],Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
36705708,36705708,2016-04-18T22:57:46,2016-08-03 14:01:22Z,0,"When ever I try to run this line or any other line which uses key(following document in 
http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/rtutorial.html
)


iris.hex = h2o.uploadFile(localH2O, path = irisPath, key = ""iris.hex"")



I get an error in the key calling it as unused argument. 


This is the first time I am using H2O and I am new to R as well. Please let me know what is the function of key and only when I run this, I get error. I could create a dataframe with the following statement. But still I would want to understand this key error


h2o.init(ip = ""localhost"", port = 54321, startH2O = TRUE)
irisPath = system.file(""extdata"", ""iris.csv"", package = ""h2o"")
iris.hex = h2o.uploadFile(path = prosPath, destination_frame = ""iris.hex"")
iris.data.frame<- as.data.frame(iris.hex)
summary(iris.data.frame)","['r', 'file-upload', 'key', 'h2o']",Nandhini Charumathy,https://stackoverflow.com/users/6222157/nandhini-charumathy,23
36696541,36696541,2016-04-18T14:15:10,2016-04-19 07:59:50Z,749,"In the Python API for the machine learning library H2O, what is the correct way to convert a sparse Pandas DataFrame object to an H2OFrame object?","['python', 'pandas', 'h2o']",nojka_kruva,https://stackoverflow.com/users/1095879/nojka-kruva,"1,454"
36680046,36680046,2016-04-17T17:50:50,2016-04-18 04:37:47Z,0,"Does h2o make use of all the available cores on an ec2 server (not instance) on the fly?


I ask because I run several instances of rstudio on ec2. Some are bigger, some are smaller. My spaghetti code gets transferred from one window to another.


When setting cores on a t2.micro


h2o.init(nthreads = -1)



It will connect to the ""max"" number of cores I explicitly created for the instance. However,


H2O cluster total nodes:    1 
H2O cluster total memory:   0.23 GB 
H2O cluster total cores:    15 
H2O cluster allowed cores:  1 



Seems to suggest that there are more cores available.


I noticed this when I transferred code over from another instance with 16 cores to the instance with 1 core.


h2o.init(nthreads = 16)



This code created


H2O cluster total nodes:    1 
H2O cluster total memory:   0.23 GB 
H2O cluster total cores:    15 
H2O cluster allowed cores:  15



Another example, on the 16 core instance, I was curious, and set nthreads=128, but it maxes out at 40.


Is h2o grabbing extra allowable cores on the fly? I ask because this would be much easier and quicker than setting up a cluster.","['r', 'amazon-ec2', 'h2o', 'rstudio-server']",Bhargav Rao,https://stackoverflow.com/users/4099593/bhargav-rao,51.9k
36620585,36620585,2016-04-14T10:36:57,2016-08-29 15:53:47Z,0,"Early stopping is turned on by default for 
h2o.deeplearning()
. But, from R, how do I find out if it did stop early, and how many epochs it did? I've tried this:



model = h2o.deeplearning(...)
print(model)



which tells me information on the layers, the MSE, R2, etc. but nothing about how many epochs were run.


Over on Flow I can see the information (e.g. where the x-axis stops in the ""Scoring History - Deviance"" chart, or in the Scoring History table).","['r', 'h2o']",Darren Cook,https://stackoverflow.com/users/841830/darren-cook,28.8k
36583415,36583415,2016-04-12T20:26:14,2016-04-28 22:17:27Z,826,"I have some trouble with Sparkling Water to run a python script as a Spark Application. I use this command to execute my script on Spark :




./bin/spark-submit \


--packages ai.h2o:sparkling-water-core_2.10:1.5.12 \


--py-files $SPARKLING_HOME/py/dist/pySparkling-1.5.12-py2.7.egg  $SPARKLING_HOME/Python/test.py 




and I have this falling error : 




py4j.protocol.Py4JError: Trying to call a package.




logs : 


> Traceback (most recent call last):
  File ""/Users/Documents/sparkling-water-1.5.12/Python/test.py"", line 5, in <module>
    hc= H2OContext(sc).start()
  File ""/Users/Documents/sparkling-water-1.5.12/py/dist/pySparkling-1.5.12-py2.7.egg/pysparkling/context.py"", line 72, in __init__
  File ""/Users/Documents/sparkling-water-1.5.12/py/dist/pySparkling-1.5.12-py2.7.egg/pysparkling/context.py"", line 96, in _do_init
  File ""/Users/Documents/spark-1.5.2-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 726, in __getattr__
py4j.protocol.Py4JError: Trying to call a package.
16/04/11 16:58:39 INFO SparkContext: Invoking stop() from shutdown hook
16/04/11 16:58:39 INFO SparkUI: Stopped Spark web UI at http://192.168.181.84:4042
16/04/11 16:58:39 INFO DAGScheduler: Stopping DAGScheduler
16/04/11 16:58:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/04/11 16:58:39 INFO MemoryStore: MemoryStore cleared
16/04/11 16:58:39 INFO BlockManager: BlockManager stopped
16/04/11 16:58:39 INFO BlockManagerMaster: BlockManagerMaster stopped
16/04/11 16:58:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/04/11 16:58:39 INFO SparkContext: Successfully stopped SparkContext
16/04/11 16:58:39 INFO ShutdownHookManager: Shutdown hook called
16/04/11 16:58:39 INFO ShutdownHookManager: Deleting directory /private/var/fold



How can I resolve this issue ? I am following exactly the command from the booklet : 
https://h2o-release.s3.amazonaws.com/h2o/rel-turan/3/docs-website/h2o-docs/booklets/SparklingWaterVignette.pdf","['python', 'pyspark', 'h2o', 'sparkling-water']",pierre_comalada,https://stackoverflow.com/users/6195399/pierre-comalada,300
36547434,36547434,2016-04-11T11:28:21,2017-04-07 23:05:36Z,0,"I am trying to use H2O package in R with 32-bit java. Unfortunately I am restricted by the comapny's IT to install the 64 bit version of java.


How can I make H2O work with 32-bit java, i.e. if possible?


OS - Windows 7","['r', 'windows', 'windows-7', 'h2o']",smci,https://stackoverflow.com/users/202229/smci,33.8k
36483153,36483153,2016-04-07T17:22:30,2018-06-21 00:46:26Z,483,"This question is similar to this 
Converting R dataframe to H2O Frame without writing to disk

except applicable to Java object 


My data is generated in Java application, then saved as text and passed to H2O (through R or Flow). I guess I can avoid some overhead if I create (and save) H2O DataFrames on the fly inside my application. I suspect it's pretty straightforward, but quick look at the 
docs
 didn't give an easy (SO-style) answer","['java', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
36465240,36465240,2016-04-07T01:40:42,2017-05-18 05:49:16Z,493,"When running the 
sparkling-shell
 :


bin/sparkling-shell



The following error occurs:


org.apache.spark.SparkException: Asked to launch cluster with 2048 MB RAM / worker but requested 20480 MB/worker



We have set following in 
spark-env.sh
 :


export SPARK_WORKER_MEMORY=108GB
export SPARK_WORKER_INSTANCES=4



And the following in 
spark-defaults.conf
 :


spark.executor.memory   20g
spark.driver.memory     8g



The only way we can get the shell to launch is to reset to 2GB the worker memory:


spark.executor.memory   2g



But that is simply way insufficient for running our jobs.   Anyone found a workaround for this issue?","['apache-spark', 'h2o', 'sparkling-water']",WestCoastProjects,https://stackoverflow.com/users/1056563/westcoastprojects,62.7k
36434082,36434082,2016-04-05T18:19:43,2016-04-06 00:27:51Z,0,"Closed
. This question needs 
details or clarity
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Add details and clarify the problem by 
editing this post
.






Closed 
8 years ago
.















                        Improve this question
                    








H2O Deep Learning is running regression by default even though I have ensured that the target variable is a factor (with only two levels). Any leads on how to resolve this ?


Below is the code :


dnn_mod <- 
  h2o.deeplearning(x = 2:321,  # column numbers for predictors
                   y = 322,   # column number for label
                   training_frame = sdcs_data, # data in H2O format
                   activation = ""TanhWithDropout"", # or 'Tanh'
                   input_dropout_ratio = 0.2, # % of inputs dropout
                   hidden_dropout_ratios = c(0.3,0.3,0.3), # % for nodes dropout
                   balance_classes = FALSE, 
                   hidden = c(150,150,150),
                   epochs = 500,
                   #standardize = TRUE,
                   epsilon = 1.0e-5,
                   loss = ""CrossEntropy"",
                   stopping_rounds = 50,
                   stopping_metric = ""AUC"")
                   #classification = TRUE)","['r', 'machine-learning', 'h2o']",Vibhor Kalra,https://stackoverflow.com/users/4425827/vibhor-kalra,71
36242191,36242191,2016-03-26T23:43:08,2016-03-27 03:23:20Z,113,"I have installed h2o on my hortonworks sandbox 2.1 platform which is on virtual box. It works fine at localhost:54321. But, when I restart virtual box, it does not launch at localhost:54321 again. How can I start it?","['hadoop', 'sandbox', 'hortonworks-data-platform', 'h2o']",Ceng,https://stackoverflow.com/users/5175342/ceng,57
36212815,36212815,2016-03-25T01:44:38,2016-03-25 20:07:37Z,338,"I am taking input values from a django model admin screen and on pre_save calling h2o to do predictions for other values and save them. 


Currently I convert my input from pandas (trying to work with sklearn preprocessing easily here) by using:


modelH2OFrame = h2o.H2OFrame(python_obj = model_data_frame.to_dict('list'))
 


It parses and loads. Hell it even creates a frame with values when I do it step by step.


BUT. When I run this inside of the Django pre_save, the H2OFrame comes back completely empty. 


Ideas for why this may be happening? Sometimes I get errors connecting to the h2o cluster or timeouts--maybe that is a related issue? I load the H2O models in the pre_save call and do the predictions, allocate them to model fields, and then shut down the h2o cluster (in one function).","['python', 'django', 'pandas', 'scikit-learn', 'h2o']",Anisotropic,https://stackoverflow.com/users/5275025/anisotropic,645
36211698,36211698,2016-03-24T23:28:41,2016-03-25 01:51:23Z,583,"I am trying to instal h20 on hortonworks sandbox 2.4 by following 
http://www.h2o.ai/download/h2o/hadoop
. Everything runs well, I see the messages
""Blocking until the H2O cluster shuts down..."" and ""open h2o web flow through 10.0.2.15:54321"".
But when I go to that page, it is not loaded giving ERR_CONNECTION_TIMED_OUT error. 
What should I do to connect h2o web page?
Thanks.","['hadoop', 'sandbox', 'hortonworks-data-platform', 'h2o']",Ceng,https://stackoverflow.com/users/5175342/ceng,57
36080568,36080568,2016-03-18T09:24:09,2016-03-19 01:03:19Z,0,"The folks over at H2O.ai mention in several slides and documents that H2O.ai can run on top of SQL. How do I connect my H2O.ai instance to a SQL Server? The only suggestion I could find so far looks rather disappointing: 
https://groups.google.com/forum/#!topic/h2ostream/x8BLSGbyvhA


Is there a more elegant way to connect directly to a SQL Server?",['h2o'],JimBoy,https://stackoverflow.com/users/2817036/jimboy,597
36059640,36059640,2016-03-17T11:46:14,2017-03-05 17:36:22Z,149,"I try to locally compile a POJO of a GBM prediction model generated with H2o 3.8.1.3; I follow the instructions in the POJO class:




create a folder 


Download the h2o-genmodel.jar into the folder with:


curl http://myh2oinstancesurl:myh2oinstancesport/3/h2o-genmodel.jar > h2o-genmodel.jar



Download the successfully trained GBM model named 154 into the folder with: 


curl http://myh2oinstancesurl:myh2oinstancesport/3/Models/154/java > 154.java



Compile the sources in the folder with javac 1.8.0_45 under Max OSX 10.11.3 or Fedora 23:


 javac -cp h2o-genmodel.jar 154.java



Result are a bunch of compilation errors:


154.java:24: error: <identifier> expected
public class 154 extends GenModel {
        ^
154.java:24: error: illegal start of type
public class 154 extends GenModel {
             ^
154.java:24: error: ';' expected
public class 154 extends GenModel {
                             ^
154.java:25: error: illegal start of expression
  public hex.ModelCategory getModelCategory() { return hex.ModelCategory.Binomial; }
  ^
154.java:25: error: ';' expected
   public hex.ModelCategory getModelCategory() { return hex.ModelCategory.Binomial; }
                                       ^
154.java:27: error: illegal start of expression
   public boolean isSupervised() { return true; }
  ^
154.java:27: error: ';' expected
  public boolean isSupervised() { return true; }
                         ^
154.java:28: error: illegal start of expression
  public int nfeatures() { return 14; }
  ^
154.java:28: error: ';' expected
  public int nfeatures() { return 14; }
                  ^
154.java:29: error: illegal start of expression
  public int nclasses() { return 2; }
  ^



...


100 errors



Is there an issue with my procedure? Or is this a bug with my setup? Is there anybody who currently can compile GBM POJOs? Thanks for your responses!","['javac', 'pojo', 'h2o']",p_r,https://stackoverflow.com/users/5027705/p-r,41
36021137,36021137,2016-03-15T20:09:07,2017-11-11 20:50:43Z,0,"Here is my code:


set.seed(1)

#Boruta on the HouseVotes84 data from mlbench
library(mlbench) #has HouseVotes84 data
library(h2o)     #has rf

#spin up h2o
myh20 <- h2o.init(nthreads = -1)

#read in data, throw some away
data(HouseVotes84)
hvo <- na.omit(HouseVotes84)

#move from R to h2o
mydata <- as.h2o(x=hvo,
                 destination_frame= ""mydata"")

#RF columns (input vs. output)
idxy <- 1
idxx <- 2:ncol(hvo)

#split data
splits <- h2o.splitFrame(mydata,           
                         c(0.8,0.1))     

train <- h2o.assign(splits[[1]], key=""train"")   
valid <- h2o.assign(splits[[2]], key=""valid"") 

# make random forest
my_imp.rf<- h2o.randomForest(y=idxy,x=idxx,
                      training_frame = train,
                      validation_frame = valid,
                      model_id = ""my_imp.rf"",
                      ntrees=200)

# find importance
my_varimp <- h2o.varimp(my_imp.rf)
my_varimp



The output that I am getting is ""variable importance"".


The classic measures are ""mean decrease in accuracy"" and ""mean decrease in gini coefficient"".  


My results are:


> my_varimp
Variable Importances: 
   variable relative_importance scaled_importance percentage
1        V4         3255.193604          1.000000   0.410574
2        V5         1131.646484          0.347643   0.142733
3        V3          921.106567          0.282965   0.116178
4       V12          759.443176          0.233302   0.095788
5       V14          492.264954          0.151224   0.062089
6        V8          342.811554          0.105312   0.043238
7       V11          205.392654          0.063097   0.025906
8        V9          191.110046          0.058709   0.024105
9        V7          169.117676          0.051953   0.021331
10      V15          135.097076          0.041502   0.017040
11      V13          114.906586          0.035299   0.014493
12       V2           51.939777          0.015956   0.006551
13      V10           46.716656          0.014351   0.005892
14       V6           44.336708          0.013620   0.005592
15      V16           34.779987          0.010684   0.004387
16       V1           32.528778          0.009993   0.004103



From this my relative importance of ""Vote #4"" aka V4, is ~3255.2.  


Questions:

What units is that in?
How is that derived?


I tried looking in documentation, but am not finding the answer.  I tried the help documentation.  I tried using Flow to look at parameters to see if anything in there indicated it.  In none of them do I find ""gini"" or ""decrease accuracy"".  Where should I look?","['random-forest', 'h2o', 'gini']",EngrStudent,https://stackoverflow.com/users/2259468/engrstudent,"1,972"
35976589,35976589,2016-03-13T22:18:13,2016-07-23 23:20:19Z,255,"I cloned sparkling droplet project from '
https://github.com/h2oai/h2o-droplets/tree/master/sparkling-water-droplet
'. And cleaned and build project using 
./gradlew clean
 ,  
./gradlew build
 respectively. After  that tried to run project using command


spark-submit --class water.droplets.SparklingWaterDroplet build/libs/sparkling-water-droplet-app.jar

Then got following error message: 


Exception in thread ""main"" java.lang.NoClassDefFoundError: water/fvec/Frame
at water.droplets.SparklingWaterDroplet.main(SparklingWaterDroplet.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

Caused by: java.lang.ClassNotFoundException: water.fvec.Frame
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
... 10 more","['scala', 'apache-spark', 'machine-learning', 'h2o']",maogautam,https://stackoverflow.com/users/1404655/maogautam,318
35892715,35892715,2016-03-09T13:18:28,2016-03-10 00:34:14Z,0,"I have a 5 node hadoop cluster running HDP 2.3.0. I setup a H2O cluster on Yarn as described 
here
.


On running following command 


hadoop jar h2odriver_hdp2.2.jar water.hadoop.h2odriver -libjars ../h2o.jar -mapperXmx 512m -nodes 3 -output /user/hdfs/H2OTestClusterOutput


I get the following ouput


H2O cluster (3 nodes) is up
(Note: Use the -disown option to exit the driver after cluster formation)
(Press Ctrl-C to kill the cluster)
Blocking until the H2O cluster shuts down...



When I try to execute the command 


h2o.init(ip=""10.113.57.98"", port=54321)


The process remains stuck at this stage.On trying to connect to the web UI using the ip:54321, the browser tries to endlessly load the H2O admin page but nothing ever displays.


On forcefully terminating the init process I get the following error


No instance found at ip and port: 10.113.57.98:54321. Trying to start local jar...



However if I try and use H2O with python without setting up a H2O cluster, everything runs fine.


I executed all commands as the root user. Root user has permissions to read and write from the /user/hdfs hdfs directory.


I'm not sure if this is a permissions error or that the port is not accessible.


Any help would be greatly appreciated.",['h2o'],Nitin Kumar,https://stackoverflow.com/users/4087944/nitin-kumar,775
35829491,35829491,2016-03-06T16:39:52,2016-03-07 21:23:26Z,594,"I created model using H2O's Sparkling Water. And now I'd like to apply it to huge Spark DF (populated with sparse vectors). I use python and pyspark, pysparkling. Basically I need to do map job with model.predict() function inside. But copying data into H2O context is huge overhead and not an option. What I think I gonna do is, extract POJO (Java class) model from h2o model and use it to do map in dataframe. My questions are: 




Is there a better way?


How to write pyspark wrapper for java class, from which I intend to use only one method .score(double[] data, double[] result)


How to maximally reuse wrappers from Spark ML library?




Thank you!","['apache-spark', 'pyspark', 'apache-spark-sql', 'h2o', 'py4j']",USER,https://stackoverflow.com/users/4007312/user,93
35763574,35763574,2016-03-03T05:00:39,2016-03-04 03:10:45Z,0,"I have a few hundred thousand very small 
.dat.gz
 files that I want to read into R in the most efficient way possible. I read in the file and then immediately aggregate and discard the data, so I am not worried about managing memory as I get near the end of the process. I just really want to speed up the bottleneck, which happens to be unzipping and reading in the data.


Each dataset consists of 366 rows and 17 columns. Here is a reproducible example of what I am doing so far:


Building reproducible data:


require(data.table)

# Make dir
system(""mkdir practice"")

# Function to create data
create_write_data <- function(file.nm) {
  dt <- data.table(Day=0:365)
  dt[, (paste0(""V"", 1:17)) := lapply(1:17, function(x) rnorm(n=366))]
  write.table(dt, paste0(""./practice/"",file.nm), row.names=FALSE, sep=""\t"", quote=FALSE)
  system(paste0(""gzip ./practice/"", file.nm))    
}



And here is code applying:


# Apply function to create 10 fake zipped data.frames (550 kb on disk)
tmp <- lapply(paste0(""dt"", 1:10,"".dat""), function(x) create_write_data(x))



And here is my most efficient code so far to read in the data:


# Function to read in files as fast as possible
read_Fast <- function(path.gz) {
  system(paste0(""gzip -d "", path.gz)) # Unzip file
  path.dat <- gsub("".gz"", """", path.gz)
  dat_run <- fread(path.dat)
}

# Apply above function
dat.files <- list.files(path=""./practice"", full.names = TRUE)
system.time(dat.list <- rbindlist(lapply(dat.files, read_Fast), fill=TRUE))
dat.list



I have bottled this up in a function and applied it in parallel, but it is still much much too slow for what I need this for.


I have already tried the 
h2o.importFolder
 from the wonderful 
h2o
 package, but it is actually much much slower compared to using plain 
R
 with 
data.table
. Maybe there is a way to speed up the unzipping of files, but I am unsure. From the few times that I have run this, I have noticed that the unzipping of the files usually takes about 2/3rd of the function time.","['r', 'performance', 'data.table', 'h2o']",Unknown,,N/A
35735675,35735675,2016-03-01T23:27:27,2017-03-14 20:48:51Z,0,"I'm trying to use h2o.predict but it's throwing a weird error. Any pointers on how to resolve it?


ERROR: Unexpected HTTP Status code: 400 Bad Request (url = http://localhost:54321/99/Rapids)

java.lang.IllegalArgumentException
 [1] ""water.rapids.ASTTmpAssign.apply(ASTAssign.java:254)""
 [2] ""water.rapids.ASTTmpAssign.apply(ASTAssign.java:248)""
 [3] ""water.rapids.ASTExec.exec(ASTExec.java:46)""
 [4] ""water.rapids.Session.exec(Session.java:56)""
 [5] ""water.rapids.Exec.exec(Exec.java:63)""
 [6] ""water.api.RapidsHandler.exec(RapidsHandler.java:23)""
 [7] ""sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""
 [8] ""sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)""
 [9] ""sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""
[10] ""java.lang.reflect.Method.invoke(Method.java:606)""
[11] ""water.api.Handler.handle(Handler.java:64)""
[12] ""water.api.RequestServer.handle(RequestServer.java:644)""
[13] ""water.api.RequestServer.serve(RequestServer.java:585)""
[14] ""water.JettyHTTPD$H2oDefaultServlet.doGeneric(JettyHTTPD.java:617)""
[15] ""water.JettyHTTPD$H2oDefaultServlet.doPost(JettyHTTPD.java:565)""
[16] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""
[17] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""
[18] ""org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""","['r', 'deep-learning', 'h2o']",Unknown,,N/A
35711772,35711772,2016-02-29T23:24:00,2017-07-14 15:55:02Z,0,"I have a server with 48 processors. 


The server is not virtualized and the h2o sees 48 processors, but 16 of them for some reason are not being used. 


Any advice? 


enter image description here",['h2o'],user3078500,https://stackoverflow.com/users/3078500/user3078500,302
35680363,35680363,2016-02-28T08:35:31,2016-03-01 04:17:47Z,0,"I'm trying to resolve an error that I am looking at while using h2o.predict.


Here's the problem setup:


#If you type class(DL.Model) then output is as follows:
[1] ""H2OMultinomialModel""
     attr(,""package"")
     [1] ""h2o""

xTest   <- as.h2o(xTest) # xTest is data frame in R 
DL.pred <- h2o.predict(DL.Model, xTest)

ERROR: Unexpected HTTP Status code: 404 Not Found (url = http://localhost:54321/3/Predictions/models/DeepLearning_model_R_1449882914034_72/frames/file1ca3d488cb1_csv_61.hex_62)

water.exceptions.H2OKeyNotFoundArgumentException
 [1] ""water.api.ModelMetricsHandler.predict(ModelMetricsHandler.java:209)""                  
 [2] ""sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                          
 [3] ""sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""        
 [4] ""sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""
 [5] ""java.lang.reflect.Method.invoke(Method.java:497)""                                     
 [6] ""water.api.Handler.handle(Handler.java:64)""                                            
 [7] ""water.api.RequestServer.handle(RequestServer.java:644)""                               
 [8] ""water.api.RequestServer.serve(RequestServer.java:585)""                                
 [9] ""water.JettyHTTPD$H2oDefaultServlet.doGeneric(JettyHTTPD.java:617)""                    
[10] ""water.JettyHTTPD$H2oDefaultServlet.doPost(JettyHTTPD.java:565)""                       
[11] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                         
[12] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                         
[13] ""org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""               

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  Object 'DeepLearning_model_R_1449882914034_72' not found in function: predict for argument: model



Any pointers on what might be going on here? I see that there exists a somewhat related error message in another 
question
 but the proposed solution isn't helping either. I'm using H2O version 3.6.0.8.


If we look at the logs as suggested 
here
, the last seven rows of the log is shown below:


065c 4861 a7b3 cea6 7505 00bd fd05 0031
0000 0000 0000 0000 0000 0000 0048 0000
0068 326f 6c6f 6773 5f32 3031 3630 3232
385f 3132 3530 3233 2f6e 6f64 6530 5f31
3237 2e30 2e30 2e31 5f35 3433 3231 2e7a
6970 504b 0506 0000 0000 0200 0200 a500
0000 4d76 0500 0000","['r', 'runtime-error', 'prediction', 'deep-learning', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
35657989,35657989,2016-02-26T17:24:57,2020-05-08 21:24:54Z,0,"I'm using the 
h2o
 package (v 3.6.0) in R, and I've built a grid search model. Now, I'm trying to access the model which minimizes MSE on the validation set. In python's 
sklearn
, this is easily achievable when using 
RandomizedSearchCV
:


## Pseudo code:
grid = RandomizedSearchCV(model, params, n_iter = 5)
grid.fit(X)
best = grid.best_estimator_



This, unfortunately, does not prove as straightforward in h2o. Here's an example you can recreate:


library(h2o)
## assume you got h2o initialized...

X <- as.h2o(iris[1:100,]) # Note: only using top two classes for example 
grid <- h2o.grid(
    algorithm = 'gbm',
    x = names(X[,1:4]),
    y = 'Species',
    training_frame = X,
    hyper_params = list(
        distribution = 'bernoulli',
        ntrees = c(25,50)
    )
)



Viewing 
grid
 prints a wealth of information, including this portion:


> grid
ntrees distribution status_ok                                                                 model_ids
 50    bernoulli        OK Grid_GBM_file1742e107fe5ba_csv_10.hex_11_model_R_1456492736353_16_model_1
 25    bernoulli        OK Grid_GBM_file1742e107fe5ba_csv_10.hex_11_model_R_1456492736353_16_model_0



With a bit of digging, you can access each individual model and view every metric imaginable:


> h2o.getModel(grid@model_ids[[1]])
H2OBinomialModel: gbm
Model ID:  Grid_GBM_file1742e107fe5ba_csv_10.hex_11_model_R_1456492736353_18_model_1 
Model Summary: 
  number_of_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves
1              50                4387         1         1    1.00000          2          2     2.00000


H2OBinomialMetrics: gbm
** Reported on training data. **

MSE:  1.056927e-05
R^2:  0.9999577
LogLoss:  0.003256338
AUC:  1
Gini:  1

Confusion Matrix for F1-optimal threshold:
           setosa versicolor    Error    Rate
setosa         50          0 0.000000   =0/50
versicolor      0         50 0.000000   =0/50
Totals         50         50 0.000000  =0/100

Maximum Metrics: Maximum metrics at their respective thresholds
                      metric threshold    value idx
1                     max f1  0.996749 1.000000   0
2                     max f2  0.996749 1.000000   0
3               max f0point5  0.996749 1.000000   0
4               max accuracy  0.996749 1.000000   0
5              max precision  0.996749 1.000000   0
6           max absolute_MCC  0.996749 1.000000   0
7 max min_per_class_accuracy  0.996749 1.000000   0



And with a 
lot
 of digging, you can finally get to this:


> h2o.getModel(grid@model_ids[[1]])@model$training_metrics@metrics$MSE
[1] 1.056927e-05



This seems like a lot of kludgey work to get down to a metric that ought to be top-level for model selection. In my situation, I've got a grid with hundreds of models, and my current, hacky solution just doesn't seems very ""R-esque"":


model_select_ <- function(grid) {
  model_ids <- grid@model_ids
  min = Inf
  best_model = NULL

  for(model_id in model_ids) {
    model <- h2o.getModel(model_id)
    mse <- model@model$training_metrics@metrics$MSE
    if(mse < min) {
      min <- mse
      best_model <- model
    }
  }

  best_model
}



This seems like overkill for something that is so core to the practice of machine learning, and it just strikes me as odd that h2o would not have a ""cleaner"" method of extracting the optimal model, or at least model metrics. 


Am I missing something? Is there no ""out of the box"" method for selecting the best model?","['python', 'r', 'h2o']",Unknown,,N/A
35472785,35472785,2016-02-18T04:23:30,2017-12-10 09:52:07Z,0,"We have hourly time series data having 2 columns, one is the timestamp and other is the error rate. We used H2O deep-learning model to learn and predict future error-rate but looks like it requires at least 2 features (except timestamp) for creating the model.


Is there any way h2o can learn this type of data (time, value) having only one feature and predict the value given future time?","['r', 'python-2.7', 'machine-learning', 'prediction', 'h2o']",Sourav Chatterjee,https://stackoverflow.com/users/4353319/sourav-chatterjee,55
35466616,35466616,2016-02-17T20:03:13,2016-02-17 20:03:13Z,152,"Problem which I am trying to solve:


We have h2o generated file which consists of multiple java classes - one core class which implements interface which is on the classpath and some other classes which holds values used by the core class. Goal is to have ability to load this file at runtime (save in the database) and execute it when needed, without any application restarts.


Some solutions which can be found on the SO are basically used for one class compilation. Which is not suitable at this time.","['java', 'compilation', 'h2o']",sandris,https://stackoverflow.com/users/1797171/sandris,"1,538"
35460973,35460973,2016-02-17T15:30:41,2016-08-05 18:20:35Z,0,"I am trying the tutorial from 
http://www.r-bloggers.com/diving-into-h2o/


I downloaded some of the airlines data locally.  I am able to connect R to an H2O instance and run the demo, but I am not able to upload the airlines data successfully with an h2o function.   I am not getting an error either.   This is what happens:


library(data.table)
library(h2o)
localH2O = h2o.init()
DTair <- fread(pathAirline)
dim(DTair)
[1] 7009728      29
air2008.hex = h2o.uploadFile(localH2O, path = pathAirline, destination_frame  = ""air2008.hex"", parse = FALSE)
dim(air2008.hex)
[1] -1 -1



When I import with the fread() function, I get the data as expected.  When I try to use h2o.uploadFile() I don't see any errors but the object has dimensions -1 by -1.


Any help is appreciated.","['r', 'file-io', 'upload', 'h2o']",smci,https://stackoverflow.com/users/202229/smci,33.8k
35424416,35424416,2016-02-16T05:24:16,2016-03-25 10:11:48Z,0,"I keep on getting the following error in version:3.8.0.3 when trying to predict on a frame either in R or on the website. 


I get this error even if I try to run a subset of the training set. 




Error evaluating cell


Error calling POST
  /3/Predictions/models/DeepLearning_model_R_14596238744_1/frames/t2
  with opts {""predictions_frame"":""prediction-b0eb96...


ERROR MESSAGE: Actual column must contain binary class labels, but
  found cardinality 1!




Even getting this error when I use a subset of the data frame I used to train the model:


t2 <- training_set[1:5,]","['r', 'h2o']",CuriousBeing,https://stackoverflow.com/users/2923027/curiousbeing,"1,632"
35261696,35261696,2016-02-08T02:46:23,2016-02-08 20:54:09Z,0,"I was using H2o R package (2014 version) to perform a deep learning task using textual data. I did my research in early 2015 and obtained promising results using deep learning method (function - h2o.deeplearning; e.g. fscore and recall always achieve >0.9). I found that my original R code doesn't work now (due to the change of H2o package in Nov 2015) and i revised my code. However, when i tried to run the same deep learning model (same setting), I could not achieve an outperfom results anymore!! please, I wish to know if H2o has changed any internal modeling settings since the revision of the H2o package? I wish to reproduce my old results with the new package... please kindly help.","['r', 'h2o']",sui,https://stackoverflow.com/users/5896745/sui,1
35231675,35231675,2016-02-05T19:07:49,2016-02-08 17:42:31Z,132,"Looking at H20, I was wondering if there is a way I can convert my Spark LogisticRegressionWithLBFGS() model to a POJO object. 
Currently, I ve been loading it with the LogisticRegressionModel.load() function but it takes about 40 seconds.","['scala', 'apache-spark', 'pojo', 'logistic-regression', 'h2o']",bobo32,https://stackoverflow.com/users/3291563/bobo32,"1,002"
35230033,35230033,2016-02-05T17:27:27,2016-12-25 16:39:29Z,928,"I ran a multi-class Logistic Regression with Spark but I would like to use
 SVM to cross validate results.
It looks like Spark 1.6 only supports SVM binary classifications. Should I use other tools to do this? H20 for example?","['scala', 'apache-spark', 'svm', 'logistic-regression', 'h2o']",bobo32,https://stackoverflow.com/users/3291563/bobo32,"1,002"
35119071,35119071,2016-01-31T20:26:34,2016-02-08 21:26:58Z,0,"I'm running cross-validation deep learning training (nfolds=4) iteratively for feature selection on H2O through R. Currently, I have only 2 layers (i.e. not deep) and between 8 and 50 neurons per layer. There are only 323 inputs, and 12 output classes.


To train one model takes in average around 40 seconds on my Intel 4770K, (32 GB ram). During training, H2o is able to max out all cpu cores.


Now, to try to speed up the training, I've set up an EC2 instance in the amazon cloud. I tried the largest compute unit (c4.8xlarge), but the speed up was minimal. It took around 24 seconds to train one model with the same settings. Therefore, I suspecting there's something I've overlooked.
I started the training like this:


localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '24G', nthreads=-1)



Just to compare the processors, the 4770K got 10163 on cpu benchmark, while the Intel Xeon E5-2666 v3 got 24804 (vCPU is 36).


This speedup is quite disappointing to say the least, and is not worth all the extra work of installing and setting everything up in the amazon cloud, while paying over $2/hour.


Is there something else that needs to be done to get all cores working besides setting nthreads=-1 ?


Do I need to start making several clusters in order to get the training time down, or should I just start on a new deep learning library that supports GPUs?","['r', 'amazon-ec2', 'deep-learning', 'h2o']",user979899,https://stackoverflow.com/users/979899/user979899,153
35097514,35097514,2016-01-30T03:07:41,2016-01-30 07:23:41Z,0,"I'm using Windows 10, Anaconda 2.4.1, and Python 2.7.11 and trying to run the h2o package from oxdata. 


To install h2o, I had used:


pip install h2o



I'm trying to initialize it as given in the Python Vignette of H2o package.


import h2o
h2o.init()



But I'm getting a long traceback mentioning Windows Error [5] as follows:


    No instance found at ip and port: localhost:54321. Trying to start local jar...


JVM stdout: c:\users\ashish\appdata\local\temp\tmpech694\h2o_Ashish_started_from_python.out
JVM stderr: c:\users\ashish\appdata\local\temp\tmp7aoeza\h2o_Ashish_started_from_python.err
Using ice_root: c:\users\ashish\appdata\local\temp\tmpjijmdl

Traceback (most recent call last):

  File ""<ipython-input-2-e7cfdc50af66>"", line 1, in <module>
    h2o.init()

  File ""C:\Users\Ashish\Anaconda2\lib\site-packages\h2o\h2o.py"", line 668, in init
    H2OConnection(ip=ip, port=port,start_h2o=start_h2o,enable_assertions=enable_assertions,license=license,max_mem_size_GB=max_mem_size_GB,min_mem_size_GB=min_mem_size_GB,ice_root=ice_root,strict_version_check=strict_version_check)

  File ""C:\Users\Ashish\Anaconda2\lib\site-packages\h2o\connection.py"", line 81, in __init__
    cld = self._start_local_h2o_jar(max_mem_size_GB, min_mem_size_GB, enable_assertions, license, ice_root, jar_path)

  File ""C:\Users\Ashish\Anaconda2\lib\site-packages\h2o\connection.py"", line 181, in _start_local_h2o_jar
    jver = subprocess.check_output([command, ""-version""], stderr=subprocess.STDOUT)

  File ""C:\Users\Ashish\Anaconda2\lib\subprocess.py"", line 566, in check_output
    process = Popen(stdout=PIPE, *popenargs, **kwargs)

  File ""C:\Users\Ashish\Anaconda2\lib\subprocess.py"", line 710, in __init__
    errread, errwrite)

  File ""C:\Users\Ashish\Anaconda2\lib\subprocess.py"", line 958, in _execute_child
    startupinfo)

WindowsError: [Error 5] Access is denied



But if I install h2o package in R language using 


install.packages(""h2o"")



and then try to run in R


library(h2o)
h <- h2o.init()



There is no error and h2o starts perfectly on localhost and port 54321.


Also, now if start h2o in python, i.e. in python


h2o.init()



Then it connects to the already running instance of h2o and performs all the operations without any problem.


But I want to avoid the step of pre-starting h2o instance and want to start it with python. I don't know whether my problem is reproducible on somebody else's system.


I simply want to use 


import h2o
h2o.init()



to start h2o in python.


Please try to provide a solution.","['python', 'r', 'windows', 'initialization', 'h2o']",Naimish Agarwal,https://stackoverflow.com/users/2702219/naimish-agarwal,516
35036498,35036498,2016-01-27T11:46:24,2016-03-28 03:21:10Z,0,"I am using h2o for anomaly detection in the data. The data contains several continuous and categorical features and the label could either be 0 or 1. Now, because the count of 1s is less than 1%, I am trying out anomaly detection technique instead of using usual classification methods. However, in the end I get MSE calculated per row of the data and I am not sure how to interpret it to be able to say that actual label is 0 but because of  it is an anomaly and should be 1. 


The code I am using so far:


features <- names(train.df)[!names(train.df) %in% c(""label"")]
train.df <- subset(train.df, label==0)
train.h <- as.h2o(train.df)

mod.dl <- h2o.deeplearning(
  x=features,
  autoencoder=TRUE,
  training_frame=train.h,
  activation=c(""Tanh""),
  hidden=c(10,10), epochs=20, adaptive_rate=FALSE,
  variable_importances=TRUE, 
  l1=1e-4, l2=1e-4,
  sparse=TRUE
)

pred.oc <- as.data.frame(h2o.anomaly(mod.dl.oc, train.h.oc))



head(pred.oc)
:


  Reconstruction.MSE
1        0.012059304
2        0.014490905
3        0.011002231
4        0.013142910
5        0.009631915
6        0.012897779","['r', 'deep-learning', 'h2o']",dsauce,https://stackoverflow.com/users/2935885/dsauce,612
34976985,34976985,2016-01-24T14:32:27,2016-01-26 15:04:08Z,0,"I have a train-dataset (forest cover type) which has 10 continuous variables and then 2 categorical (with 40 and 4 levels). So it seems to me that my read-in layer should contain 54 neurons (I am using 1-of-C). Similarly I found another version of the data (
here
), which indeed claims 54 attributes.


Below is my representation of my model:



The problem is that when I use H2O in R; it tells me I have 204,707 params (and 56 neurons in the first layer). I don't understand why it breaks (or how) the categorical into N+1? I checked and there are no missing values","['r', 'machine-learning', 'deep-learning', 'h2o']",mptevsion,https://stackoverflow.com/users/5479963/mptevsion,947
34875137,34875137,2016-01-19T11:05:26,2016-01-19 22:56:58Z,0,"I notice that the H2O 
packages mentions that it
:




preprocesses the data to be standardized for compatibility with the
  activation functions (recall Table 1’s summary of each activation
  function’s target space). Since the activation function does not
  generally map into the full spectrum of real numbers, R, we first
  standardize our data to be drawn from N (0, 1). Standardizing again
  after network propagation allows us to compute more precise errors in
  this standardized space, rather than in the raw feature space. For
  autoencoding, the data is normalized (instead of standardized) to the
  compact interval of mathcalU(−0.5, 0.5), to allow bounded activation
  functions like Tanh to better reconstruct the data.




However, I don't fully understand. My impression was (
here
, and 
here
) that the the 
categorical variables should be broken into 1-of-C dummies and the continuous data normalised
. 
Then, everything should be standardised to [-1,1]
.


I also don't see a way of specifying the neurons for the read-out layer. I thought that if we have a 
categorical
 output variable then we want to use softmax activation function (and encode as 1-of-C) / if we have a 
continuous
 output (e.g. price) then we scale that to [-1,1] and use 'tanh' / if we have a 
single binary
 output then we can use logistic and code it as [0,1]","['r', 'machine-learning', 'neural-network', 'deep-learning', 'h2o']",mptevsion,https://stackoverflow.com/users/5479963/mptevsion,947
34796107,34796107,2016-01-14T17:35:47,2016-01-16 22:53:20Z,77,"When running a gbm model using h2o 3.6.0.8 in Python I get the following warning: 




DeprecationWarning: 
h2o.gbm
 is deprecated. Use the estimators sub module to build an H2OGradientBoostedEstimator.




I have been looking for an example on how to build a H2OGradientBoostedEstimator but with no success. Could you please direct to the right path?


Thanks","['python', 'h2o', 'gbm']",somesingsomsing,https://stackoverflow.com/users/5240970/somesingsomsing,"3,330"
34784893,34784893,2016-01-14T08:36:44,2016-01-14 08:36:44Z,0,"I am trying to launch H2O server in R which works fine after I type in 


h2o.init()



However, when I open 127.0.0.1:54321 in the browser, the following comes up. How do I find what is the problem here and hence the solution?","['r', 'h2o']",dsauce,https://stackoverflow.com/users/2935885/dsauce,612
34725593,34725593,2016-01-11T15:46:52,2019-04-10 03:28:25Z,0,"I was using the example given by h2o for ECG anomaly detection.
When trying to compute manually the MSE, I got different results.
To demonstrate the difference I used the last test case
but all 23 cases differ.
Attached is the full code:


Thanks,
Eli.


suppressMessages(library(h2o))
localH2O = h2o.init(max_mem_size = '6g', # use 6GB of RAM of *GB available
                nthreads = -1) # use all CPUs (8 on my personal computer :3)

# Download and import ECG train and test data into the H2O cluster
train_ecg <- h2o.importFile(path = ""http://h2o-public-test-data.s3.amazonaws.com/smalldata/anomaly/ecg_discord_train.csv"",
                          header = FALSE,
                          sep = "","")
test_ecg <- h2o.importFile(path = ""http://h2o-public-test-data.s3.amazonaws.com/smalldata/anomaly/ecg_discord_test.csv"",
                         header = FALSE,
                         sep = "","")
# Train deep autoencoder learning model on ""normal""
# training data, y ignored
anomaly_model <- h2o.deeplearning(x = names(train_ecg),
                                 training_frame = train_ecg,
                                 activation = ""Tanh"",
                                 autoencoder = TRUE,
                                 hidden = c(50,20,50),
                                 l1 = 1e-4,
                                 epochs = 100)

# Compute reconstruction error with the Anomaly
# detection app (MSE between output layer and input layer)
recon_error <- h2o.anomaly(anomaly_model, test_ecg)

# Pull reconstruction error data into R and
# plot to find outliers (last 3 heartbeats)
recon_error <- as.data.frame(recon_error)
recon_error
plot.ts(recon_error)
test_recon <- h2o.predict(anomaly_model, test_ecg)

t <- as.vector(test_ecg[23,])
r <- as.vector(test_recon[23,])
mse.23 <- sum((t-r)^2)/length(t)
mse.23
recon_error[23,]

> mse.23
[1] 2.607374
> recon_error[23,]
[1] 8.264768","['r', 'machine-learning', 'deep-learning', 'h2o']",eli,https://stackoverflow.com/users/4660253/eli,81
34699443,34699443,2016-01-09T22:03:17,2016-01-12 16:05:09Z,0,"I am working with a very large dataset and I would like to keep the data in H2O as much as possible without bringing it into R. 


I noticed whenever I pass an 
H2O Frame
 to a function, any modification I make to the Frame is not reflected outside of the function. Is there a way to pass the Frame by Reference? 


If not, what's the best way to modify the original frame inside a function with copying all of the Frame? 


Another related question: does passing a Frame to other functions (read only), make extra copies on H2O side? My datasets are 30GB - 100GB. So want to make sure passing them around does not cause memory issues.


mod = function(fdx) {
  fdx[,""x""] = -1
}

d = data.frame(x = rnorm(100),y=rnorm(100))
dx = as.h2o(d)
dx[1,]
mod(dx)
dx[1,]  # does not change the original value of x


 > dx[1,]
           x         y
 1 0.3114706 0.9523058

 > dx[1,]
           x         y
 1 0.3114706 0.9523058



Thanks!","['r', 'h2o']",Ecognium,https://stackoverflow.com/users/110844/ecognium,"2,066"
34621635,34621635,2016-01-05T21:35:04,2016-01-06 23:16:35Z,0,"I've successfully run h2o from R on a linux machine and wanted to install it in Windows too.  h2o will not initialise for me.  The full output is pasted below but the key seems to be the line


[1] ""Failed to connect to 127.0.0.1 port 54321: Connection refused""
curl: (1) Protocol ""'http"" not supported or disabled in libcurl



Judging from 
this
 and 
this
 experience it might be something to do with single quotes v double quotes somewhere; but this seems unlikely because then no-one would be able to get h2o / R / Windows combination working and I gather that some people are. On the other hand, 
this question
 seems to suggest the problem will be that my curl installation may not have ssl enabled.  So I downloaded curl from scratch from 
this wizard
 as recommended on the h2o page, selecting the 64 bit version, generic, and selected the version with both SSL and SSH enabled; downloaded it and added the folder it ended up in to my Windows PATH.  But no difference.


I've just noticed my Java runtime environment is old and will update that as well.  But on the face of it it's not obvious that that could be the problem.


Any suggestions welcomed.


> library(h2o)

> h2o.init()

H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    C:\Users\PETERE~1\AppData\Local\Temp\Rtmpa6G3WA/h2o_Peter_Ellis_started_from_r.out
    C:\Users\PETERE~1\AppData\Local\Temp\Rtmpa6G3WA/h2o_Peter_Ellis_started_from_r.err

java version ""1.7.0_75""
Java(TM) SE Runtime Environment (build 1.7.0_75-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.75-b04, mixed mode)

............................................................
ERROR: Unknown argument (Ellis_cns773)


Usage:  java [-Xmx<size>] -jar h2o.jar [options]
        (Note that every option has a default and is optional.)

    -h | -help
          Print this help.

    -version
          Print version info and exit.

    -name <h2oCloudName>
          Cloud name used for discovery of other nodes.
          Nodes with the same cloud name will form an H2O cloud
          (also known as an H2O cluster).

    -flatfile <flatFileName>
          Configuration file explicitly listing H2O cloud node members.

    -ip <ipAddressOfNode>
          IP address of this node.

    -port <port>
          Port number for this node (note: port+1 is also used).
          (The default port is 0.)

    -network <IPv4network1Specification>[,<IPv4network2Specification> ...]
          The IP address discovery code will bind to the first interface
          that matches one of the networks in the comma-separated list.
          Use instead of -ip when a broad range of addresses is legal.
          (Example network specification: '10.1.2.0/24' allows 256 legal
          possibilities.)

    -ice_root <fileSystemPath>
          The directory where H2O spills temporary data to disk.

    -log_dir <fileSystemPath>
          The directory where H2O writes logs to disk.
          (This usually has a good default that you need not change.)

    -log_level <TRACE,DEBUG,INFO,WARN,ERRR,FATAL>
          Write messages at this logging level, or above.  Default is INFO.

    -flow_dir <server side directory or HDFS directory>
          The directory where H2O stores saved flows.
          (The default is 'C:\Users\Peter Ellis\h2oflows'.)

    -nthreads <#threads>
          Maximum number of threads in the low priority batch-work queue.
          (The default is 99.)

    -client
          Launch H2O node in client mode.

Cloud formation behavior:

    New H2O nodes join together to form a cloud at startup time.
    Once a cloud is given work to perform, it locks out new members
    from joining.

Examples:

    Start an H2O node with 4GB of memory and a default cloud name:
        $ java -Xmx4g -jar h2o.jar

    Start an H2O node with 6GB of memory and a specify the cloud name:
        $ java -Xmx6g -jar h2o.jar -name MyCloud

    Start an H2O cloud with three 2GB nodes and a default cloud name:
        $ java -Xmx2g -jar h2o.jar &
        $ java -Xmx2g -jar h2o.jar &
        $ java -Xmx2g -jar h2o.jar &

[1] ""127.0.0.1""
[1] 54321
[1] TRUE
[1] -1
[1] ""Failed to connect to 127.0.0.1 port 54321: Connection refused""
curl: (1) Protocol ""'http"" not supported or disabled in libcurl
[1] 1
Error in h2o.init() : H2O failed to start, stopping execution.
In addition: Warning message:
running command 'curl 'http://localhost:54321'' had status 1 

> sessionInfo()
R version 3.2.3 (2015-12-10)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=English_New Zealand.1252  LC_CTYPE=English_New Zealand.1252    LC_MONETARY=English_New Zealand.1252
[4] LC_NUMERIC=C                         LC_TIME=English_New Zealand.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] h2o_3.6.0.8    statmod_1.4.22

loaded via a namespace (and not attached):
[1] tools_3.2.3     RCurl_1.95-4.7  jsonlite_0.9.19 bitops_1.0-6","['r', 'curl', 'rcurl', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
34569246,34569246,2016-01-02T18:27:48,2019-02-23 23:28:51Z,0,"Hi hereunder a problem when I try to install h2oEnsemble. I don't know to fix this. PLease help.


My goal here is to run this exemple:


https://github.com/h2oai/h2o-world-2015-training/blob/master/tutorials/ensembles-stacking/ensembles-stacking.R




> library(devtools)



Warning message:
le package ‘devtools’ a été compilé avec la version R 3.1.3 


> install_github(""h2oai/h2o-3/h2o-r/ensemble/h2oEnsemble-package"")



Downloading GitHub repo h2oai/h2o-3@master
Error in loadNamespace(name) : there is no package called ‘RCurl’


> install.packages(""RCurl"")



trying URL '
http://cran.rstudio.com/bin/windows/contrib/3.1/RCurl_1.95-4.7.zip
'
Content type 'application/zip' length 2858866 bytes (2.7 Mb)
opened URL
downloaded 2.7 Mb


Warning in install.packages :
  downloaded length 2858866 != reported length 2858866
package ‘RCurl’ successfully unpacked and MD5 sums checked


The downloaded binary packages are in
    C:\Users\Djilo\AppData\Local\Temp\RStudioPortableTemp\RtmpmQLJyE\downloaded_packages


> install_github(""h2oai/h2o-3/h2o-r/ensemble/h2oEnsemble-package"")



Downloading GitHub repo h2oai/h2o-3@master
Installing h2oEnsemble
Installing 1 packages: RCurl
package ‘RCurl’ successfully unpacked and MD5 sums checked
Warning: cannot remove prior installation of package ‘RCurl’
""E:/DATAMI~3/R-PORT~1/App/R-PORT~1/bin/x64/R"" --no-site-file --no-environ --no-save --no-restore CMD INSTALL  \
  ""C:/Users/Djilo/AppData/Local/Temp/RStudioPortableTemp/RtmpmQLJyE/devtools177417c33da9/h2oai-h2o-3-7eaa37a/h2o-r/ensemble/h2oEnsemble-package"" --library=""E:/Data  \
  Mining - R/R-Portable/App/R-Portable/library"" --install-tests 




installing 
source
 package 'h2oEnsemble' ...
** R
** tests
** preparing package for lazy loading
Avis : package 'statmod' was built under R version 3.1.3
Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) : 
there is no package called 'RCurl'
Error : package 'h2o' could not be loaded
ERROR: lazy loading failed for package 'h2oEnsemble'


removing 'E:/Data Mining - R/R-Portable/App/R-Portable/library/h2oEnsemble'
Error: Command failed (1)
In addition: Warning message:
In download.file(url, destfile, method, mode = ""wb"", ...) :
downloaded length 2858866 != reported length 2858866","['r', 'h2o']",JJJ,https://stackoverflow.com/users/3367799/jjj,"1,029"
34548488,34548488,2015-12-31T15:34:22,2016-01-15 04:39:11Z,572,"I'm trying to run h2o.deeplearning twice, using checkpoint parameter
on 2 train sets (using same parameters except different epochs).
I'm getting the following error: 


Error: 'The columns of the training data must be the same as for the checkpointed model


even-though both sets have same columns.
Attached below the relevant code:


model <- h2o.deeplearning(x = 2:785, y = 1, training_frame = train1, 
                      activation = ""RectifierWithDropout"", 
                      hidden = c(1024,1024,2048),
                      epochs = 10, 
                      l1 = 1e-5, 
                      input_dropout_ratio = 0.2,
                      train_samples_per_iteration = -1, 
                      classification_stop = -1)

model2 <- h2o.deeplearning(x = 2:785, y = 1, training_frame = train2, 
                      checkpoint = model@model_id,
                      activation = ""RectifierWithDropout"", 
                      hidden = c(1024,1024,2048),
                      epochs = 1000, 
                      l1 = 1e-5, 
                      input_dropout_ratio = 0.2,
                      train_samples_per_iteration = -1, 
                      classification_stop = -1)


> all(colnames(train1)==colnames(train2))
[1] TRUE

> dim(train1)
[1] 54447   785
> dim(train2)
[1] 5553  785



Thanks,
Eli.","['machine-learning', 'neural-network', 'deep-learning', 'h2o', 'checkpointing']",Amir,https://stackoverflow.com/users/2838606/amir,11k
34502311,34502311,2015-12-28T23:30:17,2016-01-28 03:15:01Z,0,"It's a simple question but I couldn't find the answer anywhere : how to hide the progress bar in all the h2o functions?


I managed to hide it in Rmarkdown with results = FALSE, but how to hide it in the console?","['r', 'h2o']",Kevin P,https://stackoverflow.com/users/4011719/kevin-p,283
34490981,34490981,2015-12-28T09:35:43,2015-12-29 19:03:26Z,0,"Hi I began to use the package h2o ensemble (here :
https://github.com/h2oai/h2o-3/tree/master/h2o-r/ensemble
 for some data analysis and tried a demo code.


The code worked well before :


## ## setting up h2o
library(h2oEnsemble)
nodes <- 2 ## number of processes
localH2O <-  h2o.init(nthreads=nodes)

## ## simulated data set
dat <- matrix(rnorm(6e3), ncol=3, dimnames=list(NULL, c(""W"", ""X"", ""Y"")))
dat <- as.data.frame(dat)
Z <- as.factor(rbinom(nrow(dat), size=1, prob=plogis(.2+.1*dat$W-.2*dat$X)))
dat <- cbind(dat, Z=Z)
## W,X,Y: Input
## Z: output
dat.app <- dat[1:1e3, ]
dat.val <- dat[1e3+(1:1e3), ]

## ## h2o procedure
dat.h2o.app <- as.h2o(localH2O, dat.app) ## learning
dat.h2o.val <- as.h2o(localH2O, dat.val) ## validation

library.h2o <- c(""h2o.deeplearning.Tanh"",
                 ""h2o.randomForest.1000x100"")

h2o.randomForest.1000x100 <- function(...,ntrees=1000,nbins=100) {
    h2oEnsemble::h2o.randomForest.wrapper(..., ntrees=ntrees, nbins=nbins,seed=1)
}
h2o.deeplearning.Tanh <- function(...,hidden=c(200, 200,200),activation=""Tanh"" ) {
    h2oEnsemble::h2o.deeplearning.wrapper(..., hidden=hidden,    activation=activation,seed=1)
}
h2o.model <- h2o.ensemble(y=""Z"", x=c(""W"", ""X"", ""Y""),
                          training_frame=dat.h2o.app,
                          family=""binomial"",
                          learner=library.h2o,
                          cvControl=list(V=10, shuffle=TRUE),
                          metalearner=""h2o.glm.wrapper"") # getting the 400 bad request

h2o.pred.val <- predict(h2o.model, newdat=dat.h2o.val)$pred
table((h2o.pred.val>0.5)+0, dat.val$Z)



and it suddenly throw me a 400 bad request (RTMP_5 already exist)


R version 3.2.3 (2015-12-10) -- ""Wooden Christmas-Tree""
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ## ## setting up h2o
> library(h2oEnsemble)
> nodes <- 2 ## number of processes
> localH2O <-  h2o.init(nthreads=nodes)
Successfully connected to http://127.0.0.1:54321/ 

R is connected to the H2O cluster: 
    H2O cluster uptime:         9 days 19 hours 
    H2O cluster version:        3.6.0.8 
    H2O cluster name:           H2O_started_from_R_root_afl027 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   6.98 GB 
    H2O cluster total cores:    6 
    H2O cluster allowed cores:  2 
    H2O cluster healthy:        TRUE 

> 
> ## ## simulated data set
> dat <- matrix(rnorm(6e3), ncol=3, dimnames=list(NULL, c(""W"", ""X"", ""Y"")))
> dat <- as.data.frame(dat)
> Z <- as.factor(rbinom(nrow(dat), size=1, prob=plogis(.2+.1*dat$W-.2*dat$X)))
> dat <- cbind(dat, Z=Z)
> ## W,X,Y: input
> ## Z: output
> dat.app <- dat[1:1e3, ]
> dat.val <- dat[1e3+(1:1e3), ]
> 
> ## ## h2o procedure
> dat.h2o.app <- as.h2o(dat.app) ## apprentissage

  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |======================================================================| 100%
> dat.h2o.val <- as.h2o(dat.val) ## validation

  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |======================================================================| 100%
> 
> library.h2o <- c(""h2o.deeplearning.Tanh"",
+                  ""h2o.randomForest.1000x100"")
> 
> h2o.model <- h2o.ensemble(y=""Z"", x=c(""W"", ""X"", ""Y""),
+                           training_frame=dat.h2o.app,
+                           family=""binomial"",
+                           learner=library.h2o,
+                           cvControl=list(V=10, shuffle=TRUE),
+                           metalearner=""h2o.glm.wrapper"")

ERROR: Unexpected HTTP Status code: 400 Bad Request (url = http://127.0.0.1:54321/99/Rapids)

java.lang.IllegalArgumentException
 [1] ""water.rapids.ASTTmpAssign.apply(ASTAssign.java:254)""                                  
 [2] ""water.rapids.ASTTmpAssign.apply(ASTAssign.java:248)""                                  
 [3] ""water.rapids.ASTExec.exec(ASTExec.java:46)""                                           
 [4] ""water.rapids.Session.exec(Session.java:56)""                                           
 [5] ""water.rapids.Exec.exec(Exec.java:63)""                                                 
 [6] ""water.api.RapidsHandler.exec(RapidsHandler.java:23)""                                  
 [7] ""sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)""                          
 [8] ""sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""
 [9] ""java.lang.reflect.Method.invoke(Method.java:622)""                                     
[10] ""water.api.Handler.handle(Handler.java:64)""                                            
[11] ""water.api.RequestServer.handle(RequestServer.java:644)""                               
[12] ""water.api.RequestServer.serve(RequestServer.java:585)""                                
[13] ""water.JettyHTTPD$H2oDefaultServlet.doGeneric(JettyHTTPD.java:617)""                    
[14] ""water.JettyHTTPD$H2oDefaultServlet.doPost(JettyHTTPD.java:565)""                       
[15] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                         
[16] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                         
[17] ""org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""               

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page, :
    Temp ID RTMP_5 already exists
Calls : h2o.ensemble ... .eval.driver -> .h2o.__remoteSend -> .h2o.doSafeREST
Execution halted



I'm kind of lost there and don't understand the reason why it does not work now, the training set should be in the good format.
Did someone encounter this problem?if yes how did you get over this error?","['r', 'machine-learning', 'bad-request', 'h2o']",Unknown,,N/A
34418187,34418187,2015-12-22T14:32:39,2017-03-08 16:24:50Z,0,"I am new to sparkling water.  I have been trying to develop project for it in intellij but couldnt.I couldnt find many resources for same on internet.
So any can please tell how to develop a simple project using h20 and spark in scala with IntelliJ.


I tried this code:


import org.apache.spark.h2o.H2OContext
import org.apache.spark.sql.DataFrame
import org.apache.spark.{h2o, SparkConf, SparkContext}
import water.H2OClientApp
import water.fvec._
import org.apache.spark.h2o._
object test {
  def main(args: Array[String]) {

    val conf = new SparkConf().setMaster(""local[*]"").setAppName(""testing"")
    val sc = new SparkContext(conf)

    val source = getClass.getResource(""data.txt"")
    val distF = sc.textFile(source.getFile)
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    import sqlContext.implicits._
    val table1 = distF.map(_.split("","")).map(p => Person(p(0), p(1),p(2),p(3),p(4),p(5),p(6))).toDF()


    import org.apache.spark.h2o._
    val h2oContext = new H2OContext(sc).start()
    import h2oContext._
    import org.apache.spark.rdd.RDD

    val mydf2:h2o.RDD[Person] = h2oContext.createH2ORDD(table1)
    println(""Count of mydf2================>>>>>>>>""+mydf2.count())

  }
}

case class Person(Country: String, ASN: String,Time_Stamp: String,Metric_A: String,Co_Server: String,Bytes: String,Send_Time:String);



And for this i got error.
Error part of log generated is:


15/12/24 03:45:53 WARN TaskSetManager: Lost task 1.0 in stage 5.0 (TID 17, localhost): java.lang.IllegalArgumentException: argument type mismatch
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.apache.spark.rdd.H2ORDD$$anon$1.next(H2ORDD.scala:106)
    at org.apache.spark.rdd.H2ORDD$$anon$1.next(H2ORDD.scala:64)
    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555)
    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1121)
    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1121)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
    at org.apache.spark.scheduler.Task.run(Task.scala:88)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

15/12/24 03:45:53 ERROR TaskSetManager: Task 1 in stage 5.0 failed 1 times; aborting job
15/12/24 03:45:53 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/12/24 03:45:53 INFO TaskSetManager: Lost task 0.0 in stage 5.0 (TID 16) on executor localhost: java.lang.IllegalArgumentException (argument type mismatch) [duplicate 1]
15/12/24 03:45:53 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/12/24 03:45:53 INFO TaskSchedulerImpl: Cancelling stage 5
15/12/24 03:45:53 INFO DAGScheduler: ResultStage 5 (count at test.scala:32) failed in 0.038 s
15/12/24 03:45:53 INFO DAGScheduler: Job 5 failed: count at test.scala:32, took 0.050463 s
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 17, localhost): java.lang.IllegalArgumentException: argument type mismatch
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.apache.spark.rdd.H2ORDD$$anon$1.next(H2ORDD.scala:106)
    at org.apache.spark.rdd.H2ORDD$$anon$1.next(H2ORDD.scala:64)
    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555)
    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1121)
    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1121)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
    at org.apache.spark.scheduler.Task.run(Task.scala:88)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
    at scala.Option.foreach(Option.scala:236)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
    at org.apache.spark.rdd.RDD.count(RDD.scala:1121)
    at test$.main(test.scala:32)
    at test.main(test.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
Caused by: java.lang.IllegalArgumentException: argument type mismatch
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.apache.spark.rdd.H2ORDD$$anon$1.next(H2ORDD.scala:106)
    at org.apache.spark.rdd.H2ORDD$$anon$1.next(H2ORDD.scala:64)
    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555)
    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1121)
    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1121)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
    at org.apache.spark.scheduler.Task.run(Task.scala:88)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)



Please let me know where I wnt wrong and what changes I have to do and why.","['scala', 'apache-spark', 'h2o']",jangorecki,https://stackoverflow.com/users/2490497/jangorecki,16.7k
34354848,34354848,2015-12-18T11:39:57,2015-12-18 22:23:39Z,508,"I'm a newbie in h2o and recently started implementing machine learning algorithms,but I can't find apriori algorithm implementation or required packages to implement the algorithm.
Can anybody suggest me if it is available in h2o?","['apriori', 'h2o']",Akshay,https://stackoverflow.com/users/5669008/akshay,13
34340698,34340698,2015-12-17T17:27:07,2017-07-07 08:44:14Z,276,"I’ve been an h2o user for a little over a year and a half now, but my work has been limited to the R api; 
h2o flow
 is relatively new to me. If it's new to you as well, it's basically 0xdata's version of iPython, however iPython let's you export your notebook to a script. I can't find a similar option in flow...


I’m at the point of moving a model (built in flow) to production, and I'm wondering how to automate it. With the R api, after the model was built and saved, I could easily load it in R and make predictions on the new data simply by running a 
nohup Rscript <the_file> &
 from CLI, but I’m not sure how I can do something similar with flow, especially since it’s running on Hadoop.


As it currently stands, every run is broken into three pieces with the flow creating a relatively clunky process in the middle:




preprocess data, move it to hdfs


start h2o on hadoop, 
nslookup
 the IP address h2o is running on, manually run the flow cell-by-cell


run the post-prediction clean-up and final steps




This is a 
terribly
 intrusive production process, and I want to tie all the ends up, however flow is making it rather difficult.  To distill the question: is there a way to compress the flow into a hadoop jar and then later just run the jar like 
hadoop jar <my_flow_jar.jar> ...
?


Edit:


Here's the h2o R package 
documentation
. The R API allows you to load an H2O model, so I tried loading the flow (as if it were an H2O model), and unsurprisingly it did not work (failed with a 
water.api.FSIOException
) as it's not technically an h2o model.","['hadoop', 'h2o']",Unknown,,N/A
34267983,34267983,2015-12-14T13:12:50,2017-04-06 06:33:04Z,0,"While trying to run the example on H2OEnsemble found on 
http://learn.h2o.ai/content/tutorials/ensembles-stacking/index.html
 from within Rstudio, I encounter the following error:




Error in value[3L] : 
        argument ""training_frame"" must be a valid H2O H2OFrame or id




after defining the ensemble 


fit <- h2o.ensemble(x = x, y = y, 
                    training_frame = train, 
                     family = family, 
                     learner = learner, 
                     metalearner = metalearner,
                     cvControl = list(V = 5, shuffle = TRUE))



I installed the latest version of both 
h2o
 and 
h2oEnsemble
 but the issue remains. I have read here 
`h2o.cbind` accepts only of H2OFrame objects - R
 that the naming convention in 
h2o
 changed over time, but I assume by installing the latest version of both this should not be any longer the issue.


Any suggestions?


library(readr)
library(h2oEnsemble)  # Requires version >=0.0.4 of h2oEnsemble
library(cvAUC)  # Used to calculate test set AUC (requires version >=1.0.1 of cvAUC)
localH2O <-  h2o.init(nthreads = -1)  # Start an H2O cluster with nthreads = num cores on your machine





# Import a sample binary outcome train/test set into R
train <- h2o.importFile(""http://www.stat.berkeley.edu/~ledell/data/higgs_10k.csv"")
test <- h2o.importFile(""http://www.stat.berkeley.edu/~ledell/data/higgs_test_5k.csv"")
y <- ""C1""
x <- setdiff(names(train), y)
family <- ""binomial""

#For binary classification, response should be a factor
train[,y] <- as.factor(train[,y])  
test[,y] <- as.factor(test[,y])


# Specify the base learner library & the metalearner
learner <- c(""h2o.glm.wrapper"", ""h2o.randomForest.wrapper"", 
               ""h2o.gbm.wrapper"", ""h2o.deeplearning.wrapper"")
metalearner <- ""h2o.deeplearning.wrapper""


# Train the ensemble using 5-fold CV to generate level-one data
# More CV folds will take longer to train, but should increase performance
fit <- h2o.ensemble(x = x, y = y, 
                    training_frame = train, 
                    family = family, 
                    learner = learner, 
                    metalearner = metalearner,
                    cvControl = list(V = 5, shuffle = TRUE))","['r', 'cran', 'h2o', 'ensemble-learning']",Community,https://stackoverflow.com/users/-1/community,1
34227323,34227323,2015-12-11T15:43:41,2019-02-24 23:22:38Z,0,"I'm trying to save an h2o model in R with the next code:


h2o.saveModel(myModel, path = ""myPath/models"")



But I'm getting an error:


ERROR: Unexpected HTTP Status code: 404 Not Found (url = http://localhost:54321/99/Models.bin/DeepLearning_model_R_1449814346828_5?dir=D%3A%2FGoogle%20Drive%2FMio%2FKaggle%2FDigital%20Challenge%2Fmodels%2FDeepLearning_model_R_1449814346828_5&force=FALSE)

water.exceptions.H2OKeyNotFoundArgumentException
 [1] ""water.api.ModelsHandler.getFromDKV(ModelsHandler.java:126)""                           
 [2] ""water.api.ModelsHandler.exportModel(ModelsHandler.java:222)""                          
 [3] ""sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""                          
 [4] ""sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)""        
 [5] ""sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""
 [6] ""java.lang.reflect.Method.invoke(Method.java:497)""                                     
 [7] ""water.api.Handler.handle(Handler.java:64)""                                            
 [8] ""water.api.RequestServer.handle(RequestServer.java:644)""                               
 [9] ""water.api.RequestServer.serve(RequestServer.java:585)""                                
[10] ""water.JettyHTTPD$H2oDefaultServlet.doGeneric(JettyHTTPD.java:617)""                    
[11] ""water.JettyHTTPD$H2oDefaultServlet.doGet(JettyHTTPD.java:559)""                        
[12] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:735)""                         
[13] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                         
[14] ""org.eclipse

.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""               

Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
  Object 'DeepLearning_model_R_1449814346828_5' not found for argument: model_id



I already checked 
this question
 but it didn't solve my problem.


Is anyone able to save an h2o model in R? If not I will lose the model when I'll close the session.","['r', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
34224528,34224528,2015-12-11T13:19:02,2015-12-17 04:37:04Z,0,I have already created model in R and saved the model in .rds file. I need to load that saved model in h2o. Can anybody suggest the way to perform this?,"['r', 'h2o']",user3128477,https://stackoverflow.com/users/3128477/user3128477,31
34221145,34221145,2015-12-11T10:20:02,2015-12-17 18:35:55Z,0,"I am getting an error when trying to create deep learning predictions with h2o in R. The error occurs for about one third of predictions with the command h2o.predict. Here is the model setup:


localH2O = h2o.init(ip = ""localhost"", port = 54321, startH2O = TRUE,max_mem_size='20g',nthreads=6)
model <- h2o.deeplearning(x = 2:100, y = 1, training_frame = x, l1 = 1e-5, l2 = 1e-5, epochs=500, hidden = c(800,800,100))
prediction <- h2o.predict(model, x[,2:100])



Here is the error that occurs on and off:


ERROR: Unexpected HTTP Status code: 500 Server Error (url = http://localhost:54321/99/Rapids)

java.lang.RuntimeException
[1] ""water.MRTask.getResult(MRTask.java:505)""                                              
[2] ""water.MRTask.doAll(MRTask.java:379)""                                                  
[3] ""water.MRTask.doAll(MRTask.java:375)""                                                  
[4] ""water.rapids.ASTRowSlice.apply(ASTColSlice.java:123)""                                 
[5] ""water.rapids.ASTExec.exec(ASTExec.java:46)""                                           
[6] ""water.rapids.ASTTmpAssign.apply(ASTAssign.java:255)""                                  
[7] ""water.rapids.ASTTmpAssign.apply(ASTAssign.java:248)""                                  
[8] ""water.rapids.ASTExec.exec(ASTExec.java:46)""                                           
[9] ""water.rapids.Session.exec(Session.java:56)""                                           
[10] ""water.rapids.Exec.exec(Exec.java:63)""                                                 
[11] ""water.api.RapidsHandler.exec(RapidsHandler.java:23)""                                  
[12] ""sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)""                          
[13] ""sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)""
[14] ""java.lang.reflect.Method.invoke(Method.java:497)""                                     
[15] ""water.api.Handler.handle(Handler.java:64)""                                            
[16] ""water.api.RequestServer.handle(RequestServer.java:644)""                               
[17] ""water.api.RequestServer.serve(RequestServer.java:585)""                                
[18] ""water.JettyHTTPD$H2oDefaultServlet.doGeneric(JettyHTTPD.java:617)""                    
[19] ""water.JettyHTTPD$H2oDefaultServlet.doPost(JettyHTTPD.java:565)""                       
[20] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""                         
[21] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""                         
[22] ""org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""               

Show Traceback

Rerun with Debug
Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, urlSuffix = page,  : 
water.DException$DistributedException: from /127.0.0.1:54321; by class water.rapids.ASTRowSlice$1; class java.lang.NegativeArraySizeException: null Error in class(obj) <- ""rs.scalar"" : attempt to set an attribute on NULL



Here is something about my system architecture. Running system(""java -version"") gives:


java version ""1.8.0_65""
Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)



Here is the output of sessionInfo():


R version 3.2.2 (2015-08-14)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows Server 2008 R2 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252    LC_MONETARY=Danish_Denmark.1252
[4] LC_NUMERIC=C                    LC_TIME=Danish_Denmark.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] h2o_3.6.0.8              statmod_1.4.22           TTR_0.23-0               selectiveInference_1.1.1
[5] intervals_0.15.1         lars_1.2                 glmnet_2.0-2             foreach_1.4.3           
[9] Matrix_1.2-2             wq_0.4.4                 zoo_1.7-12               skm_1.0.2               
[13] stringi_1.0-1            devtools_1.9.1          

loaded via a namespace (and not attached):
[1] Rcpp_0.12.2      magrittr_1.5     MASS_7.3-43      munsell_0.4.2    colorspace_1.2-6
[6] lattice_0.20-33  stringr_1.0.0    plyr_1.8.3       xts_0.9-7        tools_3.2.2     
[11] grid_3.2.2       gtable_0.1.2     iterators_1.0.8  digest_0.6.8     reshape2_1.4.1  
[16] ggplot2_1.0.1    bitops_1.0-6     codetools_0.2-14 RCurl_1.95-4.7   memoise_0.2.1   
[21] scales_0.3.0     jsonlite_0.9.19  proto_0.3-10    



Any help much appreciated.","['r', 'deep-learning', 'h2o']",fifthace,https://stackoverflow.com/users/3202204/fifthace,546
34171687,34171687,2015-12-09T05:55:22,2018-02-02 14:59:28Z,0,"I am getting error while running h2o.ensemble in R. This is the error output 


[1] ""Cross-validating and training base learner 1: h2o.glm.wrapper""
  |======================================================================| 100%
[1] ""Cross-validating and training base learner 2: h2o.randomForest.1""
  |==============                                                        |  19%

Got exception 'class java.lang.AssertionError', with msg 'null'
java.lang.AssertionError
    at hex.tree.DHistogram.scoreMSE(DHistogram.java:323)
    at hex.tree.DTree$DecidedNode$FindSplits.compute2(DTree.java:441)
    at hex.tree.DTree$DecidedNode.bestCol(DTree.java:421)
    at hex.tree.DTree$DecidedNode.<init>(DTree.java:449)
    at hex.tree.SharedTree.makeDecided(SharedTree.java:489)
    at hex.tree.SharedTree$ScoreBuildOneTree.onCompletion(SharedTree.java:436)
    at jsr166y.CountedCompleter.__tryComplete(CountedCompleter.java:425)
    at jsr166y.CountedCompleter.tryComplete(CountedCompleter.java:383)
    at water.MRTask.compute2(MRTask.java:683)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:1069)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:468)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)


Error: 'null'



This is my code that i am using. I am using this script for regression problem. ""sales"" column is for output prediction. Rest of the columns are for training. 


response <- ""Sales""
predictors <- setdiff(names(train), response)

h2o.glm.1 <- function(..., alpha = 0.0) h2o.glm.wrapper(..., alpha = alpha)
h2o.glm.2 <- function(..., alpha = 0.5) h2o.glm.wrapper(..., alpha = alpha)
h2o.glm.3 <- function(..., alpha = 1.0) h2o.glm.wrapper(..., alpha = alpha)
h2o.randomForest.1 <- function(..., ntrees = 200, nbins = 50, seed = 1) h2o.randomForest.wrapper(..., ntrees = ntrees, nbins = nbins, seed = seed)
h2o.randomForest.2 <- function(..., ntrees = 200, sample_rate = 0.75, seed = 1) h2o.randomForest.wrapper(..., ntrees = ntrees, sample_rate = sample_rate, seed = seed)
h2o.gbm.1 <- function(..., ntrees = 100, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, seed = seed)
h2o.gbm.6 <- function(..., ntrees = 100, col_sample_rate = 0.6, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, col_sample_rate = col_sample_rate, seed = seed)
h2o.gbm.8 <- function(..., ntrees = 100, max_depth = 3, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, max_depth = max_depth, seed = seed)
h2o.deeplearning.1 <- function(..., hidden = c(500,500), activation = ""Rectifier"", epochs = 50, seed = 1)  h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed)
h2o.deeplearning.6 <- function(..., hidden = c(50,50), activation = ""Rectifier"", epochs = 50, seed = 1)  h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed)
h2o.deeplearning.7 <- function(..., hidden = c(100,100), activation = ""Rectifier"", epochs = 50, seed = 1)  h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed)

print(""learning starts "")
#### Customized base learner library
learner <- c(""h2o.glm.wrapper"",
             ""h2o.randomForest.1"", ""h2o.randomForest.2"",
             ""h2o.gbm.1"", ""h2o.gbm.6"", ""h2o.gbm.8"",
             ""h2o.deeplearning.1"", ""h2o.deeplearning.6"", ""h2o.deeplearning.7"")

metalearner <- ""h2o.glm.wrapper""
#
#Train with new library:
fit <- h2o.ensemble(
  x =  predictors, 
  y= response,
  training_frame=train,
  family = ""gaussian"", 
  learner = learner, 
  metalearner = metalearner,
  cvControl = list(V = 5))



All columns of train data are numeral. I am using R version 3.2.2.","['r', 'machine-learning', 'h2o']",saurabh agarwal,https://stackoverflow.com/users/3391524/saurabh-agarwal,"2,144"
34082792,34082792,2015-12-04T07:10:29,2015-12-17 18:41:58Z,0,"I am experimenting with loading data bigger than the memory size in h2o.


H2o 
blog
 mentions: 
A note on Bigger Data and GC: We do a user-mode swap-to-disk when the Java heap gets too full, i.e., you’re using more Big Data than physical DRAM. We won’t die with a GC death-spiral, but we will degrade to out-of-core speeds. We’ll go as fast as the disk will allow. I’ve personally tested loading a 12Gb dataset into a 2Gb (32bit) JVM; it took about 5 minutes to load the data, and another 5 minutes to run a Logistic Regression.


Here is the 
R
 code to connect to 
h2o 3.6.0.8
:


h2o.init(max_mem_size = '60m') # alloting 60mb for h2o, R is running on 8GB RAM machine



gives


java version ""1.8.0_65""
Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)

.Successfully connected to http://127.0.0.1:54321/ 

R is connected to the H2O cluster: 
    H2O cluster uptime:         2 seconds 561 milliseconds 
    H2O cluster version:        3.6.0.8 
    H2O cluster name:           H2O_started_from_R_RILITS-HWLTP_tkn816 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   0.06 GB 
    H2O cluster total cores:    4 
    H2O cluster allowed cores:  2 
    H2O cluster healthy:        TRUE 

Note:  As started, H2O is limited to the CRAN default of 2 CPUs.
       Shut down and restart H2O as shown below to use all your CPUs.
           > h2o.shutdown()
           > h2o.init(nthreads = -1)

IP Address: 127.0.0.1 
Port      : 54321 
Session ID: _sid_b2e0af0f0c62cd64a8fcdee65b244d75 
Key Count : 3



I tried to load a 169 MB csv into h2o.


dat.hex <- h2o.importFile('dat.csv')



which threw an error,


Error in .h2o.__checkConnectionHealth() : 
  H2O connection has been severed. Cannot connect to instance at http://127.0.0.1:54321/
Failed to connect to 127.0.0.1 port 54321: Connection refused



which is indicative of out of memory 
error
.




Question: If H2o promises loading a data set larger than its memory capacity(swap to disk mechanism as the blog quote above says), is this the correct way to load the data?","['java', 'r', 'garbage-collection', 'out-of-memory', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
34060916,34060916,2015-12-03T08:18:53,2015-12-03 09:05:07Z,0,"I am getting an error when trying to start h2o. Running the following start-up commands:


library(h2o)
localH2O = h2o.init(ip = ""localhost"", port = 54321, startH2O = TRUE)



Gives the following error:


Error in system2(command, ""-version"", stdout = TRUE, stderr = TRUE) : '""""' not found



It's not particularly enlightning. Does anyone have a hint as to what is the problem? Java is installed on the system, but unlike the question 
here
 I do not get any Java warnings.


Running system(""java -version"") gives:


java version ""1.8.0_65""
Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)



Here is the output of sessionInfo():


R version 3.2.2 (2015-08-14)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows Server 2008 R2 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252    LC_MONETARY=Danish_Denmark.1252
[4] LC_NUMERIC=C                    LC_TIME=Danish_Denmark.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] h2o_3.6.0.8              statmod_1.4.22           deepnet_0.2             
 [4] TTR_0.23-0               selectiveInference_1.1.1 intervals_0.15.1        
 [7] lars_1.2                 glmnet_2.0-2             foreach_1.4.3           
[10] Matrix_1.2-2             wq_0.4.4                 zoo_1.7-12              
[13] skm_1.0.2                stringi_1.0-1            devtools_1.9.1          

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.2      rstudioapi_0.3.1 magrittr_1.5     MASS_7.3-43      munsell_0.4.2   
 [6] colorspace_1.2-6 lattice_0.20-33  stringr_1.0.0    plyr_1.8.3       xts_0.9-7       
[11] tools_3.2.2      grid_3.2.2       gtable_0.1.2     iterators_1.0.8  digest_0.6.8    
[16] reshape2_1.4.1   ggplot2_1.0.1    bitops_1.0-6     codetools_0.2-14 RCurl_1.95-4.7  
[21] memoise_0.2.1    scales_0.3.0     jsonlite_0.9.17  proto_0.3-10","['r', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
34040136,34040136,2015-12-02T10:17:25,2015-12-02 22:49:01Z,0,"I have a model in h2o(R). Its performance generates 


h2o.performance(models[[1]],valid=T)



gives output 


H2ORegressionMetrics: deeplearning
** Reported on validation data. **
Description: Metrics reported on temporary validation frame with 9724 samples

MSE:  1.18963
R2 :  0.07689513
Mean Residual Deviance :  1.18963



I want to take MSE and save it in a variable. i tried to generate confusion matrix using


 h2o.confusionMatrix(h2o.performance(models[[i]],valid=T))



but it generates NULL.","['r', 'machine-learning', 'h2o']",saurabh agarwal,https://stackoverflow.com/users/3391524/saurabh-agarwal,"2,144"
33964062,33964062,2015-11-27T19:54:29,2015-11-27 22:05:46Z,0,"Hi I have a problem with the h2o(3) not loading my factors.


The Problem is similar to: 
Unable to convert data frame to h2o object
 


However I tried to use: 
myData<- data.frame(apply(myData, 2, factor, ordered=FALSE))
 but the error persists: 
Provided column type c(""ordered"", ""enum"") is unknown.


Is there some other method to ensure that my 
factor
 is 
unordered
?


edit


here a subset of the data


dput(droplevels(head(myData[1:5])))
structure(list(Id = structure(1:6, .Label = c(""    2"", ""    5"", 
""    6"", ""    7"", ""    8"", ""   10""), class = ""factor""), factor1 = structure(c(1L, 
1L, 1L, 1L, 1L, 1L), .Label = ""1"", class = ""factor""), factor2 = structure(c(3L, 
1L, 5L, 4L, 2L, 2L), .Label = c(""A1"", ""D2"", ""D3"", ""D4"", ""E1""), class = ""factor""), 
    factor3 = structure(c(1L, 2L, 2L, 1L, 2L, 2L), .Label = c(""10"", 
    ""26""), class = ""factor""), factor5 = structure(c(1L, 
    1L, 1L, 1L, 1L, 2L), .Label = c(""2"", ""3""), class = ""factor"")), .Names = c(""Id"", 
""factor1"", ""factor2"", ""factor3"", ""factor5""
), row.names = c(NA, 6L), class = ""data.frame"")
> head(myData[1:5])
     Id          factor1         factor2       factor3         factor5
1     2              1             D3             10              2
2     5              1             A1             26              2
3     6              1             E1             26              2
4     7              1             D4             10              2
5     8              1             D2             26              2
6    10              1             D2             26              3","['r', 'r-factor', 'unordered', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
33925864,33925864,2015-11-25T20:28:52,2015-11-26 00:09:23Z,0,"I have a question similar to this (
link
) except that my question refers to the java tool 'h2o' and its connection to 'r'.


In particular I want to assign a ""h2o"" object to part of a vector (or structure or array.  I want to loop through and store several of them without having to manually enumerate.


I tried the solution at the link but it does not work for 'h2o' objects.


Here is my longer code (warts and all):


#libraries
library(h2o)      #for tree control

#specify data
mydata <- iris[iris$Species!=""setosa"",]
mydata$Species <- as.factor(as.character(mydata$Species))

#most informative variable is petal length
x1 <- mydata$Petal.Length
x2 <- mydata$Petal.Width

#build classes
C <- matrix(0,nrow=length(x1),ncol=1)
idx1 <- which(mydata$Species == ""versicolor"",arr.ind=T)
idx2 <- which(mydata$Species != ""versicolor"",arr.ind=T)
C[idx1] <- +1
C[idx2] <- 0

#start h2o
localH2O = h2o.init(nthreads = -1)

# Run regression GBM on iris.hex data
irisPath = system.file(""extdata"", ""iris.csv"", package=""h2o"")
iris.hex = h2o.uploadFile(localH2O, path = irisPath)
names(iris.hex) <- c(""Sepal.Length"",
                     ""Sepal.Width"",
                     ""Petal.Length"",
                     ""Petal.Width"",
                     ""Species"" )

iris2 <- iris
iris2$Species <- unclass(iris$Species)
iris2.hex <- as.h2o(iris2)
iris.hex$Species <- as.factor(iris2.hex$Species)

independent <- c(""Sepal.Length"",""Sepal.Width"",""Petal.Length"",""Petal.Width"")
dependent <- ""Species""

mare <- numeric()
mae <- matrix(1,nrow=10,ncol=1)

est2.h2o <- vector(mode=""list"", length=150)

for (i in 1:150){

     est2.h2o[[i]] <- h2o.gbm(y = dependent, 
                         x = independent, 
                         training_frame = iris.hex,
                         distribution=""AUTO"",
                         ntrees = i, max_depth = 3, min_rows = 2,
                         learn_rate = 0.5)


     pred <- h2o.predict(est2.h2o,newdata=iris.hex)

     err <- iris2$Species-(as.data.frame(pred)$predict+1)

     mae[i] <- mean(abs(err))
     mare[i] <- mean(abs(err)/iris2$Species)

     print(c(i,log10(mae[i])))

}



The error that I get is:


Error in paste0(""Predictions/models/"", object@model_id, ""/frames/"", newdata@frame_id) : 
  trying to get slot ""model_id"" from an object of a basic class (""list"") with no slots



My intention is to have a list/structure/array of GBM's that I can then run predict against for the whole data-set, and cull the less informative ones.  I'm trying to make a decent ""random forest of gbt's"" following the steps of  Eugene Tuv.  I don't have his code.


Questions:

Is there a proper way to pack the h2o gbm along with a few (hundred) of its buddies, into a single store in r?


If the referenced object is thrown away in java, making this sort of approach unfeasible, is there a feasible variation using the 'gbm' library?  If I end up having to use gbm, what is the speed difference vs. h2o?","['r', 'gbm', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
33919640,33919640,2015-11-25T14:55:21,2015-11-25 15:14:12Z,0,"I'm working with the current version of H2O in R and following a 
""deeplearning"" example
 which must've been created with an earlier version.


A challenge I'm having in adapting the old example code, is that I can't programmatically access the cross-validation statistics like MSE with code like:


cvmodel@model$valid_sqr_error


Upon inspecting the structure of the H2O model object, I'm able to find the MSE and other statistics here:


> cvmodel@model$cross_validation_metrics
H2ORegressionMetrics: deeplearning
** Reported on cross-validation data. **
Description: 3-fold cross-validation on training data

MSE:  39.47373
R2 :  0.03510394
Mean Residual Deviance :  39.47373



But I can't figure out the syntax to programmatically grab just one of those numbers:


> cvmodel@model$cross_validation_metrics$MSE
Error in cvmodel@model$cross_validation_metrics$MSE : 
  $ operator not defined for this S4 class
> cvmodel@model$cross_validation_metrics@MS
Error: no slot of name ""MS"" for this object of class ""H2ORegressionMetrics""","['r', 'h2o']",user20650,https://stackoverflow.com/users/2250190/user20650,25.7k
33909907,33909907,2015-11-25T06:35:14,2018-05-15 12:37:00Z,0,"I want to use machine learning algorithms in java. Mahout with hadoop is too slow and weka is not able to work because of large datasize.
So is it possible to call H2O library from Java or any other better option available for java?","['java', 'r', 'machine-learning', 'h2o']",Unknown,,N/A
33876256,33876256,2015-11-23T16:46:48,2016-06-28 08:50:19Z,0,"I know h2o's internal data model is column oriented (namely an H2OFrame is a collection of H2OVec). However, the library I'd like to use requires to iterate through the rows of an H2OFrame.


Is there a clean way to get an iterator on the rows or do I need to resort to indexing like


iris = h2o.import_file(path="".../iris_wheader.csv"")
for i in xrange(iris.nrow):
    foo( iris[i,:].as_data_frame(use_pandas=False)[1] )



I know it's going to be slow, I'm using 
h2o.h2o.export_file
 when possible.","['python', 'h2o']",Unknown,,N/A
33875922,33875922,2015-11-23T16:31:25,2015-11-24 15:00:15Z,69,"Is there a way from the Python client to distinguish the cases where H2O was launched locally through 
java -jar h2o.jar
 and where it was launched on hadoop through 
hadoop -jar h2odriver.jar
?","['python', 'hadoop', 'h2o']",Unknown,,N/A
33801857,33801857,2015-11-19T10:54:12,2016-07-28 05:20:34Z,0,"Closed
. This question needs to be more 
focused
. It is not currently accepting answers.
                                
                            
























Want to improve this question?
 Update the question so it focuses on one problem only by 
editing this post
.






Closed 
8 years ago
.















                        Improve this question
                    








Hy,
in the last days I had a small/big problem.


I have a transaction dataset, with 1 million rows and two columns (Client Id and product id) and I want transform this in a binary matrix.
I used reshape and spread function, but in both cases I used 64mb ram and Rstudio/R goes down.
Because I only use 1 CPU, the process takes a lot of time
My question is, what is it the new steep forward in this transition between small and big data? Who can I use more cpu?


I search and I found a couple of solution but I need a expertise opinion


1 - Using Spark R? 


2 - H20.ai solution?  
http://h2o.ai/product/enterprise-support/


3 - Revolution analytics? 
http://www.revolutionanalytics.com/big-data


4 - go to the cloud? like microsoft azure? 


If I needed I can use a virtual machine with a lot of cores.. but I need to know what is the smooth way to make this transaction


My specific problem


I have this data.frame (but with 1 million rows)


Sell<-data.frame(UserId = c(1,1,1,2,2,3,4), Code = c(111,12,333,12,111,2,3))



and I did:


Sell[,3] <-1

test<-spread(Sell, Code, V3)



this works with a little data set.. but with 1 million rows this takes a long time (12 hours) and goes down because my maximum ram is 64MB. Any suggestions?","['r', 'azure', 'sparkr', 'h2o', 'data-munging']",tmthydvnprt,https://stackoverflow.com/users/2087463/tmthydvnprt,10.7k
33705423,33705423,2015-11-14T05:24:55,2015-11-18 11:48:15Z,0,"Few Doubts related to h2o.deeplearning:




Is it a deep neural network or deep belief neural network(which uses Restricted Boltzman Machine - RBM)


No where in the documentation of H2o,RBM is discussed.They are using SGD Stochastic gadient descent (which is being used by normal neural network).


If we are not using autoencoders and RBM then can we call it a deep belief neural network.




H2o is really very interesting, but with these doubts it will be very difficult to use it.","['r', 'h2o']",Unknown,,N/A
33640786,33640786,2015-11-10T22:21:22,2018-08-24 07:21:53Z,0,"I want to do a binary classification and one level is ""top"", the other is ""bottom"". I used gbm in h2o packages and get ""bottom"" as positive class and ""top"" as negative class. 
Here is my code:    


fit <- h2o.gbm(x = regr.var, y = max.var,
             training_frame = ddd, 
             nfolds = 10, 
             distribution = 'multinomial',
             balance_classes = TRUE)
pred <- as.data.frame(h2o.predict(fit, newdata = eee))
threshold <- 0.5
pred1 <- factor( ifelse(pred[, 'top'] > threshold, 'top', 'bottom') )
err.res<-confusionMatrix(pred1 , hh$score_class)
err.res



Here is the result:


Confusion Matrix and Statistics
           Reference
Prediction bottom top
bottom      420   123
top          1     6
Accuracy : 0.7745          
95% CI : (0.7373, 0.8088)
No Information Rate : 0.7655          
P-Value [Acc > NIR] : 0.3279          

Kappa : 0.0657          
Mcnemar's Test P-Value : <2e-16          

Sensitivity : 0.99762         
Specificity : 0.04651         
Pos Pred Value : 0.77348         
Neg Pred Value : 0.85714         
Prevalence : 0.76545         
Detection Rate : 0.76364         
Detection Prevalence : 0.98727         
Balanced Accuracy : 0.52207         

'Positive' Class : bottom          



But I want to correctly predict more ""top"". I tried to change the threshold to 0.3, and it performs better. However, should I change in the fitting process to make more prediction to ""top"" like ""ROC"" metric? Should I flip the ""top"" to positive class and ""bottom"" to negative class and how can I change it?","['r', 'classification', 'roc', 'gbm', 'h2o']",James Wang,https://stackoverflow.com/users/5548468/james-wang,21
33624415,33624415,2015-11-10T06:51:26,2015-11-19 00:53:33Z,0,"I know the 
as.h2o
 function from 
h2o
 library converts an R data.frame to an H2O frame. Two questions:




Does 
as.h2o()
 write data to disk during conversion?  How long is this data stored?


Are there other options that avoids the temp step of writing to disk?","['r', 'h2o']",Unknown,,N/A
33480289,33480289,2015-11-02T14:46:29,2015-12-17 13:33:51Z,0,"I accidently imported more data into 0xdata's h2o flow than I actually intended. How can I delete all my data frames?


I already tried 
Data -> List All Frames -> Delete
, but I get the following error message:




Error evaluating cell


Error calling DELETE/....ink


Object 'nfs:....lnk' not found for argument: key




Is there another way to erase those data frames? Where are those data frames physically stored?",['h2o'],Community,https://stackoverflow.com/users/-1/community,1
33442536,33442536,2015-10-30T18:19:55,2018-09-17 14:00:44Z,174,"This is what I tried. Depends on what does user put into the function I want to add String or Double to new Chunk.


package org.apache.spark.h2o.utils

import water.fvec.{NewChunk, Frame, Chunk}
import water._
import water.parser.ValueString

class ReplaceNa[T >: Any](a: T) extends MRTask{
  override def map(c: Chunk, nc: NewChunk): Unit = {
    for (row <- 0 until c.len()) {

        a match{
             case s: ValueString if(c.isNA(row)) => nc.addStr(s)           
             case d: Double      if(c.isNA(row)) => nc.addNum(d)

      }
    }
  }
}



But I got error 


 error: value outputFrame is not a member of Nothing
          pred.add(new ReplaceNa(3).doAll(1, pred.vec(4)).outputFrame(Array(""s""), null))



Thanks for your help!","['scala', 'parameters', 'match', 'h2o']",Juh_,https://stackoverflow.com/users/1206998/juh,15.4k
33439768,33439768,2015-10-30T15:41:01,2015-10-30 18:18:18Z,81,"I'm very new to Scala and H2O. I know there is extend class in Scala. But what I want to do is have two classes with same name but different parameter type.


I hope when I call test, if I put either a String or a Double parameter into it. The class will recognize the Data type I put into it and run the right function. Thanks for your help!","['scala', 'overriding', 'h2o']",Unknown,,N/A
33424324,33424324,2015-10-29T20:48:57,2015-10-30 01:09:48Z,36,"By using 
newChunk.addNum(8)
 i can add a number to a row in my new chunk. How can I add a String to a row in in the new chunk? Thanks!","['scala', 'h2o']",Gavin Niu,https://stackoverflow.com/users/5390309/gavin-niu,"1,335"
33422795,33422795,2015-10-29T19:17:25,2015-10-29 19:28:00Z,27,"I wrote a Miss Class to extends MRTask to check missing values in a H2oFrame. This is what I did


package org.apache.spark.h2o.utils

import water.fvec.{NewChunk, Frame, Chunk}
import water._

class Miss extends MRTask[Log] {
  override def map(c: Chunk, nc: NewChunk): Unit = {
    for (row <- 0 until c.len()) {
      if(c.atd(row).isNaN){
        nc.addNum(0)
      }
      else 
      nc.addNum(1)
    }
  }
}



As you can see, here when a row is NaN we can get 0 in new column. If I want to use a number other than 0 to mark missing value, I have to change my code to do it. Can I just add a new parameter X in my Miss Class? When I call this class I can just write something like 


H2OFrame.add(new Miss(8).doAll(1, pred.vec(0)).outputFrame(Array(""Miss""), null))



Thanks so much for your help!","['scala', 'h2o']",Odomontois,https://stackoverflow.com/users/210905/odomontois,16.3k
33401869,33401869,2015-10-28T21:25:02,2015-10-28 22:19:39Z,237,"This one is not duplicate I have new question. I tried to write this


package org.apache.spark.h2o.utils

import water.fvec.{NewChunk, Frame, Chunk}
import water._

class Miss extends MRTask {
  override def map(c: Chunk, nc: NewChunk): Unit = {
    for (row <- 0 until c.len()) {
      if(c.atd(row) == 0){  
       nc.addNum(0)
      }
      else
       nc.addNum(1)
    }
  }
}



Can I use na or IsNull in 
if (...)
 to check whether or not that 
row
 is null?


Code result


          A    B    C    D            E    NaN
    min                                     0
   mean                                     0
 stddev                                     0
    max                                     1
missing                                     0
      0  5.1  3.5  1.4  0.2  Iris-setosa    1
      1  4.9    3  1.4  0.2  Iris-setosa    1
      2  4.7  3.2  1.3  0.2  Iris-setosa    1
      3  4.6  3.1  1.5  0.2  Iris-setosa    1
      4    5  3.6  1.4  0.2  Iris-setosa    1
      5  5.4  3.9  1.7  0.4  Iris-setosa    1
      6  4.6  3.4  1.4  0.3  Iris-setosa    1
      7    5  3.4  1.5  0.2  Iris-setosa    1
      8  4.4  2.9  1.4  0.2  Iris-setosa    1
      9  4.9  3.1  1.5  0.1  Iris-setos...","['scala', 'h2o']",Peter Neyens,https://stackoverflow.com/users/5020846/peter-neyens,"9,800"
33399663,33399663,2015-10-28T19:14:15,2015-10-30 20:37:47Z,58,"I tried this to check whether a row is null or not.






package org.apache.spark.h2o.utils

import water.fvec.{NewChunk, Frame, Chunk}
import water._

class Miss extends MRTask {
  override def map(c: Chunk, nc: NewChunk): Unit = {
    for (row <- 0 until c.len()) {
      if(c.atd(row) == 0){  
       nc.addNum(0)
      }
      else
       nc.addNum(1)
    }
  }
}








And I can not understand the result of my code here






           A    B    C    D            E   check
    min                                     0
   mean                                     0
 stddev                                     0
    max                                     1
missing                                     0
      0  5.1  3.5  1.4  0.2  Iris-setosa    1
      1  4.9    3  1.4  0.2  Iris-setosa    1
      2  4.7  3.2  1.3  0.2  Iris-setosa    1
      3  4.6  3.1  1.5  0.2  Iris-setosa    1
      4    5  3.6  1.4  0.2  Iris-setosa    1
      5  5.4  3.9  1.7  0.4  Iris-setosa    1
      6  4.6  3.4  1.4  0.3  Iris-setosa    1
      7    5  3.4  1.5  0.2  Iris-setosa    1
      8  4.4  2.9  1.4  0.2  Iris-setosa    1
      9  4.9  3.1  1.5  0.1  Iris-setos...








In the code generate check column, Why my max row is 1? I'm new to h2oFrame, can anyone help me understand this? IS there something wrong with my code? Thx","['scala', 'h2o']",Unknown,,N/A
33396042,33396042,2015-10-28T16:07:58,2015-10-30 01:56:28Z,171,"I tried to write this


package org.apache.spark.h2o.utils

import water.fvec.{NewChunk, Frame, Chunk}
import water._

class Miss extends MRTask{
  override def map(c: Chunk, nc: NewChunk): Unit = {
    for (row <- 0 until c.len()) {
      if(  ){  
       nc.addNum(1)
      }
      else
       nc.addNum(0)
    }
  }
}



What can I put in 
if (...)
 to check whether or not there is a 
null
 value in that row?","['scala', 'h2o']",Peter Neyens,https://stackoverflow.com/users/5020846/peter-neyens,"9,800"
33345675,33345675,2015-10-26T12:22:41,2017-12-19 09:58:21Z,0,"I am trying to initialize H2O from Python. I am using python 2.7.9.
I followed the steps below to get h2o python module:


pip install requests
pip install tabulate

# Remove any preexisiting H2O module.
pip uninstall h2o
# Next, use pip to install this version of the H2O Python module.
pip install http://h2o-release.s3.amazonaws.com/h2o-dev/master/1109/Python/h2o-0.3.0.1109-py2.py3-none-any.whl



I get this error when I call h2o.init().


No instance found at ip and port: localhost:54321. Trying to start local jar...

No jar file found. Could not start local instance.
Traceback (most recent call last):
File ""abc.py"", line 3, in <module>
h2o.init()
File ""/usr/local/lib/python2.7/dist-packages/h2o/h2o.py"", line 229, in init
H2OConnection(ip=ip, port=port)
File ""/usr/local/lib/python2.7/dist-packages/h2o/connection.py"", line 64, in __init__
cld = self._connect(size)
File ""/usr/local/lib/python2.7/dist-packages/h2o/connection.py"", line 113, in _connect
cld = H2OConnection.get_json(url_suffix=""Cloud"")
File ""/usr/local/lib/python2.7/dist-packages/h2o/connection.py"", line 324, in get_json
return __H2OCONN__._rest_json(url_suffix, ""GET"", None, **kwargs)
File ""/usr/local/lib/python2.7/dist-packages/h2o/connection.py"", line 333, in _rest_json
raw_txt = self._do_raw_rest(url_suffix, method, file_upload_info, **kwargs)
File ""/usr/local/lib/python2.7/dist-packages/h2o/connection.py"", line 366, in _do_raw_rest
http_result = self._attempt_rest(url, method, post_body, file_upload_info)
File ""/usr/local/lib/python2.7/dist-packages/h2o/connection.py"", line 394, in _attempt_rest
return requests.get(url, headers=headers)
File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 69, in get
return request('get', url, params=params, **kwargs)
File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 50, in request
response = session.request(method=method, url=url, **kwargs)
File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 468, in request
resp = self.send(prep, **send_kwargs)
File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 576, in send
r = adapter.send(request, **kwargs)
File ""/usr/local/lib/python2.7/dist-packages/requests/adapters.py"", line 370, in send
timeout=timeout
File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py"", line 559, in urlopen
body=body, headers=headers)
File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py"", line 353, in _make_request
conn.request(method, url, **httplib_request_kw)
File ""/usr/lib/python2.7/httplib.py"", line 1048, in request
self._send_request(method, url, body, headers)
File ""/usr/lib/python2.7/httplib.py"", line 1087, in _send_request
self.putheader(hdr, value)
File ""/usr/lib/python2.7/httplib.py"", line 1026, in putheader
raise ValueError('Invalid header value %r' % (one_value,))
ValueError: Invalid header value 'H2O Python client/2.7.9 (default, Apr  2 2015, 15:33:21) \n[GCC 4.9.2]'



Please help me.
Thanks in advance.","['python-2.7', 'h2o']",Jithin K M,https://stackoverflow.com/users/5489124/jithin-k-m,1
33331725,33331725,2015-10-25T15:57:17,2015-10-30 01:28:45Z,613,"Why is 
h2o.randomforest
 calculating MSE on Out of bag sample and while training for a multinomail classification problem?


I have done binary classification also using h2o.randomforest, there it used to calculate 
AUC
 on 
out of bag sample
 and while 
training
 but for multi classification random forest is calculating MSE which seems suspicious. Please see this screenshot.




My target variable was a factor containing 4 factor levels 
model1
, 
model2
, 
model3
 and 
model4
. In the screenshot you would also a confusion matrix for these factors. 


Can someone please explain this behaviour?","['classification', 'random-forest', 'multinomial', 'h2o']",user3664020,https://stackoverflow.com/users/3664020/user3664020,"3,020"
33287846,33287846,2015-10-22T18:12:46,2015-11-03 15:56:41Z,0,"UPDATE:
I figured out the problem, but not the solution. It is a problem with the new Java version. I started having this problem Once I updated to the latest Java.


I've been using h2o and the deep learning package for a while with no problems. Today I used it again, but I keep getting the same value for all the rows in a specific column when I extract the features from the deeplearning model. It used to work fine before. I tried using different datasets which didn't work. so I am guessing it's either my dataset (which used to work before), or something deeper than that.


This is my code. Any help is appreciated.


library(h2o)
localH20=h2o.init(nthreads = -1)
data=h2o.importFile(localH20, path=""smsAllWords.csv"", header=T)
model= h2o.deeplearning(x=1:107, training_frame = data, shuffle_training_data=TRUE, activation = ""Tanh"", hidden = c(100,75,50,25), epochs= 5,autoencoder = TRUE)
dl.data=h2o.deepfeatures(model,data,4)` 



this is a sample of what the data and output looks like:
FYI: the variation of the data is higher for the rest of the matrix.


    data[1:10,1:10]
H2OFrame with 10 rows and 10 columns
         a1       a2       a3      a4       a5       a6       a7       a8       a9      a10
1  -0.31289 -0.10442 -0.14504 -0.1143 -0.11115 -0.12753 -0.10413 -0.28192 -0.13307 -0.27609
2  -0.31289 -0.10442 -0.14504 -0.1143 -0.11115 -0.12753 -0.10413  2.13240 -0.13307  1.80440
3  -0.31289 -0.10442 -0.14504 -0.1143 -0.11115 -0.12753  6.59090 -0.28192 -0.13307 -0.27609
4  -0.31289 -0.10442 -0.14504 -0.1143 -0.11115 -0.12753 -0.10413 -0.28192 -0.13307 -0.27609
5  -0.31289 -0.10442 -0.14504 -0.1143 -0.11115 -0.12753 -0.10413 -0.28192 -0.13307  4.40510
6  -0.31289 -0.10442 -0.14504 -0.1143 -0.11115 -0.12753 -0.10413 -0.28192 -0.13307 -0.27609
7  -0.31289 -0.10442 -0.14504 -0.1143 -0.11115 -0.12753 -0.10413 -0.28192 -0.13307 -0.27609
8  -0.31289 -0.10442 -0.14504 -0.1143 -0.11115 -0.12753 -0.10413 -0.28192 -0.13307 -0.27609
9  -0.31289 -0.10442 -0.14504 -0.1143 -0.11115 -0.12753 -0.10413 -0.28192 -0.13307 -0.27609
10 -0.31289  8.16410 -0.14504 -0.1143 -0.11115 -0.12753 -0.10413 -0.28192 -0.13307 -0.27609


dl.data[1:10,1:10]
H2OFrame with 10 rows and 10 columns
   DF.L4.C1 DF.L4.C2 DF.L4.C3 DF.L4.C4 DF.L4.C5 DF.L4.C6 DF.L4.C7 DF.L4.C8 DF.L4.C9 DF.L4.C10
1        -1       -1        1       -1        1        1        1        1       -1        -1
2        -1       -1        1       -1        1        1        1        1       -1        -1
3        -1       -1        1       -1        1        1        1        1       -1        -1
4        -1       -1        1       -1        1        1        1        1       -1        -1
5        -1       -1        1       -1        1        1        1        1       -1        -1
6        -1       -1        1       -1        1        1        1        1       -1        -1
7        -1       -1        1       -1        1        1        1        1       -1        -1
8        -1       -1        1       -1        1        1        1        1       -1        -1
9        -1       -1        1       -1        1        1        1        1       -1        -1
10       -1       -1        1       -1        1        1        1        1       -1        -1



edit:
I ran the same dataset and a different one multiple times, and I'll get different results. I am not changing my code at all, but sometimes it will work, sometimes it will not. Am I missing a parameter that can control this?


UPDATE:
I also tried the same data set on a different machine and it worked properly. So I am almost certain it's an issue with the other machine. I tried removing R and deleting everything associated with it then re-install it, but that didn't solve the problem.","['r', 'deep-learning', 'h2o']",Unknown,,N/A
33070159,33070159,2015-10-11T21:10:05,2015-10-15 21:25:14Z,0,"I'm trying to ensemble a random forest with logistic regresion with H2O in R. However, an error messages appears in the following code:


> localH2O = h2o.init()
    Successfully connected to http://137.0.0.1:43329/ 

    R is connected to the H2O cluster: 
        H2O cluster uptime:         3 hours 11 minutes 
        H2O cluster version:        3.2.0.3 
        H2O cluster name:           H2O_started_from_R_toshiba_jvd559 
        H2O cluster total nodes:    1 
        H2O cluster total memory:   0.97 GB 
        H2O cluster total cores:    4 
        H2O cluster allowed cores:  2 
        H2O cluster healthy:        TRUE 

    > 
    > # defining the training data and set data for H2O

    > 
    > training_frame <- as.h2o(localH2O, muestra.fullarbol)
      |=========================================================================================| 100%
    > validation_frame <- as.h2o(localH2O, test.fullarbol)
      |=========================================================================================| 100%
    > 
    > yn <- ""ex""
    > xn <- names(datafullarbol[,-c(1,2,3,9,10,11,12,17,19,20,21,22,23,24,29,31,32,33,34,35,36,47)])
    > 
    > 
    > 
    > 
    > learner <- c(""h2o.glm.wrapper"", ""h2o.randomForest.wrapper"")
    > metalearner <- ""SL.glm""
    > family <- ""binomial""
    > 
    > fit <- h2o.ensemble(x=xn, y=yn,training_frame = training_frame, family = family, 
    + learner = learner, metalearner = metalearner,cvControl = list(V = 5))
      |=========================================================================================| 100%
    [1] ""Cross-validating and training base learner 1: h2o.glm.wrapper""
      |=========================================================================================| 100%
    [1] ""Cross-validating and training base learner 2: h2o.randomForest.wrapper""
      |=========================================================================================| 100%
    Error in h2o.cbind(predlist) : 
      `h2o.cbind` accepts only of H2OFrame objects



Apperently my parameters are given correctly , but as you see, the message: 
h2o.cbind accepts only of H2OFrame objects appears
. What could be the reason of the error?","['r', 'machine-learning', 'h2o', 'ensemble-learning']",CreamStat,https://stackoverflow.com/users/2825079/creamstat,"2,175"
33053714,33053714,2015-10-10T11:58:14,2016-04-03 18:38:55Z,0,"I have no problems saving a h20 glm model(as this has a shorter file name) but I am having problems saving a h2o deeplearning model using the exactly the same saving procedure


I tried:


library(h2o)
localH2O = h2o.init()
a <- runif(1000)
b <- runif(1000)
c <- runif(1000)
d <- 5*a+2*b^2+c*a

df1 <- data.frame(a,b,c,d)


df1.hex <- as.h2o(df1)
test.dl <- h2o.deeplearning(x = 1:3, y = 4, training_frame = df1.hex)


dlmodel.path = h2o.saveModel(test.dl, dir = ""file:///C:/"", name = ""modeldl"")
dlmodel.path



But get an error:


Error in .h2o.doSafeREST(conn = conn, h2oRestApiVersion = h2oRestApiVersion,  : 
  FS IO Failure: 
 accessed path : file:///C://modeldl/modelmetrics_DeepLearningModel__9fe11910a85d1371379ac7d536d64359_-5064771152374762981_on_Key_Frame__C__Users_store_AppData_Local_Temp_RtmpGGylNe_file1f18787f2989_csv_1.hex_2.DeepLearningModel__9fe11910a85d1371379ac7d536d64359.temporary.train.chunks8_-6759658083019717917.bin



I am using a windows 10 computer. As has been pointed out by RHA, the filepath/name is extremely long and is too long for windows.How can I overcome this? Most of the filepath characters are generated automatically by the h20 program.  I am using the latest h20 update.
from.sessionInfo(): other attached packages: [1] h2o_3.0.0.30 


I would be grateful for your help.","['r', 'deep-learning', 'h2o']",Unknown,,N/A
32988493,32988493,2015-10-07T09:21:39,2016-07-22 18:22:00Z,0,"I'm trying to test both RStudio and H2O in a single instance of amazon EC2. Started with an AMI prepared with RStudio and had no trouble. After that, downloaded and installed H2O succesfully and the package for R.


Problem is, while both Rstudio and H2O are up and running, I cannot connect to the H2O flow web interface (DNS: h2o port). I don't know if there's some kind of conflict between the two web interfaces or there's other problems.","['amazon-ec2', 'rstudio', 'h2o']",Rwak,https://stackoverflow.com/users/2123175/rwak,326
32915566,32915566,2015-10-02T20:43:07,2017-03-29 20:55:22Z,596,"Folks,


I have some problem when try resuming h2o deep learning in R from a checkpointed model 
with validation frame provided
. It says ""Validation dataset must be the same as for the check pointed model"", which I believe I do have the same validation datasets. If I leave validation_frame blank, checkpointing model works fine. I attach my code below:


localh2o <- h2o.init(nthreads = -1)
train_image.hex <- read.csv(""mnist_train.csv"",header=FALSE)
train_image.hex[,785] <- factor(train_image.hex[,785])
train_image.hex <- as.h2o(train_image.hex)
test_image.hex <- read.csv(""mnist_test.csv"",header=FALSE)
test_image.hex[,785] <- factor(test_image.hex[,785])
test_image.hex <- as.h2o(test_image.hex)


mnist_model <- h2o.deeplearning(x=1:784, y = 785,
training_frame= train_image.hex, 
validation_frame = test_image.hex,
activation = ""RectifierWithDropout"", hidden = c(500,1000),
input_dropout_ratio = 0.2,
hidden_dropout_ratios = c(0.5,0.5), adaptive_rate=TRUE,
rho=0.98, epsilon = 1e-7,
l1 = 1e-8, l2 = 1e-7, max_w2 = 10, 
epochs = 10, export_weights_and_biases = TRUE,
variable_importances = FALSE
)
h2o.saveModel(mnist_model, path=""/tmp"",force=TRUE)



Then I shut down the h2o, quit R and restart h2o in R to resume training, where h2o errors out:


localh2o <- h2o.init(nthreads = -1)
train_image.hex <- read.csv(""mnist_train.csv"",header=FALSE)
train_image.hex[,785] <- factor(train_image.hex[,785])
train_image.hex <- as.h2o(train_image.hex)
test_image.hex <- read.csv(""mnist_test.csv"",header=FALSE)
test_image.hex[,785] <- factor(test_image.hex[,785])
test_image.hex <- as.h2o(test_image.hex)
startmodel <- h2o.loadModel(""/tmp/DeepLearning_model_R_1443812402059_20"", localh2o)

mnist_model <- h2o.deeplearning(x=1:784, y = 785,
checkpoint = startmodel@model_id,
training_frame= train_image.hex, 
validation_frame = test_image.hex,
activation = ""RectifierWithDropout"", hidden = c(500,1000),
input_dropout_ratio = 0.2,
hidden_dropout_ratios = c(0.5,0.5), adaptive_rate=TRUE,
rho=0.98, epsilon = 1e-7,
l1 = 1e-8, l2 = 1e-7, max_w2 = 10, 
epochs = 10, export_weights_and_biases = TRUE,
variable_importances = FALSE
)","['deep-learning', 'h2o', 'checkpointing']",russiancube,https://stackoverflow.com/users/5402750/russiancube,11
32896008,32896008,2015-10-01T20:08:22,2015-10-01 20:19:28Z,0,"I get the following error whenever I use 
h2o.init()
:




localh2o<-h2o.init()


H2O is not running yet, starting it now...

Error in system2(command, ""-version"", stdout = TRUE, stderr = TRUE) :

'""""' not found

In addition: Warning message:

In .h2o.checkJava() :

Found JRE at C:/Program Files (x86)/Java/jre7/bin/java.exe but H2O requires the JDK to run




I am running it on RStudio Version 0.99.473 and R version 3.2.2, 64 bit os","['r', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
32843267,32843267,2015-09-29T11:51:22,2015-10-01 00:59:03Z,0,"I am trying to get a sparse matrix into 
H2O
 and I was wondering whether that was possible. Suppose we have the following:


test <- Matrix(c(1,0,0,1,1,1,1,0,1), nrow = 3, sparse = TRUE)



and assuming my local H2O is 
localH2O
, I can't seem to do the following:


as.h2o(test)



It gives the error: 
cannot coerce class ""structure(""dgCMatrix"", package = ""Matrix"")"" to a data.frame
. That seems to be pretty logical, however assuming that test is so big that I can't transform it into a dataframe, how am I suppose to load this into H2O? Using a sparse matrix representation it is only 500MB or so.


How can I load a sparse matrix into H2O?","['r', 'sparse-matrix', 'h2o']",Snowflake,https://stackoverflow.com/users/1203670/snowflake,"3,061"
32819368,32819368,2015-09-28T09:12:43,2015-10-18 10:39:51Z,0,"I have a random forest model using h2o.randomForest().


Now, I need to score a lot of data using h2o.predict(). Due to some constraints I can not score all the data at once. So basically I want to score different data sets in a loop. So, to speed up the process I want to score multiple datasets at the same time by running the same script in 2 different R instances. But when I do it, one instance runs fine but other instances give me the following error. Sometimes both instances give this error.


Error in .h2o.__checkConnectionHealth(conn) : 
 H2O connection has been severed. Cannot connect to instance at http://127.0.0.1:54321/
Failed to connect to 127.0.0.1 port 54321: Address already in use



Above error is not even consistent, sometimes I get it sometimes I don't.


I am initializing the h2o and predicting like following in all R instances.


h2oServer = h2o.init(nthreads = -1, max_mem_size = '8g')
h2.predict(model, test_data)



How can I achieve this? How do I use the h2o cloud through 2 different R instances?


Thanks,","['r', 'parallel-processing', 'multiple-instances', 'h2o']",Unknown,,N/A
32764021,32764021,2015-09-24T14:29:42,2015-09-30 23:02:56Z,0,"After my research on h2o, I have found that h2o.randomForest can handle missing values in variables unlike R randomForest package. 


See, 
http://h2o.ai/blog/2014/04/sjsu-tutorial-h2o-random-forest/


But, after looking everywhere, I can not seem to find how exactly missing values are handled by h2o.randomForest? How similar is it to handling of missin values by R gbm() package?


Any help regarding above 2 questions will be greatly appreciated.


Thanks,","['r', 'random-forest', 'h2o']",user3664020,https://stackoverflow.com/users/3664020/user3664020,"3,020"
32652500,32652500,2015-09-18T12:50:58,2015-10-01 17:26:36Z,0,"I am using 
h2o
 package in 
R
 to build random forest models. My task requires me to score test data repeatedly at later times, so I save the random forest model object as follows.


save(""D:/model_random_forest.RData"")



To score the data, I load back the model object again in the memory later as follows


 load(""D:/model_random_forest.RData"")



But when I score using


scores <- h2o.predict(model_random_forest, test_data) 



I get the following error


    ERROR: Unexpected HTTP Status code: 404 Not Found (url =     http://127.0.0.1:54321/3/Predictions/models/DRF_model_R_1442519642868_26/frames/test17_normal.hex_2)

water.exceptions.H2OKeyNotFoundArgumentException
 [1] ""water.api.ModelMetricsHandler.predict(ModelMetricsHandler.java:203)""   
 [2] ""sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""           
 [3] ""sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)""           
 [4] ""sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)""       
 [5] ""java.lang.reflect.Method.invoke(Unknown Source)""                       
 [6] ""water.api.Handler.handle(Handler.java:58)""                             
 [7] ""water.api.RequestServer.handle(RequestServer.java:637)""                
 [8] ""water.api.RequestServer.serve(RequestServer.java:578)""                 
 [9] ""water.JettyHTTPD$H2oDefaultServlet.doGeneric(JettyHTTPD.java:617)""     
 [10] ""water.JettyHTTPD$H2oDefaultServlet.doPost(JettyHTTPD.java:565)""          
 [11] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:755)""          
 [12] ""javax.servlet.http.HttpServlet.service(HttpServlet.java:848)""          
 [13]     ""org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)""

Error in .h2o.doSafeREST(conn = conn, h2oRestApiVersion = h2oRestApiVersion,    : 
  Object 'DRF_model_R_1442519642868_26' not found in function: predict for  argument: model



The whole saving, loading and scoring approach works fine when I use R packages to build models, but somehow failing here. Can someone please provide a solution to it? I can not keep rebuilding the model again again every I want to score some data.


H2o connection seems to be fine since when the model is newly built and is still present in the memory, scoring happens smoothly.","['r', 'random-forest', 'h2o']",user3664020,https://stackoverflow.com/users/3664020/user3664020,"3,020"
32605158,32605158,2015-09-16T09:48:57,2015-09-21 06:08:58Z,0,"I want to generate random discrete values for my H2o Object (3GB data) as  shown in the below example. 


Example :


  C1    d_rand  d_status
1   0.886581278 1
2   0.117570381 0
3   0.824350102 1
4   0.356774692 0
5   0.995249866 1



I have written R-h2o code as below, However I am not able to get my result.


> rand_num <- h2o.runif(sample_3gb, seed = 123)
> sample_3gb$d_rand = rand_num
> sample_3gb$d_rand
H2OFrame with 9227049 rows and 1 column

First 10 rows:
       d_rand
1  0.06254423
2  0.15162557
3  0.18380040
4  0.66398323
5  0.92064923
6  0.54746199
7  0.45642585
8  0.69650692
9  0.54063600
10 0.77103990
> sample_3gb$d_status = 1
> sample_3gb$d_status[sample_3gb$d_rand <= 0.3] <- 0
Error in `[<-`(`*tmp*`, sample_3gb$d_rand <= 0.3, value = 0) : 
  `i` must be missing or a numeric vector



Below is are my H2o cluster details


R is connected to H2O cluster:
    H2O cluster uptime:         3 minutes 57 seconds 
    H2O cluster version:        3.0.0.30 
    H2O cluster name:           H2O_60331 
    H2O cluster total nodes:    2 
    H2O cluster total memory:   9.58 GB 
    H2O cluster total cores:    24 
    H2O cluster allowed cores:  24 
    H2O cluster healthy:        TRUE 



I think this is happening with data type issue between R and H2o object i.e R is not reading the numeric values of h2o object as numeric. I am facing the same problem for some other conditional operations as well.","['r', 'bigdata', 'h2o']",hrbrmstr,https://stackoverflow.com/users/1457051/hrbrmstr,78.6k
32586740,32586740,2015-09-15T12:58:14,2015-10-01 00:55:42Z,0,"I am looking all over the internet on how to specify classification in h2o.randomForest. Whatever I could find says that there is parameter ""classification"" which we can set to TRUE. But it is not there anymore as per the h2o package documentation. 


SEE here 
https://cran.r-project.org/web/packages/h2o/h2o.pdf


When I run this h2o.randomForest on my data in which the target variable is a binary 1-0 variable, it assumes regression.


I am not sure how to tell it that I want to do classifcation.


Any help will be greatly appreciated.


Thanks","['r', 'random-forest', 'h2o']",user3664020,https://stackoverflow.com/users/3664020/user3664020,"3,020"
32583945,32583945,2015-09-15T10:40:24,2015-10-01 19:20:20Z,0,"I am running h2o random forest with the following parameter setting


model_rf <- h2o.randomForest(x = predictors, y = labels,
                         training_frame = train_data, classification = T,
                         importance = T,
                         verbose = T, type = ""BigData"", ntree = 50)



After running I am getting the following output.


Model Details:
==============

H2ORegressionModel: drf
Model ID:  DRFModel__906d074da6ebf8057525b2b61c1c4c87 
Model Summary:
  number_of_trees model_size_in_bytes min_depth max_depth mean_depth      min_leaves  max_leaves mean_leaves
1       50.000000      2708173.000000 20.000000 20.000000   20.00000     4200.000000 5241.000000  4720.70000


H2ORegressionMetrics: drf
** Reported on training data. **
Description: Metrics reported on Out-Of-Bag training samples

MSE:  0.0006302392
R2 :  -0.03751038



Following are my questions.


1) What does MSE and R2 mean? 


2) If they are mean square error or similar why am I getting these metric for a classification setting?


3) How do I get other metrics like gini or auc? 


4) Can i say that if these 2 params decrease with a different parameter setting, my model performance has improved?","['r', 'random-forest', 'h2o']",user3664020,https://stackoverflow.com/users/3664020/user3664020,"3,020"
32482658,32482658,2015-09-09T14:42:59,2015-09-10 06:54:09Z,802,"How can I add a column based on existing columns in H2OFrame in Scala?


I want to add a new column which is the log of an existing column. how can i do that? Thanks","['scala', 'h2o']",Tong,https://stackoverflow.com/users/5293341/tong,539
32416579,32416579,2015-09-05T18:51:01,2015-12-17 04:59:35Z,824,"My code is as follows


gbm.fit.hex = h2o.gbm(x= xcols , y =1865 , training_frame = tr.hex , distribution = ""bernoulli"", model_id = ""gbm.model"" , key = ""gbm.model.key"" ,                 ntrees = gbm.trees , max_depth = gbm.depth , min_rows = gbm.min.rows ,                  learn_rate = gbm.learn.rate , nbins = 20 , balance_classes = gbm.balance , nfolds = gbm.folds )



perf <- h2o.performance(gbm.fit.hex , tr.hex)
 a = h2o.auc(perf , xval = TRUE)


What does the auc call return? does it return the AUC on training dataset or on the crossvalidation results?",['h2o'],webscale,https://stackoverflow.com/users/4397964/webscale,13
32400447,32400447,2015-09-04T14:24:38,2015-09-10 06:52:24Z,643,thanks for asking my question. I'm now working on convert RDD to DF to H2O. I'm quite stuck with the H2O part. How can I convert Scala DataFrame to H2O?,"['scala', 'rdd', 'h2o']",Tong,https://stackoverflow.com/users/5293341/tong,539
32256502,32256502,2015-08-27T18:04:22,2015-08-27 21:34:19Z,0,"I am trying to use the RStudio server installation that comes with H2O, following instructions for bringing it up on EC2 
here
. All of this completes successfully, and I get RStudio working on port 8787, however, I don't know what the default logon credentials are. Are there any? Will have to log into the EC2 instance and configure a user manually?","['amazon-ec2', 'rstudio', 'h2o']",tchakravarty,https://stackoverflow.com/users/1414455/tchakravarty,10.9k
32222780,32222780,2015-08-26T09:26:29,2015-09-01 01:13:57Z,0,"I have an h2o  deep learning object created using h20.deeplearning. How do I extract the neural net wieghts from this object?


Thanks for your help.","['r', 'deep-learning', 'h2o']",Gokul Swamy,https://stackoverflow.com/users/5267781/gokul-swamy,21
32164045,32164045,2015-08-23T06:58:22,2018-01-26 22:22:25Z,0,"When using h2o in R, I get an error when trying to call the predict function on a model object:


Error in .h2o.calcBaseURL(conn = conn, h2oRestApiVersion =    h2oRestApiVersion,  : 
no slot of name ""https"" for this object of class ""H2OConnection""



What's going on?


Edit

I noticed since I upgraded to the latest h2o version, I also get this error when trying to load previously created model objects:


Error in .model.parts(o) : 
trying to get slot ""metrics"" from an object of a basic class (""NULL"")  with no slots","['r', 'h2o']",Unknown,,N/A
32119553,32119553,2015-08-20T13:26:21,2016-12-14 22:53:43Z,0,"I have got a data frame of 15458 objects and 113 variables. I want to convert this to a h2o object with as.h2o(). But i get following error:


> data.h2o <- as.h2o(data.model)

ERROR: Unexpected HTTP Status code: 500 Internal Server Error (url = http://127.0.0.1:54321/3/ParseSetup)

java.lang.RuntimeException
 [1] ""water.MRTask.getResult(MRTask.java:489)""                           ""water.MRTask.doAll(MRTask.java:400)""                              
 [3] ""water.parser.ParseSetup.guessSetup(ParseSetup.java:211)""           ""water.api.ParseSetupHandler.guessSetup(ParseSetupHandler.java:29)""
 [5] ""sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)""       ""sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)""      
 [7] ""sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)""   ""java.lang.reflect.Method.invoke(Unknown Source)""                  
 [9] ""water.api.Handler.handle(Handler.java:56)""                         ""water.api.RequestServer.handle(RequestServer.java:676)""           
[11] ""water.api.RequestServer.serve(RequestServer.java:613)""             ""water.NanoHTTPD$HTTPSession.run(NanoHTTPD.java:437)""              
[13] ""java.lang.Thread.run(Unknown Source)""                             

Error in .h2o.doSafeREST(conn = conn, h2oRestApiVersion = h2oRestApiVersion,  : 
  water.DException$DistributedException: from /127.0.0.1:54321; by class water.parser.ParseSetup$GuessSetupTsk; class water.exceptions.H2OParseSetupException: Problem parsing C:/Users/[...]/AppData/Local/Temp/RtmpieIjLY/file25904ef1231d.csv_2
Cannot determine file type.



Of course csv_2 is not a valid file type. But i cant't influence the file generation.


Converting a small test dataframe works. 


Any ideas?","['r', 'rstudio', 'h2o']",Unknown,,N/A
31876597,31876597,2015-08-07T11:26:36,2015-08-10 09:57:21Z,307,"I want to install H2O alongside an existing web application.  I want the web app to call the H2O REST API to make predictions.  This would all be installed on a customer's data centre.  I would not have access to this remote instance of H2O at all.


Is there a way to export a model trained in an instance of H2O on my PC in a way that it can be imported into the customer's instance of H2O?",['h2o'],AndrewKelly,https://stackoverflow.com/users/5046185/andrewkelly,21
31765161,31765161,2015-08-01T19:02:16,2019-08-08 13:16:13Z,0,"I am trying to split my h2o frame and I keep getting exception below, but I am able to see the split frames in the Flow UI. I tried to lookup documentation trying to find the how to load the frame I see in Flow UI into my python shell, but no luck with that either. 


data = h2o.import_frame(""train.csv"")
Parse Progress: [##################################################] 100%
Imported train.csv. Parsed 878,035 rows and 5 cols

splits = data.split_frame(ratios=[0.80])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/h2o/frame.py"", line 598, in split_frame
    return [h2o.get_frame(i[""name""]) for i in j[""destination_frames""]]
  File ""/usr/local/lib/python2.7/dist-packages/h2o/h2o.py"", line 183, in get_frame
    return H2OFrame.get_frame(frame_id)
  File ""/usr/local/lib/python2.7/dist-packages/h2o/frame.py"", line 50, in get_frame
    res = h2o.H2OConnection.get_json(""Frames/""+urllib.quote(frame_id))[""frames""][0]
  File ""/usr/local/lib/python2.7/dist-packages/h2o/connection.py"", line 391, in get_json
    return __H2OCONN__._rest_json(url_suffix, ""GET"", None, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/h2o/connection.py"", line 400, in _rest_json
    raw_txt = self._do_raw_rest(url_suffix, method, file_upload_info, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/h2o/connection.py"", line 466, in _do_raw_rest
    .format(http_result.status_code,http_result.reason,method,url,detailed_error_msgs))
EnvironmentError: h2o-py got an unexpected HTTP status code:
 404 Not Found (method = GET; url = http://localhost:54321/3/Frames/train_part0.hex).
detailed error messages: Object 'train_part0.hex' not found for argument: key","['python', 'h2o']",Ferdi,https://stackoverflow.com/users/6359698/ferdi,560
31723236,31723236,2015-07-30T12:00:47,2024-11-05 16:22:15Z,0,"I'm following the instructions 
here
 to setup Python to use H2O.


Despite installing Java in the default location (C:\Program Files\Java), 
h2o.init()
 is unable to find Java and exits with ValueError:


Cannot find Java. Please install the latest JDK from http://www.oracle.com/technetwork/java/javase/downloads/index.html



java.exe is clearly in C:\Program Files\Java\jdk1.8.0_51\bin.


I can't figure out why 
h2o.init()
 is failing. Any ideas?","['python', 'h2o']",Devon,https://stackoverflow.com/users/2615477/devon,680
31624644,31624644,2015-07-25T08:40:54,2015-07-25 08:40:54Z,0,"This question already has answers here
:
                                
                            










How to replace NA values in a table for selected columns



                                (12 answers)
                            




Closed 
9 years ago
.








I was wandering about this and finally found an answer I would like to share here: 
https://groups.google.com/forum/#!topic/h2ostream/M9rIi0k6K08


If you have an H2OFrame like this:


  a   b    c    d    e
1 0   NA   NA   NA   NA
2 0   2    2    2    2
3 0   NA   NA   NA   NA
4 0   NA   NA   1    2
5 0   NA   NA   NA   NA
6 0   1    2    3    2



And would like to replace all NAs in column b with 0s to gain this:


  a   b    c    d    e
1 0   0    NA   NA   NA
2 0   2    2    2    2
3 0   0    NA   NA   NA
4 0   0    NA   1    2
5 0   0    NA   NA   NA
6 0   1    2    3    2","['r', 'h2o']",Sebastian Hätälä,https://stackoverflow.com/users/2407819/sebastian-h%c3%a4t%c3%a4l%c3%a4,"1,035"
31522341,31522341,2015-07-20T17:10:10,2017-01-12 10:41:26Z,0,"The user tutorial says 


Navigate to Data > View All
Choose to filter by the model key
Hit Save Model
Input for path: /data/h2o-training/...
Hit Submit



The problem is that I do not have this menu (H2o, 3.0.0.26, web interface)",['h2o'],Alex Lizz,https://stackoverflow.com/users/3315419/alex-lizz,445
31442820,31442820,2015-07-15T23:14:27,2018-05-17 04:16:31Z,0,"I am running the h2o package in Rstudio Version 0.99.447. I run version 10.9.5 OSX.


I would like to set up a local cluster within R, following the steps of this tutorial: 
http://blenditbayes.blogspot.co.uk/2014/07/things-to-try-after-user-part-1-deep.html


The first step does not seem to be a problem. What does seem to be a problem is converting my data frame to a proper h2o object.


library(mlbench)
dat = BreastCancer[,-1] #reading in data set from mlbench package

library(h2o)
localH2O <- h2o.init(ip = ""localhost"", port = 54321, startH2O = TRUE) #sets up the cluster
dat_h2o <- as.h2o(localH2O, dat, key = 'dat') #this returns an error message



The above statement as.h2o results in the following error message


Error in as.h2o(localH2O, dat, key = ""dat"") : 
unused argument (key = ""dat"")



If I remove the ""key"" parameter, letting the data reside in the H2O key-value store under a machine generated name, the following error message comes up.


Error in .h2o.doSafeREST(conn = conn, h2oRestApiVersion = h2oRestApiVersion,  
Unexpected CURL error: Empty reply from server



This
 question asks the same thing as me, but the solution leads me to the same error.


Does anyone have experience with this problem? I'm not entirely sure how to approach this.","['r', 'dataframe', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
31367694,31367694,2015-07-12T12:20:16,2015-07-12 12:23:43Z,0,"While trying to load a dataframe in R Studio into a local H2O instance I've got the following error:


tr_h2o <- as.h2o(localH2O, df)
## Error in .h2o.__remoteSend(data@h2o, .h2o.__PAGE_PARSE2, source_key = data@key,  : 
##  http://127.0.0.1:54321/2/Parse2.json  returned the following error:
##  Argument 'source_key' error: Dangling meta character '+' near index 40
##  /var/folders/oJ/oJgI4ENCErihlFUEYA4gxk+++TI/-Tmp-//RtmpjkIDBW/file2e68bf6103.csv



I unsderstand the problem has to do with the '+' character and the type of regular expression parsing that is performe by H2O.


Trying the same import directly on the browser for the local machine at 


http://127.0.0.1:54321/2/Parse2.query


I get the same error. Somehow the instance cannot interpret correctly keys with the '+' character embbeded.


I'm workin on a OS x Machine running Mac OS X 10.6.8.


Could anyone please helpme? I've seen that some machines pad the keys with 0. Can I configure the machine to pad the H2O instance with '0's instead of '+'s?


Thank you","['regex', 'r', 'macos', 'h2o']",Scott Ritchie,https://stackoverflow.com/users/2341679/scott-ritchie,10.5k
31236639,31236639,2015-07-06T01:21:41,2015-07-12 13:00:01Z,748,"I have setup H2O Sparkling water and now following the instructions at 
http://h2o-release.s3.amazonaws.com/sparkling-water/rel-1.3/6/index.html
 - where step 3 says


import org.apache.spark.h2o._
val h2oContext = new H2OContext(sc).start()



I get following error after entering the last line. The error is as follows -


    scala> val h2oContext = new H2OContext(sc).start()
java.lang.NoClassDefFoundError: org/apache/spark/sql/execution/LogicalRDD
    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:15)
    at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:20)
    at $iwC$$iwC$$iwC$$iwC.<init>(<console>:22)
    at $iwC$$iwC$$iwC.<init>(<console>:24)
    at $iwC$$iwC.<init>(<console>:26)
    at $iwC.<init>(<console>:28)
    at <init>(<console>:30)
    at .<init>(<console>:34)
    at .<clinit>(<console>)
    at .<init>(<console>:7)
    at .<clinit>(<console>)
    at $print(<console>)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:789)
    at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1062)
    at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:615)
    at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:646)
    at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:610)
    at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
    at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:859)
    at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:771)
    at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:616)
    at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:624)
    at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:629)
    at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:954)
    at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:902)
    at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:902)
    at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
    at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:902)
    at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:997)
    at org.apache.spark.repl.Main$.main(Main.scala:31)
    at org.apache.spark.repl.Main.main(Main.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:328)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.execution.LogicalRDD
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLC

lassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    ... 42 more



As H2O Sparkling water is new release, there is not much help I could found on the web. If any one has come across such error or have any idea what could be possibly wrong, please help me to figure it out.


Thanks!","['scala', 'apache-spark', 'h2o']",PRP,https://stackoverflow.com/users/2499926/prp,101
31196199,31196199,2015-07-02T23:25:40,2019-02-24 23:30:22Z,0,"I'm using h2o version 3.0.0.22 in R and I'm trying to save my model. But I can't seem to figure out what format is expected. I've tried all sorts of variations but getting all sorts of different exceptions.




h2o.saveModel(model, dir=""c:/temp"", name= ""my.model"")




ERROR: Unexpected HTTP Status code: 400 Bad Request (url = http://127.0.0.1:54321/3/Models.bin/DeepLearningModel__8412f3abf1699b5593a55c6861c8468d?dir=c%3A%2Ftemp%2Fmy.model&force=0)

java.lang.IllegalArgumentException
 [1] ""water.persist.PersistManager.getPersistForURI(PersistManager.java:407)""          
 [2] ""water.serial.ObjectTreeBinarySerializer.save(ObjectTreeBinarySerializer.java:57)""
 [3] ""water.api.ModelsHandler.exportModel(ModelsHandler.java:206)""                     
 [4] ""sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)""                     
 [5] ""sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)""                 
 [6] ""java.lang.reflect.Method.invoke(Unknown Source)""                                 
 [7] ""water.api.Handler.handle(Handler.java:56)""                                       
 [8] ""water.api.RequestServer.handle(RequestServer.java:677)""                          
 [9] ""water.api.RequestServer.serve(RequestServer.java:614)""                           
[10] ""water.NanoHTTPD$HTTPSession.run(NanoHTTPD.java:438)""                             
[11] ""java.lang.Thread.run(Unknown Source)""                                            

Error in .h2o.doSafeREST(conn = conn, h2oRestApiVersion = h2oRestApiVersion,  : 
  Cannot find persist manager for scheme c



How can I save my model on my Windows 8 machine?


UPDATE:
This command here seems to create a folder & file:




h2o.saveModel(model, filename=""file:///C:/temp/model"")




This created a file: 
C:/temp/DeepLearningModel__8412f3ab21699b5593aa5c6861c8468d.bin

But then throws a different error:


ERROR: Unexpected HTTP Status code: 400 Bad Request (url = http://127.0.0.1:54321/3/Models.bin/DeepLearningModel__8412f3abf1699b5593a55c6861c8468d?dir=file%3A%2F%2F%2FC%3A%2Ftemp%2Fmodel&force=0)

java.lang.IllegalArgumentException
 [1] ""java.net.URI.create(Unknown Source)""                                             
 [2] ""water.serial.ObjectTreeBinarySerializer.save(ObjectTreeBinarySerializer.java:70)""
 [3] ""water.api.ModelsHandler.exportModel(ModelsHandler.java:206)""                     
 [4] ""sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)""                     
 [5] ""sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)""                 
 [6] ""java.lang.reflect.Method.invoke(Unknown Source)""                                 
 [7] ""water.api.Handler.handle(Handler.java:56)""                                       
 [8] ""water.api.RequestServer.handle(RequestServer.java:677)""                          
 [9] ""water.api.RequestServer.serve(RequestServer.java:614)""                           
[10] ""water.NanoHTTPD$HTTPSession.run(NanoHTTPD.java:438)""                             
[11] ""java.lang.Thread.run(Unknown Source)""                                            

Error in .h2o.doSafeREST(conn = conn, h2oRestApiVersion = h2oRestApiVersion,  : 
  Malformed escape pair at index 165: file:///C:/temp/model/modelmetrics_DeepLearningModel__8412f3abf1699b5593a55c6861c8468d@-1218026610891888320_on_TrainingData.hex_1_part0.temporary.sample.9.91%@1615017098181529186.bin","['r', 'h2o']",Unknown,,N/A
30901595,30901595,2015-06-17T20:18:20,2017-03-21 08:29:41Z,0,"I am trying to build a sentiment classification model with R and H2O.
I have a data file with the format:


  +-----------+------------------------------------------------------+
| Sentiment | Text                                                 |
+-----------+------------------------------------------------------+
| 1         | This is a sample text. This is another sentence.     |
+-----------+------------------------------------------------------+
| 0         | Another sentence. And another!                       |
+-----------+------------------------------------------------------+
| -1        | Text text and Text! Text everywhere! So much text... |
+-----------+------------------------------------------------------+



So the sentiment values a 1, 0 and -1 and the text in each row can consist of several sentences.
I know want to prepare the dataset to use it with the deeplearning function of h2o. Therefore I wanted to use the 
tmcn.word2vec
 R package. But I can not transform it row-wise with this package. I could just get the whole text column and transform it in a word2vec document, but then my sentiment information would be lost.


Is there another way to translate the text into numerical input for a deeplearning function in R? Especially for H2O? 


Best regards","['r', 'machine-learning', 'sentiment-analysis', 'deep-learning', 'h2o']",JulianHi,https://stackoverflow.com/users/2874571/julianhi,286
30882686,30882686,2015-06-17T05:03:54,2016-08-10 22:33:51Z,0,"I'm trying a random forest classification model by using 
H2O
 library inside R on a training set having 70 million rows and 25 numeric features.The total file size is 5.6 GB.


The validation file's size is 1 GB.


I have 16 GB RAM and 8 core CPU on my system.


The system successfully able to read both of the files in H2O object.


Then I'm giving below command to build the model:


model <- h2o.randomForest(x = c(1:18,20:25), y = 19, training_frame = traindata, 
                          validation_frame = testdata, ntrees = 150, mtries = 6)



But after few minutes (without generating any tree), I'm getting following error:




""Error in .h2o.doSafeREST(conn = conn, h2oRestApiVersion = h2oRestApiVersion, : Unexpected CURL error: Recv failure: Connection reset by peer""




However If I tried above code with 1 tree, its running successfully.


Is the above error occurring because of memory issue? Any help will be appreciated.","['r', 'memory', 'out-of-memory', 'random-forest', 'h2o']",smci,https://stackoverflow.com/users/202229/smci,33.8k
30821865,30821865,2015-06-13T18:01:24,2021-06-18 03:56:38Z,0,"I am using h2o to carry out some modelling, and having tuned the model, i would now like it to be used to carry out a lot of predictions approx 6bln predictions/rows, per prediction row it needs 80 columns of data


The dataset I have already broken down the input dataset down so that it is in about 500 x 12 million row chunks each with the relevant 80 columns of data.


However to upload a 
data.table
 that is 12 million by 80 columns to h2o takes quite a long time, and doing it 500 times for me is taking a prohibitively long time...I think its because it is parsing the object first before it is uploaded.


The prediction part is relatively quick in comparison.... 


Are there any suggestions to speed this part up? Would changing the number of cores help?


Below is an reproducible example of the issues...


  # Load libraries
  library(h2o)
  library(data.table)

  # start up h2o using all cores...
  localH2O = h2o.init(nthreads=-1,max_mem_size=""16g"")

  # create a test input dataset
  temp <- CJ(v1=seq(20),
             v2=seq(7),
             v3=seq(24),
             v4=seq(60),
             v5=seq(60))
  temp <- do.call(cbind,lapply(seq(16),function(y){temp}))
  colnames(temp) <- paste0('v',seq(80))

  # this is the part that takes a long time!!
  system.time(tmp.obj <- as.h2o(localH2O,temp,key='test_input'))

  #|======================================================================| 100%
  #   user  system elapsed 
  #357.355   6.751 391.048","['r', 'h2o']",Unknown,,N/A
30790969,30790969,2015-06-11T20:37:24,2015-12-17 04:51:59Z,0,"I am writing the code for cross validation of my models' performance.In order to split data set randomly I use this method: 


h2o.runif(train.hex) 



Unfortunately it always returns me the same vector: 




0.7309678 0.2405364 0.6374174 0.5504370 0.5975453 0.3332184




I've also tried to use different seeds e.g.:


h2o.runif(train.hex, seed=-1)

h2o.runif(train.hex, seed=123)



the results are always the same. 
What is wrong with function? I would appreciate for any hint on it. 


UPDATE


Here is a full code:


library(h2o)
localH2O <- h2o.init(nthreads = -1,max_mem_size = '7g')
data(iris)
iris.hex<- as.h2o(localH2O,iris)
random <- h2o.runif(iris.hex, seed=-1) 
print(random)","['r', 'random', 'cross-validation', 'h2o']",Unknown,,N/A
30413447,30413447,2015-05-23T13:47:06,2015-08-14 01:05:53Z,0,"How to get each percentages of parameters's contribution in R h2o deeplearning package?


library(h2o)
localH2O = h2o.init(ip = ""localhost"", port = 54321, startH 2O = TRUE)
irisPath = system.file(""extdata"", ""iris.csv"", package = ""h2o"")
iris.hex = h2o.importFile(localH2O, path = irisPath)
h2o.deeplearning(x = 1:4, y = 5, data = iris.hex, activation = ""Tanh"")
h2o.shutdown(localH2O)","['r', 'parameters', 'deep-learning', 'h2o']",hfiuareogj85,https://stackoverflow.com/users/4932041/hfiuareogj85,1
30270400,30270400,2015-05-16T00:00:59,2015-05-19 23:47:04Z,301,"I've been trying to set up Hadoop/Spark/Sparkling Water on a clean Ubuntu 14.04 machine on a private cloud on Amazon.  Doing everything as root.  I successfully apt-get java-6, scala 2.10.5, then wget and unpack hadoop 2.6, spark 1.2.1, and sparkling water 0.2.1-47.


I exported HADOOP_HOME and SPARK_HOME to the correct directories, and ran bin/sparkling-shell:


import org.apache.spark.h2o._
import org.apache.spark.examples.h2o._
val h2oContext = new H2OContext(sc).start()



There then follows a massive amount of starting task X / finishing task X output, followed by 


java.lang.IllegalArgumentException: Cannot execute H2O on all Spark executors:
  numH2OWorkers = -1""
  executorStatus = (0,false),(1,false),(2,false),(0,false),(1,false),(2,false),(1,false),(1,false),(1,false),(1,false),(1,false),(1,false),(1,false),(1,false),(0,false),(1,false),(0,false),(1,false),(1,false),(0,false),(1,false),(0,false),(1,false),(1,false),(0,false),(0,false),(1,false),(1,false),(0,false),(0,false),(2,false),(2,false),(1,false),(0,false),(1,false),(0,false),(2,false),(1,false),(2,false),(1,false),(0,false),(1,false),(2,false),(0,false),(1,false),(2,false),(1,false),(2,false),(0,false),(2,false),(1,false),(0,false),(1,false),(0,false),(1,false),(2,false),(0,false),(2,false),(1,false),(1,false),(0,false),(2,false),(0,false),(2,false),(1,false),(1,false),(0,false),(1,false),(1,false),(2,false),(0,false),(2,false),(1,false),(1,false),(0,false),(2,false),(0,false),(2,false),(1,false),(1,false),(0,false),(2,false),(0,false),(2,false),(1,false),(0,false),(1,false),(2,false),(0,false),(1,false),(2,false),(1,false),(0,false),(2,false),(0,false),(2,false),(1,false),(2,false),(1,false),(2,false),(0,false),(1,false),(1,false),(2,false),(0,false),(2,false),(0,false),(1,false),(1,false),(2,false),(0,false),(2,false),(1,false),(2,false),(0,false),(1,false),(0,false),(2,false),(0,false),(1,false),(1,false),(2,false),(0,false),(2,false),(0,false),(1,false),(2,false),(1,false),(2,false),(0,false),(0,false),(1,false),(2,false),(2,false),(1,false),(1,false),(0,false),(0,false),(2,false),(2,false),(1,false),(0,false),(1,false),(2,false),(0,false),(2,false),(1,false),(0,false),(2,false),(1,false),(2,false),(0,false),(1,false),(1,false),(2,false),(0,false),(2,false),(2,false),(1,false),(1,false),(0,false),(2,false),(0,false),(1,false),(2,false),(0,false),(1,false),(2,false),(2,false),(1,false),(0,false),(0,false),(2,false),(1,false),(2,false),(0,false),(0,false),(1,false),(2,false),(1,false),(2,false),(1,false),(2,false),(0,false),(1,false),(0,false),(2,false),(1,false),(0,false),(1,false),(2,false),(0,false),(2,false),(0,false),(1,false),(0,false),(1,false),(2,false),(1,false),(2,false)
at org.apache.spark.h2o.H2OContext.start(H2OContext.scala:112)
(...)



Could somebody indicate what I might be doing wrong and/or missing?  If needed/helpful I can post my precise setup script.","['hadoop', 'apache-spark', 'h2o']",tresbot,https://stackoverflow.com/users/1464641/tresbot,"1,620"
30225921,30225921,2015-05-13T21:59:57,2016-02-26 01:15:58Z,0,"I am using h2o package to create randomForest regression model. I have some problems with the variables importance. The model I am creating is here. Everything works fine.


Some of the variables are numeric, but some are categorical.


RandomForest <- h2o.randomForest(x = c(""Year"",  ""Month"", ""Day"", ""Time"", ""Show"", ""Gen"",
                                   ""D"", ""Lead""), y = ""Ratio"", data = data.hex, importance=T, stat.type = ""GINI"",
                             ntree = 50, depth = 50, nodesize = 5, oobee = T, classification = FALSE, type = ""BigData"")



However, when I want to see the variable importance, the output looks like this. 


Classification: FALSE
Number of trees: 50
Tree statistics:
        Min.  Max.    Mean.
Depth     30    40    33.26
Leaves 20627 21450 21130.24


Variable importance:
                        Year    Month      Day     Time  Show   Gen           D   Lead
Relative importance 20536.64 77821.76 26742.55 67476.75 283447.3 60651.24   87440.38 3658.625
Standard Deviation        NA       NA       NA       NA       NA       NA       NA       NA
Z-Scores                  NA       NA       NA       NA       NA       NA       NA       NA

Overall Mean-squared Error:  



What I would like to know is:
 1) Why there could be NA values.
 2) What does actually Relative importance mean. Shouldn't it be between 1 and 100?
 3) Why there is no confusion matrix in the output?


Thanks for the help!","['r', 'random-forest', 'confusion-matrix', 'h2o']",AK47,https://stackoverflow.com/users/2969277/ak47,"1,328"
30137628,30137628,2015-05-09T08:12:08,2015-08-11 22:42:20Z,0,"I am trying to do deep learning using 
H2O
 via the R package 
h2o
,
and want to ask whether 
H2O
 can save and reload training data for future additional training?


My code:


iris.train <- irisdata[-1,]
iris.test <- irisdata[1,]

res.dl <- h2o.deeplearning(x = 1:4, y = 5_offset, data = iris.train, activation = ""Rectifier"")
pred.dl <- h2o.predict(object=res.dl, newdata=iris.test)
res.err.dl[i] <- ifelse(as.character(as.matrix(pred.dl)[1,1]) == as.character(as.matrix(iris.test)[1,5]),0,1)","['r', 'deep-learning', 'h2o']",tchakravarty,https://stackoverflow.com/users/1414455/tchakravarty,10.9k
29828978,29828978,2015-04-23T16:11:19,2015-12-20 02:25:43Z,0,"I use h2o(2.8.4.4) for hadoop from R.
I want to get some column from the data frame with 720512 rows and 788 columns.
I write something like this:


library(""h2o"");
localH2O = h2o.init(ip = ipItem, port = 54321, startH2O = F)
waterTrain <- h2o.importFile(localH2O, path=trainName, key=""trainKey"", parse=T, header=T, sep=""*"")
subset <- waterTrain[, 1:787]
Error: Expectation failed



What am I doing wrong?","['r', 'hadoop', 'h2o']",Fedorenko Kristina,https://stackoverflow.com/users/2131442/fedorenko-kristina,"2,747"
29603624,29603624,2015-04-13T10:55:40,2022-06-30 14:34:59Z,0,"Is it possible to create a deep learning net that gives multiple outputs?
The reason for doing this is to also try to capture the relationships between outputs. 
In the examples given I can only create one output.


library(h2o)
localH2O = h2o.init()
irisPath = system.file(""extdata"", ""iris.csv"", package = ""h2o"")
iris.hex = h2o.importFile(localH2O, path = irisPath)
h2o.deeplearning(x = 1:4, y = 5, data = iris.hex, activation = ""Tanh"", 
             hidden = c(10, 10), epochs = 5)","['r', 'deep-learning', 'h2o']",Unknown,,N/A
29152961,29152961,2015-03-19T19:05:20,2018-01-29 16:05:28Z,258,"H2O 
supports
 the 
Cox proportional hazards model
. But the notation on the first link does not make it clear whether it supports 
time varying covariates
. Does it?",['h2o'],tchakravarty,https://stackoverflow.com/users/1414455/tchakravarty,10.9k
29065974,29065974,2015-03-15T20:44:10,2015-03-20 19:52:48Z,395,"I am trying to do some performance analysis of HTTP/1.1 and HTTP /2. But I have not been successful in installing the later one. I need both the protocols from the same implementation (e.g. H2O). The first problem I got while installing H2O is the openssl version in ubuntu 14.04. The I updated the version to 1.0.2 as suggested by 
1
. Then I got the following error messages:


    In function `h2o_socket_ssl_get_selected_protocol':
    /tmp/h2o/lib/common/socket.c:499: undefined reference to        `SSL_get0_alpn_selected'
CMakeFiles/h2o.dir/lib/common/socket.c.o: In function `h2o_ssl_register_alpn_protocols':
/tmp/h2o/lib/common/socket.c:542: undefined reference to `SSL_CTX_set_alpn_select_cb'
collect2: error: ld returned 1 exit status
make[2]: *** [h2o] Error 1
make[1]: *** [CMakeFiles/h2o.dir/all] Error 2
make: *** [all] Error 2



Can anybody please help me here.","['http', 'webserver', 'spdy', 'http2', 'h2o']",Shaiful Chowdhury,https://stackoverflow.com/users/1151324/shaiful-chowdhury,59
28877755,28877755,2015-03-05T12:20:07,2017-05-19 14:54:20Z,0,"I am new to H2O and am having some trouble initializing H2O in RStudio (R 3.2 version in 64-bit Windows 7) to do tax fraud predictive modelling. The steps I made to start H2O where the following:




I installed H2O latest stable release (""Noether 2.8.4.4"" from (
http://0xdata.com/download/
)


I launched H2O from my terminal after unzipping using:




cd Downloads
 cd h2o-2.8.4.4
 java -jar h2o.jar




I pointed to H2O URL in Google Chrome: 
http://localhost:54321/


Run the script in Rstudio to start with the demo:




library(h2o,lib.loc=""C:/Program Files/RRO/R-3.1.2/library"")
 localH2O = h2o.init(ip = ""localhost"", port = 54321, startH2O = TRUE)


But after this fourth step I received the following message:


H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    C:\Users\AMARTI~1\AppData\Local\Temp\RtmpQ1aKXp/h2o_amartinezsistac_started_from_r.out
    C:\Users\AMARTI~1\AppData\Local\Temp\RtmpQ1aKXp/h2o_amartinezsistac_started_from_r.err

java version ""1.8.0_25""
Java(TM) SE Runtime Environment (build 1.8.0_25-b18)
Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)
Error in h2o.init(ip = ""localhost"", port = 54321, startH2O = TRUE) : 
      H2O failed to start, stopping execution.



I would appreciate any help or advice on what I am doing wrong.","['r', 'windows', 'rstudio', 'h2o']",Brian Tompsett - 汤莱恩,https://stackoverflow.com/users/4370109/brian-tompsett-%e6%b1%a4%e8%8e%b1%e6%81%a9,"5,875"
28771582,28771582,2015-02-27T18:08:50,2019-10-27 23:07:29Z,0,"I'm trying to run a gbm model in H2O via R and get one of the following errors:


  |==========================                                                                               |  25%
Polling fails:
<simpleError in .h2o.__poll(client, job_key): Got exception 'class java.lang.RuntimeException', with msg 'java.lang.AssertionError: NewChunk.dst.len = 0, oc._len = 1235'
java.lang.RuntimeException: java.lang.AssertionError: NewChunk.dst.len = 0, oc._len = 1235
    at hex.FrameExtractor.getResult(FrameExtractor.java:77)
    at water.util.CrossValUtils.crossValidate(CrossValUtils.java:29)
    at hex.gbm.GBM.execImpl(GBM.java:201)
    at water.Func.exec(Func.java:42)
    at water.Job$3.compute2(Job.java:333)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:647)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:429)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
Caused by: java.lang.AssertionError: NewChunk.dst.len = 0, oc._len = 1235
    at water.fvec.ChunkSplitter.extractChunkPart(ChunkSplitter.java:44)
    at hex.NFoldFrameExtractor$FoldExtractTask.map(NFoldFrameExtractor.java:105)
    at water.MRTask2.compute2(MRTask2.java:404)
    ... 6 more
>
  |=========================================================================================================| 100%
Error in .h2o.__remoteSend(data@h2o, model_view, `_modelKey` = xvalKey[i]) : 
  http://127.0.0.1:54321/2/GBMModelView.json  returned the following error:
   Model 'GBM_a1b17d68e29d7ba49cb6243293344b69_xval0' not found!



Or this version:


  |===================                                                         |  25%
Polling fails:
<simpleError in .h2o.__poll(client, job_key): Got exception 'class java.lang.AssertionError', with msg 'null'
java.lang.AssertionError
    at hex.gbm.GBM.buildNextKTrees(GBM.java:505)
    at hex.gbm.GBM.buildModel(GBM.java:296)
    at hex.gbm.GBM.buildModel(GBM.java:26)
    at hex.gbm.SharedTreeModelBuilder.buildModel(SharedTreeModelBuilder.java:276)
    at hex.gbm.GBM.execImpl(GBM.java:200)
    at water.Func.exec(Func.java:42)
    at water.Job.invoke(Job.java:353)
    at water.Job$ValidatedJob.genericCrossValidation(Job.java:889)
    at hex.gbm.GBM.crossValidate(GBM.java:709)
    at water.util.CrossValUtils.crossValidate(CrossValUtils.java:32)
    at hex.gbm.GBM.execImpl(GBM.java:201)
    at water.Func.exec(Func.java:42)
    at water.Job$3.compute2(Job.java:333)
    at water.H2O$H2OCountedCompleter.compute(H2O.java:647)
    at jsr166y.CountedCompleter.exec(CountedCompleter.java:429)
    at jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)
    at jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)
    at jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)
    at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)
>
  |============================================================================| 100%
Error in tail(res$cms, 1)[[1]] : subscript out of bounds



Here is the line that causes the error:


  dat1.gbm <- h2o.gbm(y = 'click_target2', x = xVars, data = train1.hex
                      , nfolds = 3
                      , importance = T
                      , distribution = 'bernoulli' 
                      , n.trees = 200
                      , interaction.depth = 10,
                      # , n.minobsinnode = 2
                      , shrinkage = 0.01
  )



Any suggestions for what's causing this error?


EDIT:


I've been trying to diagnose if there's a problem with the csv file itself and it appears that may be the issue.  I ended up writing a script in python to break the large file into individual csv files by week_number.  About 2/3's of the way thru reading the file I get a NULL byte exception error.  I'm still working to find a fix for this.","['java', 'r', 'h2o']",Unknown,,N/A
28741176,28741176,2015-02-26T11:26:09,2015-08-12 18:16:26Z,0,"I am using r package of h2o 2.8.4.4. According to documentation function h2o.addFunction pushes user defined function and existing R functions in h2o. 
e.g.


>library(h2o)
>localH2O = h2o.init()
>h2o.addFunction(localH2O, function(x) { 2*x + 5 }, ""simpleFun"")



Now, How to access this function simplefun from within R or from h2o browser UI, because i am unable to find the function even with the list command


>h2o.ls(localH2O)



Output:   list()","['r', 'h2o']",the M,https://stackoverflow.com/users/4030432/the-m,125
28703121,28703121,2015-02-24T18:09:05,2019-05-16 17:41:50Z,0,"I'm trying to use H2O via R to build multiple models using subsets of one large-ish data set (~ 10GB).  The data is one years worth of data and I'm trying to build 51 models (ie train on week 1, predict on week 2, etc.) with each week being about 1.5-2.5 million rows with 8 variables.


I've done this inside of a loop which I know is not always the best way in R.  One other issue I found was that the H2O entity would accumulate prior objects, so I created a function to remove all of them except the main data set.


h2o.clean <- function(clust = localH2O, verbose = TRUE, vte = c()){
  # Find all objects on server
  keysToKill <- h2o.ls(clust)$Key
  # Remove items to be excluded, if any
  keysToKill <- setdiff(keysToKill, vte)
  # Loop thru and remove items to be removed
  for(i in keysToKill){
    h2o.rm(object = clust, keys = i)

    if(verbose == TRUE){
      print(i);flush.console()

    }    
  }
  # Print remaining objects in cluster.
  h2o.ls(clust)
}



The script runs fine for a while and then crashes - often with a complaint about running out of memory and swapping to disk.  


Here's some pseudo code to describe the process


# load h2o library
library(h2o)
# create h2o entity
localH2O = h2o.init(nthreads = 4, max_mem_size = ""6g"")
# load data
dat1.hex = h2o.importFile(localH2O, inFile, key = ""dat1.hex"")

# Start loop
for(i in 1:51){
# create test/train hex objects
train1.hex <- dat1.hex[dat1.hex$week_num == i,]
test1.hex <- dat1.hex[dat1.hex$week_num == i + 1,]
# train gbm
dat1.gbm <- h2o.gbm(y = 'click_target2', x = xVars, data = train1.hex
                      , nfolds = 3
                      , importance = T
                      , distribution = 'bernoulli' 
                      , n.trees = 100
                      , interaction.depth = 10,
                      , shrinkage = 0.01
  )
# calculate out of sample performance
test2.hex <- cbind.H2OParsedData(test1.hex,h2o.predict(dat1.gbm, test1.hex))
colnames(test2.hex) <- names(head(test2.hex))
gbmAuc <- h2o.performance(test2.hex$X1, test2.hex$click_target2)@model$auc

# clean h2o entity
h2o.clean(clust = localH2O, verbose = F, vte = c('dat1.hex'))

} # end loop



My question is what, if any, is the correct way to manage data and memory in a stand alone entity (this is NOT running on hadoop or a cluster - just a large EC2 instance (~ 64gb RAM + 12 CPUs)) for this type of process?  Should I be killing and recreating the H2O entity after each loop (this was original process but reading data from file every time adds ~ 10 minutes per iteration)?  Is there a proper way to garbage collect or release memory after each loop? 


Any suggestions would be appreciated.","['java', 'r', 'memory-management', 'h2o']",screechOwl,https://stackoverflow.com/users/914308/screechowl,28.1k
28540882,28540882,2015-02-16T11:54:47,2016-08-27 17:29:33Z,0,"I want to use 
h2o.kmeans
 function from 
h2o
 package.
Here is my code


clust <- h2o.kmeans(data = waterM, centers = 30,  key = ""kmeansKey"", iter.max = 1000, normalize = T, init = ""none"", dropNACols = F );
summary(clust@model);



The output is


            Length Class  Mode   
params           6  -none- list   
centers      25560  -none- numeric
withinss        30  -none- numeric
tot.withinss     1  -none- numeric
size            30  -none- numeric
iter             1  -none- numeric



Why are there  no 
cluster
 object  containing the vector of integers (from 1:k), which indicate the cluster which each point is allocated to ?","['r', 'k-means', 'h2o']",jangorecki,https://stackoverflow.com/users/2490497/jangorecki,16.7k
28385028,28385028,2015-02-07T17:16:38,2015-02-21 18:42:55Z,0,"The 'h2o' package is a fun ML java tool that is accessible via R.  The R package for accessing 'h2o' is called ""
h2o
"". 


One of the input avenues is to tell 'h2o' where a csv file is and let 'h2o' upload the raw CSV.  It can be more effective to just point out the folder and tell 'h2o' to import ""everything in it"" using the 
h2o.importFolder
 command.


Is there a way to point out a folder of ""gzip"" or ""bzip"" csv files and get 'h2o' to import them?  


According to this link (
here
) the h2o can import compressed files.  I just don't see the way to specify this for the importFolder approach.


Is it faster or slower to import the compressed form?  If I have another program that makes output does it save me time in the h2o import process speed if they are compressed?  If they are raw text?  Guidelines and performance best practices are appreciated. 


as always, comments, suggestions, and feedback are solicited.","['r', 'csv', 'import', 'gzip', 'h2o']",EngrStudent,https://stackoverflow.com/users/2259468/engrstudent,"1,972"
28366209,28366209,2015-02-06T12:50:47,2018-06-11 14:32:35Z,0,"In H2O, when we parse .csv file to 
Frame
 object how can we get distinct values count of a particular column(Vec).
For example, consider a column Fruits which has apple 3 times and mango 2 times. After parsing it to a frame, we can get distinct values using the 
domain()
 method, but how do you get distinct values along with their counts? In the example, I would be looking for:


apple,3
mango,2",['h2o'],Max Langhof,https://stackoverflow.com/users/9528746/max-langhof,23.6k
28301125,28301125,2015-02-03T14:19:49,2017-11-12 09:04:13Z,0,"Is there a way to load a matrix into 
H2O
 from 
R
 without going over a file? 


i.e. a direct alternative to


m = matrix(c(1,2,3,4), ncol=2)
localH2O = h2o.init()
write.table(m, ""m.csv"", row.names=FALSE, col.names=FALSE)
h2o.importFile(localH2O, path=""m.csv"")","['r', 'h2o']",Unknown,,N/A
28220745,28220745,2015-01-29T17:19:47,2015-02-01 02:40:57Z,0,"I'm using the 
h2o
 package in R and trying to do some data manipulation but having some issues with the 
sub
/
gsub
 functions.  


Here's my code:


library(h2o)

# Start cluster
localH2O = h2o.init(nthreads = 2) 

# Create data set
dat1.mini <- structure(list(id = c(""7927751403363142656"", ""18236986451472797696"", 
""5654946373641778176"", ""14195690822403907584"", ""1693303484298446848"", 
""1.1362181921561e+19"", ""11694645532962195456"", ""1221431312630614784"", 
""1987127670789791488"", ""379819848497418688""), click = c(""0"", 
""0"", ""0"", ""0"", ""0"", ""0"", ""0"", ""1"", ""0"", ""0""), hour = c(""14102118"", 
""14102217"", ""14102812"", ""14102912"", ""14102820"", ""14102401"", ""14102117"", 
""14102312"", ""14102301"", ""14102414""), C1 = c(""1005"", ""1005"", ""1005"", 
""1002"", ""1005"", ""1005"", ""1005"", ""1005"", ""1005"", ""1005""), banner_pos = c(""1"", 
""1"", ""0"", ""0"", ""0"", ""0"", ""1"", ""1"", ""0"", ""0""), site_id = c(""b7e9786d"", 
""e151e245"", ""85f751fd"", ""ee4c822c"", ""85f751fd"", ""85f751fd"", ""e5c60a05"", 
""e151e245"", ""1fbe01fe"", ""1fbe01fe""), site_domain = c(""b12b9f85"", 
""7e091613"", ""c4e18dd6"", ""c4e18dd6"", ""c4e18dd6"", ""c4e18dd6"", ""7256c623"", 
""7e091613"", ""f3845767"", ""f3845767""), site_category = c(""f028772b"", 
""f028772b"", ""50e219e0"", ""50e219e0"", ""50e219e0"", ""50e219e0"", ""f028772b"", 
""f028772b"", ""28905ebd"", ""28905ebd""), app_id = c(""ecad2386"", ""ecad2386"", 
""685d1c4c"", ""ecad2386"", ""92f5800b"", ""f02cb7ab"", ""ecad2386"", ""ecad2386"", 
""ecad2386"", ""ecad2386""), app_domain = c(""7801e8d9"", ""7801e8d9"", 
""2347f47a"", ""7801e8d9"", ""ae637522"", ""2347f47a"", ""7801e8d9"", ""7801e8d9"", 
""7801e8d9"", ""7801e8d9""), app_category = c(""07d7df22"", ""07d7df22"", 
""8ded1f7a"", ""07d7df22"", ""0f2161f8"", ""f95efa07"", ""07d7df22"", ""07d7df22"", 
""07d7df22"", ""07d7df22""), device_id = c(""a99f214a"", ""a99f214a"", 
""a99f214a"", ""8374cacf"", ""a99f214a"", ""8a5908a5"", ""a99f214a"", ""a99f214a"", 
""a99f214a"", ""a99f214a""), device_ip = c(""3214d61e"", ""d5623936"", 
""419e166e"", ""698846d6"", ""c2d9c2f2"", ""40817190"", ""edd10fc1"", ""e4c6e857"", 
""05d3adbe"", ""6929d972""), device_model = c(""a0f5f879"", ""69f9dd0e"", 
""46a414f4"", ""12edfe21"", ""4ffd3a7e"", ""04f5b394"", ""779d90c2"", ""1f0bc64f"", 
""293291c1"", ""d787e91b""), device_type = c(""1"", ""1"", ""1"", ""0"", 
""1"", ""1"", ""1"", ""1"", ""1"", ""1""), device_conn_type = c(""0"", ""0"", 
""3"", ""0"", ""3"", ""0"", ""0"", ""0"", ""0"", ""0""), C14 = c(""16208"", ""20277"", 
""23224"", ""17566"", ""21189"", ""20633"", ""19771"", ""17264"", ""15703"", 
""20108""), C15 = c(""320"", ""320"", ""320"", ""320"", ""320"", ""320"", ""320"", 
""320"", ""320"", ""320""), C16 = c(""50"", ""50"", ""50"", ""50"", ""50"", ""50"", 
""50"", ""50"", ""50"", ""50""), C17 = c(""1800"", ""2281"", ""2676"", ""479"", 
""2424"", ""2374"", ""2227"", ""1872"", ""1722"", ""2299""), C18 = c(""3"", 
""3"", ""0"", ""3"", ""1"", ""3"", ""0"", ""3"", ""0"", ""2""), C19 = c(""167"", 
""47"", ""35"", ""39"", ""161"", ""39"", ""679"", ""39"", ""35"", ""1327""), C20 = c(""100077"", 
""100181"", ""100176"", ""100074"", ""100189"", ""-1"", ""100074"", ""-1"", 
""-1"", ""-1""), C21 = c(""23"", ""42"", ""221"", ""23"", ""71"", ""23"", ""48"", 
""23"", ""79"", ""52"")), .Names = c(""id"", ""click"", ""hour"", ""C1"", ""banner_pos"", 
""site_id"", ""site_domain"", ""site_category"", ""app_id"", ""app_domain"", 
""app_category"", ""device_id"", ""device_ip"", ""device_model"", ""device_type"", 
""device_conn_type"", ""C14"", ""C15"", ""C16"", ""C17"", ""C18"", ""C19"", 
""C20"", ""C21""), row.names = c(NA, 10L), class = ""data.frame"")

# Load data to cluster
dat.mini.hex <- as.h2o(localH2O, dat1.mini)

# Attempt to grab substring of first 6 characters from hour column
dat.mini.hex$hr <- h2o.sub('^(.{6}).*$','\\1', dat.mini.hex$hour)
dat.mini.hex$hr <- h2o.gsub('(.+)..','\\1', dat.mini.hex$hour)



All of these attempts result in the following error:


Error in .h2o.__remoteSend(client, .h2o.__PAGE_EXEC2, str = expr) : 
  http://127.0.0.1:54321/2/Exec2.json  returned the following error:
   class java.lang.NullPointerException","['regex', 'r', 'gsub', 'h2o']",JasonMArcher,https://stackoverflow.com/users/64046/jasonmarcher,14.9k
28202645,28202645,2015-01-28T21:22:52,2015-01-29 21:46:14Z,0,"I'm using H2O for some distributed computing work (via the 
h2o
 package in R).  Many of the base R functions are present but I'm unable to find a suitable substitute for the 
substr
 function.  I do have access to the 
sub
 and 
gsub
 functions and was hoping to possibly use some form of regex as a workaround.


I'm using the following code but not having any luck:


    df1 <- data.frame(id = 1:10, var1 = seq(14102201,14103200, 100))
    df1$var2 <- substr(df1$var1, 1,6)
    df1$var3 <- gsub('\\d{1,8}','\\d{1,6}', df1$var1)
    df1



The output in 
df1$var2
 is what I'm looking for.  Any suggestions?


EDIT:
Running this code:


library(h2o)
localH2O = h2o.init(nthreads = 2) 
df1 <- data.frame(id = 1:10, var1 = seq(14102201,14103200, 100))
df1.hex <- as.h2o(localH2O , df1)
df1.hex$var2 <- substr(df1.hex$var1, 1, 6)



Gets this message:


> df1.hex$var2 <- substr(df1.hex$var1, 1, 6)
Error in as.character.default(x) : 
  no method for coercing this S4 class to a vector","['regex', 'r', 'gsub', 'h2o']",Sven Hohenstein,https://stackoverflow.com/users/1627235/sven-hohenstein,81.6k
27759277,27759277,2015-01-03T20:36:45,2016-08-27 17:29:41Z,0,"I have been looking into the 
H20
 machine learning platform and was trying to figure out if its use with R allows R to process really large data (>> available RAM on a laptop) or if it is still bound by the amount of RAM? I think since it is ""in-memory"" that this means that it still requires a very large amount of RAM or server clusters? Anyone have experience with this?","['r', 'in-memory', 'h2o']",jangorecki,https://stackoverflow.com/users/2490497/jangorecki,16.7k
27553362,27553362,2014-12-18T18:32:49,2020-05-01 17:27:30Z,0,"Im new to R and H2O and I have tried to find a way to convert r data frame to a h2o object. I have spent some time research on how to do this with no luck. Other way around is possible and well documented as follows.


prosPath = system.file(""extdata"", ""prostate.csv"", package=""h2o"")
prostate.hex = h2o.importFile(localH2O, path = prosPath)
prostate.data.frame <- as.data.frame(prostate.hex)



But what i want is complete opposite of this. I wants to convert r ""prostate.data.frame"" data object converted to h2o object named ""prostate.hex"".
Thanks in advance.","['r', 'dataframe', 'h2o']",user4157124,https://stackoverflow.com/users/4157124/user4157124,"2,904"
27181616,27181616,2014-11-28T03:49:37,2014-12-04 14:36:48Z,0,"I have a h2o object.


The standard R for subset


sub1<-trans[trans$Type==1,]



I tried the same in h2o. It is not working


sub1<-trans[trans$Type==1,]



I also tried 


sub1<-h2o.exec(trans[trans$Type==1,])



note* trans is a h2o data Object.


Any idea to do it in h2o? Thanks","['r', 'subset', 'h2o']",chee.work.stuff,https://stackoverflow.com/users/2545609/chee-work-stuff,326
26393977,26393977,2014-10-15T23:44:25,2014-10-21 02:03:10Z,0,"After trying to upload a dataset (as a CSV) to H2O, and finding that the FirstName column gets converted to null/missing, I learned that the current version of H2O doesn't support columns of class string, and factors only go up to 65k unique values. So now I'm looking for another way to solve this problem.


I want to end with a model that, given any FirstName, will return:




the probability that the person is male/female (+1.0 to -1.0)


the likely age of the person (mean, stdev) if possible




Which R functions (or packages::functions) would work for this? Preferably well-documented packages/functions so I can learn more as I go.


Here's a sample of the dataset in R. The column types are: Numerical, factor, factor, numerical.


> head(TrainingNames)

  Year FirstName Gender Freq
1 1880      Mary      F 7065
2 1880      Anna      F 2604
3 1880      Emma      F 2003
4 1880 Elizabeth      F 1939
5 1880    Minnie      F 1746
6 1880  Margaret      F 1578

> summary(TrainingNames)

      Year        FirstName       Gender           Freq        
 Min.   :1880   Francis:    268   F:1062432   Min.   :    5.0  
 1st Qu.:1948   James  :    268   M: 729659   1st Qu.:    7.0  
 Median :1981   Jean   :    268               Median :   12.0  
 Mean   :1972   Jesse  :    268               Mean   :  186.1  
 3rd Qu.:2000   Jessie :    268               3rd Qu.:   32.0  
 Max.   :2013   John   :    268               Max.   :99674.0  
                (Other):1790483                                



Here's R code to pull/process the data-source.


# Create data dir, download and extract data source
dir.create('Data Files', showWarnings = F)
if(!file.exists('Data Files/names.zip')) {
  download.file(url = 'http://www.ssa.gov/oact/babynames/names.zip', destfile = 'Data Files/names.zip', cacheOK = T)
  setwd('Data Files/')
  unzip(zipfile = 'names.zip')
  setwd('../') 
}

FileList <- list.files(path = ""Data Files/"", pattern = "".txt"") # List of data files

# Create data-source of names for R/Tableau

munge <- function(f) { # Return data frame of single data file
  y <- as.numeric(gsub(pattern = '[^0-9]', replacement = """", x = f))
  l <- read.csv(file = paste0(""Data Files/"", f), header = F, quote = ""'"")
  d <- cbind(y, l)
  colnames(d) <- c(""Year"", ""FirstName"", ""Gender"", ""Freq"")
  return(data.frame(d))
}

if(!file.exists('TrainingNames.csv')) {
  pb <- txtProgressBar(min = 1, max = length(FileList), style = 3) # Start progress bar

  TrainingNames <- munge(FileList[[1]]) # Munge first data file
  for(n in 2:length(FileList)) { # Munge remaining data files
    TrainingNames <- rbind(TrainingNames, munge(FileList[[n]]))
    setTxtProgressBar(pb, n)
  }

  close(pb) # Close progress bar
  rm(n, pb)

  write.table(x = TrainingNames, file = ""TrainingNames.csv"", sep = "";"", row.names = F, col.names = T) # Write results to CSV file
}

summary(TrainingNames)","['r', 'machine-learning', 'classification', 'h2o']",Unknown,,N/A
24683428,24683428,2014-07-10T18:05:19,2015-08-11 23:03:55Z,0,"I am following the tutorial at 
revolutionanalytics
 and in the beginning where the tutorial says


library(h2o)                # Load H2O library  
localH2O = h2o.init()       # initial H2O locl instance

# Upload iris file from the H2O package into the H2O local instance
iris.hex <-  h2o.uploadFile(localH2O, path = system.file(""extdata"", ""iris.csv"", 
package=""h2o""), key = ""iris.hex"")

summary(iris.hex)



When I run the commands in R 3.1.0 x64 for windows (configuration given below) with H2O 2.0.0.9 I get the following output:


> library(h2o)                # Load H2O library  
> localH2O = h2o.init()       # initial H2O locl instance
Successfully connected to http://127.0.0.1:54321 
R is connected to H2O cluster:
Error in names(durationVector) = c(""days"", ""hours"", ""minutes"", ""seconds"",  : 
'names' attribute [5] must be the same length as the vector [0]

iris.hex <-  h2o.uploadFile(localH2O, path = system.file(""extdata"", ""iris.csv"", package=""h2o""),       
key = ""iris.hex"")
Error in h2o.uploadFile.FV(object, path, key, parse, header, sep, col.names,  : 
object 'localH2O' not found



I cannot figure out how to solve this. This issue is not there on any forum. I know that the program is trying to assign a vector of different size to an array of different size. But how do I resolve this and get this to work?


R configuration:


platform       x86_64-w64-mingw32          
arch           x86_64                      
os             mingw32                     
system         x86_64, mingw32             
status                                     
major          3                           
minor          1.0                         
year           2014                        
month          04                          
day            10                          
svn rev        65387                       
language       R                           
version.string R version 3.1.0 (2014-04-10)
nickname       Spring Dance","['r', 'h2o']",Raffael,https://stackoverflow.com/users/562440/raffael,20k
24604458,24604458,2014-07-07T06:45:36,2015-03-30 03:06:36Z,0,"Using R-hadoop which is hosted at 172.16.53.31:8787, I am trying to import file from HDFS to H2O which is hosted at 172.16.53.31:54331 (originally 54321).


This error happened. Any idea?




Or any advices? 


Please and thanks.


Sincerely,


Newb","['r', 'hadoop', 'h2o']",Community,https://stackoverflow.com/users/-1/community,1
